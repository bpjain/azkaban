
./vmlinux:     file format elf64-x86-64


Disassembly of section .text:

ffffffff812be000 <deadline_fifo_batch_show+0x22>:
}
SHOW_FUNCTION(deadline_read_expire_show, dd->fifo_expire[READ], 1);
SHOW_FUNCTION(deadline_write_expire_show, dd->fifo_expire[WRITE], 1);
SHOW_FUNCTION(deadline_writes_starved_show, dd->writes_starved, 0);
SHOW_FUNCTION(deadline_front_merges_show, dd->front_merges, 0);
SHOW_FUNCTION(deadline_fifo_batch_show, dd->fifo_batch, 0);
ffffffff812be000:	c3                   	retq   

ffffffff812be001 <deadline_read_expire_show>:
	int __data = __VAR;						\
	if (__CONV)							\
		__data = jiffies_to_msecs(__data);			\
	return deadline_var_show(__data, (page));			\
}
SHOW_FUNCTION(deadline_read_expire_show, dd->fifo_expire[READ], 1);
ffffffff812be001:	55                   	push   %rbp
ffffffff812be002:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be005:	53                   	push   %rbx
ffffffff812be006:	50                   	push   %rax
ffffffff812be007:	48 8b 47 08          	mov    0x8(%rdi),%rax
ffffffff812be00b:	48 89 f3             	mov    %rsi,%rbx
ffffffff812be00e:	48 63 78 54          	movslq 0x54(%rax),%rdi
ffffffff812be012:	e8 c2 f8 dd ff       	callq  ffffffff8109d8d9 <jiffies_to_msecs>
 */

static ssize_t
deadline_var_show(int var, char *page)
{
	return sprintf(page, "%d\n", var);
ffffffff812be017:	48 89 df             	mov    %rbx,%rdi
ffffffff812be01a:	89 c2                	mov    %eax,%edx
ffffffff812be01c:	48 c7 c6 da 27 7d 81 	mov    $0xffffffff817d27da,%rsi
ffffffff812be023:	31 c0                	xor    %eax,%eax
ffffffff812be025:	e8 eb c7 00 00       	callq  ffffffff812ca815 <sprintf>
	int __data = __VAR;						\
	if (__CONV)							\
		__data = jiffies_to_msecs(__data);			\
	return deadline_var_show(__data, (page));			\
}
SHOW_FUNCTION(deadline_read_expire_show, dd->fifo_expire[READ], 1);
ffffffff812be02a:	5a                   	pop    %rdx
ffffffff812be02b:	48 98                	cltq   
ffffffff812be02d:	5b                   	pop    %rbx
ffffffff812be02e:	5d                   	pop    %rbp
ffffffff812be02f:	c3                   	retq   

ffffffff812be030 <deadline_writes_starved_store>:
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_writes_starved_store, &dd->writes_starved, INT_MIN, INT_MAX, 0);
ffffffff812be030:	55                   	push   %rbp
ffffffff812be031:	48 89 f0             	mov    %rsi,%rax
ffffffff812be034:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be037:	41 54                	push   %r12
ffffffff812be039:	53                   	push   %rbx
ffffffff812be03a:	48 89 d3             	mov    %rdx,%rbx
static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;

	*var = simple_strtol(p, &p, 10);
ffffffff812be03d:	ba 0a 00 00 00       	mov    $0xa,%edx
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_writes_starved_store, &dd->writes_starved, INT_MIN, INT_MAX, 0);
ffffffff812be042:	48 83 ec 10          	sub    $0x10,%rsp
ffffffff812be046:	4c 8b 67 08          	mov    0x8(%rdi),%r12
}

static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;
ffffffff812be04a:	48 89 75 e8          	mov    %rsi,-0x18(%rbp)

	*var = simple_strtol(p, &p, 10);
ffffffff812be04e:	48 8d 75 e8          	lea    -0x18(%rbp),%rsi
ffffffff812be052:	48 89 c7             	mov    %rax,%rdi
ffffffff812be055:	e8 c1 ad 00 00       	callq  ffffffff812c8e1b <simple_strtol>
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_writes_starved_store, &dd->writes_starved, INT_MIN, INT_MAX, 0);
ffffffff812be05a:	41 89 44 24 60       	mov    %eax,0x60(%r12)
ffffffff812be05f:	48 63 c3             	movslq %ebx,%rax
ffffffff812be062:	5a                   	pop    %rdx
ffffffff812be063:	59                   	pop    %rcx
ffffffff812be064:	5b                   	pop    %rbx
ffffffff812be065:	41 5c                	pop    %r12
ffffffff812be067:	5d                   	pop    %rbp
ffffffff812be068:	c3                   	retq   

ffffffff812be069 <deadline_fifo_batch_store>:
STORE_FUNCTION(deadline_front_merges_store, &dd->front_merges, 0, 1, 0);
STORE_FUNCTION(deadline_fifo_batch_store, &dd->fifo_batch, 0, INT_MAX, 0);
ffffffff812be069:	55                   	push   %rbp
ffffffff812be06a:	48 89 f0             	mov    %rsi,%rax
ffffffff812be06d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be070:	41 54                	push   %r12
ffffffff812be072:	53                   	push   %rbx
ffffffff812be073:	48 89 d3             	mov    %rdx,%rbx
static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;

	*var = simple_strtol(p, &p, 10);
ffffffff812be076:	ba 0a 00 00 00       	mov    $0xa,%edx
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_writes_starved_store, &dd->writes_starved, INT_MIN, INT_MAX, 0);
STORE_FUNCTION(deadline_front_merges_store, &dd->front_merges, 0, 1, 0);
STORE_FUNCTION(deadline_fifo_batch_store, &dd->fifo_batch, 0, INT_MAX, 0);
ffffffff812be07b:	48 83 ec 10          	sub    $0x10,%rsp
}

static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;
ffffffff812be07f:	48 89 75 e8          	mov    %rsi,-0x18(%rbp)

	*var = simple_strtol(p, &p, 10);
ffffffff812be083:	48 8d 75 e8          	lea    -0x18(%rbp),%rsi
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_writes_starved_store, &dd->writes_starved, INT_MIN, INT_MAX, 0);
STORE_FUNCTION(deadline_front_merges_store, &dd->front_merges, 0, 1, 0);
STORE_FUNCTION(deadline_fifo_batch_store, &dd->fifo_batch, 0, INT_MAX, 0);
ffffffff812be087:	4c 8b 67 08          	mov    0x8(%rdi),%r12
static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;

	*var = simple_strtol(p, &p, 10);
ffffffff812be08b:	48 89 c7             	mov    %rax,%rdi
ffffffff812be08e:	e8 88 ad 00 00       	callq  ffffffff812c8e1b <simple_strtol>
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_writes_starved_store, &dd->writes_starved, INT_MIN, INT_MAX, 0);
STORE_FUNCTION(deadline_front_merges_store, &dd->front_merges, 0, 1, 0);
STORE_FUNCTION(deadline_fifo_batch_store, &dd->fifo_batch, 0, INT_MAX, 0);
ffffffff812be093:	ba 00 00 00 00       	mov    $0x0,%edx
ffffffff812be098:	85 c0                	test   %eax,%eax
ffffffff812be09a:	0f 48 c2             	cmovs  %edx,%eax
ffffffff812be09d:	41 89 44 24 5c       	mov    %eax,0x5c(%r12)
ffffffff812be0a2:	48 63 c3             	movslq %ebx,%rax
ffffffff812be0a5:	5a                   	pop    %rdx
ffffffff812be0a6:	59                   	pop    %rcx
ffffffff812be0a7:	5b                   	pop    %rbx
ffffffff812be0a8:	41 5c                	pop    %r12
ffffffff812be0aa:	5d                   	pop    %rbp
ffffffff812be0ab:	c3                   	retq   

ffffffff812be0ac <deadline_read_expire_store>:
		*(__PTR) = msecs_to_jiffies(__data);			\
	else								\
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
ffffffff812be0ac:	55                   	push   %rbp
ffffffff812be0ad:	48 89 f0             	mov    %rsi,%rax
ffffffff812be0b0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be0b3:	41 54                	push   %r12
ffffffff812be0b5:	53                   	push   %rbx
ffffffff812be0b6:	48 89 d3             	mov    %rdx,%rbx
static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;

	*var = simple_strtol(p, &p, 10);
ffffffff812be0b9:	ba 0a 00 00 00       	mov    $0xa,%edx
		*(__PTR) = msecs_to_jiffies(__data);			\
	else								\
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
ffffffff812be0be:	48 83 ec 10          	sub    $0x10,%rsp
}

static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;
ffffffff812be0c2:	48 89 75 e8          	mov    %rsi,-0x18(%rbp)

	*var = simple_strtol(p, &p, 10);
ffffffff812be0c6:	48 8d 75 e8          	lea    -0x18(%rbp),%rsi
		*(__PTR) = msecs_to_jiffies(__data);			\
	else								\
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
ffffffff812be0ca:	4c 8b 67 08          	mov    0x8(%rdi),%r12
static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;

	*var = simple_strtol(p, &p, 10);
ffffffff812be0ce:	48 89 c7             	mov    %rax,%rdi
ffffffff812be0d1:	e8 45 ad 00 00       	callq  ffffffff812c8e1b <simple_strtol>
		*(__PTR) = msecs_to_jiffies(__data);			\
	else								\
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
ffffffff812be0d6:	bf 00 00 00 00       	mov    $0x0,%edi
ffffffff812be0db:	85 c0                	test   %eax,%eax
ffffffff812be0dd:	0f 49 f8             	cmovns %eax,%edi
ffffffff812be0e0:	e8 e5 f8 dd ff       	callq  ffffffff8109d9ca <msecs_to_jiffies>
ffffffff812be0e5:	41 89 44 24 54       	mov    %eax,0x54(%r12)
ffffffff812be0ea:	48 63 c3             	movslq %ebx,%rax
ffffffff812be0ed:	5a                   	pop    %rdx
ffffffff812be0ee:	59                   	pop    %rcx
ffffffff812be0ef:	5b                   	pop    %rbx
ffffffff812be0f0:	41 5c                	pop    %r12
ffffffff812be0f2:	5d                   	pop    %rbp
ffffffff812be0f3:	c3                   	retq   

ffffffff812be0f4 <deadline_write_expire_store>:
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
ffffffff812be0f4:	55                   	push   %rbp
ffffffff812be0f5:	48 89 f0             	mov    %rsi,%rax
ffffffff812be0f8:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be0fb:	41 54                	push   %r12
ffffffff812be0fd:	53                   	push   %rbx
ffffffff812be0fe:	48 89 d3             	mov    %rdx,%rbx
static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;

	*var = simple_strtol(p, &p, 10);
ffffffff812be101:	ba 0a 00 00 00       	mov    $0xa,%edx
	else								\
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
ffffffff812be106:	48 83 ec 10          	sub    $0x10,%rsp
}

static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;
ffffffff812be10a:	48 89 75 e8          	mov    %rsi,-0x18(%rbp)

	*var = simple_strtol(p, &p, 10);
ffffffff812be10e:	48 8d 75 e8          	lea    -0x18(%rbp),%rsi
	else								\
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
ffffffff812be112:	4c 8b 67 08          	mov    0x8(%rdi),%r12
static ssize_t
deadline_var_store(int *var, const char *page, size_t count)
{
	char *p = (char *) page;

	*var = simple_strtol(p, &p, 10);
ffffffff812be116:	48 89 c7             	mov    %rax,%rdi
ffffffff812be119:	e8 fd ac 00 00       	callq  ffffffff812c8e1b <simple_strtol>
	else								\
		*(__PTR) = __data;					\
	return ret;							\
}
STORE_FUNCTION(deadline_read_expire_store, &dd->fifo_expire[READ], 0, INT_MAX, 1);
STORE_FUNCTION(deadline_write_expire_store, &dd->fifo_expire[WRITE], 0, INT_MAX, 1);
ffffffff812be11e:	bf 00 00 00 00       	mov    $0x0,%edi
ffffffff812be123:	85 c0                	test   %eax,%eax
ffffffff812be125:	0f 49 f8             	cmovns %eax,%edi
ffffffff812be128:	e8 9d f8 dd ff       	callq  ffffffff8109d9ca <msecs_to_jiffies>
ffffffff812be12d:	41 89 44 24 58       	mov    %eax,0x58(%r12)
ffffffff812be132:	48 63 c3             	movslq %ebx,%rax
ffffffff812be135:	5a                   	pop    %rdx
ffffffff812be136:	59                   	pop    %rcx
ffffffff812be137:	5b                   	pop    %rbx
ffffffff812be138:	41 5c                	pop    %r12
ffffffff812be13a:	5d                   	pop    %rbp
ffffffff812be13b:	c3                   	retq   

ffffffff812be13c <deadline_add_request>:
/*
 * add rq to rbtree and fifo
 */
static void
deadline_add_request(struct request_queue *q, struct request *rq)
{
ffffffff812be13c:	55                   	push   %rbp
ffffffff812be13d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be140:	41 55                	push   %r13
ffffffff812be142:	41 54                	push   %r12
ffffffff812be144:	53                   	push   %rbx
ffffffff812be145:	50                   	push   %rax
ffffffff812be146:	49 89 f4             	mov    %rsi,%r12
	struct deadline_data *dd = q->elevator->elevator_data;
ffffffff812be149:	48 8b 47 18          	mov    0x18(%rdi),%rax
static void deadline_move_request(struct deadline_data *, struct request *);

static inline struct rb_root *
deadline_rb_root(struct deadline_data *dd, struct request *rq)
{
	return &dd->sort_list[rq_data_dir(rq)];
ffffffff812be14d:	48 8b 5e 40          	mov    0x40(%rsi),%rbx
 * add rq to rbtree and fifo
 */
static void
deadline_add_request(struct request_queue *q, struct request *rq)
{
	struct deadline_data *dd = q->elevator->elevator_data;
ffffffff812be151:	4c 8b 68 08          	mov    0x8(%rax),%r13
static void deadline_move_request(struct deadline_data *, struct request *);

static inline struct rb_root *
deadline_rb_root(struct deadline_data *dd, struct request *rq)
{
	return &dd->sort_list[rq_data_dir(rq)];
ffffffff812be155:	83 e3 01             	and    $0x1,%ebx
ffffffff812be158:	49 8d 7c dd 00       	lea    0x0(%r13,%rbx,8),%rdi
static void
deadline_add_rq_rb(struct deadline_data *dd, struct request *rq)
{
	struct rb_root *root = deadline_rb_root(dd, rq);

	elv_rb_add(root, rq);
ffffffff812be15d:	e8 aa a8 fe ff       	callq  ffffffff812a8a0c <elv_rb_add>
	deadline_add_rq_rb(dd, rq);

	/*
	 * set expire time and add to fifo list
	 */
	rq->fifo_time = jiffies + dd->fifo_expire[data_dir];
ffffffff812be162:	48 8d 53 14          	lea    0x14(%rbx),%rdx
ffffffff812be166:	48 8b 05 93 8e 74 00 	mov    0x748e93(%rip),%rax        # ffffffff81a07000 <jiffies>
	list_add_tail(&rq->queuelist, &dd->fifo_list[data_dir]);
ffffffff812be16d:	48 ff c3             	inc    %rbx
ffffffff812be170:	48 c1 e3 04          	shl    $0x4,%rbx
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
ffffffff812be174:	4c 89 e7             	mov    %r12,%rdi
	deadline_add_rq_rb(dd, rq);

	/*
	 * set expire time and add to fifo list
	 */
	rq->fifo_time = jiffies + dd->fifo_expire[data_dir];
ffffffff812be177:	49 63 54 95 04       	movslq 0x4(%r13,%rdx,4),%rdx
	list_add_tail(&rq->queuelist, &dd->fifo_list[data_dir]);
ffffffff812be17c:	4c 01 eb             	add    %r13,%rbx
	deadline_add_rq_rb(dd, rq);

	/*
	 * set expire time and add to fifo list
	 */
	rq->fifo_time = jiffies + dd->fifo_expire[data_dir];
ffffffff812be17f:	48 01 d0             	add    %rdx,%rax
ffffffff812be182:	48 89 da             	mov    %rbx,%rdx
ffffffff812be185:	49 89 44 24 10       	mov    %rax,0x10(%r12)
ffffffff812be18a:	48 8b 73 08          	mov    0x8(%rbx),%rsi
ffffffff812be18e:	e8 d7 6c 01 00       	callq  ffffffff812d4e6a <__list_add>
	list_add_tail(&rq->queuelist, &dd->fifo_list[data_dir]);
}
ffffffff812be193:	5a                   	pop    %rdx
ffffffff812be194:	5b                   	pop    %rbx
ffffffff812be195:	41 5c                	pop    %r12
ffffffff812be197:	41 5d                	pop    %r13
ffffffff812be199:	5d                   	pop    %rbp
ffffffff812be19a:	c3                   	retq   

ffffffff812be19b <dune_dev_ioctl>:
	return vmx_launch(conf, ret);
}

static long dune_dev_ioctl(struct file *filp,
			  unsigned int ioctl, unsigned long arg)
{
ffffffff812be19b:	55                   	push   %rbp
ffffffff812be19c:	48 89 d7             	mov    %rdx,%rdi
ffffffff812be19f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be1a2:	53                   	push   %rbx
ffffffff812be1a3:	48 83 ec 28          	sub    $0x28,%rsp
	__label__ out;
	long r = -EINVAL;
	struct dune_config conf;
	struct dune_layout layout;

	switch (ioctl) {
ffffffff812be1a7:	81 fe 03 e9 18 40    	cmp    $0x4018e903,%esi
ffffffff812be1ad:	0f 84 88 00 00 00    	je     ffffffff812be23b <dune_dev_ioctl+0xa0>
ffffffff812be1b3:	81 fe 01 e9 20 80    	cmp    $0x8020e901,%esi
ffffffff812be1b9:	74 3b                	je     ffffffff812be1f6 <dune_dev_ioctl+0x5b>
ffffffff812be1bb:	81 fe 02 e9 00 00    	cmp    $0xe902,%esi
ffffffff812be1c1:	0f 85 dd 00 00 00    	jne    ffffffff812be2a4 <dune_dev_ioctl+0x109>

static inline unsigned long long native_read_msr(unsigned int msr)
{
	DECLARE_ARGS(val, low, high);

	asm volatile("rdmsr" : EAX_EDX_RET(val, low, high) : "c" (msr));
ffffffff812be1c7:	b9 82 00 00 c0       	mov    $0xc0000082,%ecx
ffffffff812be1cc:	0f 32                	rdmsr  
	return EAX_EDX_VAL(val, low, high);
ffffffff812be1ce:	48 89 d1             	mov    %rdx,%rcx
ffffffff812be1d1:	89 c0                	mov    %eax,%eax
//		r = 0;
		break;

	case DUNE_GET_SYSCALL:
		rdmsrl(MSR_LSTAR, r);
		printk(KERN_INFO "R %lx\n", (unsigned long) r);
ffffffff812be1d3:	48 c7 c7 9d 54 7b 81 	mov    $0xffffffff817b549d,%rdi
ffffffff812be1da:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812be1de:	48 89 cb             	mov    %rcx,%rbx
ffffffff812be1e1:	48 09 c3             	or     %rax,%rbx
ffffffff812be1e4:	31 c0                	xor    %eax,%eax
ffffffff812be1e6:	48 89 de             	mov    %rbx,%rsi
ffffffff812be1e9:	e8 a4 4d 17 00       	callq  ffffffff81432f92 <printk>
*/
//		r = 0;
		break;

	case DUNE_GET_SYSCALL:
		rdmsrl(MSR_LSTAR, r);
ffffffff812be1ee:	48 89 de             	mov    %rbx,%rsi
		printk(KERN_INFO "R %lx\n", (unsigned long) r);
		break;
ffffffff812be1f1:	e9 b5 00 00 00       	jmpq   ffffffff812be2ab <dune_dev_ioctl+0x110>
	 * Therefore limit the compile time checking to the constant size
	 * case, and do only runtime checking for non-constant sizes.
	 */

	if (likely(sz < 0 || sz >= n))
		n = _copy_from_user(to, from, n);
ffffffff812be1f6:	48 89 fe             	mov    %rdi,%rsi
ffffffff812be1f9:	48 8d 7d d0          	lea    -0x30(%rbp),%rdi
ffffffff812be1fd:	ba 20 00 00 00       	mov    $0x20,%edx
ffffffff812be202:	e8 f9 c8 00 00       	callq  ffffffff812cab00 <_copy_from_user>

	switch (ioctl) {
	case DUNE_ENTER:
		r = copy_from_user(&conf, (int __user *) arg,
				   sizeof(struct dune_config));
		if (r) {
ffffffff812be207:	48 85 c0             	test   %rax,%rax
			r = -EIO;
ffffffff812be20a:	48 c7 c6 fb ff ff ff 	mov    $0xfffffffffffffffb,%rsi

	switch (ioctl) {
	case DUNE_ENTER:
		r = copy_from_user(&conf, (int __user *) arg,
				   sizeof(struct dune_config));
		if (r) {
ffffffff812be211:	0f 85 94 00 00 00    	jne    ffffffff812be2ab <dune_dev_ioctl+0x110>
			r = -EIO;
			goto out;
		}

		conf.rip = (unsigned long)0xffffffff812be2ab;
		asm("mov %%rsp, %0": "=r"(conf.rsp));
ffffffff812be217:	48 89 e0             	mov    %rsp,%rax
ffffffff812be21a:	48 89 45 d8          	mov    %rax,-0x28(%rbp)
MODULE_LICENSE("GPL");
MODULE_DESCRIPTION("A driver for Dune");

static int dune_enter(struct dune_config *conf, int64_t *ret)
{
	return vmx_launch(conf, ret);
ffffffff812be21e:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
		if (r) {
			r = -EIO;
			goto out;
		}

		conf.rip = (unsigned long)0xffffffff812be2ab;
ffffffff812be222:	48 c7 45 d0 ab e2 2b 	movq   $0xffffffff812be2ab,-0x30(%rbp)
ffffffff812be229:	81 
MODULE_LICENSE("GPL");
MODULE_DESCRIPTION("A driver for Dune");

static int dune_enter(struct dune_config *conf, int64_t *ret)
{
	return vmx_launch(conf, ret);
ffffffff812be22a:	48 8d 70 18          	lea    0x18(%rax),%rsi
ffffffff812be22e:	48 89 c7             	mov    %rax,%rdi
ffffffff812be231:	e8 56 08 00 00       	callq  ffffffff812bea8c <vmx_launch>
			goto out;
		}

		conf.rip = (unsigned long)0xffffffff812be2ab;
		asm("mov %%rsp, %0": "=r"(conf.rsp));
		r = dune_enter(&conf, &conf.ret);
ffffffff812be236:	48 63 f0             	movslq %eax,%rsi
ffffffff812be239:	eb 70                	jmp    ffffffff812be2ab <dune_dev_ioctl+0x110>
		rdmsrl(MSR_LSTAR, r);
		printk(KERN_INFO "R %lx\n", (unsigned long) r);
		break;

	case DUNE_GET_LAYOUT:
		layout.base_proc = 0;
ffffffff812be23b:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
ffffffff812be242:	00 
		layout.base_map = LG_ALIGN(current->mm->mmap_base) - GPA_SIZE;
ffffffff812be243:	48 ba 00 00 00 00 fc 	movabs $0xfffffffc00000000,%rdx
ffffffff812be24a:	ff ff ff 

	might_fault();

	/* See the comment in copy_from_user() above. */
	if (likely(sz < 0 || sz >= n))
		n = _copy_to_user(to, from, n);
ffffffff812be24d:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi

DECLARE_PER_CPU(struct task_struct *, current_task);

static __always_inline struct task_struct *get_current(void)
{
	return this_cpu_read_stable(current_task);
ffffffff812be251:	65 48 8b 0c 25 00 aa 	mov    %gs:0xaa00,%rcx
ffffffff812be258:	00 00 
ffffffff812be25a:	48 8b 81 80 02 00 00 	mov    0x280(%rcx),%rax
ffffffff812be261:	48 8b 40 20          	mov    0x20(%rax),%rax
ffffffff812be265:	48 05 ff ff 1f 00    	add    $0x1fffff,%rax
ffffffff812be26b:	48 25 00 00 e0 ff    	and    $0xffffffffffe00000,%rax
ffffffff812be271:	48 01 d0             	add    %rdx,%rax
ffffffff812be274:	48 89 45 d8          	mov    %rax,-0x28(%rbp)
		layout.base_stack = ((unsigned long) current->mm->context.vdso & ~GPA_MASK);
ffffffff812be278:	48 8b 81 80 02 00 00 	mov    0x280(%rcx),%rax
ffffffff812be27f:	48 23 90 48 03 00 00 	and    0x348(%rax),%rdx
ffffffff812be286:	48 89 55 e0          	mov    %rdx,-0x20(%rbp)
ffffffff812be28a:	ba 18 00 00 00       	mov    $0x18,%edx
ffffffff812be28f:	e8 3c c8 00 00       	callq  ffffffff812caad0 <_copy_to_user>
		r = copy_to_user((void __user *)arg, &layout,
				 sizeof(struct dune_layout));
		if (r) {
ffffffff812be294:	48 83 f8 01          	cmp    $0x1,%rax
ffffffff812be298:	48 19 f6             	sbb    %rsi,%rsi
ffffffff812be29b:	48 f7 d6             	not    %rsi
ffffffff812be29e:	48 83 e6 fb          	and    $0xfffffffffffffffb,%rsi
ffffffff812be2a2:	eb 07                	jmp    ffffffff812be2ab <dune_dev_ioctl+0x110>
			goto out;
		}
		break;

	default:
		return -ENOTTY;
ffffffff812be2a4:	48 c7 c6 e7 ff ff ff 	mov    $0xffffffffffffffe7,%rsi
	}

out:
//	kvm_hypercall1(2,r);
	return r;
}
ffffffff812be2ab:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812be2af:	48 89 f0             	mov    %rsi,%rax
ffffffff812be2b2:	5b                   	pop    %rbx
ffffffff812be2b3:	5d                   	pop    %rbp
ffffffff812be2b4:	c3                   	retq   

ffffffff812be2b5 <segment_base>:
	asm("sldt %0" : "=g"(ldt));
	return ldt;
}

static unsigned long segment_base(u16 selector)
{
ffffffff812be2b5:	55                   	push   %rbp
	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
ffffffff812be2b6:	48 c7 c2 30 09 01 00 	mov    $0x10930,%rdx
	asm("sldt %0" : "=g"(ldt));
	return ldt;
}

static unsigned long segment_base(u16 selector)
{
ffffffff812be2bd:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be2c0:	53                   	push   %rbx
ffffffff812be2c1:	51                   	push   %rcx
	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
ffffffff812be2c2:	65 48 03 15 4e be d4 	add    %gs:0x7ed4be4e(%rip),%rdx        # a118 <this_cpu_off>
ffffffff812be2c9:	7e 
	struct desc_struct *d;
	unsigned long table_base;
	unsigned long v;

	if (!(selector & ~3))
ffffffff812be2ca:	0f b7 df             	movzwl %di,%ebx
		return 0;
ffffffff812be2cd:	31 c0                	xor    %eax,%eax
	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
	struct desc_struct *d;
	unsigned long table_base;
	unsigned long v;

	if (!(selector & ~3))
ffffffff812be2cf:	f7 c3 fc ff 00 00    	test   $0xfffc,%ebx
ffffffff812be2d5:	74 68                	je     ffffffff812be33f <segment_base+0x8a>
		return 0;

	table_base = gdt->address;

	if (selector & 4) {           /* from ldt */
ffffffff812be2d7:	40 80 e7 04          	and    $0x4,%dil
ffffffff812be2db:	75 06                	jne    ffffffff812be2e3 <segment_base+0x2e>
	unsigned long v;

	if (!(selector & ~3))
		return 0;

	table_base = gdt->address;
ffffffff812be2dd:	48 8b 42 02          	mov    0x2(%rdx),%rax
ffffffff812be2e1:	eb 14                	jmp    ffffffff812be2f7 <segment_base+0x42>
}

static inline u16 vmx_read_ldt(void)
{
	u16 ldt;
	asm("sldt %0" : "=g"(ldt));
ffffffff812be2e3:	66 0f 00 c7          	sldt   %di
	table_base = gdt->address;

	if (selector & 4) {           /* from ldt */
		u16 ldt_selector = vmx_read_ldt();

		if (!(ldt_selector & ~3))
ffffffff812be2e7:	0f b7 ff             	movzwl %di,%edi
ffffffff812be2ea:	f7 c7 fc ff 00 00    	test   $0xfffc,%edi
ffffffff812be2f0:	74 4d                	je     ffffffff812be33f <segment_base+0x8a>
			return 0;

		table_base = segment_base(ldt_selector);
ffffffff812be2f2:	e8 be ff ff ff       	callq  ffffffff812be2b5 <segment_base>
	}
	d = (struct desc_struct *)(table_base + (selector & ~7));
ffffffff812be2f7:	81 e3 f8 ff 00 00    	and    $0xfff8,%ebx
ffffffff812be2fd:	48 8d 14 03          	lea    (%rbx,%rax,1),%rdx
	preempt_enable();
}

static inline unsigned long get_desc_base(const struct desc_struct *desc)
{
	return (unsigned)(desc->base0 | ((desc->base1) << 16) | ((desc->base2) << 24));
ffffffff812be301:	0f b6 42 04          	movzbl 0x4(%rdx),%eax
ffffffff812be305:	c1 e0 10             	shl    $0x10,%eax
ffffffff812be308:	89 c1                	mov    %eax,%ecx
ffffffff812be30a:	0f b6 42 07          	movzbl 0x7(%rdx),%eax
ffffffff812be30e:	c1 e0 18             	shl    $0x18,%eax
ffffffff812be311:	09 c8                	or     %ecx,%eax
ffffffff812be313:	0f b7 4a 02          	movzwl 0x2(%rdx),%ecx
ffffffff812be317:	09 c8                	or     %ecx,%eax
	v = get_desc_base(d);
#ifdef CONFIG_X86_64
       if (d->s == 0 && (d->type == 2 || d->type == 9 || d->type == 11))
ffffffff812be319:	8a 4a 05             	mov    0x5(%rdx),%cl
ffffffff812be31c:	f6 c1 10             	test   $0x10,%cl
ffffffff812be31f:	75 1e                	jne    ffffffff812be33f <segment_base+0x8a>
ffffffff812be321:	40 88 ce             	mov    %cl,%sil
ffffffff812be324:	83 e6 0f             	and    $0xf,%esi
ffffffff812be327:	40 80 fe 02          	cmp    $0x2,%sil
ffffffff812be32b:	74 08                	je     ffffffff812be335 <segment_base+0x80>
ffffffff812be32d:	83 e1 0d             	and    $0xd,%ecx
ffffffff812be330:	80 f9 09             	cmp    $0x9,%cl
ffffffff812be333:	75 0a                	jne    ffffffff812be33f <segment_base+0x8a>
               v |= ((unsigned long)((struct ldttss_desc64 *)d)->base3) << 32;
ffffffff812be335:	8b 52 08             	mov    0x8(%rdx),%edx
ffffffff812be338:	48 c1 e2 20          	shl    $0x20,%rdx
ffffffff812be33c:	48 09 d0             	or     %rdx,%rax
#endif
	return v;
}
ffffffff812be33f:	5a                   	pop    %rdx
ffffffff812be340:	5b                   	pop    %rbx
ffffffff812be341:	5d                   	pop    %rbp
ffffffff812be342:	c3                   	retq   

ffffffff812be343 <is_benign>:
	vcpu->regs[VCPU_REGS_RDX] = edx;
}


static int is_benign(u32 vector)
{
ffffffff812be343:	55                   	push   %rbp
ffffffff812be344:	31 c0                	xor    %eax,%eax
	if((vector > 0 && vector < 8) || (vector == 9) || (vector > 15 && vector < 20))
ffffffff812be346:	83 ff 13             	cmp    $0x13,%edi
	vcpu->regs[VCPU_REGS_RDX] = edx;
}


static int is_benign(u32 vector)
{
ffffffff812be349:	48 89 e5             	mov    %rsp,%rbp
	if((vector > 0 && vector < 8) || (vector == 9) || (vector > 15 && vector < 20))
ffffffff812be34c:	77 0e                	ja     ffffffff812be35c <is_benign+0x19>
		return 1;
ffffffff812be34e:	b8 fe 02 0f 00       	mov    $0xf02fe,%eax
ffffffff812be353:	40 88 f9             	mov    %dil,%cl
ffffffff812be356:	48 d3 e8             	shr    %cl,%rax
ffffffff812be359:	83 e0 01             	and    $0x1,%eax
	return 0;
}
ffffffff812be35c:	5d                   	pop    %rbp
ffffffff812be35d:	c3                   	retq   

ffffffff812be35e <vmcs_clear>:
	else
		vpid_sync_vcpu_global();
}

static void vmcs_clear(struct vmcs *vmcs)
{
ffffffff812be35e:	55                   	push   %rbp
ffffffff812be35f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be362:	53                   	push   %rbx
ffffffff812be363:	48 89 fb             	mov    %rdi,%rbx
ffffffff812be366:	48 83 ec 18          	sub    $0x18,%rsp
	u64 phys_addr = __pa(vmcs);
ffffffff812be36a:	e8 47 45 da ff       	callq  ffffffff810628b6 <__phys_addr>
ffffffff812be36f:	48 89 45 e8          	mov    %rax,-0x18(%rbp)
	u8 error;

	asm volatile (ASM_VMX_VMCLEAR_RAX "; setna %0"
ffffffff812be373:	48 8d 45 e8          	lea    -0x18(%rbp),%rax
ffffffff812be377:	66 0f c7 30          	vmclear (%rax)
ffffffff812be37b:	0f 96 c0             	setbe  %al
		      : "=qm"(error) : "a"(&phys_addr), "m"(phys_addr)
		      : "cc", "memory");
	if (error)
ffffffff812be37e:	84 c0                	test   %al,%al
ffffffff812be380:	74 15                	je     ffffffff812be397 <vmcs_clear+0x39>
		printk(KERN_ERR "kvm: vmclear fail: %p/%llx\n",
ffffffff812be382:	48 8b 55 e8          	mov    -0x18(%rbp),%rdx
ffffffff812be386:	48 89 de             	mov    %rbx,%rsi
ffffffff812be389:	48 c7 c7 d7 59 78 81 	mov    $0xffffffff817859d7,%rdi
ffffffff812be390:	31 c0                	xor    %eax,%eax
ffffffff812be392:	e8 fb 4b 17 00       	callq  ffffffff81432f92 <printk>
		       vmcs, phys_addr);
}
ffffffff812be397:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812be39b:	5b                   	pop    %rbx
ffffffff812be39c:	5d                   	pop    %rbp
ffffffff812be39d:	c3                   	retq   

ffffffff812be39e <vmcs_load>:

static void vmcs_load(struct vmcs *vmcs)
{
ffffffff812be39e:	55                   	push   %rbp
ffffffff812be39f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be3a2:	53                   	push   %rbx
ffffffff812be3a3:	48 89 fb             	mov    %rdi,%rbx
ffffffff812be3a6:	48 83 ec 18          	sub    $0x18,%rsp
	u64 phys_addr = __pa(vmcs);
ffffffff812be3aa:	e8 07 45 da ff       	callq  ffffffff810628b6 <__phys_addr>
ffffffff812be3af:	48 89 45 e8          	mov    %rax,-0x18(%rbp)
	u8 error;

	asm volatile (ASM_VMX_VMPTRLD_RAX "; setna %0"
ffffffff812be3b3:	48 8d 45 e8          	lea    -0x18(%rbp),%rax
ffffffff812be3b7:	0f c7 30             	vmptrld (%rax)
ffffffff812be3ba:	0f 96 c0             	setbe  %al
			: "=qm"(error) : "a"(&phys_addr), "m"(phys_addr)
			: "cc", "memory");
	if (error)
ffffffff812be3bd:	84 c0                	test   %al,%al
ffffffff812be3bf:	74 15                	je     ffffffff812be3d6 <vmcs_load+0x38>
		printk(KERN_ERR "vmx: vmptrld %p/%llx failed\n",
ffffffff812be3c1:	48 8b 55 e8          	mov    -0x18(%rbp),%rdx
ffffffff812be3c5:	48 89 de             	mov    %rbx,%rsi
ffffffff812be3c8:	48 c7 c7 ab 54 7b 81 	mov    $0xffffffff817b54ab,%rdi
ffffffff812be3cf:	31 c0                	xor    %eax,%eax
ffffffff812be3d1:	e8 bc 4b 17 00       	callq  ffffffff81432f92 <printk>
		       vmcs, phys_addr);
}
ffffffff812be3d6:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812be3da:	5b                   	pop    %rbx
ffffffff812be3db:	5d                   	pop    %rbp
ffffffff812be3dc:	c3                   	retq   

ffffffff812be3dd <vmwrite_error>:
	return vmcs_readl(field) | ((u64)vmcs_readl(field+1) << 32);
#endif
}

static noinline void vmwrite_error(unsigned long field, unsigned long value)
{
ffffffff812be3dd:	55                   	push   %rbp

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812be3de:	ba 00 44 00 00       	mov    $0x4400,%edx
	return vmcs_readl(field) | ((u64)vmcs_readl(field+1) << 32);
#endif
}

static noinline void vmwrite_error(unsigned long field, unsigned long value)
{
ffffffff812be3e3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be3e6:	0f 78 d0             	vmread %rdx,%rax
	printk(KERN_ERR "vmwrite error: reg %lx value %lx (err %d)\n",
ffffffff812be3e9:	89 c1                	mov    %eax,%ecx
ffffffff812be3eb:	48 89 f2             	mov    %rsi,%rdx
ffffffff812be3ee:	31 c0                	xor    %eax,%eax
ffffffff812be3f0:	48 89 fe             	mov    %rdi,%rsi
ffffffff812be3f3:	48 c7 c7 aa 59 78 81 	mov    $0xffffffff817859aa,%rdi
ffffffff812be3fa:	e8 93 4b 17 00       	callq  ffffffff81432f92 <printk>
	       field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
	dump_stack();
ffffffff812be3ff:	e8 65 66 17 00       	callq  ffffffff81434a69 <dump_stack>
}
ffffffff812be404:	5d                   	pop    %rbp
ffffffff812be405:	c3                   	retq   

ffffffff812be406 <vmcs_writel>:

static void vmcs_writel(unsigned long field, unsigned long value)
{
	u8 error;

	asm volatile (ASM_VMX_VMWRITE_RAX_RDX "; setna %0"
ffffffff812be406:	48 89 f0             	mov    %rsi,%rax
ffffffff812be409:	48 89 fa             	mov    %rdi,%rdx
ffffffff812be40c:	0f 79 d0             	vmwrite %rax,%rdx
ffffffff812be40f:	0f 96 c0             	setbe  %al
		       : "=q"(error) : "a"(value), "d"(field) : "cc");
	if (unlikely(error))
ffffffff812be412:	84 c0                	test   %al,%al
ffffffff812be414:	74 0a                	je     ffffffff812be420 <vmcs_writel+0x1a>
	       field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
	dump_stack();
}

static void vmcs_writel(unsigned long field, unsigned long value)
{
ffffffff812be416:	55                   	push   %rbp
ffffffff812be417:	48 89 e5             	mov    %rsp,%rbp
	u8 error;

	asm volatile (ASM_VMX_VMWRITE_RAX_RDX "; setna %0"
		       : "=q"(error) : "a"(value), "d"(field) : "cc");
	if (unlikely(error))
		vmwrite_error(field, value);
ffffffff812be41a:	e8 be ff ff ff       	callq  ffffffff812be3dd <vmwrite_error>
}
ffffffff812be41f:	5d                   	pop    %rbp
ffffffff812be420:	c3                   	retq   

ffffffff812be421 <vmx_free_vpid.isra.9>:

/**
 * vmx_free_vpid - frees a vpid
 * @vmx: the VCPU
 */
static void vmx_free_vpid(struct vmx_vcpu *vmx)
ffffffff812be421:	55                   	push   %rbp
ffffffff812be422:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be425:	53                   	push   %rbx
ffffffff812be426:	52                   	push   %rdx
ffffffff812be427:	48 89 fb             	mov    %rdi,%rbx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812be42a:	48 c7 c7 b0 5e a3 81 	mov    $0xffffffff81a35eb0,%rdi
ffffffff812be431:	e8 fa a1 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
{
	spin_lock(&vmx_vpid_lock);
	if (vmx->vpid != 0)
ffffffff812be436:	48 63 03             	movslq (%rbx),%rax
ffffffff812be439:	85 c0                	test   %eax,%eax
ffffffff812be43b:	74 08                	je     ffffffff812be445 <vmx_free_vpid.isra.9+0x24>
	clear_bit(nr, addr);
}

static inline void __clear_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("btr %1,%0" : ADDR : "Ir" (nr));
ffffffff812be43d:	48 0f b3 05 3b 13 8f 	btr    %rax,0x8f133b(%rip)        # ffffffff81baf780 <vmx_vpid_bitmap>
ffffffff812be444:	00 
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812be445:	48 c7 c7 b0 5e a3 81 	mov    $0xffffffff81a35eb0,%rdi
ffffffff812be44c:	e8 5b a2 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
		__clear_bit(vmx->vpid, vmx_vpid_bitmap);
	spin_unlock(&vmx_vpid_lock);
}
ffffffff812be451:	58                   	pop    %rax
ffffffff812be452:	5b                   	pop    %rbx
ffffffff812be453:	5d                   	pop    %rbp
ffffffff812be454:	c3                   	retq   

ffffffff812be455 <__vmx_get_cpu_helper>:

static void __vmx_get_cpu_helper(void *ptr)
{
	struct vmx_vcpu *vcpu = ptr;

	BUG_ON(raw_smp_processor_id() != vcpu->cpu);
ffffffff812be455:	65 8b 05 b4 bc d4 7e 	mov    %gs:0x7ed4bcb4(%rip),%eax        # a110 <cpu_number>
ffffffff812be45c:	3b 07                	cmp    (%rdi),%eax
ffffffff812be45e:	74 02                	je     ffffffff812be462 <__vmx_get_cpu_helper+0xd>
ffffffff812be460:	0f 0b                	ud2    
	rdmsrl(MSR_GS_BASE, tmpl);
	vmcs_writel(HOST_GS_BASE, tmpl); /* 22.2.4 */
}

static void __vmx_get_cpu_helper(void *ptr)
{
ffffffff812be462:	55                   	push   %rbp
ffffffff812be463:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be466:	53                   	push   %rbx
ffffffff812be467:	48 89 fb             	mov    %rdi,%rbx
ffffffff812be46a:	52                   	push   %rdx
	struct vmx_vcpu *vcpu = ptr;

	BUG_ON(raw_smp_processor_id() != vcpu->cpu);
	vmcs_clear(vcpu->vmcs);
ffffffff812be46b:	48 8b bf 10 02 00 00 	mov    0x210(%rdi),%rdi
ffffffff812be472:	e8 e7 fe ff ff       	callq  ffffffff812be35e <vmcs_clear>
	if (*this_cpu_ptr(&local_vcpu) == vcpu)
ffffffff812be477:	48 c7 c0 20 09 01 00 	mov    $0x10920,%rax
ffffffff812be47e:	48 89 c2             	mov    %rax,%rdx
ffffffff812be481:	65 48 03 15 8f bc d4 	add    %gs:0x7ed4bc8f(%rip),%rdx        # a118 <this_cpu_off>
ffffffff812be488:	7e 
ffffffff812be489:	48 3b 1a             	cmp    (%rdx),%rbx
ffffffff812be48c:	75 0f                	jne    ffffffff812be49d <__vmx_get_cpu_helper+0x48>
		*this_cpu_ptr(&local_vcpu) = NULL;
ffffffff812be48e:	65 48 03 05 82 bc d4 	add    %gs:0x7ed4bc82(%rip),%rax        # a118 <this_cpu_off>
ffffffff812be495:	7e 
ffffffff812be496:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)
}
ffffffff812be49d:	58                   	pop    %rax
ffffffff812be49e:	5b                   	pop    %rbx
ffffffff812be49f:	5d                   	pop    %rbp
ffffffff812be4a0:	c3                   	retq   

ffffffff812be4a1 <vmx_disable>:

/**
 * vmx_disable - disables VMX mode on the current CPU
 */
static void vmx_disable(void *unused)
{
ffffffff812be4a1:	55                   	push   %rbp
	if (*this_cpu_ptr(&vmx_enabled)) {
ffffffff812be4a2:	48 c7 c0 28 09 01 00 	mov    $0x10928,%rax
ffffffff812be4a9:	48 89 c2             	mov    %rax,%rdx

/**
 * vmx_disable - disables VMX mode on the current CPU
 */
static void vmx_disable(void *unused)
{
ffffffff812be4ac:	48 89 e5             	mov    %rsp,%rbp
	if (*this_cpu_ptr(&vmx_enabled)) {
ffffffff812be4af:	65 48 03 15 61 bc d4 	add    %gs:0x7ed4bc61(%rip),%rdx        # a118 <this_cpu_off>
ffffffff812be4b6:	7e 
ffffffff812be4b7:	83 3a 00             	cmpl   $0x0,(%rdx)
ffffffff812be4ba:	74 2f                	je     ffffffff812be4eb <vmx_disable+0x4a>
ffffffff812be4bc:	0f 01 c4             	vmxoff 
/* Clear in this cpu's CR4. */
static inline void cr4_clear_bits(unsigned long mask)
{
	unsigned long cr4;

	cr4 = this_cpu_read(cpu_tlbstate.cr4);
ffffffff812be4bf:	65 48 8b 0d 09 60 d5 	mov    %gs:0x7ed56009(%rip),%rcx        # 144d0 <cpu_tlbstate+0x10>
ffffffff812be4c6:	7e 
	if ((cr4 & ~mask) != cr4) {
ffffffff812be4c7:	48 89 ca             	mov    %rcx,%rdx
ffffffff812be4ca:	80 e6 df             	and    $0xdf,%dh
ffffffff812be4cd:	48 39 d1             	cmp    %rdx,%rcx
ffffffff812be4d0:	74 0b                	je     ffffffff812be4dd <vmx_disable+0x3c>
		cr4 &= ~mask;
		this_cpu_write(cpu_tlbstate.cr4, cr4);
ffffffff812be4d2:	65 48 89 15 f6 5f d5 	mov    %rdx,%gs:0x7ed55ff6(%rip)        # 144d0 <cpu_tlbstate+0x10>
ffffffff812be4d9:	7e 
	return val;
}

static inline void native_write_cr4(unsigned long val)
{
	asm volatile("mov %0,%%cr4": : "r" (val), "m" (__force_order));
ffffffff812be4da:	0f 22 e2             	mov    %rdx,%cr4
		__vmxoff();
		cr4_clear_bits(X86_CR4_VMXE);
		*this_cpu_ptr(&vmx_enabled) = 0;
ffffffff812be4dd:	65 48 03 05 33 bc d4 	add    %gs:0x7ed4bc33(%rip),%rax        # a118 <this_cpu_off>
ffffffff812be4e4:	7e 
ffffffff812be4e5:	c7 00 00 00 00 00    	movl   $0x0,(%rax)
	}
}
ffffffff812be4eb:	5d                   	pop    %rbp
ffffffff812be4ec:	c3                   	retq   

ffffffff812be4ed <__vmx_alloc_vmcs.isra.13>:
				   VM_EXIT_LOAD_IA32_EFER);

	return 0;
}

static struct vmcs *__vmx_alloc_vmcs(int cpu)
ffffffff812be4ed:	55                   	push   %rbp

static inline struct page *
__alloc_pages(gfp_t gfp_mask, unsigned int order,
		struct zonelist *zonelist)
{
	return __alloc_pages_nodemask(gfp_mask, order, zonelist, NULL);
ffffffff812be4ee:	8b 35 50 12 8f 00    	mov    0x8f1250(%rip),%esi        # ffffffff81baf744 <vmcs_config+0x4>
ffffffff812be4f4:	31 c9                	xor    %ecx,%ecx
ffffffff812be4f6:	48 c7 c2 80 5e a5 81 	mov    $0xffffffff81a55e80,%rdx
ffffffff812be4fd:	bf d0 00 00 00       	mov    $0xd0,%edi
ffffffff812be502:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be505:	e8 f3 b5 e1 ff       	callq  ffffffff810d9afd <__alloc_pages_nodemask>
ffffffff812be50a:	31 d2                	xor    %edx,%edx
	int node = cpu_to_node(cpu);
	struct page *pages;
	struct vmcs *vmcs;

	pages = alloc_pages_exact_node(node, GFP_KERNEL, vmcs_config.order);
	if (!pages)
ffffffff812be50c:	48 85 c0             	test   %rax,%rax
ffffffff812be50f:	74 38                	je     ffffffff812be549 <__vmx_alloc_vmcs.isra.13+0x5c>
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
ffffffff812be511:	48 ba 00 00 00 00 00 	movabs $0x160000000000,%rdx
ffffffff812be518:	16 00 00 
ffffffff812be51b:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
ffffffff812be522:	88 ff ff 
ffffffff812be525:	48 01 c2             	add    %rax,%rdx
		return NULL;
	vmcs = page_address(pages);
	memset(vmcs, 0, vmcs_config.size);
ffffffff812be528:	31 c0                	xor    %eax,%eax
ffffffff812be52a:	48 c1 fa 06          	sar    $0x6,%rdx
ffffffff812be52e:	48 c1 e2 0c          	shl    $0xc,%rdx
ffffffff812be532:	48 01 ca             	add    %rcx,%rdx
ffffffff812be535:	48 63 0d 04 12 8f 00 	movslq 0x8f1204(%rip),%rcx        # ffffffff81baf740 <vmcs_config>
ffffffff812be53c:	48 89 d7             	mov    %rdx,%rdi
ffffffff812be53f:	f3 aa                	rep stos %al,%es:(%rdi)
	vmcs->revision_id = vmcs_config.revision_id; /* vmcs revision id */
ffffffff812be541:	8b 05 01 12 8f 00    	mov    0x8f1201(%rip),%eax        # ffffffff81baf748 <vmcs_config+0x8>
ffffffff812be547:	89 02                	mov    %eax,(%rdx)
	return vmcs;
}
ffffffff812be549:	48 89 d0             	mov    %rdx,%rax
ffffffff812be54c:	5d                   	pop    %rbp
ffffffff812be54d:	c3                   	retq   

ffffffff812be54e <ept_sync_global>:
			: : "a" (&operand), "c" (ext) : "cc", "memory");
}

static inline void ept_sync_global(void)
{
	if (cpu_has_vmx_invept_global())
ffffffff812be54e:	f6 05 ce 11 8f 00 04 	testb  $0x4,0x8f11ce(%rip)        # ffffffff81baf723 <vmx_capability+0x3>
ffffffff812be555:	74 2b                	je     ffffffff812be582 <ept_sync_global+0x34>
			"; ja 1f ; ud2 ; 1:\n"
			: : "a" (&operand), "c" (ext) : "cc", "memory");
}

static inline void ept_sync_global(void)
{
ffffffff812be557:	55                   	push   %rbp
{
	struct {
		u64 eptp, gpa;
	} operand = {eptp, gpa};

	asm volatile (ASM_VMX_INVEPT
ffffffff812be558:	b9 02 00 00 00       	mov    $0x2,%ecx
			"; ja 1f ; ud2 ; 1:\n"
			: : "a" (&operand), "c" (ext) : "cc", "memory");
}

static inline void ept_sync_global(void)
{
ffffffff812be55d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be560:	48 83 ec 10          	sub    $0x10,%rsp
{
	struct {
		u64 eptp, gpa;
	} operand = {eptp, gpa};

	asm volatile (ASM_VMX_INVEPT
ffffffff812be564:	48 8d 45 f0          	lea    -0x10(%rbp),%rax

static inline void __invept(int ext, u64 eptp, gpa_t gpa)
{
	struct {
		u64 eptp, gpa;
	} operand = {eptp, gpa};
ffffffff812be568:	48 c7 45 f0 00 00 00 	movq   $0x0,-0x10(%rbp)
ffffffff812be56f:	00 
ffffffff812be570:	48 c7 45 f8 00 00 00 	movq   $0x0,-0x8(%rbp)
ffffffff812be577:	00 
ffffffff812be578:	66 0f 38 80 08       	invept (%rax),%rcx

	asm volatile (ASM_VMX_INVEPT
ffffffff812be57d:	77 02                	ja     ffffffff812be581 <ept_sync_global+0x33>
ffffffff812be57f:	0f 0b                	ud2    

static inline void ept_sync_global(void)
{
	if (cpu_has_vmx_invept_global())
		__invept(VMX_EPT_EXTENT_GLOBAL, 0, 0);
}
ffffffff812be581:	c9                   	leaveq 
ffffffff812be582:	c3                   	retq   

ffffffff812be583 <ept_sync_context>:

static inline void ept_sync_context(u64 eptp)
{
ffffffff812be583:	55                   	push   %rbp
ffffffff812be584:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be587:	48 83 ec 10          	sub    $0x10,%rsp
	if (cpu_has_vmx_invept_context())
ffffffff812be58b:	f6 05 91 11 8f 00 02 	testb  $0x2,0x8f1191(%rip)        # ffffffff81baf723 <vmx_capability+0x3>
ffffffff812be592:	74 20                	je     ffffffff812be5b4 <ept_sync_context+0x31>

static inline void __invept(int ext, u64 eptp, gpa_t gpa)
{
	struct {
		u64 eptp, gpa;
	} operand = {eptp, gpa};
ffffffff812be594:	48 89 7d f0          	mov    %rdi,-0x10(%rbp)
ffffffff812be598:	48 c7 45 f8 00 00 00 	movq   $0x0,-0x8(%rbp)
ffffffff812be59f:	00 

	asm volatile (ASM_VMX_INVEPT
ffffffff812be5a0:	48 8d 45 f0          	lea    -0x10(%rbp),%rax
ffffffff812be5a4:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812be5a9:	66 0f 38 80 08       	invept (%rax),%rcx
ffffffff812be5ae:	77 02                	ja     ffffffff812be5b2 <ept_sync_context+0x2f>
ffffffff812be5b0:	0f 0b                	ud2    
ffffffff812be5b2:	eb 05                	jmp    ffffffff812be5b9 <ept_sync_context+0x36>
static inline void ept_sync_context(u64 eptp)
{
	if (cpu_has_vmx_invept_context())
		__invept(VMX_EPT_EXTENT_CONTEXT, eptp, 0);
	else
		ept_sync_global();
ffffffff812be5b4:	e8 95 ff ff ff       	callq  ffffffff812be54e <ept_sync_global>
}
ffffffff812be5b9:	c9                   	leaveq 
ffffffff812be5ba:	c3                   	retq   

ffffffff812be5bb <__vmx_sync_individual_addr_helper>:
	struct vmx_vcpu *vcpu;
	gpa_t gpa;
};

static void __vmx_sync_individual_addr_helper(void *ptr)
{
ffffffff812be5bb:	55                   	push   %rbp
ffffffff812be5bc:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be5bf:	48 83 ec 10          	sub    $0x10,%rsp
		ept_sync_global();
}

static inline void ept_sync_individual_addr(u64 eptp, gpa_t gpa)
{
	if (cpu_has_vmx_invept_individual_addr())
ffffffff812be5c3:	f6 05 59 11 8f 00 01 	testb  $0x1,0x8f1159(%rip)        # ffffffff81baf723 <vmx_capability+0x3>

static void __vmx_sync_individual_addr_helper(void *ptr)
{
	struct sync_addr_args *args = ptr;

	ept_sync_individual_addr(args->vcpu->eptp,
ffffffff812be5ca:	48 8b 07             	mov    (%rdi),%rax
ffffffff812be5cd:	48 8b 50 48          	mov    0x48(%rax),%rdx
		ept_sync_global();
}

static inline void ept_sync_individual_addr(u64 eptp, gpa_t gpa)
{
	if (cpu_has_vmx_invept_individual_addr())
ffffffff812be5d1:	74 23                	je     ffffffff812be5f6 <__vmx_sync_individual_addr_helper+0x3b>
static void __vmx_sync_individual_addr_helper(void *ptr)
{
	struct sync_addr_args *args = ptr;

	ept_sync_individual_addr(args->vcpu->eptp,
				 (args->gpa & ~(PAGE_SIZE - 1)));
ffffffff812be5d3:	48 8b 47 08          	mov    0x8(%rdi),%rax

static inline void __invept(int ext, u64 eptp, gpa_t gpa)
{
	struct {
		u64 eptp, gpa;
	} operand = {eptp, gpa};
ffffffff812be5d7:	48 89 55 f0          	mov    %rdx,-0x10(%rbp)

	asm volatile (ASM_VMX_INVEPT
ffffffff812be5db:	31 c9                	xor    %ecx,%ecx

static inline void __invept(int ext, u64 eptp, gpa_t gpa)
{
	struct {
		u64 eptp, gpa;
	} operand = {eptp, gpa};
ffffffff812be5dd:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
ffffffff812be5e3:	48 89 45 f8          	mov    %rax,-0x8(%rbp)

	asm volatile (ASM_VMX_INVEPT
ffffffff812be5e7:	48 8d 45 f0          	lea    -0x10(%rbp),%rax
ffffffff812be5eb:	66 0f 38 80 08       	invept (%rax),%rcx
ffffffff812be5f0:	77 02                	ja     ffffffff812be5f4 <__vmx_sync_individual_addr_helper+0x39>
ffffffff812be5f2:	0f 0b                	ud2    
ffffffff812be5f4:	eb 08                	jmp    ffffffff812be5fe <__vmx_sync_individual_addr_helper+0x43>
{
	if (cpu_has_vmx_invept_individual_addr())
		__invept(VMX_EPT_EXTENT_INDIVIDUAL_ADDR,
				eptp, gpa);
	else
		ept_sync_context(eptp);
ffffffff812be5f6:	48 89 d7             	mov    %rdx,%rdi
ffffffff812be5f9:	e8 85 ff ff ff       	callq  ffffffff812be583 <ept_sync_context>
{
	struct sync_addr_args *args = ptr;

	ept_sync_individual_addr(args->vcpu->eptp,
				 (args->gpa & ~(PAGE_SIZE - 1)));
}
ffffffff812be5fe:	c9                   	leaveq 
ffffffff812be5ff:	c3                   	retq   

ffffffff812be600 <__vmx_sync_helper>:
{
	put_cpu();
}

static void __vmx_sync_helper(void *ptr)
{
ffffffff812be600:	55                   	push   %rbp
	struct vmx_vcpu *vcpu = ptr;

	ept_sync_context(vcpu->eptp);
ffffffff812be601:	48 8b 7f 48          	mov    0x48(%rdi),%rdi
{
	put_cpu();
}

static void __vmx_sync_helper(void *ptr)
{
ffffffff812be605:	48 89 e5             	mov    %rsp,%rbp
	struct vmx_vcpu *vcpu = ptr;

	ept_sync_context(vcpu->eptp);
ffffffff812be608:	e8 76 ff ff ff       	callq  ffffffff812be583 <ept_sync_context>
}
ffffffff812be60d:	5d                   	pop    %rbp
ffffffff812be60e:	c3                   	retq   

ffffffff812be60f <vpid_sync_vcpu_global>:
		__invvpid(VMX_VPID_EXTENT_SINGLE_CONTEXT, vpid, 0);
}

static inline void vpid_sync_vcpu_global(void)
{
	if (cpu_has_vmx_invvpid_global())
ffffffff812be60f:	f6 05 0f 11 8f 00 04 	testb  $0x4,0x8f110f(%rip)        # ffffffff81baf725 <vmx_capability+0x5>
ffffffff812be616:	74 31                	je     ffffffff812be649 <vpid_sync_vcpu_global+0x3a>
	if (cpu_has_vmx_invvpid_single())
		__invvpid(VMX_VPID_EXTENT_SINGLE_CONTEXT, vpid, 0);
}

static inline void vpid_sync_vcpu_global(void)
{
ffffffff812be618:	55                   	push   %rbp
	u64 vpid : 16;
	u64 rsvd : 48;
	u64 gva;
    } operand = { vpid, 0, gva };

    asm volatile (ASM_VMX_INVVPID
ffffffff812be619:	b9 02 00 00 00       	mov    $0x2,%ecx
	if (cpu_has_vmx_invvpid_single())
		__invvpid(VMX_VPID_EXTENT_SINGLE_CONTEXT, vpid, 0);
}

static inline void vpid_sync_vcpu_global(void)
{
ffffffff812be61e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be621:	48 83 ec 10          	sub    $0x10,%rsp
{
    struct {
	u64 vpid : 16;
	u64 rsvd : 48;
	u64 gva;
    } operand = { vpid, 0, gva };
ffffffff812be625:	66 c7 45 f0 00 00    	movw   $0x0,-0x10(%rbp)
ffffffff812be62b:	0f b7 45 f0          	movzwl -0x10(%rbp),%eax
ffffffff812be62f:	48 c7 45 f8 00 00 00 	movq   $0x0,-0x8(%rbp)
ffffffff812be636:	00 
ffffffff812be637:	48 89 45 f0          	mov    %rax,-0x10(%rbp)

    asm volatile (ASM_VMX_INVVPID
ffffffff812be63b:	48 8d 45 f0          	lea    -0x10(%rbp),%rax
ffffffff812be63f:	66 0f 38 81 08       	invvpid (%rax),%rcx
ffffffff812be644:	77 02                	ja     ffffffff812be648 <vpid_sync_vcpu_global+0x39>
ffffffff812be646:	0f 0b                	ud2    

static inline void vpid_sync_vcpu_global(void)
{
	if (cpu_has_vmx_invvpid_global())
		__invvpid(VMX_VPID_EXTENT_ALL_CONTEXT, 0, 0);
}
ffffffff812be648:	c9                   	leaveq 
ffffffff812be649:	c3                   	retq   

ffffffff812be64a <vmx_free_vmxon_areas>:

/**
 * vmx_free_vmxon_areas - cleanup helper function to free all VMXON buffers
 */
static void vmx_free_vmxon_areas(void)
{
ffffffff812be64a:	55                   	push   %rbp
ffffffff812be64b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be64e:	41 55                	push   %r13
ffffffff812be650:	41 54                	push   %r12
ffffffff812be652:	53                   	push   %rbx
ffffffff812be653:	52                   	push   %rdx
	int cpu;

	for_each_possible_cpu(cpu) {
ffffffff812be654:	41 83 cc ff          	or     $0xffffffff,%r12d
		if (per_cpu(vmxarea, cpu)) {
ffffffff812be658:	49 c7 c5 40 09 01 00 	mov    $0x10940,%r13
static inline unsigned int cpumask_next(int n, const struct cpumask *srcp)
{
	/* -1 is a legal arg here. */
	if (n != -1)
		cpumask_check(n);
	return find_next_bit(cpumask_bits(srcp), nr_cpumask_bits, n+1);
ffffffff812be65f:	41 8d 54 24 01       	lea    0x1(%r12),%edx
ffffffff812be664:	48 8b 3d c5 db 35 00 	mov    0x35dbc5(%rip),%rdi        # ffffffff8161c230 <cpu_possible_mask>
ffffffff812be66b:	be 20 00 00 00       	mov    $0x20,%esi
ffffffff812be670:	48 63 d2             	movslq %edx,%rdx
ffffffff812be673:	e8 a0 26 01 00       	callq  ffffffff812d0d18 <find_next_bit>
 */
static void vmx_free_vmxon_areas(void)
{
	int cpu;

	for_each_possible_cpu(cpu) {
ffffffff812be678:	3b 05 6e ac 79 00    	cmp    0x79ac6e(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812be67e:	41 89 c4             	mov    %eax,%r12d
ffffffff812be681:	7d 34                	jge    ffffffff812be6b7 <vmx_free_vmxon_areas+0x6d>
		if (per_cpu(vmxarea, cpu)) {
ffffffff812be683:	48 63 d8             	movslq %eax,%rbx
ffffffff812be686:	48 8b 04 dd 00 8e a5 	mov    -0x7e5a7200(,%rbx,8),%rax
ffffffff812be68d:	81 
ffffffff812be68e:	49 83 7c 05 00 00    	cmpq   $0x0,0x0(%r13,%rax,1)
ffffffff812be694:	74 c9                	je     ffffffff812be65f <vmx_free_vmxon_areas+0x15>
/**
 * vmx_free_vmcs - frees a VMCS region
 */
static void vmx_free_vmcs(struct vmcs *vmcs)
{
	free_pages((unsigned long)vmcs, vmcs_config.order);
ffffffff812be696:	4a 8b 3c 28          	mov    (%rax,%r13,1),%rdi
ffffffff812be69a:	8b 35 a4 10 8f 00    	mov    0x8f10a4(%rip),%esi        # ffffffff81baf744 <vmcs_config+0x4>
ffffffff812be6a0:	e8 ad a8 e1 ff       	callq  ffffffff810d8f52 <free_pages>
	int cpu;

	for_each_possible_cpu(cpu) {
		if (per_cpu(vmxarea, cpu)) {
			vmx_free_vmcs(per_cpu(vmxarea, cpu));
			per_cpu(vmxarea, cpu) = NULL;
ffffffff812be6a5:	48 8b 04 dd 00 8e a5 	mov    -0x7e5a7200(,%rbx,8),%rax
ffffffff812be6ac:	81 
ffffffff812be6ad:	4a c7 04 28 00 00 00 	movq   $0x0,(%rax,%r13,1)
ffffffff812be6b4:	00 
ffffffff812be6b5:	eb a8                	jmp    ffffffff812be65f <vmx_free_vmxon_areas+0x15>
		}
	}
}
ffffffff812be6b7:	58                   	pop    %rax
ffffffff812be6b8:	5b                   	pop    %rbx
ffffffff812be6b9:	41 5c                	pop    %r12
ffffffff812be6bb:	41 5d                	pop    %r13
ffffffff812be6bd:	5d                   	pop    %rbp
ffffffff812be6be:	c3                   	retq   

ffffffff812be6bf <vmx_get_cpu>:
 * @vcpu: VCPU that will be loaded.
 *
 * Disables preemption. Call vmx_put_cpu() when finished.
 */
static void vmx_get_cpu(struct vmx_vcpu *vcpu)
{
ffffffff812be6bf:	55                   	push   %rbp
ffffffff812be6c0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be6c3:	41 55                	push   %r13
ffffffff812be6c5:	41 54                	push   %r12
ffffffff812be6c7:	53                   	push   %rbx
ffffffff812be6c8:	48 83 ec 18          	sub    $0x18,%rsp
	int cur_cpu = get_cpu();

	if (*this_cpu_ptr(&local_vcpu) != vcpu) {
ffffffff812be6cc:	48 c7 c0 20 09 01 00 	mov    $0x10920,%rax
 *
 * Disables preemption. Call vmx_put_cpu() when finished.
 */
static void vmx_get_cpu(struct vmx_vcpu *vcpu)
{
	int cur_cpu = get_cpu();
ffffffff812be6d3:	65 44 8b 25 35 ba d4 	mov    %gs:0x7ed4ba35(%rip),%r12d        # a110 <cpu_number>
ffffffff812be6da:	7e 

	if (*this_cpu_ptr(&local_vcpu) != vcpu) {
ffffffff812be6db:	48 89 c2             	mov    %rax,%rdx
ffffffff812be6de:	65 48 03 15 32 ba d4 	add    %gs:0x7ed4ba32(%rip),%rdx        # a118 <this_cpu_off>
ffffffff812be6e5:	7e 
ffffffff812be6e6:	48 39 3a             	cmp    %rdi,(%rdx)
ffffffff812be6e9:	0f 84 3b 01 00 00    	je     ffffffff812be82a <vmx_get_cpu+0x16b>
ffffffff812be6ef:	48 89 fb             	mov    %rdi,%rbx
		*this_cpu_ptr(&local_vcpu) = vcpu;
ffffffff812be6f2:	65 48 03 05 1e ba d4 	add    %gs:0x7ed4ba1e(%rip),%rax        # a118 <this_cpu_off>
ffffffff812be6f9:	7e 
ffffffff812be6fa:	48 89 38             	mov    %rdi,(%rax)

		if (vcpu->cpu != cur_cpu) {
ffffffff812be6fd:	8b 3f                	mov    (%rdi),%edi
ffffffff812be6ff:	41 39 fc             	cmp    %edi,%r12d
ffffffff812be702:	0f 84 16 01 00 00    	je     ffffffff812be81e <vmx_get_cpu+0x15f>
			if (vcpu->cpu >= 0)
ffffffff812be708:	85 ff                	test   %edi,%edi
ffffffff812be70a:	78 16                	js     ffffffff812be722 <vmx_get_cpu+0x63>
				smp_call_function_single(vcpu->cpu,
ffffffff812be70c:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812be711:	48 89 da             	mov    %rbx,%rdx
ffffffff812be714:	48 c7 c6 55 e4 2b 81 	mov    $0xffffffff812be455,%rsi
ffffffff812be71b:	e8 9f de de ff       	callq  ffffffff810ac5bf <smp_call_function_single>
ffffffff812be720:	eb 0c                	jmp    ffffffff812be72e <vmx_get_cpu+0x6f>
					__vmx_get_cpu_helper, (void *) vcpu, 1);
			else
				vmcs_clear(vcpu->vmcs);
ffffffff812be722:	48 8b bb 10 02 00 00 	mov    0x210(%rbx),%rdi
ffffffff812be729:	e8 30 fc ff ff       	callq  ffffffff812be35e <vmcs_clear>
		__invvpid(VMX_VPID_EXTENT_ALL_CONTEXT, 0, 0);
}

static inline void vpid_sync_context(u16 vpid)
{
	if (cpu_has_vmx_invvpid_single())
ffffffff812be72e:	f6 05 f0 0f 8f 00 02 	testb  $0x2,0x8f0ff0(%rip)        # ffffffff81baf725 <vmx_capability+0x5>
ffffffff812be735:	74 2b                	je     ffffffff812be762 <vmx_get_cpu+0xa3>
ffffffff812be737:	8b 43 04             	mov    0x4(%rbx),%eax
		  : : "a"(&operand), "c"(ext) : "cc", "memory");
}

static inline void vpid_sync_vcpu_single(u16 vpid)
{
	if (vpid == 0)
ffffffff812be73a:	66 85 c0             	test   %ax,%ax
ffffffff812be73d:	74 28                	je     ffffffff812be767 <vmx_get_cpu+0xa8>
{
    struct {
	u64 vpid : 16;
	u64 rsvd : 48;
	u64 gva;
    } operand = { vpid, 0, gva };
ffffffff812be73f:	0f b7 c0             	movzwl %ax,%eax
ffffffff812be742:	48 c7 45 d8 00 00 00 	movq   $0x0,-0x28(%rbp)
ffffffff812be749:	00 

    asm volatile (ASM_VMX_INVVPID
ffffffff812be74a:	b9 01 00 00 00       	mov    $0x1,%ecx
{
    struct {
	u64 vpid : 16;
	u64 rsvd : 48;
	u64 gva;
    } operand = { vpid, 0, gva };
ffffffff812be74f:	48 89 45 d0          	mov    %rax,-0x30(%rbp)

    asm volatile (ASM_VMX_INVVPID
ffffffff812be753:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
ffffffff812be757:	66 0f 38 81 08       	invvpid (%rax),%rcx
ffffffff812be75c:	77 02                	ja     ffffffff812be760 <vmx_get_cpu+0xa1>
ffffffff812be75e:	0f 0b                	ud2    
ffffffff812be760:	eb 05                	jmp    ffffffff812be767 <vmx_get_cpu+0xa8>
static inline void vpid_sync_context(u16 vpid)
{
	if (cpu_has_vmx_invvpid_single())
		vpid_sync_vcpu_single(vpid);
	else
		vpid_sync_vcpu_global();
ffffffff812be762:	e8 a8 fe ff ff       	callq  ffffffff812be60f <vpid_sync_vcpu_global>
					__vmx_get_cpu_helper, (void *) vcpu, 1);
			else
				vmcs_clear(vcpu->vmcs);

			vpid_sync_context(vcpu->vpid);
			ept_sync_context(vcpu->eptp);
ffffffff812be767:	48 8b 7b 48          	mov    0x48(%rbx),%rdi
	return segment_base(tr);
}

static void __vmx_setup_cpu(void)
{
	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
ffffffff812be76b:	49 c7 c5 30 09 01 00 	mov    $0x10930,%r13
					__vmx_get_cpu_helper, (void *) vcpu, 1);
			else
				vmcs_clear(vcpu->vmcs);

			vpid_sync_context(vcpu->vpid);
			ept_sync_context(vcpu->eptp);
ffffffff812be772:	e8 0c fe ff ff       	callq  ffffffff812be583 <ept_sync_context>

			vcpu->launched = 0;
			vmcs_load(vcpu->vmcs);
ffffffff812be777:	48 8b bb 10 02 00 00 	mov    0x210(%rbx),%rdi
				vmcs_clear(vcpu->vmcs);

			vpid_sync_context(vcpu->vpid);
			ept_sync_context(vcpu->eptp);

			vcpu->launched = 0;
ffffffff812be77e:	c7 43 08 00 00 00 00 	movl   $0x0,0x8(%rbx)
			vmcs_load(vcpu->vmcs);
ffffffff812be785:	e8 14 fc ff ff       	callq  ffffffff812be39e <vmcs_load>
	return segment_base(tr);
}

static void __vmx_setup_cpu(void)
{
	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
ffffffff812be78a:	65 4c 03 2d 86 b9 d4 	add    %gs:0x7ed4b986(%rip),%r13        # a118 <this_cpu_off>
ffffffff812be791:	7e 
}

static inline unsigned long vmx_read_tr_base(void)
{
	u16 tr;
	asm("str %0" : "=g"(tr));
ffffffff812be792:	66 0f 00 cf          	str    %di
	return segment_base(tr);
ffffffff812be796:	0f b7 ff             	movzwl %di,%edi
ffffffff812be799:	e8 17 fb ff ff       	callq  ffffffff812be2b5 <segment_base>

	/*
	 * Linux uses per-cpu TSS and GDT, so set these when switching
	 * processors.
	 */
	vmcs_writel(HOST_TR_BASE, vmx_read_tr_base()); /* 22.2.4 */
ffffffff812be79e:	bf 0a 6c 00 00       	mov    $0x6c0a,%edi
ffffffff812be7a3:	48 89 c6             	mov    %rax,%rsi
ffffffff812be7a6:	e8 5b fc ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(HOST_GDTR_BASE, gdt->address);   /* 22.2.4 */
ffffffff812be7ab:	49 8b 75 02          	mov    0x2(%r13),%rsi
ffffffff812be7af:	bf 0c 6c 00 00       	mov    $0x6c0c,%edi
ffffffff812be7b4:	e8 4d fc ff ff       	callq  ffffffff812be406 <vmcs_writel>

static inline unsigned long long native_read_msr(unsigned int msr)
{
	DECLARE_ARGS(val, low, high);

	asm volatile("rdmsr" : EAX_EDX_RET(val, low, high) : "c" (msr));
ffffffff812be7b9:	b9 75 01 00 00       	mov    $0x175,%ecx
ffffffff812be7be:	0f 32                	rdmsr  

	rdmsrl(MSR_IA32_SYSENTER_ESP, sysenter_esp);
	vmcs_writel(HOST_IA32_SYSENTER_ESP, sysenter_esp); /* 22.2.3 */
ffffffff812be7c0:	48 89 d1             	mov    %rdx,%rcx
ffffffff812be7c3:	89 c0                	mov    %eax,%eax
ffffffff812be7c5:	bf 10 6c 00 00       	mov    $0x6c10,%edi
ffffffff812be7ca:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812be7ce:	48 89 ce             	mov    %rcx,%rsi
ffffffff812be7d1:	48 09 c6             	or     %rax,%rsi
ffffffff812be7d4:	e8 2d fc ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812be7d9:	b9 00 01 00 c0       	mov    $0xc0000100,%ecx
ffffffff812be7de:	0f 32                	rdmsr  

	rdmsrl(MSR_FS_BASE, tmpl);
	vmcs_writel(HOST_FS_BASE, tmpl); /* 22.2.4 */
ffffffff812be7e0:	48 89 d1             	mov    %rdx,%rcx
ffffffff812be7e3:	89 c0                	mov    %eax,%eax
ffffffff812be7e5:	bf 06 6c 00 00       	mov    $0x6c06,%edi
ffffffff812be7ea:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812be7ee:	48 89 ce             	mov    %rcx,%rsi
ffffffff812be7f1:	48 09 c6             	or     %rax,%rsi
ffffffff812be7f4:	e8 0d fc ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812be7f9:	b9 01 01 00 c0       	mov    $0xc0000101,%ecx
ffffffff812be7fe:	0f 32                	rdmsr  
	rdmsrl(MSR_GS_BASE, tmpl);
	vmcs_writel(HOST_GS_BASE, tmpl); /* 22.2.4 */
ffffffff812be800:	48 89 d1             	mov    %rdx,%rcx
ffffffff812be803:	89 c0                	mov    %eax,%eax
ffffffff812be805:	bf 08 6c 00 00       	mov    $0x6c08,%edi
ffffffff812be80a:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812be80e:	48 89 ce             	mov    %rcx,%rsi
ffffffff812be811:	48 09 c6             	or     %rax,%rsi
ffffffff812be814:	e8 ed fb ff ff       	callq  ffffffff812be406 <vmcs_writel>
			ept_sync_context(vcpu->eptp);

			vcpu->launched = 0;
			vmcs_load(vcpu->vmcs);
			__vmx_setup_cpu();
			vcpu->cpu = cur_cpu;
ffffffff812be819:	44 89 23             	mov    %r12d,(%rbx)
ffffffff812be81c:	eb 0c                	jmp    ffffffff812be82a <vmx_get_cpu+0x16b>
		} else {
			vmcs_load(vcpu->vmcs);
ffffffff812be81e:	48 8b bb 10 02 00 00 	mov    0x210(%rbx),%rdi
ffffffff812be825:	e8 74 fb ff ff       	callq  ffffffff812be39e <vmcs_load>
		}
	}
}
ffffffff812be82a:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812be82e:	5b                   	pop    %rbx
ffffffff812be82f:	41 5c                	pop    %r12
ffffffff812be831:	41 5d                	pop    %r13
ffffffff812be833:	5d                   	pop    %rbp
ffffffff812be834:	c3                   	retq   

ffffffff812be835 <vmx_dump_cpu>:
/**
 * vmx_dump_cpu - prints the CPU state
 * @vcpu: VCPU to print
 */
static void vmx_dump_cpu(struct vmx_vcpu *vcpu)
{
ffffffff812be835:	55                   	push   %rbp
ffffffff812be836:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be839:	41 54                	push   %r12
ffffffff812be83b:	53                   	push   %rbx
ffffffff812be83c:	48 89 fb             	mov    %rdi,%rbx
	unsigned long flags;

	vmx_get_cpu(vcpu);
ffffffff812be83f:	e8 7b fe ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812be844:	ba 1e 68 00 00       	mov    $0x681e,%edx
ffffffff812be849:	0f 78 d0             	vmread %rdx,%rax
ffffffff812be84c:	ba 1c 68 00 00       	mov    $0x681c,%edx
static void vmx_dump_cpu(struct vmx_vcpu *vcpu)
{
	unsigned long flags;

	vmx_get_cpu(vcpu);
	vcpu->regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);
ffffffff812be851:	48 89 83 e8 00 00 00 	mov    %rax,0xe8(%rbx)
ffffffff812be858:	0f 78 d0             	vmread %rdx,%rax

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812be85b:	ba 20 68 00 00       	mov    $0x6820,%edx
{
	unsigned long flags;

	vmx_get_cpu(vcpu);
	vcpu->regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);
	vcpu->regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);
ffffffff812be860:	48 89 83 88 00 00 00 	mov    %rax,0x88(%rbx)
ffffffff812be867:	0f 78 d0             	vmread %rdx,%rax

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812be86a:	49 89 c4             	mov    %rax,%r12
	vcpu->regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);
	vcpu->regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);
	flags = vmcs_readl(GUEST_RFLAGS);
	vmx_put_cpu(vcpu);

	printk(KERN_INFO "vmx: --- Begin VCPU Dump ---\n");
ffffffff812be86d:	48 c7 c7 1d 55 7b 81 	mov    $0xffffffff817b551d,%rdi
ffffffff812be874:	31 c0                	xor    %eax,%eax
ffffffff812be876:	e8 17 47 17 00       	callq  ffffffff81432f92 <printk>
	printk(KERN_INFO "vmx: CPU %d VPID %d\n", vcpu->cpu, vcpu->vpid);
ffffffff812be87b:	8b 53 04             	mov    0x4(%rbx),%edx
ffffffff812be87e:	8b 33                	mov    (%rbx),%esi
ffffffff812be880:	48 c7 c7 3d 55 7b 81 	mov    $0xffffffff817b553d,%rdi
ffffffff812be887:	31 c0                	xor    %eax,%eax
ffffffff812be889:	e8 04 47 17 00       	callq  ffffffff81432f92 <printk>
	printk(KERN_INFO "vmx: RIP 0x%016llx RFLAGS 0x%08lx\n",
ffffffff812be88e:	48 8b b3 e8 00 00 00 	mov    0xe8(%rbx),%rsi
ffffffff812be895:	4c 89 e2             	mov    %r12,%rdx
ffffffff812be898:	48 c7 c7 54 55 7b 81 	mov    $0xffffffff817b5554,%rdi
ffffffff812be89f:	31 c0                	xor    %eax,%eax
ffffffff812be8a1:	e8 ec 46 17 00       	callq  ffffffff81432f92 <printk>
	       vcpu->regs[VCPU_REGS_RIP], flags);
	printk(KERN_INFO "vmx: RAX 0x%016llx RCX 0x%016llx\n",
ffffffff812be8a6:	48 8b 53 70          	mov    0x70(%rbx),%rdx
ffffffff812be8aa:	48 8b 73 68          	mov    0x68(%rbx),%rsi
ffffffff812be8ae:	48 c7 c7 79 55 7b 81 	mov    $0xffffffff817b5579,%rdi
ffffffff812be8b5:	31 c0                	xor    %eax,%eax
ffffffff812be8b7:	e8 d6 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_RAX], vcpu->regs[VCPU_REGS_RCX]);
	printk(KERN_INFO "vmx: RDX 0x%016llx RBX 0x%016llx\n",
ffffffff812be8bc:	48 8b 93 80 00 00 00 	mov    0x80(%rbx),%rdx
ffffffff812be8c3:	48 8b 73 78          	mov    0x78(%rbx),%rsi
ffffffff812be8c7:	48 c7 c7 9d 55 7b 81 	mov    $0xffffffff817b559d,%rdi
ffffffff812be8ce:	31 c0                	xor    %eax,%eax
ffffffff812be8d0:	e8 bd 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_RDX], vcpu->regs[VCPU_REGS_RBX]);
	printk(KERN_INFO "vmx: RSP 0x%016llx RBP 0x%016llx\n",
ffffffff812be8d5:	48 8b 93 90 00 00 00 	mov    0x90(%rbx),%rdx
ffffffff812be8dc:	48 8b b3 88 00 00 00 	mov    0x88(%rbx),%rsi
ffffffff812be8e3:	48 c7 c7 c1 55 7b 81 	mov    $0xffffffff817b55c1,%rdi
ffffffff812be8ea:	31 c0                	xor    %eax,%eax
ffffffff812be8ec:	e8 a1 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_RSP], vcpu->regs[VCPU_REGS_RBP]);
	printk(KERN_INFO "vmx: RSI 0x%016llx RDI 0x%016llx\n",
ffffffff812be8f1:	48 8b 93 a0 00 00 00 	mov    0xa0(%rbx),%rdx
ffffffff812be8f8:	48 8b b3 98 00 00 00 	mov    0x98(%rbx),%rsi
ffffffff812be8ff:	48 c7 c7 e5 55 7b 81 	mov    $0xffffffff817b55e5,%rdi
ffffffff812be906:	31 c0                	xor    %eax,%eax
ffffffff812be908:	e8 85 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_RSI], vcpu->regs[VCPU_REGS_RDI]);
	printk(KERN_INFO "vmx: R8  0x%016llx R9  0x%016llx\n",
ffffffff812be90d:	48 8b 93 b0 00 00 00 	mov    0xb0(%rbx),%rdx
ffffffff812be914:	48 8b b3 a8 00 00 00 	mov    0xa8(%rbx),%rsi
ffffffff812be91b:	48 c7 c7 09 56 7b 81 	mov    $0xffffffff817b5609,%rdi
ffffffff812be922:	31 c0                	xor    %eax,%eax
ffffffff812be924:	e8 69 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_R8], vcpu->regs[VCPU_REGS_R9]);
	printk(KERN_INFO "vmx: R10 0x%016llx R11 0x%016llx\n",
ffffffff812be929:	48 8b 93 c0 00 00 00 	mov    0xc0(%rbx),%rdx
ffffffff812be930:	48 8b b3 b8 00 00 00 	mov    0xb8(%rbx),%rsi
ffffffff812be937:	48 c7 c7 2d 56 7b 81 	mov    $0xffffffff817b562d,%rdi
ffffffff812be93e:	31 c0                	xor    %eax,%eax
ffffffff812be940:	e8 4d 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_R10], vcpu->regs[VCPU_REGS_R11]);
	printk(KERN_INFO "vmx: R12 0x%016llx R13 0x%016llx\n",
ffffffff812be945:	48 8b 93 d0 00 00 00 	mov    0xd0(%rbx),%rdx
ffffffff812be94c:	48 8b b3 c8 00 00 00 	mov    0xc8(%rbx),%rsi
ffffffff812be953:	48 c7 c7 51 56 7b 81 	mov    $0xffffffff817b5651,%rdi
ffffffff812be95a:	31 c0                	xor    %eax,%eax
ffffffff812be95c:	e8 31 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_R12], vcpu->regs[VCPU_REGS_R13]);
	printk(KERN_INFO "vmx: R14 0x%016llx R15 0x%016llx\n",
ffffffff812be961:	48 8b 93 e0 00 00 00 	mov    0xe0(%rbx),%rdx
ffffffff812be968:	48 8b b3 d8 00 00 00 	mov    0xd8(%rbx),%rsi
ffffffff812be96f:	48 c7 c7 75 56 7b 81 	mov    $0xffffffff817b5675,%rdi
ffffffff812be976:	31 c0                	xor    %eax,%eax
ffffffff812be978:	e8 15 46 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->regs[VCPU_REGS_R14], vcpu->regs[VCPU_REGS_R15]);
	printk(KERN_INFO "vmx: --- End VCPU Dump ---\n");
ffffffff812be97d:	48 c7 c7 99 56 7b 81 	mov    $0xffffffff817b5699,%rdi
ffffffff812be984:	31 c0                	xor    %eax,%eax
ffffffff812be986:	e8 07 46 17 00       	callq  ffffffff81432f92 <printk>
}
ffffffff812be98b:	5b                   	pop    %rbx
ffffffff812be98c:	41 5c                	pop    %r12
ffffffff812be98e:	5d                   	pop    %rbp
ffffffff812be98f:	c3                   	retq   

ffffffff812be990 <vmcs_readl>:
ffffffff812be990:	55                   	push   %rbp
ffffffff812be991:	48 89 fa             	mov    %rdi,%rdx
ffffffff812be994:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be997:	0f 78 d0             	vmread %rdx,%rax
ffffffff812be99a:	5d                   	pop    %rbp
ffffffff812be99b:	c3                   	retq   

ffffffff812be99c <vmcs_read16>:
		      : "=a"(value) : "d"(field) : "cc");
	return value;
}

u16 vmcs_read16(unsigned long field)
{
ffffffff812be99c:	55                   	push   %rbp

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812be99d:	48 89 fa             	mov    %rdi,%rdx
		      : "=a"(value) : "d"(field) : "cc");
	return value;
}

u16 vmcs_read16(unsigned long field)
{
ffffffff812be9a0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be9a3:	0f 78 d0             	vmread %rdx,%rax
	return vmcs_readl(field);
}
ffffffff812be9a6:	5d                   	pop    %rbp
ffffffff812be9a7:	c3                   	retq   

ffffffff812be9a8 <vmcs_read32>:

u32 vmcs_read32(unsigned long field)
{
ffffffff812be9a8:	55                   	push   %rbp

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812be9a9:	48 89 fa             	mov    %rdi,%rdx
{
	return vmcs_readl(field);
}

u32 vmcs_read32(unsigned long field)
{
ffffffff812be9ac:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be9af:	0f 78 d0             	vmread %rdx,%rax
	return vmcs_readl(field);
}
ffffffff812be9b2:	5d                   	pop    %rbp
ffffffff812be9b3:	c3                   	retq   

ffffffff812be9b4 <vmcs_read64>:

u64 vmcs_read64(unsigned long field)
{
ffffffff812be9b4:	55                   	push   %rbp

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812be9b5:	48 89 fa             	mov    %rdi,%rdx
{
	return vmcs_readl(field);
}

u64 vmcs_read64(unsigned long field)
{
ffffffff812be9b8:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be9bb:	0f 78 d0             	vmread %rdx,%rax
#ifdef CONFIG_X86_64
	return vmcs_readl(field);
#else
	return vmcs_readl(field) | ((u64)vmcs_readl(field+1) << 32);
#endif
}
ffffffff812be9be:	5d                   	pop    %rbp
ffffffff812be9bf:	c3                   	retq   

ffffffff812be9c0 <vmx_ept_sync_vcpu>:
/**
 * vmx_ept_sync_global - used to evict everything in the EPT
 * @vcpu: the vcpu
 */
void vmx_ept_sync_vcpu(struct vmx_vcpu *vcpu)
{
ffffffff812be9c0:	55                   	push   %rbp
	smp_call_function_single(vcpu->cpu,
ffffffff812be9c1:	48 89 fa             	mov    %rdi,%rdx
ffffffff812be9c4:	8b 3f                	mov    (%rdi),%edi
ffffffff812be9c6:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812be9cb:	48 c7 c6 00 e6 2b 81 	mov    $0xffffffff812be600,%rsi
/**
 * vmx_ept_sync_global - used to evict everything in the EPT
 * @vcpu: the vcpu
 */
void vmx_ept_sync_vcpu(struct vmx_vcpu *vcpu)
{
ffffffff812be9d2:	48 89 e5             	mov    %rsp,%rbp
	smp_call_function_single(vcpu->cpu,
ffffffff812be9d5:	e8 e5 db de ff       	callq  ffffffff810ac5bf <smp_call_function_single>
		__vmx_sync_helper, (void *) vcpu, 1);
}
ffffffff812be9da:	5d                   	pop    %rbp
ffffffff812be9db:	c3                   	retq   

ffffffff812be9dc <vmx_ept_sync_individual_addr>:
 * vmx_ept_sync_individual_addr - used to evict an individual address
 * @vcpu: the vcpu
 * @gpa: the guest-physical address
 */
void vmx_ept_sync_individual_addr(struct vmx_vcpu *vcpu, gpa_t gpa)
{
ffffffff812be9dc:	55                   	push   %rbp
	struct sync_addr_args args;
	args.vcpu = vcpu;
	args.gpa = gpa;

	smp_call_function_single(vcpu->cpu,
ffffffff812be9dd:	b9 01 00 00 00       	mov    $0x1,%ecx
 * vmx_ept_sync_individual_addr - used to evict an individual address
 * @vcpu: the vcpu
 * @gpa: the guest-physical address
 */
void vmx_ept_sync_individual_addr(struct vmx_vcpu *vcpu, gpa_t gpa)
{
ffffffff812be9e2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812be9e5:	48 83 ec 10          	sub    $0x10,%rsp
	struct sync_addr_args args;
	args.vcpu = vcpu;
ffffffff812be9e9:	48 89 7d f0          	mov    %rdi,-0x10(%rbp)
	args.gpa = gpa;

	smp_call_function_single(vcpu->cpu,
ffffffff812be9ed:	8b 3f                	mov    (%rdi),%edi
ffffffff812be9ef:	48 8d 55 f0          	lea    -0x10(%rbp),%rdx
 */
void vmx_ept_sync_individual_addr(struct vmx_vcpu *vcpu, gpa_t gpa)
{
	struct sync_addr_args args;
	args.vcpu = vcpu;
	args.gpa = gpa;
ffffffff812be9f3:	48 89 75 f8          	mov    %rsi,-0x8(%rbp)

	smp_call_function_single(vcpu->cpu,
ffffffff812be9f7:	48 c7 c6 bb e5 2b 81 	mov    $0xffffffff812be5bb,%rsi
ffffffff812be9fe:	e8 bc db de ff       	callq  ffffffff810ac5bf <smp_call_function_single>
		__vmx_sync_individual_addr_helper, (void *) &args, 1);
}
ffffffff812bea03:	c9                   	leaveq 
ffffffff812bea04:	c3                   	retq   

ffffffff812bea05 <vmx_destroy_vm>:

	return ret;
}

void vmx_destroy_vm(struct vmx_vcpu* vcpu)
{
ffffffff812bea05:	55                   	push   %rbp
ffffffff812bea06:	48 89 e5             	mov    %rsp,%rbp
ffffffff812bea09:	53                   	push   %rbx
ffffffff812bea0a:	50                   	push   %rax
	printk(KERN_ERR "vmx: destroying VCPU (VPID %d)\n",
ffffffff812bea0b:	8b 77 04             	mov    0x4(%rdi),%esi

	return ret;
}

void vmx_destroy_vm(struct vmx_vcpu* vcpu)
{
ffffffff812bea0e:	48 89 fb             	mov    %rdi,%rbx
	printk(KERN_ERR "vmx: destroying VCPU (VPID %d)\n",
ffffffff812bea11:	31 c0                	xor    %eax,%eax
ffffffff812bea13:	48 c7 c7 95 57 7b 81 	mov    $0xffffffff817b5795,%rdi
ffffffff812bea1a:	e8 73 45 17 00       	callq  ffffffff81432f92 <printk>
			vcpu->vpid);
	vmx_dump_cpu(vcpu);
ffffffff812bea1f:	48 89 df             	mov    %rbx,%rdi
ffffffff812bea22:	e8 0e fe ff ff       	callq  ffffffff812be835 <vmx_dump_cpu>
 * vmx_destroy_vcpu - destroys and frees an existing virtual cpu
 * @vcpu: the VCPU to destroy
 */
static void vmx_destroy_vcpu(struct vmx_vcpu *vcpu)
{
	vmx_destroy_ept(vcpu);
ffffffff812bea27:	48 89 df             	mov    %rbx,%rdi
ffffffff812bea2a:	e8 e1 20 00 00       	callq  ffffffff812c0b10 <vmx_destroy_ept>
	vmx_get_cpu(vcpu);
ffffffff812bea2f:	48 89 df             	mov    %rbx,%rdi
ffffffff812bea32:	e8 88 fc ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>
	ept_sync_context(vcpu->eptp);
ffffffff812bea37:	48 8b 7b 48          	mov    0x48(%rbx),%rdi
ffffffff812bea3b:	e8 43 fb ff ff       	callq  ffffffff812be583 <ept_sync_context>
	vmcs_clear(vcpu->vmcs);
ffffffff812bea40:	48 8b bb 10 02 00 00 	mov    0x210(%rbx),%rdi
ffffffff812bea47:	e8 12 f9 ff ff       	callq  ffffffff812be35e <vmcs_clear>
	*this_cpu_ptr(&local_vcpu) = NULL;
ffffffff812bea4c:	48 c7 c0 20 09 01 00 	mov    $0x10920,%rax
ffffffff812bea53:	65 48 03 05 bd b6 d4 	add    %gs:0x7ed4b6bd(%rip),%rax        # a118 <this_cpu_off>
ffffffff812bea5a:	7e 
ffffffff812bea5b:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)
 * vmx_put_cpu - called after using a cpu
 * @vcpu: VCPU that was loaded.
 */
static void vmx_put_cpu(struct vmx_vcpu *vcpu)
{
	put_cpu();
ffffffff812bea62:	48 8d 7b 04          	lea    0x4(%rbx),%rdi
	vmx_get_cpu(vcpu);
	ept_sync_context(vcpu->eptp);
	vmcs_clear(vcpu->vmcs);
	*this_cpu_ptr(&local_vcpu) = NULL;
	vmx_put_cpu(vcpu);
	vmx_free_vpid(vcpu);
ffffffff812bea66:	e8 b6 f9 ff ff       	callq  ffffffff812be421 <vmx_free_vpid.isra.9>
/**
 * vmx_free_vmcs - frees a VMCS region
 */
static void vmx_free_vmcs(struct vmcs *vmcs)
{
	free_pages((unsigned long)vmcs, vmcs_config.order);
ffffffff812bea6b:	48 8b bb 10 02 00 00 	mov    0x210(%rbx),%rdi
ffffffff812bea72:	8b 35 cc 0c 8f 00    	mov    0x8f0ccc(%rip),%esi        # ffffffff81baf744 <vmcs_config+0x4>
ffffffff812bea78:	e8 d5 a4 e1 ff       	callq  ffffffff810d8f52 <free_pages>
	vmcs_clear(vcpu->vmcs);
	*this_cpu_ptr(&local_vcpu) = NULL;
	vmx_put_cpu(vcpu);
	vmx_free_vpid(vcpu);
	vmx_free_vmcs(vcpu->vmcs);
	kfree(vcpu);
ffffffff812bea7d:	48 89 df             	mov    %rbx,%rdi
ffffffff812bea80:	e8 04 1f e4 ff       	callq  ffffffff81100989 <kfree>
			vcpu->vpid);
	vmx_dump_cpu(vcpu);
	vmx_destroy_vcpu(vcpu);
	/* Write the context cleanup code */
//	panic("VM just exited.");
	do_exit(0);
ffffffff812bea85:	31 ff                	xor    %edi,%edi
ffffffff812bea87:	e8 21 94 da ff       	callq  ffffffff81067ead <do_exit>

ffffffff812bea8c <vmx_launch>:
/**
 * vmx_launch - the main loop for a VMX Dune process
 * @conf: the launch configuration
 */
int vmx_launch(struct dune_config *conf, int64_t *ret_code)
{
ffffffff812bea8c:	55                   	push   %rbp
	int ret;
	struct vmx_vcpu *vcpu;
	printk(KERN_INFO "vmx_launch: Entering \n");
ffffffff812bea8d:	31 c0                	xor    %eax,%eax
/**
 * vmx_launch - the main loop for a VMX Dune process
 * @conf: the launch configuration
 */
int vmx_launch(struct dune_config *conf, int64_t *ret_code)
{
ffffffff812bea8f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812bea92:	41 57                	push   %r15
ffffffff812bea94:	41 56                	push   %r14
ffffffff812bea96:	41 55                	push   %r13
ffffffff812bea98:	41 54                	push   %r12
ffffffff812bea9a:	49 89 fe             	mov    %rdi,%r14
ffffffff812bea9d:	53                   	push   %rbx
	int ret;
	struct vmx_vcpu *vcpu;
	printk(KERN_INFO "vmx_launch: Entering \n");
ffffffff812bea9e:	48 c7 c7 b7 57 7b 81 	mov    $0xffffffff817b57b7,%rdi
/**
 * vmx_launch - the main loop for a VMX Dune process
 * @conf: the launch configuration
 */
int vmx_launch(struct dune_config *conf, int64_t *ret_code)
{
ffffffff812beaa5:	48 83 ec 28          	sub    $0x28,%rsp
	int ret;
	struct vmx_vcpu *vcpu;
	printk(KERN_INFO "vmx_launch: Entering \n");
ffffffff812beaa9:	e8 e4 44 17 00       	callq  ffffffff81432f92 <printk>
 * Returns: A new VCPU structure
 */
static struct vmx_vcpu * vmx_create_vcpu(struct dune_config *conf)
{
	struct vmx_vcpu *vcpu;
	printk(KERN_INFO "vmx_create_vcpu Entering\n");
ffffffff812beaae:	48 c7 c7 d0 57 7b 81 	mov    $0xffffffff817b57d0,%rdi
ffffffff812beab5:	31 c0                	xor    %eax,%eax
ffffffff812beab7:	e8 d6 44 17 00       	callq  ffffffff81432f92 <printk>

#else /* CONFIG_TRACING */
static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
		gfp_t flags, size_t size)
{
	void *ret = kmem_cache_alloc(s, flags);
ffffffff812beabc:	48 8b 3d cd 3b 8e 00 	mov    0x8e3bcd(%rip),%rdi        # ffffffff81ba2690 <kmalloc_caches+0x50>
ffffffff812beac3:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812beac8:	e8 04 26 e4 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
	vcpu  = kmalloc(sizeof(struct vmx_vcpu), GFP_KERNEL);
	if (!vcpu)
ffffffff812beacd:	48 85 c0             	test   %rax,%rax
ffffffff812bead0:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812bead4:	0f 84 ef 0d 00 00    	je     ffffffff812bf8c9 <vmx_launch+0xe3d>
		return NULL;

	memset(vcpu, 0, sizeof(*vcpu));
ffffffff812beada:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812beade:	31 c0                	xor    %eax,%eax
ffffffff812beae0:	b9 88 00 00 00       	mov    $0x88,%ecx
ffffffff812beae5:	f3 ab                	rep stos %eax,%es:(%rdi)
 *
 * Returns a valid VMCS region.
 */
static struct vmcs *vmx_alloc_vmcs(void)
{
	return __vmx_alloc_vmcs(raw_smp_processor_id());
ffffffff812beae7:	e8 01 fa ff ff       	callq  ffffffff812be4ed <__vmx_alloc_vmcs.isra.13>
	if (!vcpu)
		return NULL;

	memset(vcpu, 0, sizeof(*vcpu));

	vcpu->vmcs = vmx_alloc_vmcs();
ffffffff812beaec:	48 8b 5d b8          	mov    -0x48(%rbp),%rbx
	if (!vcpu->vmcs)
ffffffff812beaf0:	48 85 c0             	test   %rax,%rax
	if (!vcpu)
		return NULL;

	memset(vcpu, 0, sizeof(*vcpu));

	vcpu->vmcs = vmx_alloc_vmcs();
ffffffff812beaf3:	48 89 83 10 02 00 00 	mov    %rax,0x210(%rbx)
	if (!vcpu->vmcs)
ffffffff812beafa:	0f 84 c0 0d 00 00    	je     ffffffff812bf8c0 <vmx_launch+0xe34>
 */
static int vmx_allocate_vpid(struct vmx_vcpu *vmx)
{
	int vpid;

	vmx->vpid = 0;
ffffffff812beb00:	c7 43 04 00 00 00 00 	movl   $0x0,0x4(%rbx)
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812beb07:	48 c7 c7 b0 5e a3 81 	mov    $0xffffffff81a35eb0,%rdi
ffffffff812beb0e:	e8 1d 9b 17 00       	callq  ffffffff81438630 <_raw_spin_lock>

	spin_lock(&vmx_vpid_lock);
	vpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);
ffffffff812beb13:	be 00 00 01 00       	mov    $0x10000,%esi
ffffffff812beb18:	48 c7 c7 80 f7 ba 81 	mov    $0xffffffff81baf780,%rdi
ffffffff812beb1f:	e8 5e 21 01 00       	callq  ffffffff812d0c82 <find_first_zero_bit>
	if (vpid < VMX_NR_VPIDS) {
ffffffff812beb24:	3d ff ff 00 00       	cmp    $0xffff,%eax
	int vpid;

	vmx->vpid = 0;

	spin_lock(&vmx_vpid_lock);
	vpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);
ffffffff812beb29:	48 89 c3             	mov    %rax,%rbx
	if (vpid < VMX_NR_VPIDS) {
ffffffff812beb2c:	7f 12                	jg     ffffffff812beb40 <vmx_launch+0xb4>
		vmx->vpid = vpid;
ffffffff812beb2e:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812beb32:	89 58 04             	mov    %ebx,0x4(%rax)
		__set_bit(vpid, vmx_vpid_bitmap);
ffffffff812beb35:	48 63 c3             	movslq %ebx,%rax
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
ffffffff812beb38:	48 0f ab 05 40 0c 8f 	bts    %rax,0x8f0c40(%rip)        # ffffffff81baf780 <vmx_vpid_bitmap>
ffffffff812beb3f:	00 
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812beb40:	48 c7 c7 b0 5e a3 81 	mov    $0xffffffff81a35eb0,%rdi
ffffffff812beb47:	e8 60 9b 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>

	vcpu->vmcs = vmx_alloc_vmcs();
	if (!vcpu->vmcs)
		goto fail_vmcs;

	if (vmx_allocate_vpid(vcpu))
ffffffff812beb4c:	81 fb ff ff 00 00    	cmp    $0xffff,%ebx
ffffffff812beb52:	0f 8f 52 0d 00 00    	jg     ffffffff812bf8aa <vmx_launch+0xe1e>
		goto fail_vpid;

	vcpu->cpu = -1;
ffffffff812beb58:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812beb5c:	c7 00 ff ff ff ff    	movl   $0xffffffff,(%rax)
struct vmx_capability vmx_capability;

static inline unsigned long azk_read_cr3(void)
{
        unsigned long val;
        asm volatile("mov %%cr3,%0\n\t" : "=r" (val), "=m" (__force_order));
ffffffff812beb62:	0f 20 d8             	mov    %cr3,%rax
		goto fail_vpid;

	vcpu->cpu = -1;
	// XXX : See later
	//vcpu->syscall_tbl = (void *) SYSCALL_TBL;
	conf->cr3 = azk_read_cr3();
ffffffff812beb65:	49 89 46 10          	mov    %rax,0x10(%r14)
	spin_lock_init(&vcpu->ept_lock);
ffffffff812beb69:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812beb6d:	48 c7 c2 2c f7 ba 81 	mov    $0xffffffff81baf72c,%rdx
ffffffff812beb74:	48 c7 c6 ec 57 7b 81 	mov    $0xffffffff817b57ec,%rsi
ffffffff812beb7b:	48 8d 78 28          	lea    0x28(%rax),%rdi
ffffffff812beb7f:	e8 ef 11 dd ff       	callq  ffffffff8108fd73 <__raw_spin_lock_init>
	// TODO: create one to one mapping for each physical address in cr3 page table 
	if (vmx_init_ept(vcpu, conf))
ffffffff812beb84:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812beb88:	4c 89 f6             	mov    %r14,%rsi
ffffffff812beb8b:	e8 a9 1e 00 00       	callq  ffffffff812c0a39 <vmx_init_ept>
ffffffff812beb90:	85 c0                	test   %eax,%eax
ffffffff812beb92:	0f 85 f7 0c 00 00    	jne    ffffffff812bf88f <vmx_launch+0xe03>
		goto fail_ept;
	vcpu->eptp = construct_eptp(vcpu->ept_root);
ffffffff812beb98:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812beb9c:	48 8b 5d b8          	mov    -0x48(%rbp),%rbx
ffffffff812beba0:	48 8b 40 40          	mov    0x40(%rax),%rax

	vmx_get_cpu(vcpu);
ffffffff812beba4:	48 89 df             	mov    %rbx,%rdi
	conf->cr3 = azk_read_cr3();
	spin_lock_init(&vcpu->ept_lock);
	// TODO: create one to one mapping for each physical address in cr3 page table 
	if (vmx_init_ept(vcpu, conf))
		goto fail_ept;
	vcpu->eptp = construct_eptp(vcpu->ept_root);
ffffffff812beba7:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
	u64 eptp;

	/* TODO write the value reading from MSR */
	eptp = VMX_EPT_DEFAULT_MT |
		VMX_EPT_DEFAULT_GAW << VMX_EPT_GAW_EPTP_SHIFT;
	if (cpu_has_vmx_ept_ad_bits())
ffffffff812bebab:	8b 05 6f 0b 8f 00    	mov    0x8f0b6f(%rip),%eax        # ffffffff81baf720 <vmx_capability>
	conf->cr3 = azk_read_cr3();
	spin_lock_init(&vcpu->ept_lock);
	// TODO: create one to one mapping for each physical address in cr3 page table 
	if (vmx_init_ept(vcpu, conf))
		goto fail_ept;
	vcpu->eptp = construct_eptp(vcpu->ept_root);
ffffffff812bebb1:	48 8b 55 b0          	mov    -0x50(%rbp),%rdx
	u64 eptp;

	/* TODO write the value reading from MSR */
	eptp = VMX_EPT_DEFAULT_MT |
		VMX_EPT_DEFAULT_GAW << VMX_EPT_GAW_EPTP_SHIFT;
	if (cpu_has_vmx_ept_ad_bits())
ffffffff812bebb5:	25 00 00 20 00       	and    $0x200000,%eax
		eptp |= VMX_EPT_AD_ENABLE_BIT;
ffffffff812bebba:	83 f8 01             	cmp    $0x1,%eax
ffffffff812bebbd:	48 19 c0             	sbb    %rax,%rax
	conf->cr3 = azk_read_cr3();
	spin_lock_init(&vcpu->ept_lock);
	// TODO: create one to one mapping for each physical address in cr3 page table 
	if (vmx_init_ept(vcpu, conf))
		goto fail_ept;
	vcpu->eptp = construct_eptp(vcpu->ept_root);
ffffffff812bebc0:	48 81 e2 00 f0 ff ff 	and    $0xfffffffffffff000,%rdx

	/* TODO write the value reading from MSR */
	eptp = VMX_EPT_DEFAULT_MT |
		VMX_EPT_DEFAULT_GAW << VMX_EPT_GAW_EPTP_SHIFT;
	if (cpu_has_vmx_ept_ad_bits())
		eptp |= VMX_EPT_AD_ENABLE_BIT;
ffffffff812bebc7:	48 83 e0 c0          	and    $0xffffffffffffffc0,%rax
ffffffff812bebcb:	48 83 c0 5e          	add    $0x5e,%rax
	conf->cr3 = azk_read_cr3();
	spin_lock_init(&vcpu->ept_lock);
	// TODO: create one to one mapping for each physical address in cr3 page table 
	if (vmx_init_ept(vcpu, conf))
		goto fail_ept;
	vcpu->eptp = construct_eptp(vcpu->ept_root);
ffffffff812bebcf:	48 09 d0             	or     %rdx,%rax
ffffffff812bebd2:	48 89 43 48          	mov    %rax,0x48(%rbx)

	vmx_get_cpu(vcpu);
ffffffff812bebd6:	e8 e4 fa ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>
		vmwrite_error(field, value);
}

static void vmcs_write16(unsigned long field, u16 value)
{
	vmcs_writel(field, value);
ffffffff812bebdb:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812bebdf:	31 ff                	xor    %edi,%edi
ffffffff812bebe1:	0f b7 70 04          	movzwl 0x4(%rax),%esi
ffffffff812bebe5:	e8 1c f8 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bebea:	48 83 ce ff          	or     $0xffffffffffffffff,%rsi
ffffffff812bebee:	bf 00 28 00 00       	mov    $0x2800,%edi
ffffffff812bebf3:	e8 0e f8 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bebf8:	8b 35 4e 0b 8f 00    	mov    0x8f0b4e(%rip),%esi        # ffffffff81baf74c <vmcs_config+0xc>
ffffffff812bebfe:	bf 00 40 00 00       	mov    $0x4000,%edi
ffffffff812bec03:	e8 fe f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bec08:	8b 35 42 0b 8f 00    	mov    0x8f0b42(%rip),%esi        # ffffffff81baf750 <vmcs_config+0x10>
ffffffff812bec0e:	bf 02 40 00 00       	mov    $0x4002,%edi
ffffffff812bec13:	e8 ee f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
		vmcs_config.pin_based_exec_ctrl);

	vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
		vmcs_config.cpu_based_exec_ctrl);

	if (cpu_has_secondary_exec_ctrls()) {
ffffffff812bec18:	83 3d 31 0b 8f 00 00 	cmpl   $0x0,0x8f0b31(%rip)        # ffffffff81baf750 <vmcs_config+0x10>
ffffffff812bec1f:	79 10                	jns    ffffffff812bec31 <vmx_launch+0x1a5>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bec21:	8b 35 2d 0b 8f 00    	mov    0x8f0b2d(%rip),%esi        # ffffffff81baf754 <vmcs_config+0x14>
ffffffff812bec27:	bf 1e 40 00 00       	mov    $0x401e,%edi
ffffffff812bec2c:	e8 d5 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bec31:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812bec35:	bf 1a 20 00 00       	mov    $0x201a,%edi
ffffffff812bec3a:	48 8b 70 48          	mov    0x48(%rax),%rsi
ffffffff812bec3e:	e8 c3 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bec43:	31 f6                	xor    %esi,%esi
ffffffff812bec45:	bf 06 40 00 00       	mov    $0x4006,%edi
ffffffff812bec4a:	e8 b7 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bec4f:	31 f6                	xor    %esi,%esi
ffffffff812bec51:	bf 08 40 00 00       	mov    $0x4008,%edi
ffffffff812bec56:	e8 ab f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bec5b:	be 00 40 00 00       	mov    $0x4000,%esi
ffffffff812bec60:	bf 04 40 00 00       	mov    $0x4004,%edi
ffffffff812bec65:	e8 9c f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bec6a:	31 f6                	xor    %esi,%esi
ffffffff812bec6c:	bf 0a 40 00 00       	mov    $0x400a,%edi
ffffffff812bec71:	e8 90 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>

	sz = 0;

	BUILD_BUG_ON(sz > NR_AUTOLOAD_MSRS);

	vcpu->msr_autoload.nr = sz;
ffffffff812bec76:	48 8b 45 b8          	mov    -0x48(%rbp),%rax

	/* XXX enable only MSRs in set */
	vmcs_write64(MSR_BITMAP, __pa(msr_bitmap));
ffffffff812bec7a:	48 8b 3d df 0a 8f 00 	mov    0x8f0adf(%rip),%rdi        # ffffffff81baf760 <msr_bitmap>

	sz = 0;

	BUILD_BUG_ON(sz > NR_AUTOLOAD_MSRS);

	vcpu->msr_autoload.nr = sz;
ffffffff812bec81:	c7 80 00 01 00 00 00 	movl   $0x0,0x100(%rax)
ffffffff812bec88:	00 00 00 

	/* XXX enable only MSRs in set */
	vmcs_write64(MSR_BITMAP, __pa(msr_bitmap));
ffffffff812bec8b:	e8 26 3c da ff       	callq  ffffffff810628b6 <__phys_addr>
	vmcs_writel(field, value);
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bec90:	bf 04 20 00 00       	mov    $0x2004,%edi
ffffffff812bec95:	48 89 c6             	mov    %rax,%rsi
ffffffff812bec98:	e8 69 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bec9d:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812beca1:	bf 0e 40 00 00       	mov    $0x400e,%edi
ffffffff812beca6:	8b b0 00 01 00 00    	mov    0x100(%rax),%esi
ffffffff812becac:	e8 55 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812becb1:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812becb5:	bf 10 40 00 00       	mov    $0x4010,%edi
ffffffff812becba:	8b b0 00 01 00 00    	mov    0x100(%rax),%esi
ffffffff812becc0:	e8 41 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812becc5:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812becc9:	bf 14 40 00 00       	mov    $0x4014,%edi
ffffffff812becce:	8b b0 00 01 00 00    	mov    0x100(%rax),%esi
ffffffff812becd4:	e8 2d f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>

	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, vcpu->msr_autoload.nr);
	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vcpu->msr_autoload.nr);
	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vcpu->msr_autoload.nr);

	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vcpu->msr_autoload.host));
ffffffff812becd9:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812becdd:	48 8d b8 90 01 00 00 	lea    0x190(%rax),%rdi
ffffffff812bece4:	e8 cd 3b da ff       	callq  ffffffff810628b6 <__phys_addr>
	vmcs_writel(field, value);
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bece9:	bf 08 20 00 00       	mov    $0x2008,%edi
ffffffff812becee:	48 89 c6             	mov    %rax,%rsi
ffffffff812becf1:	e8 10 f7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, vcpu->msr_autoload.nr);
	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vcpu->msr_autoload.nr);
	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vcpu->msr_autoload.nr);

	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vcpu->msr_autoload.host));
	vmcs_write64(VM_EXIT_MSR_STORE_ADDR, __pa(vcpu->msr_autoload.guest));
ffffffff812becf6:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812becfa:	48 8d 98 10 01 00 00 	lea    0x110(%rax),%rbx
ffffffff812bed01:	48 89 df             	mov    %rbx,%rdi
ffffffff812bed04:	e8 ad 3b da ff       	callq  ffffffff810628b6 <__phys_addr>
	vmcs_writel(field, value);
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bed09:	bf 06 20 00 00       	mov    $0x2006,%edi
ffffffff812bed0e:	48 89 c6             	mov    %rax,%rsi
ffffffff812bed11:	e8 f0 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vcpu->msr_autoload.nr);
	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vcpu->msr_autoload.nr);

	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vcpu->msr_autoload.host));
	vmcs_write64(VM_EXIT_MSR_STORE_ADDR, __pa(vcpu->msr_autoload.guest));
	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vcpu->msr_autoload.guest));
ffffffff812bed16:	48 89 df             	mov    %rbx,%rdi
ffffffff812bed19:	e8 98 3b da ff       	callq  ffffffff810628b6 <__phys_addr>
	vmcs_writel(field, value);
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bed1e:	bf 0a 20 00 00       	mov    $0x200a,%edi
ffffffff812bed23:	48 89 c6             	mov    %rax,%rsi
ffffffff812bed26:	e8 db f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bed2b:	8b 35 27 0a 8f 00    	mov    0x8f0a27(%rip),%esi        # ffffffff81baf758 <vmcs_config+0x18>
ffffffff812bed31:	bf 0c 40 00 00       	mov    $0x400c,%edi
		vmx->guest_msrs[j].mask = -1ull;
		++vmx->nmsrs;
	}
#endif

	vmcs_config.vmentry_ctrl |= VM_ENTRY_IA32E_MODE;
ffffffff812bed36:	81 0d 1c 0a 8f 00 00 	orl    $0x200,0x8f0a1c(%rip)        # ffffffff81baf75c <vmcs_config+0x1c>
ffffffff812bed3d:	02 00 00 
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bed40:	e8 c1 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bed45:	8b 35 11 0a 8f 00    	mov    0x8f0a11(%rip),%esi        # ffffffff81baf75c <vmcs_config+0x1c>
ffffffff812bed4b:	bf 12 40 00 00       	mov    $0x4012,%edi
ffffffff812bed50:	e8 b1 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_config.vmentry_ctrl |= VM_ENTRY_IA32E_MODE;

	vmcs_write32(VM_EXIT_CONTROLS, vmcs_config.vmexit_ctrl);
	vmcs_write32(VM_ENTRY_CONTROLS, vmcs_config.vmentry_ctrl);

	vmcs_writel(CR0_GUEST_HOST_MASK, ~0ul);
ffffffff812bed55:	48 83 ce ff          	or     $0xffffffffffffffff,%rsi
ffffffff812bed59:	bf 00 60 00 00       	mov    $0x6000,%edi
ffffffff812bed5e:	e8 a3 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(CR4_GUEST_HOST_MASK, ~0ul);
ffffffff812bed63:	48 83 ce ff          	or     $0xffffffffffffffff,%rsi
ffffffff812bed67:	bf 02 60 00 00       	mov    $0x6002,%edi
ffffffff812bed6c:	e8 95 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>

	//kvm_write_tsc(&vmx->vcpu, 0);
	vmcs_writel(TSC_OFFSET, 0);
ffffffff812bed71:	31 f6                	xor    %esi,%esi
ffffffff812bed73:	bf 10 20 00 00       	mov    $0x2010,%edi
ffffffff812bed78:	e8 89 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
extern unsigned long __force_order;

static inline unsigned long native_read_cr0(void)
{
	unsigned long val;
	asm volatile("mov %%cr0,%0\n\t" : "=r" (val), "=m" (__force_order));
ffffffff812bed7d:	0f 20 c6             	mov    %cr0,%rsi
{
	u32 low32, high32;
	unsigned long tmpl, new_rsp;
	struct desc_ptr dt;

	vmcs_writel(HOST_CR0, native_read_cr0() & ~X86_CR0_TS);  /* 22.2.3 */
ffffffff812bed80:	bf 00 6c 00 00       	mov    $0x6c00,%edi
ffffffff812bed85:	48 83 e6 f7          	and    $0xfffffffffffffff7,%rsi
ffffffff812bed89:	e8 78 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
}

static inline unsigned long native_read_cr4(void)
{
	unsigned long val;
	asm volatile("mov %%cr4,%0\n\t" : "=r" (val), "=m" (__force_order));
ffffffff812bed8e:	0f 20 e6             	mov    %cr4,%rsi
	vmcs_writel(HOST_CR4, native_read_cr4());  /* 22.2.3, 22.2.5 */
ffffffff812bed91:	bf 04 6c 00 00       	mov    $0x6c04,%edi
ffffffff812bed96:	e8 6b f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
struct vmx_capability vmx_capability;

static inline unsigned long azk_read_cr3(void)
{
        unsigned long val;
        asm volatile("mov %%cr3,%0\n\t" : "=r" (val), "=m" (__force_order));
ffffffff812bed9b:	0f 20 de             	mov    %cr3,%rsi
	unsigned long tmpl, new_rsp;
	struct desc_ptr dt;

	vmcs_writel(HOST_CR0, native_read_cr0() & ~X86_CR0_TS);  /* 22.2.3 */
	vmcs_writel(HOST_CR4, native_read_cr4());  /* 22.2.3, 22.2.5 */
	vmcs_writel(HOST_CR3, azk_read_cr3());  /* 22.2.3 */
ffffffff812bed9e:	bf 02 6c 00 00       	mov    $0x6c02,%edi
ffffffff812beda3:	e8 5e f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
		vmwrite_error(field, value);
}

static void vmcs_write16(unsigned long field, u16 value)
{
	vmcs_writel(field, value);
ffffffff812beda8:	be 10 00 00 00       	mov    $0x10,%esi
ffffffff812bedad:	bf 02 0c 00 00       	mov    $0xc02,%edi
ffffffff812bedb2:	e8 4f f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bedb7:	be 18 00 00 00       	mov    $0x18,%esi
ffffffff812bedbc:	bf 06 0c 00 00       	mov    $0xc06,%edi
ffffffff812bedc1:	e8 40 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bedc6:	be 18 00 00 00       	mov    $0x18,%esi
ffffffff812bedcb:	bf 00 0c 00 00       	mov    $0xc00,%edi
ffffffff812bedd0:	e8 31 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bedd5:	be 18 00 00 00       	mov    $0x18,%esi
ffffffff812bedda:	bf 04 0c 00 00       	mov    $0xc04,%edi
ffffffff812beddf:	e8 22 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bede4:	be 40 00 00 00       	mov    $0x40,%esi
ffffffff812bede9:	bf 0c 0c 00 00       	mov    $0xc0c,%edi
ffffffff812bedee:	e8 13 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	asm volatile("sgdt %0":"=m" (*dtr));
}

static inline void native_store_idt(struct desc_ptr *dtr)
{
	asm volatile("sidt %0":"=m" (*dtr));
ffffffff812bedf3:	0f 01 4d c6          	sidt   -0x3a(%rbp)
	vmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
	vmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);  /* 22.2.4 */
	vmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */

	native_store_idt(&dt);
	vmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */
ffffffff812bedf7:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
ffffffff812bedfb:	bf 0e 6c 00 00       	mov    $0x6c0e,%edi
ffffffff812bee00:	e8 01 f6 ff ff       	callq  ffffffff812be406 <vmcs_writel>

	asm("mov $.Lkvm_vmx_return, %0" : "=r"(tmpl));
	vmcs_writel(HOST_RIP, tmpl); /* 22.2.5 */
ffffffff812bee05:	bf 16 6c 00 00       	mov    $0x6c16,%edi
	vmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);  /* 22.2.4 */

	native_store_idt(&dt);
	vmcs_writel(HOST_IDTR_BASE, dt.address);   /* 22.2.4 */

	asm("mov $.Lkvm_vmx_return, %0" : "=r"(tmpl));
ffffffff812bee0a:	48 c7 c6 9d f7 2b 81 	mov    $0xffffffff812bf79d,%rsi
	vmcs_writel(HOST_RIP, tmpl); /* 22.2.5 */
ffffffff812bee11:	e8 f0 f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bee16:	b9 74 01 00 00       	mov    $0x174,%ecx
ffffffff812bee1b:	0f 32                	rdmsr  
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bee1d:	bf 00 4c 00 00       	mov    $0x4c00,%edi
ffffffff812bee22:	89 c6                	mov    %eax,%esi
ffffffff812bee24:	e8 dd f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bee29:	b9 76 01 00 00       	mov    $0x176,%ecx
ffffffff812bee2e:	0f 32                	rdmsr  
	vmcs_writel(HOST_RIP, tmpl); /* 22.2.5 */

	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);
	rdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);
	vmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */
ffffffff812bee30:	48 89 d1             	mov    %rdx,%rcx
ffffffff812bee33:	89 c0                	mov    %eax,%eax
ffffffff812bee35:	bf 12 6c 00 00       	mov    $0x6c12,%edi
ffffffff812bee3a:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812bee3e:	48 09 c1             	or     %rax,%rcx
ffffffff812bee41:	48 89 ce             	mov    %rcx,%rsi
ffffffff812bee44:	e8 bd f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bee49:	b9 80 00 00 c0       	mov    $0xc0000080,%ecx
ffffffff812bee4e:	0f 32                	rdmsr  
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bee50:	bf 02 2c 00 00       	mov    $0x2c02,%edi
ffffffff812bee55:	89 c6                	mov    %eax,%esi
ffffffff812bee57:	e8 aa f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);   /* 22.2.3 */

	rdmsr(MSR_EFER, low32, high32);
	vmcs_write32(HOST_IA32_EFER, low32);

	if (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {
ffffffff812bee5c:	f6 05 f7 08 8f 00 08 	testb  $0x8,0x8f08f7(%rip)        # ffffffff81baf75a <vmcs_config+0x1a>
ffffffff812bee63:	74 20                	je     ffffffff812bee85 <vmx_launch+0x3f9>
ffffffff812bee65:	b9 77 02 00 00       	mov    $0x277,%ecx
ffffffff812bee6a:	0f 32                	rdmsr  
	vmcs_writel(field, value);
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bee6c:	48 89 d1             	mov    %rdx,%rcx
ffffffff812bee6f:	89 c0                	mov    %eax,%eax
ffffffff812bee71:	bf 00 2c 00 00       	mov    $0x2c00,%edi
ffffffff812bee76:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812bee7a:	48 09 c1             	or     %rax,%rcx
ffffffff812bee7d:	48 89 ce             	mov    %rcx,%rsi
ffffffff812bee80:	e8 81 f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
		vmwrite_error(field, value);
}

static void vmcs_write16(unsigned long field, u16 value)
{
	vmcs_writel(field, value);
ffffffff812bee85:	31 f6                	xor    %esi,%esi
ffffffff812bee87:	bf 08 0c 00 00       	mov    $0xc08,%edi
ffffffff812bee8c:	e8 75 f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bee91:	31 f6                	xor    %esi,%esi
ffffffff812bee93:	bf 0a 0c 00 00       	mov    $0xc0a,%edi
ffffffff812bee98:	e8 69 f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
		vmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));
	}

	vmcs_write16(HOST_FS_SELECTOR, 0);            /* 22.2.4 */
	vmcs_write16(HOST_GS_SELECTOR, 0);            /* 22.2.4 */
	new_rsp = __get_free_pages(GFP_KERNEL, 0);
ffffffff812bee9d:	31 f6                	xor    %esi,%esi
ffffffff812bee9f:	bf d0 00 00 00       	mov    $0xd0,%edi
ffffffff812beea4:	e8 1b b2 e1 ff       	callq  ffffffff810da0c4 <__get_free_pages>
	BUG_ON(new_rsp == 0);
ffffffff812beea9:	48 85 c0             	test   %rax,%rax
ffffffff812beeac:	75 02                	jne    ffffffff812beeb0 <vmx_launch+0x424>
ffffffff812beeae:	0f 0b                	ud2    
	new_rsp += PAGE_SIZE - sizeof(unsigned long);
ffffffff812beeb0:	48 8d 98 f8 0f 00 00 	lea    0xff8(%rax),%rbx
	printk("NEW_RSP address is %016lx\n", new_rsp);
ffffffff812beeb7:	48 c7 c7 06 58 7b 81 	mov    $0xffffffff817b5806,%rdi
ffffffff812beebe:	31 c0                	xor    %eax,%eax
ffffffff812beec0:	48 89 de             	mov    %rbx,%rsi
ffffffff812beec3:	e8 ca 40 17 00       	callq  ffffffff81432f92 <printk>
	vmcs_writel(HOST_RSP, new_rsp);
ffffffff812beec8:	48 89 de             	mov    %rbx,%rsi
ffffffff812beecb:	bf 14 6c 00 00       	mov    $0x6c14,%edi
ffffffff812beed0:	e8 31 f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812beed5:	b9 00 01 00 c0       	mov    $0xc0000100,%ecx
ffffffff812beeda:	0f 32                	rdmsr  
#ifdef CONFIG_X86_64
	rdmsrl(MSR_FS_BASE, tmpl);
	vmcs_writel(HOST_FS_BASE, tmpl); /* 22.2.4 */
ffffffff812beedc:	48 89 d6             	mov    %rdx,%rsi
ffffffff812beedf:	89 c0                	mov    %eax,%eax
ffffffff812beee1:	bf 06 6c 00 00       	mov    $0x6c06,%edi
ffffffff812beee6:	48 c1 e6 20          	shl    $0x20,%rsi
ffffffff812beeea:	41 bc 01 01 00 c0    	mov    $0xc0000101,%r12d
ffffffff812beef0:	48 09 c6             	or     %rax,%rsi
ffffffff812beef3:	e8 0e f5 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812beef8:	44 89 e1             	mov    %r12d,%ecx
ffffffff812beefb:	0f 32                	rdmsr  
	rdmsrl(MSR_GS_BASE, tmpl);
	vmcs_writel(HOST_GS_BASE, tmpl); /* 22.2.4 */
ffffffff812beefd:	48 89 d6             	mov    %rdx,%rsi
ffffffff812bef00:	89 c0                	mov    %eax,%eax
ffffffff812bef02:	bf 08 6c 00 00       	mov    $0x6c08,%edi
ffffffff812bef07:	48 c1 e6 20          	shl    $0x20,%rsi
ffffffff812bef0b:	48 09 c6             	or     %rax,%rsi
ffffffff812bef0e:	e8 f3 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bef13:	ba 14 6c 00 00       	mov    $0x6c14,%edx
ffffffff812bef18:	0f 78 d0             	vmread %rdx,%rax

	//kvm_write_tsc(&vmx->vcpu, 0);
	vmcs_writel(TSC_OFFSET, 0);

	vmx_setup_constant_host_state();
	vcpu->host_rsp = vmcs_readl(HOST_RSP);
ffffffff812bef1b:	48 8b 5d b8          	mov    -0x48(%rbp),%rbx
{
	// TODO: Change all the state to host kernel. Newchange
	unsigned long tmpl;
	unsigned long cr4 = X86_CR4_PAE | X86_CR4_VMXE | X86_CR4_OSXMMEXCPT |
			    X86_CR4_PGE | X86_CR4_OSFXSR;
	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
ffffffff812bef1f:	49 c7 c7 30 09 01 00 	mov    $0x10930,%r15

	//kvm_write_tsc(&vmx->vcpu, 0);
	vmcs_writel(TSC_OFFSET, 0);

	vmx_setup_constant_host_state();
	vcpu->host_rsp = vmcs_readl(HOST_RSP);
ffffffff812bef26:	48 89 43 60          	mov    %rax,0x60(%rbx)
{
	// TODO: Change all the state to host kernel. Newchange
	unsigned long tmpl;
	unsigned long cr4 = X86_CR4_PAE | X86_CR4_VMXE | X86_CR4_OSXMMEXCPT |
			    X86_CR4_PGE | X86_CR4_OSFXSR;
	struct desc_ptr *gdt = this_cpu_ptr(&host_gdt);
ffffffff812bef2a:	65 4c 03 3d e6 b1 d4 	add    %gs:0x7ed4b1e6(%rip),%r15        # a118 <this_cpu_off>
ffffffff812bef31:	7e 
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
ffffffff812bef32:	48 8b 05 ab 85 79 00 	mov    0x7985ab(%rip),%rax        # ffffffff81a574e4 <boot_cpu_data+0x24>
ffffffff812bef39:	48 8b 15 a4 85 79 00 	mov    0x7985a4(%rip),%rdx        # ffffffff81a574e4 <boot_cpu_data+0x24>
/*	if (boot_cpu_has(X86_FEATURE_FSGSBASE))
		cr4 |= X86_CR4_FSGSBASE;*/
//		cr4 |= X86_CR4_RDWRGSFS;

	/* configure control and data registers */
	vmcs_writel(GUEST_CR0, X86_CR0_PG | X86_CR0_PE | X86_CR0_WP |
ffffffff812bef40:	be 33 00 01 80       	mov    $0x80010033,%esi
ffffffff812bef45:	bf 00 68 00 00       	mov    $0x6800,%edi
	unsigned long cr3;
	struct desc_ptr idt;
        uint64_t cs_val, es_val, ds_val, ss_val;
	u32 low32, high32;

	if (boot_cpu_has(X86_FEATURE_PCID))
ffffffff812bef4a:	48 c1 e8 11          	shr    $0x11,%rax
ffffffff812bef4e:	83 e0 01             	and    $0x1,%eax
		cr4 |= X86_CR4_PCIDE;
ffffffff812bef51:	48 83 f8 01          	cmp    $0x1,%rax
ffffffff812bef55:	4d 19 ed             	sbb    %r13,%r13
ffffffff812bef58:	49 81 e5 00 00 fe ff 	and    $0xfffffffffffe0000,%r13
ffffffff812bef5f:	49 81 c5 a0 26 02 00 	add    $0x226a0,%r13
	if (boot_cpu_has(X86_FEATURE_OSXSAVE))
		cr4 |= X86_CR4_OSXSAVE;
ffffffff812bef66:	4c 89 e8             	mov    %r13,%rax
ffffffff812bef69:	48 0d 00 00 04 00    	or     $0x40000,%rax
ffffffff812bef6f:	f7 c2 00 00 00 08    	test   $0x8000000,%edx
ffffffff812bef75:	4c 0f 45 e8          	cmovne %rax,%r13
/*	if (boot_cpu_has(X86_FEATURE_FSGSBASE))
		cr4 |= X86_CR4_FSGSBASE;*/
//		cr4 |= X86_CR4_RDWRGSFS;

	/* configure control and data registers */
	vmcs_writel(GUEST_CR0, X86_CR0_PG | X86_CR0_PE | X86_CR0_WP |
ffffffff812bef79:	e8 88 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
			       X86_CR0_MP | X86_CR0_ET | X86_CR0_NE);
	vmcs_writel(CR0_READ_SHADOW, X86_CR0_PG | X86_CR0_PE | X86_CR0_WP |
ffffffff812bef7e:	be 33 00 01 80       	mov    $0x80010033,%esi
ffffffff812bef83:	bf 04 60 00 00       	mov    $0x6004,%edi
ffffffff812bef88:	e8 79 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
				     X86_CR0_MP | X86_CR0_ET | X86_CR0_NE);
	//TODO: Use host CR3 istead of the one from conf
	//asm("\t movq %%cr3,%0" : "=r"(cr3));
	//cr3 = virt_to_phys(current->active_mm->pgd);
	//vmcs_writel(GUEST_CR3, cr3 /*conf->cr3*/);
	cr3 = conf->cr3;
ffffffff812bef8d:	49 8b 5e 10          	mov    0x10(%r14),%rbx
	printk(KERN_INFO "Cr3 is 0x%016llx. rip is 0x%016llx. rsp is 0x%016llx \n", (long long unsigned int)cr3, conf->rip, conf->rsp);
ffffffff812bef91:	49 8b 4e 08          	mov    0x8(%r14),%rcx
ffffffff812bef95:	48 c7 c7 21 58 7b 81 	mov    $0xffffffff817b5821,%rdi
ffffffff812bef9c:	49 8b 16             	mov    (%r14),%rdx
ffffffff812bef9f:	31 c0                	xor    %eax,%eax
ffffffff812befa1:	48 89 de             	mov    %rbx,%rsi
ffffffff812befa4:	e8 e9 3f 17 00       	callq  ffffffff81432f92 <printk>
	vmcs_writel(GUEST_CR3, cr3);
ffffffff812befa9:	48 89 de             	mov    %rbx,%rsi
ffffffff812befac:	bf 02 68 00 00       	mov    $0x6802,%edi
ffffffff812befb1:	e8 50 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_CR4, cr4);
ffffffff812befb6:	4c 89 ee             	mov    %r13,%rsi
ffffffff812befb9:	bf 04 68 00 00       	mov    $0x6804,%edi
ffffffff812befbe:	e8 43 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(CR4_READ_SHADOW, cr4);
ffffffff812befc3:	4c 89 ee             	mov    %r13,%rsi
ffffffff812befc6:	bf 06 60 00 00       	mov    $0x6006,%edi
ffffffff812befcb:	e8 36 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_IA32_EFER, EFER_LME | EFER_LMA |
ffffffff812befd0:	be 01 45 00 00       	mov    $0x4501,%esi
ffffffff812befd5:	bf 06 28 00 00       	mov    $0x2806,%edi
ffffffff812befda:	e8 27 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812befdf:	0f 01 4d c6          	sidt   -0x3a(%rbp)

	native_store_idt(&idt);
//        uint16_t xdtr_limit;
//        uint64_t xdtr_base;
//        read_gdtr( &xdtr_base, &xdtr_limit );
        vmcs_writel(GUEST_GDTR_BASE, gdt->address);
ffffffff812befe3:	49 8b 77 02          	mov    0x2(%r15),%rsi
ffffffff812befe7:	bf 16 68 00 00       	mov    $0x6816,%edi
ffffffff812befec:	e8 15 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
        vmcs_writel(GUEST_GDTR_LIMIT, gdt->size);
ffffffff812beff1:	41 0f b7 37          	movzwl (%r15),%esi
ffffffff812beff5:	bf 10 48 00 00       	mov    $0x4810,%edi
ffffffff812beffa:	e8 07 f4 ff ff       	callq  ffffffff812be406 <vmcs_writel>
//        vmcs_writel(GUEST_GDTR_BASE, xdtr_base);
//        vmcs_writel(GUEST_GDTR_LIMIT, xdtr_limit);
 
//        read_idtr( &xdtr_base, &xdtr_limit );
        vmcs_writel(GUEST_IDTR_BASE, idt.address);
ffffffff812befff:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
ffffffff812bf003:	bf 18 68 00 00       	mov    $0x6818,%edi
ffffffff812bf008:	e8 f9 f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
        vmcs_writel(GUEST_IDTR_LIMIT, idt.size);
ffffffff812bf00d:	0f b7 75 c6          	movzwl -0x3a(%rbp),%esi
ffffffff812bf011:	bf 12 48 00 00       	mov    $0x4812,%edi
ffffffff812bf016:	e8 eb f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
//        vmcs_writel(GUEST_GDTR_BASE, 0);
//        vmcs_writel(GUEST_GDTR_LIMIT, 0);
//        vmcs_writel(GUEST_IDTR_BASE, 0);
//        vmcs_writel(GUEST_IDTR_LIMIT, 0);

	vmcs_writel(GUEST_RIP, conf->rip);
ffffffff812bf01b:	49 8b 36             	mov    (%r14),%rsi
ffffffff812bf01e:	bf 1e 68 00 00       	mov    $0x681e,%edi
ffffffff812bf023:	e8 de f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_RSP, conf->rsp);
ffffffff812bf028:	49 8b 76 08          	mov    0x8(%r14),%rsi
ffffffff812bf02c:	bf 1c 68 00 00       	mov    $0x681c,%edi
ffffffff812bf031:	e8 d0 f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	/*
	 * "=rm" is safe here, because "pop" adjusts the stack before
	 * it evaluates its effective address -- this is part of the
	 * documented behavior of the "pop" instruction.
	 */
	asm volatile("# __raw_save_flags\n\t"
ffffffff812bf036:	9c                   	pushfq 
ffffffff812bf037:	5e                   	pop    %rsi
	vmcs_writel(GUEST_RFLAGS, native_save_fl());
ffffffff812bf038:	bf 20 68 00 00       	mov    $0x6820,%edi
ffffffff812bf03d:	e8 c4 f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_DR7, 0);
ffffffff812bf042:	31 f6                	xor    %esi,%esi
ffffffff812bf044:	bf 1a 68 00 00       	mov    $0x681a,%edi
ffffffff812bf049:	e8 b8 f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
//	vmcs_writel(GUEST_ES_BASE, 0);

	asm("mov %%cs, %0": "=r"(cs_val));
	asm("mov %%ds, %0": "=r"(ds_val));
	asm("mov %%es, %0": "=r"(es_val));
        vmcs_writel(GUEST_CS_BASE, cs_val);
ffffffff812bf04e:	bf 08 68 00 00       	mov    $0x6808,%edi
	// TODO: get these bases from the current task_struct Newchange
//	vmcs_writel(GUEST_CS_BASE, 0);
//	vmcs_writel(GUEST_DS_BASE, 0);
//	vmcs_writel(GUEST_ES_BASE, 0);

	asm("mov %%cs, %0": "=r"(cs_val));
ffffffff812bf053:	48 8c ce             	mov    %cs,%rsi
	asm("mov %%ds, %0": "=r"(ds_val));
ffffffff812bf056:	49 8c de             	mov    %ds,%r14
	asm("mov %%es, %0": "=r"(es_val));
        vmcs_writel(GUEST_CS_BASE, cs_val);
ffffffff812bf059:	e8 a8 f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
        vmcs_writel(GUEST_DS_BASE, ds_val);
ffffffff812bf05e:	4c 89 f6             	mov    %r14,%rsi
ffffffff812bf061:	bf 0c 68 00 00       	mov    $0x680c,%edi
ffffffff812bf066:	e8 9b f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
//	vmcs_writel(GUEST_DS_BASE, 0);
//	vmcs_writel(GUEST_ES_BASE, 0);

	asm("mov %%cs, %0": "=r"(cs_val));
	asm("mov %%ds, %0": "=r"(ds_val));
	asm("mov %%es, %0": "=r"(es_val));
ffffffff812bf06b:	49 8c c5             	mov    %es,%r13
        vmcs_writel(GUEST_CS_BASE, cs_val);
        vmcs_writel(GUEST_DS_BASE, ds_val);
        vmcs_writel(GUEST_ES_BASE, es_val);
ffffffff812bf06e:	bf 06 68 00 00       	mov    $0x6806,%edi
ffffffff812bf073:	4c 89 ee             	mov    %r13,%rsi
ffffffff812bf076:	e8 8b f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf07b:	44 89 e1             	mov    %r12d,%ecx
ffffffff812bf07e:	0f 32                	rdmsr  

	rdmsrl(MSR_GS_BASE, tmpl);
	vmcs_writel(GUEST_GS_BASE, tmpl);
ffffffff812bf080:	48 89 d6             	mov    %rdx,%rsi
ffffffff812bf083:	89 c0                	mov    %eax,%eax
ffffffff812bf085:	bf 10 68 00 00       	mov    $0x6810,%edi
ffffffff812bf08a:	48 c1 e6 20          	shl    $0x20,%rsi
ffffffff812bf08e:	48 09 c6             	or     %rax,%rsi
ffffffff812bf091:	e8 70 f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
        asm("mov %%ss, %0": "=r"(ss_val));
	vmcs_writel(GUEST_SS_BASE, ss_val);
ffffffff812bf096:	bf 0a 68 00 00       	mov    $0x680a,%edi
        vmcs_writel(GUEST_DS_BASE, ds_val);
        vmcs_writel(GUEST_ES_BASE, es_val);

	rdmsrl(MSR_GS_BASE, tmpl);
	vmcs_writel(GUEST_GS_BASE, tmpl);
        asm("mov %%ss, %0": "=r"(ss_val));
ffffffff812bf09b:	48 8c d6             	mov    %ss,%rsi
	vmcs_writel(GUEST_SS_BASE, ss_val);
ffffffff812bf09e:	e8 63 f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf0a3:	b9 00 01 00 c0       	mov    $0xc0000100,%ecx
ffffffff812bf0a8:	0f 32                	rdmsr  
	rdmsrl(MSR_FS_BASE, tmpl);
	vmcs_writel(GUEST_FS_BASE, tmpl);
ffffffff812bf0aa:	48 89 d6             	mov    %rdx,%rsi
ffffffff812bf0ad:	89 c0                	mov    %eax,%eax
ffffffff812bf0af:	bf 0e 68 00 00       	mov    $0x680e,%edi
ffffffff812bf0b4:	48 c1 e6 20          	shl    $0x20,%rsi
ffffffff812bf0b8:	48 09 c6             	or     %rax,%rsi
}

static inline unsigned long vmx_read_tr_base(void)
{
	u16 tr;
	asm("str %0" : "=g"(tr));
ffffffff812bf0bb:	66 0f 00 cb          	str    %bx
	return segment_base(tr);
ffffffff812bf0bf:	0f b7 db             	movzwl %bx,%ebx
	rdmsrl(MSR_GS_BASE, tmpl);
	vmcs_writel(GUEST_GS_BASE, tmpl);
        asm("mov %%ss, %0": "=r"(ss_val));
	vmcs_writel(GUEST_SS_BASE, ss_val);
	rdmsrl(MSR_FS_BASE, tmpl);
	vmcs_writel(GUEST_FS_BASE, tmpl);
ffffffff812bf0c2:	e8 3f f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>

static inline unsigned long vmx_read_tr_base(void)
{
	u16 tr;
	asm("str %0" : "=g"(tr));
	return segment_base(tr);
ffffffff812bf0c7:	89 df                	mov    %ebx,%edi
ffffffff812bf0c9:	e8 e7 f1 ff ff       	callq  ffffffff812be2b5 <segment_base>
	vmcs_writel(GUEST_GS_BASE, tmpl);
        asm("mov %%ss, %0": "=r"(ss_val));
	vmcs_writel(GUEST_SS_BASE, ss_val);
	rdmsrl(MSR_FS_BASE, tmpl);
	vmcs_writel(GUEST_FS_BASE, tmpl);
	vmcs_writel(GUEST_TR_BASE, vmx_read_tr_base());
ffffffff812bf0ce:	bf 14 68 00 00       	mov    $0x6814,%edi
ffffffff812bf0d3:	48 89 c6             	mov    %rax,%rsi
ffffffff812bf0d6:	e8 2b f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>

	/* guest segment access rights */
	vmcs_writel(GUEST_CS_AR_BYTES, 0xA09B);
ffffffff812bf0db:	be 9b a0 00 00       	mov    $0xa09b,%esi
ffffffff812bf0e0:	bf 16 48 00 00       	mov    $0x4816,%edi
ffffffff812bf0e5:	e8 1c f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_DS_AR_BYTES, 0xA093);
ffffffff812bf0ea:	be 93 a0 00 00       	mov    $0xa093,%esi
ffffffff812bf0ef:	bf 1a 48 00 00       	mov    $0x481a,%edi
ffffffff812bf0f4:	e8 0d f3 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_ES_AR_BYTES, 0xA093);
ffffffff812bf0f9:	be 93 a0 00 00       	mov    $0xa093,%esi
ffffffff812bf0fe:	bf 14 48 00 00       	mov    $0x4814,%edi
ffffffff812bf103:	e8 fe f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_FS_AR_BYTES, 0xA093);
ffffffff812bf108:	be 93 a0 00 00       	mov    $0xa093,%esi
ffffffff812bf10d:	bf 1c 48 00 00       	mov    $0x481c,%edi
ffffffff812bf112:	e8 ef f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_GS_AR_BYTES, 0xA093);
ffffffff812bf117:	be 93 a0 00 00       	mov    $0xa093,%esi
ffffffff812bf11c:	bf 1e 48 00 00       	mov    $0x481e,%edi
ffffffff812bf121:	e8 e0 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_SS_AR_BYTES, 0xA093);
ffffffff812bf126:	be 93 a0 00 00       	mov    $0xa093,%esi
ffffffff812bf12b:	bf 18 48 00 00       	mov    $0x4818,%edi
ffffffff812bf130:	e8 d1 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bf135:	be ff ff ff ff       	mov    $0xffffffff,%esi
ffffffff812bf13a:	bf 02 48 00 00       	mov    $0x4802,%edi
ffffffff812bf13f:	e8 c2 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf144:	be ff ff ff ff       	mov    $0xffffffff,%esi
ffffffff812bf149:	bf 06 48 00 00       	mov    $0x4806,%edi
ffffffff812bf14e:	e8 b3 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf153:	be ff ff ff ff       	mov    $0xffffffff,%esi
ffffffff812bf158:	bf 00 48 00 00       	mov    $0x4800,%edi
ffffffff812bf15d:	e8 a4 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf162:	be ff ff ff ff       	mov    $0xffffffff,%esi
ffffffff812bf167:	bf 08 48 00 00       	mov    $0x4808,%edi
ffffffff812bf16c:	e8 95 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf171:	be ff ff ff ff       	mov    $0xffffffff,%esi
ffffffff812bf176:	bf 0a 48 00 00       	mov    $0x480a,%edi
ffffffff812bf17b:	e8 86 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf180:	be ff ff ff ff       	mov    $0xffffffff,%esi
ffffffff812bf185:	bf 04 48 00 00       	mov    $0x4804,%edi
ffffffff812bf18a:	e8 77 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
		vmwrite_error(field, value);
}

static void vmcs_write16(unsigned long field, u16 value)
{
	vmcs_writel(field, value);
ffffffff812bf18f:	be 10 00 00 00       	mov    $0x10,%esi
ffffffff812bf194:	bf 02 08 00 00       	mov    $0x802,%edi
ffffffff812bf199:	e8 68 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf19e:	be 18 00 00 00       	mov    $0x18,%esi
ffffffff812bf1a3:	bf 06 08 00 00       	mov    $0x806,%edi
ffffffff812bf1a8:	e8 59 f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf1ad:	be 18 00 00 00       	mov    $0x18,%esi
ffffffff812bf1b2:	bf 00 08 00 00       	mov    $0x800,%edi
ffffffff812bf1b7:	e8 4a f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf1bc:	be 63 00 00 00       	mov    $0x63,%esi
ffffffff812bf1c1:	bf 08 08 00 00       	mov    $0x808,%edi
ffffffff812bf1c6:	e8 3b f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf1cb:	be 6b 00 00 00       	mov    $0x6b,%esi
ffffffff812bf1d0:	bf 0a 08 00 00       	mov    $0x80a,%edi
ffffffff812bf1d5:	e8 2c f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf1da:	be 18 00 00 00       	mov    $0x18,%esi
ffffffff812bf1df:	bf 04 08 00 00       	mov    $0x804,%edi
ffffffff812bf1e4:	e8 1d f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf1e9:	be 50 00 00 00       	mov    $0x50,%esi
ffffffff812bf1ee:	bf 0c 08 00 00       	mov    $0x80c,%edi
ffffffff812bf1f3:	e8 0e f2 ff ff       	callq  ffffffff812be406 <vmcs_writel>
        vmcs_write16(GUEST_TR_SELECTOR, 0);
*/

	/* guest LDTR */
	vmcs_write16(GUEST_LDTR_SELECTOR, GDT_ENTRY_LDT*8);
	vmcs_writel(GUEST_LDTR_AR_BYTES, 0x0082 | AR_TYPE_LDT);
ffffffff812bf1f8:	be 82 00 00 00       	mov    $0x82,%esi
ffffffff812bf1fd:	bf 20 48 00 00       	mov    $0x4820,%edi
ffffffff812bf202:	e8 ff f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
}

static inline u16 vmx_read_ldt(void)
{
	u16 ldt;
	asm("sldt %0" : "=g"(ldt));
ffffffff812bf207:	66 0f 00 c7          	sldt   %di
*/

	/* guest LDTR */
	vmcs_write16(GUEST_LDTR_SELECTOR, GDT_ENTRY_LDT*8);
	vmcs_writel(GUEST_LDTR_AR_BYTES, 0x0082 | AR_TYPE_LDT);
	vmcs_writel(GUEST_LDTR_BASE, segment_base(vmx_read_ldt()));
ffffffff812bf20b:	0f b7 ff             	movzwl %di,%edi
ffffffff812bf20e:	e8 a2 f0 ff ff       	callq  ffffffff812be2b5 <segment_base>
ffffffff812bf213:	bf 12 68 00 00       	mov    $0x6812,%edi
ffffffff812bf218:	48 89 c6             	mov    %rax,%rsi
ffffffff812bf21b:	e8 e6 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_LDTR_LIMIT, 0xffff);
ffffffff812bf220:	be ff ff 00 00       	mov    $0xffff,%esi
ffffffff812bf225:	bf 0c 48 00 00       	mov    $0x480c,%edi
ffffffff812bf22a:	e8 d7 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
		vmwrite_error(field, value);
}

static void vmcs_write16(unsigned long field, u16 value)
{
	vmcs_writel(field, value);
ffffffff812bf22f:	be 40 00 00 00       	mov    $0x40,%esi
ffffffff812bf234:	bf 0e 08 00 00       	mov    $0x80e,%edi
ffffffff812bf239:	e8 c8 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(GUEST_LDTR_LIMIT, 0xffff);

	/* guest TSS */
	//Newchange - which one?
	vmcs_write16(GUEST_TR_SELECTOR, GDT_ENTRY_TSS*8);
	vmcs_writel(GUEST_TR_AR_BYTES, 0x0080 | AR_TYPE_BUSY_64_TSS);
ffffffff812bf23e:	be 8b 00 00 00       	mov    $0x8b,%esi
ffffffff812bf243:	bf 22 48 00 00       	mov    $0x4822,%edi
ffffffff812bf248:	e8 b9 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>

static inline unsigned long vmx_read_tr_base(void)
{
	u16 tr;
	asm("str %0" : "=g"(tr));
	return segment_base(tr);
ffffffff812bf24d:	89 df                	mov    %ebx,%edi
ffffffff812bf24f:	e8 61 f0 ff ff       	callq  ffffffff812be2b5 <segment_base>

	/* guest TSS */
	//Newchange - which one?
	vmcs_write16(GUEST_TR_SELECTOR, GDT_ENTRY_TSS*8);
	vmcs_writel(GUEST_TR_AR_BYTES, 0x0080 | AR_TYPE_BUSY_64_TSS);
	vmcs_writel(GUEST_TR_BASE, vmx_read_tr_base());
ffffffff812bf254:	bf 14 68 00 00       	mov    $0x6814,%edi
ffffffff812bf259:	48 89 c6             	mov    %rax,%rsi
ffffffff812bf25c:	e8 a5 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
//	vmcs_writel(GUEST_TR_BASE, 0x38);
	vmcs_writel(GUEST_TR_LIMIT, 0xff);
ffffffff812bf261:	be ff 00 00 00       	mov    $0xff,%esi
ffffffff812bf266:	bf 0e 48 00 00       	mov    $0x480e,%edi
ffffffff812bf26b:	e8 96 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf270:	b9 74 01 00 00       	mov    $0x174,%ecx
ffffffff812bf275:	0f 32                	rdmsr  
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bf277:	bf 2a 48 00 00       	mov    $0x482a,%edi
ffffffff812bf27c:	89 c6                	mov    %eax,%esi
ffffffff812bf27e:	e8 83 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf283:	b9 75 01 00 00       	mov    $0x175,%ecx
ffffffff812bf288:	0f 32                	rdmsr  
	/*Change: This is changed from dune.*/
	/* initialize sysenter */
	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
	vmcs_write32(GUEST_SYSENTER_CS, low32);
        rdmsrl(MSR_IA32_SYSENTER_ESP, tmpl);
	vmcs_writel(GUEST_SYSENTER_ESP, tmpl);
ffffffff812bf28a:	48 89 d1             	mov    %rdx,%rcx
ffffffff812bf28d:	89 c0                	mov    %eax,%eax
ffffffff812bf28f:	bf 24 68 00 00       	mov    $0x6824,%edi
ffffffff812bf294:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812bf298:	48 09 c1             	or     %rax,%rcx
ffffffff812bf29b:	48 89 ce             	mov    %rcx,%rsi
ffffffff812bf29e:	e8 63 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf2a3:	b9 76 01 00 00       	mov    $0x176,%ecx
ffffffff812bf2a8:	0f 32                	rdmsr  
	rdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);
	vmcs_writel(GUEST_SYSENTER_EIP, tmpl);
ffffffff812bf2aa:	48 89 d1             	mov    %rdx,%rcx
ffffffff812bf2ad:	89 c0                	mov    %eax,%eax
ffffffff812bf2af:	bf 26 68 00 00       	mov    $0x6826,%edi
ffffffff812bf2b4:	48 c1 e1 20          	shl    $0x20,%rcx
ffffffff812bf2b8:	48 09 c1             	or     %rax,%rcx
ffffffff812bf2bb:	48 89 ce             	mov    %rcx,%rsi
ffffffff812bf2be:	e8 43 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bf2c3:	31 f6                	xor    %esi,%esi
ffffffff812bf2c5:	bf 26 48 00 00       	mov    $0x4826,%edi
ffffffff812bf2ca:	e8 37 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf2cf:	31 f6                	xor    %esi,%esi
ffffffff812bf2d1:	bf 24 48 00 00       	mov    $0x4824,%edi
ffffffff812bf2d6:	e8 2b f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bf2db:	31 f6                	xor    %esi,%esi
ffffffff812bf2dd:	bf 22 68 00 00       	mov    $0x6822,%edi
ffffffff812bf2e2:	e8 1f f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
}

static void vmcs_write64(unsigned long field, u64 value)
{
	vmcs_writel(field, value);
ffffffff812bf2e7:	31 f6                	xor    %esi,%esi
ffffffff812bf2e9:	bf 02 28 00 00       	mov    $0x2802,%edi
ffffffff812bf2ee:	e8 13 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bf2f3:	31 f6                	xor    %esi,%esi
ffffffff812bf2f5:	bf 16 40 00 00       	mov    $0x4016,%edi
ffffffff812bf2fa:	e8 07 f1 ff ff       	callq  ffffffff812be406 <vmcs_writel>

	vmx_get_cpu(vcpu);
	vmx_setup_vmcs(vcpu);
	vmx_setup_initial_guest_state(conf);
	
	if (cpu_has_vmx_ept_ad_bits()) {
ffffffff812bf2ff:	f6 05 1c 04 8f 00 20 	testb  $0x20,0x8f041c(%rip)        # ffffffff81baf722 <vmx_capability+0x2>
ffffffff812bf306:	74 16                	je     ffffffff812bf31e <vmx_launch+0x892>
		vcpu->ept_ad_enabled = true;
ffffffff812bf308:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
		printk(KERN_INFO "vmx: enabled EPT A/D bits");
ffffffff812bf30c:	48 c7 c7 5a 58 7b 81 	mov    $0xffffffff817b585a,%rdi
	vmx_get_cpu(vcpu);
	vmx_setup_vmcs(vcpu);
	vmx_setup_initial_guest_state(conf);
	
	if (cpu_has_vmx_ept_ad_bits()) {
		vcpu->ept_ad_enabled = true;
ffffffff812bf313:	c6 40 50 01          	movb   $0x1,0x50(%rax)
		printk(KERN_INFO "vmx: enabled EPT A/D bits");
ffffffff812bf317:	31 c0                	xor    %eax,%eax
ffffffff812bf319:	e8 74 3c 17 00       	callq  ffffffff81432f92 <printk>
	}
	if (vmx_create_ept(vcpu))
ffffffff812bf31e:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812bf322:	e8 a2 17 00 00       	callq  ffffffff812c0ac9 <vmx_create_ept>
ffffffff812bf327:	85 c0                	test   %eax,%eax
ffffffff812bf329:	89 45 b0             	mov    %eax,-0x50(%rbp)
ffffffff812bf32c:	0f 85 5d 05 00 00    	jne    ffffffff812bf88f <vmx_launch+0xe03>
	printk(KERN_INFO "vmx_launch: Entering \n");
        vcpu = vmx_create_vcpu(conf);
	if (!vcpu)
                return -ENOMEM;

        printk(KERN_ERR "vmx: created VCPU (VPID %d)\n",
ffffffff812bf332:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812bf336:	48 c7 c7 76 58 7b 81 	mov    $0xffffffff817b5876,%rdi
ffffffff812bf33d:	8b 70 04             	mov    0x4(%rax),%esi
ffffffff812bf340:	31 c0                	xor    %eax,%eax
ffffffff812bf342:	e8 4b 3c 17 00       	callq  ffffffff81432f92 <printk>
               vcpu->vpid);

start:
	vmx_get_cpu(vcpu);
ffffffff812bf347:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812bf34b:	e8 6f f3 ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>
 * vmx_run_vcpu - launches the CPU into non-root mode
 * @vcpu: the vmx instance to launch
 */
static int __noclone vmx_run_vcpu(struct vmx_vcpu *vcpu)
{
	printk(KERN_INFO "Launching or resuming VM.\n");
ffffffff812bf350:	48 c7 c7 95 58 7b 81 	mov    $0xffffffff817b5895,%rdi
ffffffff812bf357:	31 c0                	xor    %eax,%eax
ffffffff812bf359:	e8 34 3c 17 00       	callq  ffffffff81432f92 <printk>
		printk(KERN_INFO "vmx: --- Begin VMCS DUMP ---\n");
ffffffff812bf35e:	48 c7 c7 b2 58 7b 81 	mov    $0xffffffff817b58b2,%rdi
ffffffff812bf365:	31 c0                	xor    %eax,%eax
ffffffff812bf367:	e8 26 3c 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf36c:	ba 02 08 00 00       	mov    $0x802,%edx
ffffffff812bf371:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf374:	ba 00 08 00 00       	mov    $0x800,%edx
ffffffff812bf379:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf37c:	0f 78 d0             	vmread %rdx,%rax
 */
static int __noclone vmx_run_vcpu(struct vmx_vcpu *vcpu)
{
	printk(KERN_INFO "Launching or resuming VM.\n");
		printk(KERN_INFO "vmx: --- Begin VMCS DUMP ---\n");
                printk(KERN_INFO "GUEST_ES_SELECTOR: 0x%04x, GUEST_CS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_ES_SELECTOR), vmcs_read16(GUEST_CS_SELECTOR));
ffffffff812bf37f:	48 c7 c7 d2 58 7b 81 	mov    $0xffffffff817b58d2,%rdi
ffffffff812bf386:	0f b7 d1             	movzwl %cx,%edx
ffffffff812bf389:	0f b7 f0             	movzwl %ax,%esi
ffffffff812bf38c:	31 c0                	xor    %eax,%eax
ffffffff812bf38e:	e8 ff 3b 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf393:	ba 06 08 00 00       	mov    $0x806,%edx
ffffffff812bf398:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf39b:	ba 04 08 00 00       	mov    $0x804,%edx
ffffffff812bf3a0:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf3a3:	0f 78 d0             	vmread %rdx,%rax
static int __noclone vmx_run_vcpu(struct vmx_vcpu *vcpu)
{
	printk(KERN_INFO "Launching or resuming VM.\n");
		printk(KERN_INFO "vmx: --- Begin VMCS DUMP ---\n");
                printk(KERN_INFO "GUEST_ES_SELECTOR: 0x%04x, GUEST_CS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_ES_SELECTOR), vmcs_read16(GUEST_CS_SELECTOR));
                printk(KERN_INFO "GUEST_SS_SELECTOR: 0x%04x, GUEST_DS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_SS_SELECTOR), vmcs_read16(GUEST_DS_SELECTOR));
ffffffff812bf3a6:	48 c7 c7 0a 59 7b 81 	mov    $0xffffffff817b590a,%rdi
ffffffff812bf3ad:	0f b7 d1             	movzwl %cx,%edx
ffffffff812bf3b0:	0f b7 f0             	movzwl %ax,%esi
ffffffff812bf3b3:	31 c0                	xor    %eax,%eax
ffffffff812bf3b5:	e8 d8 3b 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf3ba:	ba 0a 08 00 00       	mov    $0x80a,%edx
ffffffff812bf3bf:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf3c2:	ba 08 08 00 00       	mov    $0x808,%edx
ffffffff812bf3c7:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf3ca:	0f 78 d0             	vmread %rdx,%rax
{
	printk(KERN_INFO "Launching or resuming VM.\n");
		printk(KERN_INFO "vmx: --- Begin VMCS DUMP ---\n");
                printk(KERN_INFO "GUEST_ES_SELECTOR: 0x%04x, GUEST_CS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_ES_SELECTOR), vmcs_read16(GUEST_CS_SELECTOR));
                printk(KERN_INFO "GUEST_SS_SELECTOR: 0x%04x, GUEST_DS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_SS_SELECTOR), vmcs_read16(GUEST_DS_SELECTOR));
		printk(KERN_INFO "GUEST_FS_SELECTOR: 0x%04x, GUEST_GS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_FS_SELECTOR), vmcs_read16(GUEST_GS_SELECTOR));
ffffffff812bf3cd:	48 c7 c7 42 59 7b 81 	mov    $0xffffffff817b5942,%rdi
ffffffff812bf3d4:	0f b7 d1             	movzwl %cx,%edx
ffffffff812bf3d7:	0f b7 f0             	movzwl %ax,%esi
ffffffff812bf3da:	31 c0                	xor    %eax,%eax
ffffffff812bf3dc:	e8 b1 3b 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf3e1:	ba 0e 08 00 00       	mov    $0x80e,%edx
ffffffff812bf3e6:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf3e9:	ba 0c 08 00 00       	mov    $0x80c,%edx
ffffffff812bf3ee:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf3f1:	0f 78 d0             	vmread %rdx,%rax
	printk(KERN_INFO "Launching or resuming VM.\n");
		printk(KERN_INFO "vmx: --- Begin VMCS DUMP ---\n");
                printk(KERN_INFO "GUEST_ES_SELECTOR: 0x%04x, GUEST_CS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_ES_SELECTOR), vmcs_read16(GUEST_CS_SELECTOR));
                printk(KERN_INFO "GUEST_SS_SELECTOR: 0x%04x, GUEST_DS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_SS_SELECTOR), vmcs_read16(GUEST_DS_SELECTOR));
		printk(KERN_INFO "GUEST_FS_SELECTOR: 0x%04x, GUEST_GS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_FS_SELECTOR), vmcs_read16(GUEST_GS_SELECTOR));
                printk(KERN_INFO "GUEST_LDTR_SELECTOR: 0x%04x, GUEST_TR_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
ffffffff812bf3f4:	48 c7 c7 7a 59 7b 81 	mov    $0xffffffff817b597a,%rdi
ffffffff812bf3fb:	0f b7 d1             	movzwl %cx,%edx
ffffffff812bf3fe:	0f b7 f0             	movzwl %ax,%esi
ffffffff812bf401:	31 c0                	xor    %eax,%eax
ffffffff812bf403:	e8 8a 3b 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf408:	ba 02 48 00 00       	mov    $0x4802,%edx
ffffffff812bf40d:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf410:	ba 00 48 00 00       	mov    $0x4800,%edx
ffffffff812bf415:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf418:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_ES_SELECTOR: 0x%04x, GUEST_CS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_ES_SELECTOR), vmcs_read16(GUEST_CS_SELECTOR));
                printk(KERN_INFO "GUEST_SS_SELECTOR: 0x%04x, GUEST_DS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_SS_SELECTOR), vmcs_read16(GUEST_DS_SELECTOR));
		printk(KERN_INFO "GUEST_FS_SELECTOR: 0x%04x, GUEST_GS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_FS_SELECTOR), vmcs_read16(GUEST_GS_SELECTOR));
                printk(KERN_INFO "GUEST_LDTR_SELECTOR: 0x%04x, GUEST_TR_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
//                printk(KERN_INFO "GUEST_IA32_DEBUGCTL: 0x%016lx, GUEST_IA32_PAT: 0x%04x\n",vmcs_readl(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
ffffffff812bf41b:	48 c7 c7 b4 59 7b 81 	mov    $0xffffffff817b59b4,%rdi
ffffffff812bf422:	89 ca                	mov    %ecx,%edx
ffffffff812bf424:	89 c6                	mov    %eax,%esi
ffffffff812bf426:	31 c0                	xor    %eax,%eax
ffffffff812bf428:	e8 65 3b 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf42d:	ba 06 48 00 00       	mov    $0x4806,%edx
ffffffff812bf432:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf435:	ba 04 48 00 00       	mov    $0x4804,%edx
ffffffff812bf43a:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf43d:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_SS_SELECTOR: 0x%04x, GUEST_DS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_SS_SELECTOR), vmcs_read16(GUEST_DS_SELECTOR));
		printk(KERN_INFO "GUEST_FS_SELECTOR: 0x%04x, GUEST_GS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_FS_SELECTOR), vmcs_read16(GUEST_GS_SELECTOR));
                printk(KERN_INFO "GUEST_LDTR_SELECTOR: 0x%04x, GUEST_TR_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
//                printk(KERN_INFO "GUEST_IA32_DEBUGCTL: 0x%016lx, GUEST_IA32_PAT: 0x%04x\n",vmcs_readl(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
ffffffff812bf440:	48 c7 c7 e6 59 7b 81 	mov    $0xffffffff817b59e6,%rdi
ffffffff812bf447:	89 ca                	mov    %ecx,%edx
ffffffff812bf449:	89 c6                	mov    %eax,%esi
ffffffff812bf44b:	31 c0                	xor    %eax,%eax
ffffffff812bf44d:	e8 40 3b 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf452:	ba 0a 48 00 00       	mov    $0x480a,%edx
ffffffff812bf457:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf45a:	ba 08 48 00 00       	mov    $0x4808,%edx
ffffffff812bf45f:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf462:	0f 78 d0             	vmread %rdx,%rax
		printk(KERN_INFO "GUEST_FS_SELECTOR: 0x%04x, GUEST_GS_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_FS_SELECTOR), vmcs_read16(GUEST_GS_SELECTOR));
                printk(KERN_INFO "GUEST_LDTR_SELECTOR: 0x%04x, GUEST_TR_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
//                printk(KERN_INFO "GUEST_IA32_DEBUGCTL: 0x%016lx, GUEST_IA32_PAT: 0x%04x\n",vmcs_readl(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
ffffffff812bf465:	48 c7 c7 18 5a 7b 81 	mov    $0xffffffff817b5a18,%rdi
ffffffff812bf46c:	89 ca                	mov    %ecx,%edx
ffffffff812bf46e:	89 c6                	mov    %eax,%esi
ffffffff812bf470:	31 c0                	xor    %eax,%eax
ffffffff812bf472:	e8 1b 3b 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf477:	ba 0e 48 00 00       	mov    $0x480e,%edx
ffffffff812bf47c:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf47f:	ba 0c 48 00 00       	mov    $0x480c,%edx
ffffffff812bf484:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf487:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_LDTR_SELECTOR: 0x%04x, GUEST_TR_SELECTOR: 0x%04x\n",vmcs_read16(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
//                printk(KERN_INFO "GUEST_IA32_DEBUGCTL: 0x%016lx, GUEST_IA32_PAT: 0x%04x\n",vmcs_readl(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
ffffffff812bf48a:	48 c7 c7 4a 5a 7b 81 	mov    $0xffffffff817b5a4a,%rdi
ffffffff812bf491:	89 ca                	mov    %ecx,%edx
ffffffff812bf493:	89 c6                	mov    %eax,%esi
ffffffff812bf495:	31 c0                	xor    %eax,%eax
ffffffff812bf497:	e8 f6 3a 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf49c:	ba 16 48 00 00       	mov    $0x4816,%edx
ffffffff812bf4a1:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf4a4:	ba 14 48 00 00       	mov    $0x4814,%edx
ffffffff812bf4a9:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf4ac:	0f 78 d0             	vmread %rdx,%rax
//                printk(KERN_INFO "GUEST_IA32_DEBUGCTL: 0x%016lx, GUEST_IA32_PAT: 0x%04x\n",vmcs_readl(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
ffffffff812bf4af:	48 c7 c7 7e 5a 7b 81 	mov    $0xffffffff817b5a7e,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf4b6:	48 89 c6             	mov    %rax,%rsi
//                printk(KERN_INFO "GUEST_IA32_DEBUGCTL: 0x%016lx, GUEST_IA32_PAT: 0x%04x\n",vmcs_readl(GUEST_LDTR_SELECTOR), vmcs_read16(GUEST_TR_SELECTOR));
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
ffffffff812bf4b9:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf4bc:	31 c0                	xor    %eax,%eax
ffffffff812bf4be:	e8 cf 3a 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf4c3:	ba 1a 48 00 00       	mov    $0x481a,%edx
ffffffff812bf4c8:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf4cb:	ba 18 48 00 00       	mov    $0x4818,%edx
ffffffff812bf4d0:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf4d3:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
ffffffff812bf4d6:	48 c7 c7 b8 5a 7b 81 	mov    $0xffffffff817b5ab8,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf4dd:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_ES_LIMIT: 0x%08x, GUEST_CS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_ES_LIMIT), vmcs_read32(GUEST_CS_LIMIT));
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
ffffffff812bf4e0:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf4e3:	31 c0                	xor    %eax,%eax
ffffffff812bf4e5:	e8 a8 3a 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf4ea:	ba 1e 48 00 00       	mov    $0x481e,%edx
ffffffff812bf4ef:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf4f2:	ba 1c 48 00 00       	mov    $0x481c,%edx
ffffffff812bf4f7:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf4fa:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
ffffffff812bf4fd:	48 c7 c7 f2 5a 7b 81 	mov    $0xffffffff817b5af2,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf504:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_SS_LIMIT: 0x%08x, GUEST_DS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_SS_LIMIT), vmcs_read32(GUEST_DS_LIMIT));
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
ffffffff812bf507:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf50a:	31 c0                	xor    %eax,%eax
ffffffff812bf50c:	e8 81 3a 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf511:	ba 22 48 00 00       	mov    $0x4822,%edx
ffffffff812bf516:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf519:	ba 20 48 00 00       	mov    $0x4820,%edx
ffffffff812bf51e:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf521:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
ffffffff812bf524:	48 c7 c7 2c 5b 7b 81 	mov    $0xffffffff817b5b2c,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf52b:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_FS_LIMIT: 0x%08x, GUEST_GS_LIMIT: 0x%08x\n",vmcs_read32(GUEST_FS_LIMIT), vmcs_read32(GUEST_GS_LIMIT));
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
ffffffff812bf52e:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf531:	31 c0                	xor    %eax,%eax
ffffffff812bf533:	e8 5a 3a 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf538:	ba 26 48 00 00       	mov    $0x4826,%edx
ffffffff812bf53d:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf540:	ba 24 48 00 00       	mov    $0x4824,%edx
ffffffff812bf545:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf548:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
ffffffff812bf54b:	48 c7 c7 68 5b 7b 81 	mov    $0xffffffff817b5b68,%rdi
ffffffff812bf552:	89 ca                	mov    %ecx,%edx
ffffffff812bf554:	89 c6                	mov    %eax,%esi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf556:	bb 22 68 00 00       	mov    $0x6822,%ebx
                printk(KERN_INFO "GUEST_LDTR_LIMIT: 0x%08x, GUEST_TR_LIMIT: 0x%08x\n",vmcs_read32(GUEST_LDTR_LIMIT), vmcs_read32(GUEST_TR_LIMIT));
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
ffffffff812bf55b:	31 c0                	xor    %eax,%eax
ffffffff812bf55d:	e8 30 3a 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf562:	48 89 da             	mov    %rbx,%rdx
ffffffff812bf565:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf568:	ba 2a 48 00 00       	mov    $0x482a,%edx
ffffffff812bf56d:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf570:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_ES_AR_BYTES: 0x%16lx, GUEST_CS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_ES_AR_BYTES), vmcs_readl(GUEST_CS_AR_BYTES));
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
ffffffff812bf573:	48 c7 c7 ad 5b 7b 81 	mov    $0xffffffff817b5bad,%rdi
ffffffff812bf57a:	89 ca                	mov    %ecx,%edx
ffffffff812bf57c:	89 c6                	mov    %eax,%esi
ffffffff812bf57e:	31 c0                	xor    %eax,%eax
ffffffff812bf580:	e8 0d 3a 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf585:	ba 08 68 00 00       	mov    $0x6808,%edx
ffffffff812bf58a:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf58d:	ba 06 68 00 00       	mov    $0x6806,%edx
ffffffff812bf592:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf595:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
ffffffff812bf598:	48 c7 c7 f0 5b 7b 81 	mov    $0xffffffff817b5bf0,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf59f:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_SS_AR_BYTES: 0x%16lx, GUEST_DS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_SS_AR_BYTES), vmcs_readl(GUEST_DS_AR_BYTES));
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
ffffffff812bf5a2:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf5a5:	31 c0                	xor    %eax,%eax
ffffffff812bf5a7:	e8 e6 39 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf5ac:	ba 0c 68 00 00       	mov    $0x680c,%edx
ffffffff812bf5b1:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf5b4:	ba 0a 68 00 00       	mov    $0x680a,%edx
ffffffff812bf5b9:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf5bc:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
ffffffff812bf5bf:	48 c7 c7 22 5c 7b 81 	mov    $0xffffffff817b5c22,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf5c6:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_FS_AR_BYTES: 0x%16lx, GUEST_GS_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_FS_AR_BYTES), vmcs_readl(GUEST_GS_AR_BYTES));
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
ffffffff812bf5c9:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf5cc:	31 c0                	xor    %eax,%eax
ffffffff812bf5ce:	e8 bf 39 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf5d3:	ba 10 68 00 00       	mov    $0x6810,%edx
ffffffff812bf5d8:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf5db:	ba 0e 68 00 00       	mov    $0x680e,%edx
ffffffff812bf5e0:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf5e3:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
ffffffff812bf5e6:	48 c7 c7 54 5c 7b 81 	mov    $0xffffffff817b5c54,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf5ed:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_LDTR_AR_BYTES: 0x%16lx, GUEST_TR_AR_BYTES: 0x%16lx\n",vmcs_readl(GUEST_LDTR_AR_BYTES), vmcs_readl(GUEST_TR_AR_BYTES));
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
ffffffff812bf5f0:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf5f3:	31 c0                	xor    %eax,%eax
ffffffff812bf5f5:	e8 98 39 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf5fa:	ba 14 68 00 00       	mov    $0x6814,%edx
ffffffff812bf5ff:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf602:	ba 12 68 00 00       	mov    $0x6812,%edx
ffffffff812bf607:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf60a:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
ffffffff812bf60d:	48 c7 c7 86 5c 7b 81 	mov    $0xffffffff817b5c86,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf614:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_INTERRUPTIBILITY_INFO: 0x%08x, GUEST_ACTIVITY_STATE: 0x%08x\n",vmcs_read32(GUEST_INTERRUPTIBILITY_INFO), vmcs_read32(GUEST_ACTIVITY_STATE));
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
ffffffff812bf617:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf61a:	31 c0                	xor    %eax,%eax
ffffffff812bf61c:	e8 71 39 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf621:	ba 18 68 00 00       	mov    $0x6818,%edx
ffffffff812bf626:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf629:	ba 16 68 00 00       	mov    $0x6816,%edx
ffffffff812bf62e:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf631:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
ffffffff812bf634:	48 c7 c7 ba 5c 7b 81 	mov    $0xffffffff817b5cba,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf63b:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_SYSENTER_CS: 0x%08x, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_read32(GUEST_SYSENTER_CS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
ffffffff812bf63e:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf641:	31 c0                	xor    %eax,%eax
ffffffff812bf643:	e8 4a 39 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf648:	ba 02 68 00 00       	mov    $0x6802,%edx
ffffffff812bf64d:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf650:	ba 00 68 00 00       	mov    $0x6800,%edx
ffffffff812bf655:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf658:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
ffffffff812bf65b:	48 c7 c7 f0 5c 7b 81 	mov    $0xffffffff817b5cf0,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf662:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_ES_BASE: 0x%16lx, GUEST_CS_BASE: 0x%16lx\n",vmcs_readl(GUEST_ES_BASE), vmcs_readl(GUEST_CS_BASE));
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
ffffffff812bf665:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf668:	31 c0                	xor    %eax,%eax
ffffffff812bf66a:	e8 23 39 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf66f:	ba 1a 68 00 00       	mov    $0x681a,%edx
ffffffff812bf674:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf677:	ba 04 68 00 00       	mov    $0x6804,%edx
ffffffff812bf67c:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf67f:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
ffffffff812bf682:	48 c7 c7 1a 5d 7b 81 	mov    $0xffffffff817b5d1a,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf689:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_SS_BASE: 0x%16lx, GUEST_DS_BASE: 0x%16lx\n",vmcs_readl(GUEST_SS_BASE), vmcs_readl(GUEST_DS_BASE));
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
ffffffff812bf68c:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf68f:	31 c0                	xor    %eax,%eax
ffffffff812bf691:	e8 fc 38 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf696:	ba 1e 68 00 00       	mov    $0x681e,%edx
ffffffff812bf69b:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf69e:	ba 1c 68 00 00       	mov    $0x681c,%edx
ffffffff812bf6a3:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf6a6:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
                printk(KERN_INFO "GUEST_RSP: 0x%16lx, GUEST_RIP: 0x%16lx\n",vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
ffffffff812bf6a9:	48 c7 c7 44 5d 7b 81 	mov    $0xffffffff817b5d44,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf6b0:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_FS_BASE: 0x%16lx, GUEST_GS_BASE: 0x%16lx\n",vmcs_readl(GUEST_FS_BASE), vmcs_readl(GUEST_GS_BASE));
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
                printk(KERN_INFO "GUEST_RSP: 0x%16lx, GUEST_RIP: 0x%16lx\n",vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
ffffffff812bf6b3:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf6b6:	31 c0                	xor    %eax,%eax
ffffffff812bf6b8:	e8 d5 38 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf6bd:	48 89 da             	mov    %rbx,%rdx
ffffffff812bf6c0:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf6c3:	ba 20 68 00 00       	mov    $0x6820,%edx
ffffffff812bf6c8:	48 89 c3             	mov    %rax,%rbx
ffffffff812bf6cb:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
                printk(KERN_INFO "GUEST_RSP: 0x%16lx, GUEST_RIP: 0x%16lx\n",vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
                printk(KERN_INFO "GUEST_RFLAGS: 0x%16lx, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_readl(GUEST_RFLAGS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
ffffffff812bf6ce:	48 c7 c7 6e 5d 7b 81 	mov    $0xffffffff817b5d6e,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf6d5:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_LDTR_BASE: 0x%16lx, GUEST_TR_BASE: 0x%16lx\n",vmcs_readl(GUEST_LDTR_BASE), vmcs_readl(GUEST_TR_BASE));
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
                printk(KERN_INFO "GUEST_RSP: 0x%16lx, GUEST_RIP: 0x%16lx\n",vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
                printk(KERN_INFO "GUEST_RFLAGS: 0x%16lx, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_readl(GUEST_RFLAGS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
ffffffff812bf6d8:	89 da                	mov    %ebx,%edx
ffffffff812bf6da:	31 c0                	xor    %eax,%eax
ffffffff812bf6dc:	e8 b1 38 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf6e1:	ba 26 68 00 00       	mov    $0x6826,%edx
ffffffff812bf6e6:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf6e9:	ba 24 68 00 00       	mov    $0x6824,%edx
ffffffff812bf6ee:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf6f1:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
                printk(KERN_INFO "GUEST_RSP: 0x%16lx, GUEST_RIP: 0x%16lx\n",vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
                printk(KERN_INFO "GUEST_RFLAGS: 0x%16lx, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_readl(GUEST_RFLAGS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_SYSENTER_ESP: 0x%16lx, GUEST_SYSENTER_EIP: 0x%16lx\n",vmcs_readl(GUEST_SYSENTER_ESP), vmcs_readl(GUEST_SYSENTER_EIP));
ffffffff812bf6f4:	48 c7 c7 ad 5d 7b 81 	mov    $0xffffffff817b5dad,%rdi

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf6fb:	48 89 c6             	mov    %rax,%rsi
                printk(KERN_INFO "GUEST_GDTR_BASE: 0x%16lx, GUEST_IDTR_BASE: 0x%16lx\n",vmcs_readl(GUEST_GDTR_BASE), vmcs_readl(GUEST_IDTR_BASE));
                printk(KERN_INFO "GUEST_CR0: 0x%16lx, GUEST_CR3: 0x%16lx\n",vmcs_readl(GUEST_CR0), vmcs_readl(GUEST_CR3));
                printk(KERN_INFO "GUEST_CR4: 0x%16lx, GUEST_DR7: 0x%16lx\n",vmcs_readl(GUEST_CR4), vmcs_readl(GUEST_DR7));
                printk(KERN_INFO "GUEST_RSP: 0x%16lx, GUEST_RIP: 0x%16lx\n",vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));
                printk(KERN_INFO "GUEST_RFLAGS: 0x%16lx, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_readl(GUEST_RFLAGS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_SYSENTER_ESP: 0x%16lx, GUEST_SYSENTER_EIP: 0x%16lx\n",vmcs_readl(GUEST_SYSENTER_ESP), vmcs_readl(GUEST_SYSENTER_EIP));
ffffffff812bf6fe:	48 89 ca             	mov    %rcx,%rdx
ffffffff812bf701:	31 c0                	xor    %eax,%eax
ffffffff812bf703:	e8 8a 38 17 00       	callq  ffffffff81432f92 <printk>
                printk(KERN_INFO "vmx: --- End VMCS DUMP ---\n");
ffffffff812bf708:	48 c7 c7 e9 5d 7b 81 	mov    $0xffffffff817b5de9,%rdi
ffffffff812bf70f:	31 c0                	xor    %eax,%eax
ffffffff812bf711:	e8 7c 38 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf716:	ba 14 6c 00 00       	mov    $0x6c14,%edx
ffffffff812bf71b:	0f 78 d0             	vmread %rdx,%rax
                printk(KERN_INFO "GUEST_RFLAGS: 0x%16lx, GUEST_PENDING_DBG_EXCEPTIONS: 0x%08x\n",vmcs_readl(GUEST_RFLAGS), vmcs_read32(GUEST_PENDING_DBG_EXCEPTIONS));
                printk(KERN_INFO "GUEST_SYSENTER_ESP: 0x%16lx, GUEST_SYSENTER_EIP: 0x%16lx\n",vmcs_readl(GUEST_SYSENTER_ESP), vmcs_readl(GUEST_SYSENTER_EIP));
                printk(KERN_INFO "vmx: --- End VMCS DUMP ---\n");

	
	*(unsigned long *)(vmcs_readl(HOST_RSP)) = (unsigned long)vcpu;			
ffffffff812bf71e:	48 8b 5d b8          	mov    -0x48(%rbp),%rbx
ffffffff812bf722:	48 89 18             	mov    %rbx,(%rax)
	
	asm(
ffffffff812bf725:	48 89 d9             	mov    %rbx,%rcx
ffffffff812bf728:	48 8b 81 f0 00 00 00 	mov    0xf0(%rcx),%rax
ffffffff812bf72f:	0f 20 d2             	mov    %cr2,%rdx
ffffffff812bf732:	48 39 c2             	cmp    %rax,%rdx
ffffffff812bf735:	74 03                	je     ffffffff812bf73a <vmx_launch+0xcae>
ffffffff812bf737:	0f 22 d0             	mov    %rax,%cr2
ffffffff812bf73a:	48 8b 41 68          	mov    0x68(%rcx),%rax
ffffffff812bf73e:	48 8b 99 80 00 00 00 	mov    0x80(%rcx),%rbx
ffffffff812bf745:	48 8b 51 78          	mov    0x78(%rcx),%rdx
ffffffff812bf749:	48 8b b1 98 00 00 00 	mov    0x98(%rcx),%rsi
ffffffff812bf750:	48 8b b9 a0 00 00 00 	mov    0xa0(%rcx),%rdi
ffffffff812bf757:	48 8b a9 90 00 00 00 	mov    0x90(%rcx),%rbp
ffffffff812bf75e:	4c 8b 81 a8 00 00 00 	mov    0xa8(%rcx),%r8
ffffffff812bf765:	4c 8b 89 b0 00 00 00 	mov    0xb0(%rcx),%r9
ffffffff812bf76c:	4c 8b 91 b8 00 00 00 	mov    0xb8(%rcx),%r10
ffffffff812bf773:	4c 8b 99 c0 00 00 00 	mov    0xc0(%rcx),%r11
ffffffff812bf77a:	4c 8b a1 c8 00 00 00 	mov    0xc8(%rcx),%r12
ffffffff812bf781:	4c 8b a9 d0 00 00 00 	mov    0xd0(%rcx),%r13
ffffffff812bf788:	4c 8b b1 d8 00 00 00 	mov    0xd8(%rcx),%r14
ffffffff812bf78f:	4c 8b b9 e0 00 00 00 	mov    0xe0(%rcx),%r15
ffffffff812bf796:	48 8b 49 70          	mov    0x70(%rcx),%rcx
ffffffff812bf79a:	0f 01 c2             	vmlaunch 
ffffffff812bf79d:	51                   	push   %rcx
ffffffff812bf79e:	48 8b 4c 24 08       	mov    0x8(%rsp),%rcx
ffffffff812bf7a3:	48 89 41 68          	mov    %rax,0x68(%rcx)
ffffffff812bf7a7:	48 89 99 80 00 00 00 	mov    %rbx,0x80(%rcx)
ffffffff812bf7ae:	8f 41 70             	popq   0x70(%rcx)
ffffffff812bf7b1:	48 89 51 78          	mov    %rdx,0x78(%rcx)
ffffffff812bf7b5:	48 89 b1 98 00 00 00 	mov    %rsi,0x98(%rcx)
ffffffff812bf7bc:	48 89 b9 a0 00 00 00 	mov    %rdi,0xa0(%rcx)
ffffffff812bf7c3:	48 89 a9 90 00 00 00 	mov    %rbp,0x90(%rcx)
ffffffff812bf7ca:	4c 89 81 a8 00 00 00 	mov    %r8,0xa8(%rcx)
ffffffff812bf7d1:	4c 89 89 b0 00 00 00 	mov    %r9,0xb0(%rcx)
ffffffff812bf7d8:	4c 89 91 b8 00 00 00 	mov    %r10,0xb8(%rcx)
ffffffff812bf7df:	4c 89 99 c0 00 00 00 	mov    %r11,0xc0(%rcx)
ffffffff812bf7e6:	4c 89 a1 c8 00 00 00 	mov    %r12,0xc8(%rcx)
ffffffff812bf7ed:	4c 89 a9 d0 00 00 00 	mov    %r13,0xd0(%rcx)
ffffffff812bf7f4:	4c 89 b1 d8 00 00 00 	mov    %r14,0xd8(%rcx)
ffffffff812bf7fb:	4c 89 b9 e0 00 00 00 	mov    %r15,0xe0(%rcx)
ffffffff812bf802:	0f 20 d0             	mov    %cr2,%rax
ffffffff812bf805:	48 89 81 f0 00 00 00 	mov    %rax,0xf0(%rcx)
ffffffff812bf80c:	48 89 cf             	mov    %rcx,%rdi
ffffffff812bf80f:	e8 ce 00 00 00       	callq  ffffffff812bf8e2 <vmx_handle_exits>
ffffffff812bf814:	48 8b 0c 24          	mov    (%rsp),%rcx
ffffffff812bf818:	48 8b 81 f0 00 00 00 	mov    0xf0(%rcx),%rax
ffffffff812bf81f:	0f 20 d2             	mov    %cr2,%rdx
ffffffff812bf822:	48 39 c2             	cmp    %rax,%rdx
ffffffff812bf825:	74 03                	je     ffffffff812bf82a <vmx_launch+0xd9e>
ffffffff812bf827:	0f 22 d0             	mov    %rax,%cr2
ffffffff812bf82a:	48 8b 41 68          	mov    0x68(%rcx),%rax
ffffffff812bf82e:	48 8b 99 80 00 00 00 	mov    0x80(%rcx),%rbx
ffffffff812bf835:	48 8b 51 78          	mov    0x78(%rcx),%rdx
ffffffff812bf839:	48 8b b1 98 00 00 00 	mov    0x98(%rcx),%rsi
ffffffff812bf840:	48 8b b9 a0 00 00 00 	mov    0xa0(%rcx),%rdi
ffffffff812bf847:	48 8b a9 90 00 00 00 	mov    0x90(%rcx),%rbp
ffffffff812bf84e:	4c 8b 81 a8 00 00 00 	mov    0xa8(%rcx),%r8
ffffffff812bf855:	4c 8b 89 b0 00 00 00 	mov    0xb0(%rcx),%r9
ffffffff812bf85c:	4c 8b 91 b8 00 00 00 	mov    0xb8(%rcx),%r10
ffffffff812bf863:	4c 8b 99 c0 00 00 00 	mov    0xc0(%rcx),%r11
ffffffff812bf86a:	4c 8b a1 c8 00 00 00 	mov    0xc8(%rcx),%r12
ffffffff812bf871:	4c 8b a9 d0 00 00 00 	mov    0xd0(%rcx),%r13
ffffffff812bf878:	4c 8b b1 d8 00 00 00 	mov    0xd8(%rcx),%r14
ffffffff812bf87f:	4c 8b b9 e0 00 00 00 	mov    0xe0(%rcx),%r15
ffffffff812bf886:	48 8b 49 70          	mov    0x70(%rcx),%rcx
ffffffff812bf88a:	0f 01 c3             	vmresume 
		vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, x);
		goto start;
	}
*/
	ret = vmx_run_vcpu(vcpu);
	return ret;
ffffffff812bf88d:	eb 41                	jmp    ffffffff812bf8d0 <vmx_launch+0xe44>
        vmx_put_cpu(vcpu);

	return vcpu;

fail_ept:
	printk(KERN_INFO "vmx: fail ept");
ffffffff812bf88f:	48 c7 c7 07 5e 7b 81 	mov    $0xffffffff817b5e07,%rdi
ffffffff812bf896:	31 c0                	xor    %eax,%eax
ffffffff812bf898:	e8 f5 36 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812bf89d:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812bf8a1:	48 8d 78 04          	lea    0x4(%rax),%rdi
	vmx_free_vpid(vcpu);
ffffffff812bf8a5:	e8 77 eb ff ff       	callq  ffffffff812be421 <vmx_free_vpid.isra.9>
/**
 * vmx_free_vmcs - frees a VMCS region
 */
static void vmx_free_vmcs(struct vmcs *vmcs)
{
	free_pages((unsigned long)vmcs, vmcs_config.order);
ffffffff812bf8aa:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812bf8ae:	8b 35 90 fe 8e 00    	mov    0x8efe90(%rip),%esi        # ffffffff81baf744 <vmcs_config+0x4>
ffffffff812bf8b4:	48 8b b8 10 02 00 00 	mov    0x210(%rax),%rdi
ffffffff812bf8bb:	e8 92 96 e1 ff       	callq  ffffffff810d8f52 <free_pages>
	printk(KERN_INFO "vmx: fail ept");
	vmx_free_vpid(vcpu);
fail_vpid:
	vmx_free_vmcs(vcpu->vmcs);
fail_vmcs:
	kfree(vcpu);
ffffffff812bf8c0:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812bf8c4:	e8 c0 10 e4 ff       	callq  ffffffff81100989 <kfree>
	int ret;
	struct vmx_vcpu *vcpu;
	printk(KERN_INFO "vmx_launch: Entering \n");
        vcpu = vmx_create_vcpu(conf);
	if (!vcpu)
                return -ENOMEM;
ffffffff812bf8c9:	c7 45 b0 f4 ff ff ff 	movl   $0xfffffff4,-0x50(%rbp)
		goto start;
	}
*/
	ret = vmx_run_vcpu(vcpu);
	return ret;
}
ffffffff812bf8d0:	8b 45 b0             	mov    -0x50(%rbp),%eax
ffffffff812bf8d3:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812bf8d7:	5b                   	pop    %rbx
ffffffff812bf8d8:	41 5c                	pop    %r12
ffffffff812bf8da:	41 5d                	pop    %r13
ffffffff812bf8dc:	41 5e                	pop    %r14
ffffffff812bf8de:	41 5f                	pop    %r15
ffffffff812bf8e0:	5d                   	pop    %rbp
ffffffff812bf8e1:	c3                   	retq   

ffffffff812bf8e2 <vmx_handle_exits>:
//		if(!((ret & 0x0000ffff) == 1 && vmcs_read32(EXIT_QUALIFICATION) == 0))i



int vmx_handle_exits(struct vmx_vcpu* vcpu)
{
ffffffff812bf8e2:	55                   	push   %rbp
ffffffff812bf8e3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812bf8e6:	41 57                	push   %r15
ffffffff812bf8e8:	41 56                	push   %r14
ffffffff812bf8ea:	41 55                	push   %r13
ffffffff812bf8ec:	41 54                	push   %r12
ffffffff812bf8ee:	49 89 fc             	mov    %rdi,%r12
ffffffff812bf8f1:	53                   	push   %rbx
ffffffff812bf8f2:	48 83 ec 18          	sub    $0x18,%rsp
	int ret, done;
	if (unlikely(vcpu->fail)) {
ffffffff812bf8f6:	80 7f 51 00          	cmpb   $0x0,0x51(%rdi)
ffffffff812bf8fa:	74 18                	je     ffffffff812bf914 <vmx_handle_exits+0x32>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf8fc:	ba 00 44 00 00       	mov    $0x4400,%edx
ffffffff812bf901:	0f 78 d0             	vmread %rdx,%rax

int vmx_handle_exits(struct vmx_vcpu* vcpu)
{
	int ret, done;
	if (unlikely(vcpu->fail)) {
		printk(KERN_ERR "vmx: failure detected (err %x)\n",
ffffffff812bf904:	48 c7 c7 17 5e 7b 81 	mov    $0xffffffff817b5e17,%rdi
ffffffff812bf90b:	89 c6                	mov    %eax,%esi
ffffffff812bf90d:	31 c0                	xor    %eax,%eax
ffffffff812bf90f:	e8 7e 36 17 00       	callq  ffffffff81432f92 <printk>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf914:	ba 02 44 00 00       	mov    $0x4402,%edx
ffffffff812bf919:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf91c:	41 bd 1e 68 00 00    	mov    $0x681e,%r13d
ffffffff812bf922:	48 89 c3             	mov    %rax,%rbx
ffffffff812bf925:	4c 89 ea             	mov    %r13,%rdx
ffffffff812bf928:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf92b:	48 89 c1             	mov    %rax,%rcx
ffffffff812bf92e:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf931:	ba 1c 68 00 00       	mov    $0x681c,%edx
ffffffff812bf936:	48 89 c7             	mov    %rax,%rdi
ffffffff812bf939:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf93c:	48 89 c6             	mov    %rax,%rsi
	printk(KERN_INFO "Guest was accessing 0x%16lx linear and 0x%16lx physical.", 
			vmcs_readl(GUEST_LINEAR_ADDRESS), (long unsigned int)vmcs_read64(GUEST_PHYSICAL_ADDRESS));
	printk(KERN_INFO "IDT vectoring info is 0x%16x [%d]. IDT vectoring error is 0x%16x [%d]",
			vmcs_read32(IDT_VECTORING_INFO_FIELD), vmcs_read32(IDT_VECTORING_INFO_FIELD), 
			vmcs_read32(IDT_VECTORING_ERROR_CODE), vmcs_read32(IDT_VECTORING_ERROR_CODE));*/
	printk(KERN_ERR "GUEST_RSP: 0x%16lx, GUEST_RIP: [<%p>] %pS\n",vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP), vmcs_readl(GUEST_RIP));
ffffffff812bf93f:	48 89 fa             	mov    %rdi,%rdx
ffffffff812bf942:	31 c0                	xor    %eax,%eax
ffffffff812bf944:	48 c7 c7 39 5e 7b 81 	mov    $0xffffffff817b5e39,%rdi
ffffffff812bf94b:	e8 42 36 17 00       	callq  ffffffff81432f92 <printk>
	/*local_irq_enable();*/

	if (ret == EXIT_REASON_VMCALL ||
ffffffff812bf950:	8d 43 f6             	lea    -0xa(%rbx),%eax
ffffffff812bf953:	83 e0 f7             	and    $0xfffffff7,%eax
ffffffff812bf956:	75 20                	jne    ffffffff812bf978 <vmx_handle_exits+0x96>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bf958:	4c 89 ea             	mov    %r13,%rdx
ffffffff812bf95b:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bf95e:	ba 0c 44 00 00       	mov    $0x440c,%edx
ffffffff812bf963:	49 89 c5             	mov    %rax,%r13
ffffffff812bf966:	0f 78 d0             	vmread %rdx,%rax



static void vmx_step_instruction(void)
{
	vmcs_writel(GUEST_RIP, vmcs_readl(GUEST_RIP) +
ffffffff812bf969:	89 c6                	mov    %eax,%esi
ffffffff812bf96b:	bf 1e 68 00 00       	mov    $0x681e,%edi
ffffffff812bf970:	4c 01 ee             	add    %r13,%rsi
ffffffff812bf973:	e8 8e ea ff ff       	callq  ffffffff812be406 <vmcs_writel>
		vmx_step_instruction();
	}

	vmx_put_cpu(vcpu);

	if (ret == EXIT_REASON_VMCALL)
ffffffff812bf978:	83 fb 12             	cmp    $0x12,%ebx
ffffffff812bf97b:	75 5d                	jne    ffffffff812bf9da <vmx_handle_exits+0xf8>
	a0 = vcpu->regs[VCPU_REGS_RBX];
	a1 = vcpu->regs[VCPU_REGS_RCX];
	a2 = vcpu->regs[VCPU_REGS_RDX];
	a3 = vcpu->regs[VCPU_REGS_RSI];

	switch(nr)
ffffffff812bf97d:	49 8b 44 24 68       	mov    0x68(%r12),%rax
ffffffff812bf982:	48 83 f8 02          	cmp    $0x2,%rax
ffffffff812bf986:	74 10                	je     ffffffff812bf998 <vmx_handle_exits+0xb6>
ffffffff812bf988:	48 83 f8 03          	cmp    $0x3,%rax
ffffffff812bf98c:	74 36                	je     ffffffff812bf9c4 <vmx_handle_exits+0xe2>
ffffffff812bf98e:	48 ff c8             	dec    %rax
ffffffff812bf991:	75 36                	jne    ffffffff812bf9c9 <vmx_handle_exits+0xe7>
ffffffff812bf993:	e9 ca 02 00 00       	jmpq   ffffffff812bfc62 <vmx_handle_exits+0x380>
	{
		case DUNE_HC_EXIT_VM:
			vmx_destroy_vm(vcpu);
			break;
		case DUNE_HC_PRINT:
			printk(KERN_ERR "Print hypercall: %016lx %016lx %016lx %016lx \n", a0, a1, a2, a3);
ffffffff812bf998:	49 8b 4c 24 78       	mov    0x78(%r12),%rcx
ffffffff812bf99d:	49 8b 54 24 70       	mov    0x70(%r12),%rdx
ffffffff812bf9a2:	48 c7 c7 66 5e 7b 81 	mov    $0xffffffff817b5e66,%rdi
ffffffff812bf9a9:	49 8b b4 24 80 00 00 	mov    0x80(%r12),%rsi
ffffffff812bf9b0:	00 
ffffffff812bf9b1:	4d 8b 84 24 98 00 00 	mov    0x98(%r12),%r8
ffffffff812bf9b8:	00 
ffffffff812bf9b9:	31 c0                	xor    %eax,%eax
ffffffff812bf9bb:	e8 d2 35 17 00       	callq  ffffffff81432f92 <printk>
	do_exit(0);
}

static void vmx_handle_hypercall(struct vmx_vcpu *vcpu)
{
	unsigned long nr, a0, a1, a2, a3, ret = 0;
ffffffff812bf9c0:	31 c0                	xor    %eax,%eax
ffffffff812bf9c2:	eb 0c                	jmp    ffffffff812bf9d0 <vmx_handle_exits+0xee>
			break;
		case DUNE_HC_PRINT:
			printk(KERN_ERR "Print hypercall: %016lx %016lx %016lx %016lx \n", a0, a1, a2, a3);
			break;
		case DUNE_HC_SCHEDULE:
			schedule();
ffffffff812bf9c4:	e8 c3 5e 17 00       	callq  ffffffff8143588c <schedule>
		default:
			ret = -EINVAL;
ffffffff812bf9c9:	48 c7 c0 ea ff ff ff 	mov    $0xffffffffffffffea,%rax
	}
	vcpu->regs[VCPU_REGS_RAX] = ret;
ffffffff812bf9d0:	49 89 44 24 68       	mov    %rax,0x68(%r12)
ffffffff812bf9d5:	e9 c1 00 00 00       	jmpq   ffffffff812bfa9b <vmx_handle_exits+0x1b9>
	vmx_put_cpu(vcpu);

	if (ret == EXIT_REASON_VMCALL)
		vmx_handle_hypercall(vcpu);
	// TODO Dont virtualize CPUID Newchange
	else if (ret == EXIT_REASON_CPUID)
ffffffff812bf9da:	83 fb 0a             	cmp    $0xa,%ebx
ffffffff812bf9dd:	75 41                	jne    ffffffff812bfa20 <vmx_handle_exits+0x13e>

static void vmx_handle_cpuid(struct vmx_vcpu *vcpu)
{
	unsigned int eax, ebx, ecx, edx;
#ifdef CONFIG_AZKABAN_DEBUG
	printk(KERN_INFO "Handling CPUID for the guest \n");
ffffffff812bf9df:	48 c7 c7 97 5e 7b 81 	mov    $0xffffffff817b5e97,%rdi
ffffffff812bf9e6:	31 c0                	xor    %eax,%eax
ffffffff812bf9e8:	e8 a5 35 17 00       	callq  ffffffff81432f92 <printk>
#endif
static inline void native_cpuid(unsigned int *eax, unsigned int *ebx,
				unsigned int *ecx, unsigned int *edx)
{
	/* ecx is often an input as well as an output. */
	asm volatile("cpuid"
ffffffff812bf9ed:	41 8b 44 24 68       	mov    0x68(%r12),%eax
ffffffff812bf9f2:	41 8b 4c 24 70       	mov    0x70(%r12),%ecx
ffffffff812bf9f7:	0f a2                	cpuid  
#endif
	eax = vcpu->regs[VCPU_REGS_RAX];
	ecx = vcpu->regs[VCPU_REGS_RCX];
	native_cpuid(&eax, &ebx, &ecx, &edx);
	ecx = ecx | (1 << (X86_FEATURE_HYPERVISOR & 31));
	vcpu->regs[VCPU_REGS_RAX] = eax;
ffffffff812bf9f9:	89 c0                	mov    %eax,%eax
	vcpu->regs[VCPU_REGS_RBX] = ebx;
	vcpu->regs[VCPU_REGS_RCX] = ecx;
ffffffff812bf9fb:	81 c9 00 00 00 80    	or     $0x80000000,%ecx
#endif
	eax = vcpu->regs[VCPU_REGS_RAX];
	ecx = vcpu->regs[VCPU_REGS_RCX];
	native_cpuid(&eax, &ebx, &ecx, &edx);
	ecx = ecx | (1 << (X86_FEATURE_HYPERVISOR & 31));
	vcpu->regs[VCPU_REGS_RAX] = eax;
ffffffff812bfa01:	49 89 44 24 68       	mov    %rax,0x68(%r12)
	vcpu->regs[VCPU_REGS_RBX] = ebx;
ffffffff812bfa06:	89 d8                	mov    %ebx,%eax
ffffffff812bfa08:	49 89 84 24 80 00 00 	mov    %rax,0x80(%r12)
ffffffff812bfa0f:	00 
	vcpu->regs[VCPU_REGS_RCX] = ecx;
ffffffff812bfa10:	89 c8                	mov    %ecx,%eax
ffffffff812bfa12:	49 89 44 24 70       	mov    %rax,0x70(%r12)
	vcpu->regs[VCPU_REGS_RDX] = edx;
ffffffff812bfa17:	89 d0                	mov    %edx,%eax
ffffffff812bfa19:	49 89 44 24 78       	mov    %rax,0x78(%r12)
ffffffff812bfa1e:	eb 7b                	jmp    ffffffff812bfa9b <vmx_handle_exits+0x1b9>
	if (ret == EXIT_REASON_VMCALL)
		vmx_handle_hypercall(vcpu);
	// TODO Dont virtualize CPUID Newchange
	else if (ret == EXIT_REASON_CPUID)
		vmx_handle_cpuid(vcpu);
	else if (ret == EXIT_REASON_EPT_VIOLATION){
ffffffff812bfa20:	83 fb 30             	cmp    $0x30,%ebx
ffffffff812bfa23:	0f 85 b3 00 00 00    	jne    ffffffff812bfadc <vmx_handle_exits+0x1fa>
{
	unsigned long gva, gpa;
	int exit_qual, ret;

//	printk(KERN_ERR "EPT: vmx_handle_ept_violation: Entering\n");
	vmx_get_cpu(vcpu);
ffffffff812bfa29:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfa2c:	e8 8e ec ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bfa31:	ba 00 64 00 00       	mov    $0x6400,%edx
ffffffff812bfa36:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bfa39:	ba 0a 64 00 00       	mov    $0x640a,%edx
ffffffff812bfa3e:	49 89 c5             	mov    %rax,%r13
ffffffff812bfa41:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bfa44:	ba 00 24 00 00       	mov    $0x2400,%edx
ffffffff812bfa49:	49 89 c6             	mov    %rax,%r14
ffffffff812bfa4c:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bfa4f:	48 89 c3             	mov    %rax,%rbx
	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
	vmx_put_cpu(vcpu);
	
//	printk(KERN_ERR "EPT: vmx_handle_ept_violation: GVA: 0x%lx, GPA 0x%lx \n", gva, gpa);

	if (exit_qual & (1 << 6)) {
ffffffff812bfa52:	41 f6 c5 40          	test   $0x40,%r13b
		printk(KERN_ERR "EPT: GPA 0x%lx exceeds GAW!\n", gpa);
ffffffff812bfa56:	48 89 c6             	mov    %rax,%rsi
ffffffff812bfa59:	48 c7 c7 b8 5e 7b 81 	mov    $0xffffffff817b5eb8,%rdi
	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
	vmx_put_cpu(vcpu);
	
//	printk(KERN_ERR "EPT: vmx_handle_ept_violation: GVA: 0x%lx, GPA 0x%lx \n", gva, gpa);

	if (exit_qual & (1 << 6)) {
ffffffff812bfa60:	75 10                	jne    ffffffff812bfa72 <vmx_handle_exits+0x190>
		printk(KERN_ERR "EPT: GPA 0x%lx exceeds GAW!\n", gpa);
		return -EINVAL;
	}
	
	if (!(exit_qual & (1 << 7))) {
ffffffff812bfa62:	41 f6 c5 80          	test   $0x80,%r13b
ffffffff812bfa66:	75 16                	jne    ffffffff812bfa7e <vmx_handle_exits+0x19c>
		printk(KERN_ERR "EPT: linear address is not valid, GPA: 0x%lx!\n", gpa);
ffffffff812bfa68:	48 89 c6             	mov    %rax,%rsi
ffffffff812bfa6b:	48 c7 c7 d7 5e 7b 81 	mov    $0xffffffff817b5ed7,%rdi
ffffffff812bfa72:	31 c0                	xor    %eax,%eax
ffffffff812bfa74:	e8 19 35 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812bfa79:	e9 e4 01 00 00       	jmpq   ffffffff812bfc62 <vmx_handle_exits+0x380>
		return -EINVAL;
	}

	vmx_dump_cpu(vcpu);
ffffffff812bfa7e:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfa81:	e8 af ed ff ff       	callq  ffffffff812be835 <vmx_dump_cpu>

	ret = vmx_do_ept_fault(vcpu, gpa, gva, exit_qual);
ffffffff812bfa86:	44 89 e9             	mov    %r13d,%ecx
ffffffff812bfa89:	4c 89 f2             	mov    %r14,%rdx
ffffffff812bfa8c:	48 89 de             	mov    %rbx,%rsi
ffffffff812bfa8f:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfa92:	e8 51 0c 00 00       	callq  ffffffff812c06e8 <vmx_do_ept_fault>

	if (ret) {
ffffffff812bfa97:	85 c0                	test   %eax,%eax
ffffffff812bfa99:	75 14                	jne    ffffffff812bfaaf <vmx_handle_exits+0x1cd>
	else if (ret != EXIT_REASON_EXTERNAL_INTERRUPT) {
//		printk(KERN_INFO "unhandled exit: reason 0x%08x [%d], exit qualification %x\n",
//				ret, ret, vmcs_read32(EXIT_QUALIFICATION));
		done = 1; //Bhu: allow it to continue
	}
	if (done || vcpu->shutdown)
ffffffff812bfa9b:	41 83 bc 24 f8 00 00 	cmpl   $0x0,0xf8(%r12)
ffffffff812bfaa2:	00 00 
ffffffff812bfaa4:	0f 84 d4 01 00 00    	je     ffffffff812bfc7e <vmx_handle_exits+0x39c>
ffffffff812bfaaa:	e9 b3 01 00 00       	jmpq   ffffffff812bfc62 <vmx_handle_exits+0x380>
	vmx_dump_cpu(vcpu);

	ret = vmx_do_ept_fault(vcpu, gpa, gva, exit_qual);

	if (ret) {
		printk(KERN_ERR "vmx: page fault failure "
ffffffff812bfaaf:	4c 89 f2             	mov    %r14,%rdx
ffffffff812bfab2:	48 89 de             	mov    %rbx,%rsi
ffffffff812bfab5:	48 c7 c7 08 5f 7b 81 	mov    $0xffffffff817b5f08,%rdi
ffffffff812bfabc:	31 c0                	xor    %eax,%eax
ffffffff812bfabe:	e8 cf 34 17 00       	callq  ffffffff81432f92 <printk>
		       "GPA: 0x%lx, GVA: 0x%lx\n",
		       gpa, gva);
		vcpu->ret_code = ((EFAULT) << 8);
ffffffff812bfac3:	41 c7 84 24 fc 00 00 	movl   $0xe00,0xfc(%r12)
ffffffff812bfaca:	00 00 0e 00 00 
		vmx_dump_cpu(vcpu);
ffffffff812bfacf:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfad2:	e8 5e ed ff ff       	callq  ffffffff812be835 <vmx_dump_cpu>
ffffffff812bfad7:	e9 86 01 00 00       	jmpq   ffffffff812bfc62 <vmx_handle_exits+0x380>
		vmx_handle_cpuid(vcpu);
	else if (ret == EXIT_REASON_EPT_VIOLATION){
//		printk(KERN_INFO "VMX: EXIT_REASON_EPT_VIOLATION occured\n");
		done = vmx_handle_ept_violation(vcpu);
	}
	else if (ret == EXIT_REASON_EXCEPTION_NMI) {
ffffffff812bfadc:	85 db                	test   %ebx,%ebx
ffffffff812bfade:	0f 85 76 01 00 00    	jne    ffffffff812bfc5a <vmx_handle_exits+0x378>
{
	u32 intr_info, intr_err, idt_info, idt_err;
        u32 intr_type, intr_valid, intr_err_valid, intr_nmi_unblock, intr_vector;
        u32 idt_type, idt_valid, idt_err_valid, idt_vector;

	vmx_get_cpu(vcpu);
ffffffff812bfae4:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfae7:	e8 d3 eb ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>

unsigned long vmcs_readl(unsigned long field)
{
	unsigned long value;

	asm volatile (ASM_VMX_VMREAD_RDX_RAX
ffffffff812bfaec:	ba 04 44 00 00       	mov    $0x4404,%edx
ffffffff812bfaf1:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bfaf4:	ba 06 44 00 00       	mov    $0x4406,%edx
ffffffff812bfaf9:	48 89 c3             	mov    %rax,%rbx
ffffffff812bfafc:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bfaff:	ba 08 44 00 00       	mov    $0x4408,%edx
ffffffff812bfb04:	49 89 c5             	mov    %rax,%r13
ffffffff812bfb07:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bfb0a:	ba 0a 44 00 00       	mov    $0x440a,%edx
ffffffff812bfb0f:	49 89 c6             	mov    %rax,%r14
ffffffff812bfb12:	0f 78 d0             	vmread %rdx,%rax
ffffffff812bfb15:	49 89 c7             	mov    %rax,%r15
	intr_err = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
	idt_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
	idt_err = vmcs_read32(IDT_VECTORING_ERROR_CODE);
	vmx_put_cpu(vcpu);

	printk(KERN_INFO "vmx: got an exception\n");
ffffffff812bfb18:	48 c7 c7 3a 5f 7b 81 	mov    $0xffffffff817b5f3a,%rdi
ffffffff812bfb1f:	31 c0                	xor    %eax,%eax
ffffffff812bfb21:	e8 6c 34 17 00       	callq  ffffffff81432f92 <printk>
                 vmx->vnmi_blocked_time +=
                         ktime_to_ns(ktime_sub(ktime_get(), vmx->entry_time));

#endif

	printk(KERN_ERR "vmx: unhandled nmi, intr_info %x, and intr_err %x\n", intr_info, intr_err);
ffffffff812bfb26:	89 de                	mov    %ebx,%esi
ffffffff812bfb28:	44 89 ea             	mov    %r13d,%edx
ffffffff812bfb2b:	48 c7 c7 53 5f 7b 81 	mov    $0xffffffff817b5f53,%rdi
ffffffff812bfb32:	31 c0                	xor    %eax,%eax
ffffffff812bfb34:	e8 59 34 17 00       	callq  ffffffff81432f92 <printk>
	//detail_dump(vcpu);
	printk(KERN_ERR "Reinserting the exception.");
ffffffff812bfb39:	31 c0                	xor    %eax,%eax
ffffffff812bfb3b:	48 c7 c7 88 5f 7b 81 	mov    $0xffffffff817b5f88,%rdi
ffffffff812bfb42:	e8 4b 34 17 00       	callq  ffffffff81432f92 <printk>
	intr_nmi_unblock = intr_info & INTR_INFO_UNBLOCK_NMI;
	intr_vector =  intr_info & INTR_INFO_VECTOR_MASK;

	
        idt_type = idt_info & INTR_INFO_INTR_TYPE_MASK;
        idt_valid = idt_info & INTR_INFO_VALID_MASK;
ffffffff812bfb47:	44 89 f6             	mov    %r14d,%esi

	intr_type = intr_info & INTR_INFO_INTR_TYPE_MASK;
	intr_valid = intr_info & INTR_INFO_VALID_MASK;
	intr_err_valid = intr_info & INTR_INFO_DELIVER_CODE_MASK;
	intr_nmi_unblock = intr_info & INTR_INFO_UNBLOCK_NMI;
	intr_vector =  intr_info & INTR_INFO_VECTOR_MASK;
ffffffff812bfb4a:	0f b6 c3             	movzbl %bl,%eax

	
        idt_type = idt_info & INTR_INFO_INTR_TYPE_MASK;
        idt_valid = idt_info & INTR_INFO_VALID_MASK;
ffffffff812bfb4d:	81 e6 00 00 00 80    	and    $0x80000000,%esi
        idt_err_valid = idt_info & INTR_INFO_DELIVER_CODE_MASK;
        idt_vector =  idt_info & INTR_INFO_VECTOR_MASK;

	if(intr_valid)
ffffffff812bfb53:	85 db                	test   %ebx,%ebx

	intr_type = intr_info & INTR_INFO_INTR_TYPE_MASK;
	intr_valid = intr_info & INTR_INFO_VALID_MASK;
	intr_err_valid = intr_info & INTR_INFO_DELIVER_CODE_MASK;
	intr_nmi_unblock = intr_info & INTR_INFO_UNBLOCK_NMI;
	intr_vector =  intr_info & INTR_INFO_VECTOR_MASK;
ffffffff812bfb55:	89 45 cc             	mov    %eax,-0x34(%rbp)

	
        idt_type = idt_info & INTR_INFO_INTR_TYPE_MASK;
        idt_valid = idt_info & INTR_INFO_VALID_MASK;
ffffffff812bfb58:	89 75 c8             	mov    %esi,-0x38(%rbp)
        idt_err_valid = idt_info & INTR_INFO_DELIVER_CODE_MASK;
        idt_vector =  idt_info & INTR_INFO_VECTOR_MASK;

	if(intr_valid)
ffffffff812bfb5b:	79 2d                	jns    ffffffff812bfb8a <vmx_handle_exits+0x2a8>
	{
                if(intr_type != INTR_TYPE_HARD_EXCEPTION)
ffffffff812bfb5d:	89 d8                	mov    %ebx,%eax
ffffffff812bfb5f:	25 00 07 00 00       	and    $0x700,%eax
ffffffff812bfb64:	3d 00 03 00 00       	cmp    $0x300,%eax
ffffffff812bfb69:	0f 85 c5 00 00 00    	jne    ffffffff812bfc34 <vmx_handle_exits+0x352>
                {
                        goto REFLECT;
                }
                if(is_benign(intr_vector))
ffffffff812bfb6f:	0f b6 fb             	movzbl %bl,%edi
ffffffff812bfb72:	e8 cc e7 ff ff       	callq  ffffffff812be343 <is_benign>
                {
                        goto REFLECT;
                }
		if(!idt_valid)
ffffffff812bfb77:	83 7d c8 00          	cmpl   $0x0,-0x38(%rbp)
ffffffff812bfb7b:	0f 84 b3 00 00 00    	je     ffffffff812bfc34 <vmx_handle_exits+0x352>
ffffffff812bfb81:	85 c0                	test   %eax,%eax
ffffffff812bfb83:	74 0b                	je     ffffffff812bfb90 <vmx_handle_exits+0x2ae>
ffffffff812bfb85:	e9 aa 00 00 00       	jmpq   ffffffff812bfc34 <vmx_handle_exits+0x352>
		{
			goto REFLECT;
		}
	}

	if(idt_valid)
ffffffff812bfb8a:	83 7d c8 00          	cmpl   $0x0,-0x38(%rbp)
ffffffff812bfb8e:	74 62                	je     ffffffff812bfbf2 <vmx_handle_exits+0x310>
	{
		if(idt_type != INTR_TYPE_HARD_EXCEPTION)
ffffffff812bfb90:	44 89 f0             	mov    %r14d,%eax
ffffffff812bfb93:	25 00 07 00 00       	and    $0x700,%eax
ffffffff812bfb98:	3d 00 03 00 00       	cmp    $0x300,%eax
ffffffff812bfb9d:	0f 85 91 00 00 00    	jne    ffffffff812bfc34 <vmx_handle_exits+0x352>

	
        idt_type = idt_info & INTR_INFO_INTR_TYPE_MASK;
        idt_valid = idt_info & INTR_INFO_VALID_MASK;
        idt_err_valid = idt_info & INTR_INFO_DELIVER_CODE_MASK;
        idt_vector =  idt_info & INTR_INFO_VECTOR_MASK;
ffffffff812bfba3:	41 0f b6 c6          	movzbl %r14b,%eax
		if(idt_type != INTR_TYPE_HARD_EXCEPTION)
		{
			goto REFLECT;
		}

		if(is_benign(idt_vector))
ffffffff812bfba7:	89 c7                	mov    %eax,%edi

	
        idt_type = idt_info & INTR_INFO_INTR_TYPE_MASK;
        idt_valid = idt_info & INTR_INFO_VALID_MASK;
        idt_err_valid = idt_info & INTR_INFO_DELIVER_CODE_MASK;
        idt_vector =  idt_info & INTR_INFO_VECTOR_MASK;
ffffffff812bfba9:	89 45 c8             	mov    %eax,-0x38(%rbp)
		if(idt_type != INTR_TYPE_HARD_EXCEPTION)
		{
			goto REFLECT;
		}

		if(is_benign(idt_vector))
ffffffff812bfbac:	e8 92 e7 ff ff       	callq  ffffffff812be343 <is_benign>
ffffffff812bfbb1:	85 c0                	test   %eax,%eax
ffffffff812bfbb3:	75 7f                	jne    ffffffff812bfc34 <vmx_handle_exits+0x352>
		{
                        goto REFLECT;
		}
		if(is_benign(intr_vector))
ffffffff812bfbb5:	0f b6 fb             	movzbl %bl,%edi
ffffffff812bfbb8:	e8 86 e7 ff ff       	callq  ffffffff812be343 <is_benign>
ffffffff812bfbbd:	85 c0                	test   %eax,%eax
ffffffff812bfbbf:	75 73                	jne    ffffffff812bfc34 <vmx_handle_exits+0x352>
	return 0;
}

static int is_contributory(u32 vector)
{
        if((vector > 9 && vector < 14) || (vector == 0))
ffffffff812bfbc1:	41 0f b6 c6          	movzbl %r14b,%eax
ffffffff812bfbc5:	83 e8 0a             	sub    $0xa,%eax
ffffffff812bfbc8:	83 f8 03             	cmp    $0x3,%eax
ffffffff812bfbcb:	0f 86 99 00 00 00    	jbe    ffffffff812bfc6a <vmx_handle_exits+0x388>
ffffffff812bfbd1:	83 7d c8 00          	cmpl   $0x0,-0x38(%rbp)
ffffffff812bfbd5:	0f 84 8f 00 00 00    	je     ffffffff812bfc6a <vmx_handle_exits+0x388>
		
		if(is_contributory(idt_vector) && is_contributory(intr_vector))
		{
			goto DOUBLE_FAULT;	
		}
		if(idt_vector == 14 && (is_contributory(intr_vector) || intr_vector == 14))
ffffffff812bfbdb:	83 7d c8 0e          	cmpl   $0xe,-0x38(%rbp)
ffffffff812bfbdf:	75 11                	jne    ffffffff812bfbf2 <vmx_handle_exits+0x310>
ffffffff812bfbe1:	0f b6 c3             	movzbl %bl,%eax
ffffffff812bfbe4:	83 e8 0a             	sub    $0xa,%eax
ffffffff812bfbe7:	83 f8 04             	cmp    $0x4,%eax
ffffffff812bfbea:	76 2d                	jbe    ffffffff812bfc19 <vmx_handle_exits+0x337>
ffffffff812bfbec:	83 7d cc 00          	cmpl   $0x0,-0x34(%rbp)
ffffffff812bfbf0:	74 27                	je     ffffffff812bfc19 <vmx_handle_exits+0x337>
                {
                        goto DOUBLE_FAULT;
                }			
	}
	
        printk(KERN_ERR "vmx: No case found to reflect, intr_info %x, and intr_err %x, idt_info %x, and idt_err %x\n", intr_info, intr_err, idt_info, idt_err);
ffffffff812bfbf2:	45 89 f8             	mov    %r15d,%r8d
ffffffff812bfbf5:	44 89 f1             	mov    %r14d,%ecx
ffffffff812bfbf8:	44 89 ea             	mov    %r13d,%edx
ffffffff812bfbfb:	89 de                	mov    %ebx,%esi
ffffffff812bfbfd:	48 c7 c7 a5 5f 7b 81 	mov    $0xffffffff817b5fa5,%rdi
ffffffff812bfc04:	31 c0                	xor    %eax,%eax
ffffffff812bfc06:	e8 87 33 17 00       	callq  ffffffff81432f92 <printk>
//		return 0;
//	}
//	else
//	{
ERR:
		vcpu->ret_code = ((EFAULT) << 8);
ffffffff812bfc0b:	41 c7 84 24 fc 00 00 	movl   $0xe00,0xfc(%r12)
ffffffff812bfc12:	00 00 0e 00 00 
ffffffff812bfc17:	eb 49                	jmp    ffffffff812bfc62 <vmx_handle_exits+0x380>
	
        printk(KERN_ERR "vmx: No case found to reflect, intr_info %x, and intr_err %x, idt_info %x, and idt_err %x\n", intr_info, intr_err, idt_info, idt_err);
	goto ERR;
	
DOUBLE_FAULT:
        vmx_get_cpu(vcpu);
ffffffff812bfc19:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfc1c:	e8 9e ea ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bfc21:	be 08 0b 00 80       	mov    $0x80000b08,%esi
ffffffff812bfc26:	bf 16 40 00 00       	mov    $0x4016,%edi
ffffffff812bfc2b:	e8 d6 e7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bfc30:	31 f6                	xor    %esi,%esi
ffffffff812bfc32:	eb 17                	jmp    ffffffff812bfc4b <vmx_handle_exits+0x369>
	vmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, 0);
        vmx_put_cpu(vcpu);
	return 0;

REFLECT:
        vmx_get_cpu(vcpu);
ffffffff812bfc34:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfc37:	e8 83 ea ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>
	vmcs_writel(field, value);
}

static void vmcs_write32(unsigned long field, u32 value)
{
	vmcs_writel(field, value);
ffffffff812bfc3c:	89 de                	mov    %ebx,%esi
ffffffff812bfc3e:	bf 16 40 00 00       	mov    $0x4016,%edi
ffffffff812bfc43:	e8 be e7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
ffffffff812bfc48:	44 89 ee             	mov    %r13d,%esi
ffffffff812bfc4b:	bf 18 40 00 00       	mov    $0x4018,%edi
ffffffff812bfc50:	e8 b1 e7 ff ff       	callq  ffffffff812be406 <vmcs_writel>
 * vmx_put_cpu - called after using a cpu
 * @vcpu: VCPU that was loaded.
 */
static void vmx_put_cpu(struct vmx_vcpu *vcpu)
{
	put_cpu();
ffffffff812bfc55:	e9 41 fe ff ff       	jmpq   ffffffff812bfa9b <vmx_handle_exits+0x1b9>
	else if (ret == EXIT_REASON_HLT)
	{
//		printk("GUEST CPU HALTED\n");
		done = 1; //Bhu: allow it to continue
	}
	else if (ret != EXIT_REASON_EXTERNAL_INTERRUPT) {
ffffffff812bfc5a:	ff cb                	dec    %ebx
ffffffff812bfc5c:	0f 84 39 fe ff ff    	je     ffffffff812bfa9b <vmx_handle_exits+0x1b9>
		done = 1; //Bhu: allow it to continue
	}
	if (done || vcpu->shutdown)
	{
done:
		vmx_destroy_vm(vcpu);
ffffffff812bfc62:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfc65:	e8 9b ed ff ff       	callq  ffffffff812bea05 <vmx_destroy_vm>
		}
		if(is_benign(intr_vector))
                {
                        goto REFLECT;
                }
		if(is_contributory(idt_vector) && intr_vector == 14)
ffffffff812bfc6a:	83 7d cc 0e          	cmpl   $0xe,-0x34(%rbp)
ffffffff812bfc6e:	74 c4                	je     ffffffff812bfc34 <vmx_handle_exits+0x352>
	return 0;
}

static int is_contributory(u32 vector)
{
        if((vector > 9 && vector < 14) || (vector == 0))
ffffffff812bfc70:	0f b6 c3             	movzbl %bl,%eax
ffffffff812bfc73:	83 e8 0a             	sub    $0xa,%eax
ffffffff812bfc76:	83 f8 03             	cmp    $0x3,%eax
ffffffff812bfc79:	e9 6c ff ff ff       	jmpq   ffffffff812bfbea <vmx_handle_exits+0x308>
done:
		vmx_destroy_vm(vcpu);
	}
	
start:
	vmx_get_cpu(vcpu);
ffffffff812bfc7e:	4c 89 e7             	mov    %r12,%rdi
ffffffff812bfc81:	e8 39 ea ff ff       	callq  ffffffff812be6bf <vmx_get_cpu>
		vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, x);
		goto start;
	}
*/
	return 0;
}
ffffffff812bfc86:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812bfc8a:	31 c0                	xor    %eax,%eax
ffffffff812bfc8c:	5b                   	pop    %rbx
ffffffff812bfc8d:	41 5c                	pop    %r12
ffffffff812bfc8f:	41 5d                	pop    %r13
ffffffff812bfc91:	41 5e                	pop    %r14
ffffffff812bfc93:	41 5f                	pop    %r15
ffffffff812bfc95:	5d                   	pop    %rbp
ffffffff812bfc96:	c3                   	retq   

ffffffff812bfc97 <vmx_exit>:

/**
 * vmx_exit - the main removal routine for this driver
 */
void vmx_exit(void)
{
ffffffff812bfc97:	55                   	push   %rbp
	on_each_cpu(vmx_disable, NULL, 1);
ffffffff812bfc98:	ba 01 00 00 00       	mov    $0x1,%edx
ffffffff812bfc9d:	31 f6                	xor    %esi,%esi
ffffffff812bfc9f:	48 c7 c7 a1 e4 2b 81 	mov    $0xffffffff812be4a1,%rdi

/**
 * vmx_exit - the main removal routine for this driver
 */
void vmx_exit(void)
{
ffffffff812bfca6:	48 89 e5             	mov    %rsp,%rbp
	on_each_cpu(vmx_disable, NULL, 1);
ffffffff812bfca9:	e8 7f cc de ff       	callq  ffffffff810ac92d <on_each_cpu>
	vmx_free_vmxon_areas();
ffffffff812bfcae:	e8 97 e9 ff ff       	callq  ffffffff812be64a <vmx_free_vmxon_areas>
	free_page((unsigned long)msr_bitmap);
ffffffff812bfcb3:	48 8b 3d a6 fa 8e 00 	mov    0x8efaa6(%rip),%rdi        # ffffffff81baf760 <msr_bitmap>
ffffffff812bfcba:	31 f6                	xor    %esi,%esi
ffffffff812bfcbc:	e8 91 92 e1 ff       	callq  ffffffff810d8f52 <free_pages>
}
ffffffff812bfcc1:	5d                   	pop    %rbp
ffffffff812bfcc2:	c3                   	retq   

ffffffff812bfcc3 <ept_mmu_notifier_invalidate_range_end>:

static void ept_mmu_notifier_invalidate_range_end(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long start,
		unsigned long end)
{
ffffffff812bfcc3:	55                   	push   %rbp
ffffffff812bfcc4:	48 89 e5             	mov    %rsp,%rbp
}
ffffffff812bfcc7:	5d                   	pop    %rbp
ffffffff812bfcc8:	c3                   	retq   

ffffffff812bfcc9 <ept_mmu_notifier_release>:
		return ept_check_page_accessed(vcpu, mm, address, false);
}

static void ept_mmu_notifier_release(struct mmu_notifier *mn,
		struct mm_struct *mm)
{
ffffffff812bfcc9:	55                   	push   %rbp
ffffffff812bfcca:	48 89 e5             	mov    %rsp,%rbp
}
ffffffff812bfccd:	5d                   	pop    %rbp
ffffffff812bfcce:	c3                   	retq   

ffffffff812bfccf <add_to_ept>:
	ret = add_to_ept(__va(vcpu->ept_root), pte_val(pa) & PTE_PFN_MASK, pte_kernel(pa), !pte_write(pa), pte_exec(pa), make_write);
	spin_unlock(&vcpu->ept_lock);
	return ret;
}

static int add_to_ept(pte_t *epgd, int address, int kernel, int kernel_ro, int kernel_rx, int make_write){
ffffffff812bfccf:	55                   	push   %rbp
	pte_t *epmd;	// epmd table
	pte_t *epte;	// epte table
	int i;		// index
	void *page;
#if DEBUG
	printk(KERN_INFO "add_to_ept: address = %lx, epgd = %lx, is_kernel = %d, is_ro = %d, is_rx = %d, make_write = %d\n", address, epgd, kernel, kernel_ro, kernel_rx, make_write);
ffffffff812bfcd0:	31 c0                	xor    %eax,%eax
	ret = add_to_ept(__va(vcpu->ept_root), pte_val(pa) & PTE_PFN_MASK, pte_kernel(pa), !pte_write(pa), pte_exec(pa), make_write);
	spin_unlock(&vcpu->ept_lock);
	return ret;
}

static int add_to_ept(pte_t *epgd, int address, int kernel, int kernel_ro, int kernel_rx, int make_write){
ffffffff812bfcd2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812bfcd5:	41 57                	push   %r15
ffffffff812bfcd7:	41 56                	push   %r14
ffffffff812bfcd9:	41 55                	push   %r13
ffffffff812bfcdb:	41 54                	push   %r12
ffffffff812bfcdd:	41 89 d7             	mov    %edx,%r15d
ffffffff812bfce0:	53                   	push   %rbx
ffffffff812bfce1:	49 89 fc             	mov    %rdi,%r12
ffffffff812bfce4:	89 f3                	mov    %esi,%ebx
ffffffff812bfce6:	45 89 cd             	mov    %r9d,%r13d
ffffffff812bfce9:	48 83 ec 20          	sub    $0x20,%rsp
ffffffff812bfced:	89 4d c8             	mov    %ecx,-0x38(%rbp)
ffffffff812bfcf0:	44 89 45 c0          	mov    %r8d,-0x40(%rbp)
	pte_t *epmd;	// epmd table
	pte_t *epte;	// epte table
	int i;		// index
	void *page;
#if DEBUG
	printk(KERN_INFO "add_to_ept: address = %lx, epgd = %lx, is_kernel = %d, is_ro = %d, is_rx = %d, make_write = %d\n", address, epgd, kernel, kernel_ro, kernel_rx, make_write);
ffffffff812bfcf4:	41 51                	push   %r9
ffffffff812bfcf6:	45 89 c1             	mov    %r8d,%r9d
ffffffff812bfcf9:	41 89 c8             	mov    %ecx,%r8d
ffffffff812bfcfc:	89 d1                	mov    %edx,%ecx
ffffffff812bfcfe:	48 89 fa             	mov    %rdi,%rdx
ffffffff812bfd01:	48 c7 c7 02 60 7b 81 	mov    $0xffffffff817b6002,%rdi
ffffffff812bfd08:	e8 85 32 17 00       	callq  ffffffff81432f92 <printk>

#if DEBUG
//        printk(KERN_INFO "add_to_ept: i = %d\n", i);
#endif

	if (!pte_present(epgd[i])) {
ffffffff812bfd0d:	89 d8                	mov    %ebx,%eax
ffffffff812bfd0f:	b1 27                	mov    $0x27,%cl
ffffffff812bfd11:	d3 f8                	sar    %cl,%eax
ffffffff812bfd13:	25 ff 01 00 00       	and    $0x1ff,%eax
ffffffff812bfd18:	4d 8d 34 c4          	lea    (%r12,%rax,8),%r14
ffffffff812bfd1c:	49 bc ff 0f 00 00 00 	movabs $0xffffc00000000fff,%r12
ffffffff812bfd23:	c0 ff ff 
ffffffff812bfd26:	58                   	pop    %rax
ffffffff812bfd27:	4d 23 26             	and    (%r14),%r12
ffffffff812bfd2a:	5a                   	pop    %rdx
ffffffff812bfd2b:	41 81 e4 01 01 00 00 	and    $0x101,%r12d
ffffffff812bfd32:	75 37                	jne    ffffffff812bfd6b <add_to_ept+0x9c>
		page = (void *) __get_free_page(GFP_AZK);
ffffffff812bfd34:	31 f6                	xor    %esi,%esi
ffffffff812bfd36:	bf 10 80 00 00       	mov    $0x8010,%edi
ffffffff812bfd3b:	e8 84 a3 e1 ff       	callq  ffffffff810da0c4 <__get_free_pages>
		if (!page) {
ffffffff812bfd40:	48 85 c0             	test   %rax,%rax
#if DEBUG
//        printk(KERN_INFO "add_to_ept: i = %d\n", i);
#endif

	if (!pte_present(epgd[i])) {
		page = (void *) __get_free_page(GFP_AZK);
ffffffff812bfd43:	48 89 c2             	mov    %rax,%rdx
		if (!page) {
#if DEBUG
        printk(KERN_INFO "add_to_ept: error 1\n");
ffffffff812bfd46:	48 c7 c7 64 60 7b 81 	mov    $0xffffffff817b6064,%rdi
//        printk(KERN_INFO "add_to_ept: i = %d\n", i);
#endif

	if (!pte_present(epgd[i])) {
		page = (void *) __get_free_page(GFP_AZK);
		if (!page) {
ffffffff812bfd4d:	74 76                	je     ffffffff812bfdc5 <add_to_ept+0xf6>
#endif

		goto ERROR;
		}

		memset(page, 0, PAGE_SIZE);
ffffffff812bfd4f:	48 89 c7             	mov    %rax,%rdi
ffffffff812bfd52:	b9 00 04 00 00       	mov    $0x400,%ecx
ffffffff812bfd57:	44 89 e0             	mov    %r12d,%eax
ffffffff812bfd5a:	f3 ab                	rep stos %eax,%es:(%rdi)
 *	this function
 */

static inline phys_addr_t virt_to_phys(volatile void *address)
{
	return __pa(address);
ffffffff812bfd5c:	48 89 d7             	mov    %rdx,%rdi
ffffffff812bfd5f:	e8 52 2b da ff       	callq  ffffffff810628b6 <__phys_addr>
		epgd[i] = __pte(virt_to_phys(page) | __EPTE_FULL);
ffffffff812bfd64:	48 83 c8 07          	or     $0x7,%rax
ffffffff812bfd68:	49 89 06             	mov    %rax,(%r14)
	}

	epud = (pte_t *)phys_to_virt(pte_val(epgd[i]) & PTE_PFN_MASK);
	i = ((address >> PUD_SHIFT) & (PTRS_PER_PUD - 1));

	if (!pte_present(epud[i])) {
ffffffff812bfd6b:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
ffffffff812bfd72:	3f 00 00 
ffffffff812bfd75:	49 23 06             	and    (%r14),%rax
ffffffff812bfd78:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
ffffffff812bfd7f:	88 ff ff 
ffffffff812bfd82:	49 bc ff 0f 00 00 00 	movabs $0xffffc00000000fff,%r12
ffffffff812bfd89:	c0 ff ff 
ffffffff812bfd8c:	48 01 d0             	add    %rdx,%rax
ffffffff812bfd8f:	89 da                	mov    %ebx,%edx
ffffffff812bfd91:	c1 fa 1e             	sar    $0x1e,%edx
ffffffff812bfd94:	81 e2 ff 01 00 00    	and    $0x1ff,%edx
ffffffff812bfd9a:	4c 8d 34 d0          	lea    (%rax,%rdx,8),%r14
ffffffff812bfd9e:	4d 23 26             	and    (%r14),%r12
ffffffff812bfda1:	41 81 e4 01 01 00 00 	and    $0x101,%r12d
ffffffff812bfda8:	75 43                	jne    ffffffff812bfded <add_to_ept+0x11e>
		page = (void *) __get_free_page(GFP_AZK);
ffffffff812bfdaa:	31 f6                	xor    %esi,%esi
ffffffff812bfdac:	bf 10 80 00 00       	mov    $0x8010,%edi
ffffffff812bfdb1:	e8 0e a3 e1 ff       	callq  ffffffff810da0c4 <__get_free_pages>
		if (!page) {
ffffffff812bfdb6:	48 85 c0             	test   %rax,%rax

	epud = (pte_t *)phys_to_virt(pte_val(epgd[i]) & PTE_PFN_MASK);
	i = ((address >> PUD_SHIFT) & (PTRS_PER_PUD - 1));

	if (!pte_present(epud[i])) {
		page = (void *) __get_free_page(GFP_AZK);
ffffffff812bfdb9:	48 89 c2             	mov    %rax,%rdx
		if (!page) {
ffffffff812bfdbc:	75 13                	jne    ffffffff812bfdd1 <add_to_ept+0x102>
#if DEBUG
        printk(KERN_INFO "add_to_ept: error 2\n");
ffffffff812bfdbe:	48 c7 c7 7b 60 7b 81 	mov    $0xffffffff817b607b,%rdi
ffffffff812bfdc5:	31 c0                	xor    %eax,%eax
ffffffff812bfdc7:	e8 c6 31 17 00       	callq  ffffffff81432f92 <printk>
#endif
			goto ERROR;
ffffffff812bfdcc:	e9 66 01 00 00       	jmpq   ffffffff812bff37 <add_to_ept+0x268>
		}

		memset(page, 0, PAGE_SIZE);
ffffffff812bfdd1:	48 89 c7             	mov    %rax,%rdi
ffffffff812bfdd4:	b9 00 04 00 00       	mov    $0x400,%ecx
ffffffff812bfdd9:	44 89 e0             	mov    %r12d,%eax
ffffffff812bfddc:	f3 ab                	rep stos %eax,%es:(%rdi)
ffffffff812bfdde:	48 89 d7             	mov    %rdx,%rdi
ffffffff812bfde1:	e8 d0 2a da ff       	callq  ffffffff810628b6 <__phys_addr>
		epud[i] = __pte(virt_to_phys(page) | __EPTE_FULL);
ffffffff812bfde6:	48 83 c8 07          	or     $0x7,%rax
ffffffff812bfdea:	49 89 06             	mov    %rax,(%r14)
	}

	epmd = (pte_t *)phys_to_virt(pte_val(epud[i]) & PTE_PFN_MASK);
	i = ((address >> PMD_SHIFT) & (PTRS_PER_PMD - 1));

	if (!pte_present(epmd[i])) {
ffffffff812bfded:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
ffffffff812bfdf4:	3f 00 00 
ffffffff812bfdf7:	49 23 06             	and    (%r14),%rax
ffffffff812bfdfa:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
ffffffff812bfe01:	88 ff ff 
ffffffff812bfe04:	49 bc ff 0f 00 00 00 	movabs $0xffffc00000000fff,%r12
ffffffff812bfe0b:	c0 ff ff 
ffffffff812bfe0e:	48 01 d0             	add    %rdx,%rax
ffffffff812bfe11:	89 da                	mov    %ebx,%edx
ffffffff812bfe13:	c1 fa 15             	sar    $0x15,%edx
ffffffff812bfe16:	81 e2 ff 01 00 00    	and    $0x1ff,%edx
ffffffff812bfe1c:	4c 8d 34 d0          	lea    (%rax,%rdx,8),%r14
ffffffff812bfe20:	4d 23 26             	and    (%r14),%r12
ffffffff812bfe23:	41 81 e4 01 01 00 00 	and    $0x101,%r12d
ffffffff812bfe2a:	75 3b                	jne    ffffffff812bfe67 <add_to_ept+0x198>
		page = (void *) __get_free_page(GFP_AZK);
ffffffff812bfe2c:	31 f6                	xor    %esi,%esi
ffffffff812bfe2e:	bf 10 80 00 00       	mov    $0x8010,%edi
ffffffff812bfe33:	e8 8c a2 e1 ff       	callq  ffffffff810da0c4 <__get_free_pages>
		if (!page) {
ffffffff812bfe38:	48 85 c0             	test   %rax,%rax

	epmd = (pte_t *)phys_to_virt(pte_val(epud[i]) & PTE_PFN_MASK);
	i = ((address >> PMD_SHIFT) & (PTRS_PER_PMD - 1));

	if (!pte_present(epmd[i])) {
		page = (void *) __get_free_page(GFP_AZK);
ffffffff812bfe3b:	48 89 c2             	mov    %rax,%rdx
		if (!page) {
#if DEBUG
        printk(KERN_INFO "add_to_ept: error 3\n");
ffffffff812bfe3e:	48 c7 c7 92 60 7b 81 	mov    $0xffffffff817b6092,%rdi
	epmd = (pte_t *)phys_to_virt(pte_val(epud[i]) & PTE_PFN_MASK);
	i = ((address >> PMD_SHIFT) & (PTRS_PER_PMD - 1));

	if (!pte_present(epmd[i])) {
		page = (void *) __get_free_page(GFP_AZK);
		if (!page) {
ffffffff812bfe45:	0f 84 7a ff ff ff    	je     ffffffff812bfdc5 <add_to_ept+0xf6>
        printk(KERN_INFO "add_to_ept: error 3\n");
#endif
			goto ERROR;
		}

		memset(page, 0, PAGE_SIZE);
ffffffff812bfe4b:	48 89 c7             	mov    %rax,%rdi
ffffffff812bfe4e:	b9 00 04 00 00       	mov    $0x400,%ecx
ffffffff812bfe53:	44 89 e0             	mov    %r12d,%eax
ffffffff812bfe56:	f3 ab                	rep stos %eax,%es:(%rdi)
ffffffff812bfe58:	48 89 d7             	mov    %rdx,%rdi
ffffffff812bfe5b:	e8 56 2a da ff       	callq  ffffffff810628b6 <__phys_addr>
		epmd[i] = __pte(virt_to_phys(page) | __EPTE_FULL);
ffffffff812bfe60:	48 83 c8 07          	or     $0x7,%rax
ffffffff812bfe64:	49 89 06             	mov    %rax,(%r14)
	}

	epte = (pte_t *)phys_to_virt(pte_val(epmd[i]) & PTE_PFN_MASK);
	i = pte_index(address);

	if (!pte_present(epte[i])) {
ffffffff812bfe67:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
ffffffff812bfe6e:	3f 00 00 
ffffffff812bfe71:	49 23 16             	and    (%r14),%rdx
		memset(page, 0, PAGE_SIZE);
		epmd[i] = __pte(virt_to_phys(page) | __EPTE_FULL);
	}

	epte = (pte_t *)phys_to_virt(pte_val(epmd[i]) & PTE_PFN_MASK);
	i = pte_index(address);
ffffffff812bfe74:	4c 63 c3             	movslq %ebx,%r8

	if (!pte_present(epte[i])) {
ffffffff812bfe77:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
ffffffff812bfe7e:	88 ff ff 
	return a.pte == b.pte;
}

static inline int pte_present(pte_t a)
{
	return pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE);
ffffffff812bfe81:	49 bc ff 0f 00 00 00 	movabs $0xffffc00000000fff,%r12
ffffffff812bfe88:	c0 ff ff 
ffffffff812bfe8b:	48 8d 04 0a          	lea    (%rdx,%rcx,1),%rax
ffffffff812bfe8f:	4c 89 c2             	mov    %r8,%rdx
ffffffff812bfe92:	48 c1 ea 09          	shr    $0x9,%rdx
ffffffff812bfe96:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
ffffffff812bfe9c:	4c 8d 34 10          	lea    (%rax,%rdx,1),%r14
ffffffff812bfea0:	4d 23 26             	and    (%r14),%r12
ffffffff812bfea3:	41 81 e4 01 01 00 00 	and    $0x101,%r12d
ffffffff812bfeaa:	75 26                	jne    ffffffff812bfed2 <add_to_ept+0x203>
		if(kernel) {
ffffffff812bfeac:	45 85 ff             	test   %r15d,%r15d
ffffffff812bfeaf:	74 16                	je     ffffffff812bfec7 <add_to_ept+0x1f8>
			if(kernel_rx)
ffffffff812bfeb1:	83 7d c0 00          	cmpl   $0x0,-0x40(%rbp)
ffffffff812bfeb5:	74 05                	je     ffffffff812bfebc <add_to_ept+0x1ed>
				epte[i] = __pte(address | __EPTE_READ | __EPTE_EXEC | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);
ffffffff812bfeb7:	83 cb 75             	or     $0x75,%ebx
ffffffff812bfeba:	eb 0e                	jmp    ffffffff812bfeca <add_to_ept+0x1fb>
			else if(kernel_ro)
ffffffff812bfebc:	83 7d c8 00          	cmpl   $0x0,-0x38(%rbp)
ffffffff812bfec0:	74 05                	je     ffffffff812bfec7 <add_to_ept+0x1f8>
				epte[i] = __pte(address | __EPTE_READ | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);
ffffffff812bfec2:	83 cb 71             	or     $0x71,%ebx
ffffffff812bfec5:	eb 03                	jmp    ffffffff812bfeca <add_to_ept+0x1fb>
			else
				epte[i] = __pte(address | __EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);
		}
		else
			epte[i] = __pte(address | __EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);
ffffffff812bfec7:	83 cb 77             	or     $0x77,%ebx
ffffffff812bfeca:	48 63 db             	movslq %ebx,%rbx
ffffffff812bfecd:	49 89 1e             	mov    %rbx,(%r14)
ffffffff812bfed0:	eb 79                	jmp    ffffffff812bff4b <add_to_ept+0x27c>
		goto ERROR;
		}
		memcpy(page, __va(address), PAGE_SIZE);
		epte[i] = __pte(__pa(page) | __EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);	
	}
	return 0;
ffffffff812bfed2:	45 31 e4             	xor    %r12d,%r12d
				epte[i] = __pte(address | __EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);
		}
		else
			epte[i] = __pte(address | __EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);
	}
	else if(make_write){
ffffffff812bfed5:	45 85 ed             	test   %r13d,%r13d
ffffffff812bfed8:	74 71                	je     ffffffff812bff4b <add_to_ept+0x27c>
#if DEBUG
        printk(KERN_INFO "add_to_ept: count is %d\n", count++);
ffffffff812bfeda:	8b 35 a4 18 8f 00    	mov    0x8f18a4(%rip),%esi        # ffffffff81bb1784 <count>
ffffffff812bfee0:	48 c7 c7 a9 60 7b 81 	mov    $0xffffffff817b60a9,%rdi
ffffffff812bfee7:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
ffffffff812bfeeb:	4c 89 45 c8          	mov    %r8,-0x38(%rbp)
ffffffff812bfeef:	8d 46 01             	lea    0x1(%rsi),%eax
ffffffff812bfef2:	89 05 8c 18 8f 00    	mov    %eax,0x8f188c(%rip)        # ffffffff81bb1784 <count>
ffffffff812bfef8:	31 c0                	xor    %eax,%eax
ffffffff812bfefa:	e8 93 30 17 00       	callq  ffffffff81432f92 <printk>
#if DEBUG
//        printk(KERN_INFO "add_to_ept: error 4\n");
#endif
//		goto ERROR;
//}
		page = (void *) __get_free_page(GFP_AZK);
ffffffff812bfeff:	31 f6                	xor    %esi,%esi
ffffffff812bff01:	bf 10 80 00 00       	mov    $0x8010,%edi
ffffffff812bff06:	e8 b9 a1 e1 ff       	callq  ffffffff810da0c4 <__get_free_pages>
		if(!page){
ffffffff812bff0b:	48 85 c0             	test   %rax,%rax
ffffffff812bff0e:	74 27                	je     ffffffff812bff37 <add_to_ept+0x268>
		goto ERROR;
		}
		memcpy(page, __va(address), PAGE_SIZE);
ffffffff812bff10:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
ffffffff812bff14:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
ffffffff812bff18:	48 89 c7             	mov    %rax,%rdi
ffffffff812bff1b:	49 8d 34 08          	lea    (%r8,%rcx,1),%rsi
ffffffff812bff1f:	b9 00 04 00 00       	mov    $0x400,%ecx
ffffffff812bff24:	f3 a5                	rep movsl %ds:(%rsi),%es:(%rdi)
		epte[i] = __pte(__pa(page) | __EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);	
ffffffff812bff26:	48 89 c7             	mov    %rax,%rdi
ffffffff812bff29:	e8 88 29 da ff       	callq  ffffffff810628b6 <__phys_addr>
ffffffff812bff2e:	48 83 c8 77          	or     $0x77,%rax
ffffffff812bff32:	49 89 06             	mov    %rax,(%r14)
ffffffff812bff35:	eb 14                	jmp    ffffffff812bff4b <add_to_ept+0x27c>
	}
	return 0;

ERROR:
	printk(KERN_ERR "add_to_ept: error in ept\n");
ffffffff812bff37:	48 c7 c7 c4 60 7b 81 	mov    $0xffffffff817b60c4,%rdi
ffffffff812bff3e:	31 c0                	xor    %eax,%eax
	// TODO: free_ept_
	return -ENOMEM;
ffffffff812bff40:	41 bc f4 ff ff ff    	mov    $0xfffffff4,%r12d
		epte[i] = __pte(__pa(page) | __EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT);	
	}
	return 0;

ERROR:
	printk(KERN_ERR "add_to_ept: error in ept\n");
ffffffff812bff46:	e8 47 30 17 00       	callq  ffffffff81432f92 <printk>
	// TODO: free_ept_
	return -ENOMEM;

}
ffffffff812bff4b:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812bff4f:	44 89 e0             	mov    %r12d,%eax
ffffffff812bff52:	5b                   	pop    %rbx
ffffffff812bff53:	41 5c                	pop    %r12
ffffffff812bff55:	41 5d                	pop    %r13
ffffffff812bff57:	41 5e                	pop    %r14
ffffffff812bff59:	41 5f                	pop    %r15
ffffffff812bff5b:	5d                   	pop    %rbp
ffffffff812bff5c:	c3                   	retq   

ffffffff812bff5d <free_ept_page>:

	return ept_lookup_gpa(vcpu, gpa, level, create, epte_out);
}

static void free_ept_page(epte_t epte)
{
ffffffff812bff5d:	55                   	push   %rbp
	struct page *page = pfn_to_page(epte_addr(epte) >> PAGE_SHIFT);
ffffffff812bff5e:	48 b8 00 00 00 00 00 	movabs $0xffffea0000000000,%rax
ffffffff812bff65:	ea ff ff 

	return ept_lookup_gpa(vcpu, gpa, level, create, epte_out);
}

static void free_ept_page(epte_t epte)
{
ffffffff812bff68:	48 89 e5             	mov    %rsp,%rbp
ffffffff812bff6b:	53                   	push   %rbx
	struct page *page = pfn_to_page(epte_addr(epte) >> PAGE_SHIFT);
ffffffff812bff6c:	48 89 fb             	mov    %rdi,%rbx
ffffffff812bff6f:	48 c1 eb 0c          	shr    $0xc,%rbx

	return ept_lookup_gpa(vcpu, gpa, level, create, epte_out);
}

static void free_ept_page(epte_t epte)
{
ffffffff812bff73:	52                   	push   %rdx
	struct page *page = pfn_to_page(epte_addr(epte) >> PAGE_SHIFT);
ffffffff812bff74:	48 c1 e3 06          	shl    $0x6,%rbx
ffffffff812bff78:	48 01 c3             	add    %rax,%rbx

	if (epte & __EPTE_WRITE)
ffffffff812bff7b:	40 80 e7 02          	and    $0x2,%dil
ffffffff812bff7f:	74 08                	je     ffffffff812bff89 <free_ept_page+0x2c>
		set_page_dirty_lock(page);
ffffffff812bff81:	48 89 df             	mov    %rbx,%rdi
ffffffff812bff84:	e8 b8 a9 e1 ff       	callq  ffffffff810da941 <set_page_dirty_lock>
	put_page(page);
ffffffff812bff89:	48 89 df             	mov    %rbx,%rdi
ffffffff812bff8c:	e8 d5 d3 e1 ff       	callq  ffffffff810dd366 <put_page>
}
ffffffff812bff91:	58                   	pop    %rax
ffffffff812bff92:	5b                   	pop    %rbx
ffffffff812bff93:	5d                   	pop    %rbp
ffffffff812bff94:	c3                   	retq   

ffffffff812bff95 <vmx_free_ept>:

static void vmx_free_ept(unsigned long ept_root)
{
ffffffff812bff95:	55                   	push   %rbp
ffffffff812bff96:	48 89 e5             	mov    %rsp,%rbp
ffffffff812bff99:	41 57                	push   %r15
ffffffff812bff9b:	41 56                	push   %r14
ffffffff812bff9d:	41 55                	push   %r13
ffffffff812bff9f:	41 54                	push   %r12
ffffffff812bffa1:	53                   	push   %rbx
	epte_t *pgd = (epte_t *) __va(ept_root);
ffffffff812bffa2:	48 bb 00 00 00 00 00 	movabs $0xffff880000000000,%rbx
ffffffff812bffa9:	88 ff ff 
ffffffff812bffac:	48 8d 04 1f          	lea    (%rdi,%rbx,1),%rax
		set_page_dirty_lock(page);
	put_page(page);
}

static void vmx_free_ept(unsigned long ept_root)
{
ffffffff812bffb0:	48 83 ec 38          	sub    $0x38,%rsp
	epte_t *pgd = (epte_t *) __va(ept_root);
ffffffff812bffb4:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
ffffffff812bffb8:	48 b8 00 10 00 00 00 	movabs $0xffff880000001000,%rax
ffffffff812bffbf:	88 ff ff 
ffffffff812bffc2:	4c 8b 65 c8          	mov    -0x38(%rbp),%r12
ffffffff812bffc6:	48 01 f8             	add    %rdi,%rax
ffffffff812bffc9:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
	int i, j, k, l;

	for (i = 0; i < PTRS_PER_PGD; i++) {
		epte_t *pud = (epte_t *) epte_page_vaddr(pgd[i]);
ffffffff812bffcd:	49 8b 04 24          	mov    (%r12),%rax
		if (!epte_present(pgd[i]))
ffffffff812bffd1:	a8 07                	test   $0x7,%al
ffffffff812bffd3:	0f 84 9c 00 00 00    	je     ffffffff812c0075 <vmx_free_ept+0xe0>
#define EPTE_ADDR	(~(PAGE_SIZE - 1))
#define EPTE_FLAGS	(PAGE_SIZE - 1)

static inline uintptr_t epte_addr(epte_t epte)
{
	return (epte & EPTE_ADDR);
ffffffff812bffd9:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
ffffffff812bffdf:	48 ba 00 10 00 00 00 	movabs $0xffff880000001000,%rdx
ffffffff812bffe6:	88 ff ff 
}

static inline uintptr_t epte_page_vaddr(epte_t epte)
{
	return (uintptr_t) __va(epte_addr(epte));
ffffffff812bffe9:	48 8d 0c 18          	lea    (%rax,%rbx,1),%rcx
ffffffff812bffed:	48 01 d0             	add    %rdx,%rax
ffffffff812bfff0:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
ffffffff812bfff4:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
ffffffff812bfff8:	49 89 cd             	mov    %rcx,%r13
		epte_t *pud = (epte_t *) epte_page_vaddr(pgd[i]);
		if (!epte_present(pgd[i]))
			continue;

		for (j = 0; j < PTRS_PER_PUD; j++) {
			epte_t *pmd = (epte_t *) epte_page_vaddr(pud[j]);
ffffffff812bfffb:	49 8b 45 00          	mov    0x0(%r13),%rax
			if (!epte_present(pud[j]))
ffffffff812bffff:	a8 07                	test   $0x7,%al
ffffffff812c0001:	74 5d                	je     ffffffff812c0060 <vmx_free_ept+0xcb>
				continue;
			if (epte_flags(pud[j]) & __EPTE_SZ)
ffffffff812c0003:	a8 80                	test   $0x80,%al
ffffffff812c0005:	75 59                	jne    ffffffff812c0060 <vmx_free_ept+0xcb>
#define EPTE_ADDR	(~(PAGE_SIZE - 1))
#define EPTE_FLAGS	(PAGE_SIZE - 1)

static inline uintptr_t epte_addr(epte_t epte)
{
	return (epte & EPTE_ADDR);
ffffffff812c0007:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
ffffffff812c000d:	48 ba 00 10 00 00 00 	movabs $0xffff880000001000,%rdx
ffffffff812c0014:	88 ff ff 
}

static inline uintptr_t epte_page_vaddr(epte_t epte)
{
	return (uintptr_t) __va(epte_addr(epte));
ffffffff812c0017:	4c 8d 3c 18          	lea    (%rax,%rbx,1),%r15
ffffffff812c001b:	48 01 d0             	add    %rdx,%rax
ffffffff812c001e:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812c0022:	4d 89 fe             	mov    %r15,%r14
				continue;
			if (epte_flags(pud[j]) & __EPTE_SZ)
				continue;

			for (k = 0; k < PTRS_PER_PMD; k++) {
				epte_t *pte = (epte_t *) epte_page_vaddr(pmd[k]);
ffffffff812c0025:	49 8b 3e             	mov    (%r14),%rdi
				if (!epte_present(pmd[k]))
ffffffff812c0028:	40 f6 c7 07          	test   $0x7,%dil
ffffffff812c002c:	74 1e                	je     ffffffff812c004c <vmx_free_ept+0xb7>
					continue;
				if (epte_flags(pmd[k]) & __EPTE_SZ) {
ffffffff812c002e:	40 f6 c7 80          	test   $0x80,%dil
ffffffff812c0032:	74 07                	je     ffffffff812c003b <vmx_free_ept+0xa6>
					free_ept_page(pmd[k]);
ffffffff812c0034:	e8 24 ff ff ff       	callq  ffffffff812bff5d <free_ept_page>
					continue;
ffffffff812c0039:	eb 11                	jmp    ffffffff812c004c <vmx_free_ept+0xb7>
						continue;

					//free_ept_page(pte[l]);
				}

				free_page((unsigned long) pte);
ffffffff812c003b:	48 81 e7 00 f0 ff ff 	and    $0xfffffffffffff000,%rdi
ffffffff812c0042:	31 f6                	xor    %esi,%esi
ffffffff812c0044:	48 01 df             	add    %rbx,%rdi
ffffffff812c0047:	e8 06 8f e1 ff       	callq  ffffffff810d8f52 <free_pages>
ffffffff812c004c:	49 83 c6 08          	add    $0x8,%r14
			if (!epte_present(pud[j]))
				continue;
			if (epte_flags(pud[j]) & __EPTE_SZ)
				continue;

			for (k = 0; k < PTRS_PER_PMD; k++) {
ffffffff812c0050:	4c 39 75 b8          	cmp    %r14,-0x48(%rbp)
ffffffff812c0054:	75 cf                	jne    ffffffff812c0025 <vmx_free_ept+0x90>
				}

				free_page((unsigned long) pte);
			}

			free_page((unsigned long) pmd);
ffffffff812c0056:	31 f6                	xor    %esi,%esi
ffffffff812c0058:	4c 89 ff             	mov    %r15,%rdi
ffffffff812c005b:	e8 f2 8e e1 ff       	callq  ffffffff810d8f52 <free_pages>
ffffffff812c0060:	49 83 c5 08          	add    $0x8,%r13
	for (i = 0; i < PTRS_PER_PGD; i++) {
		epte_t *pud = (epte_t *) epte_page_vaddr(pgd[i]);
		if (!epte_present(pgd[i]))
			continue;

		for (j = 0; j < PTRS_PER_PUD; j++) {
ffffffff812c0064:	4c 3b 6d a8          	cmp    -0x58(%rbp),%r13
ffffffff812c0068:	75 91                	jne    ffffffff812bfffb <vmx_free_ept+0x66>
			}

			free_page((unsigned long) pmd);
		}

		free_page((unsigned long) pud);
ffffffff812c006a:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
ffffffff812c006e:	31 f6                	xor    %esi,%esi
ffffffff812c0070:	e8 dd 8e e1 ff       	callq  ffffffff810d8f52 <free_pages>
ffffffff812c0075:	49 83 c4 08          	add    $0x8,%r12
static void vmx_free_ept(unsigned long ept_root)
{
	epte_t *pgd = (epte_t *) __va(ept_root);
	int i, j, k, l;

	for (i = 0; i < PTRS_PER_PGD; i++) {
ffffffff812c0079:	4c 3b 65 b0          	cmp    -0x50(%rbp),%r12
ffffffff812c007d:	0f 85 4a ff ff ff    	jne    ffffffff812bffcd <vmx_free_ept+0x38>
		}

		free_page((unsigned long) pud);
	}

	free_page((unsigned long) pgd);
ffffffff812c0083:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
ffffffff812c0087:	31 f6                	xor    %esi,%esi
ffffffff812c0089:	e8 c4 8e e1 ff       	callq  ffffffff810d8f52 <free_pages>
}
ffffffff812c008e:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812c0092:	5b                   	pop    %rbx
ffffffff812c0093:	41 5c                	pop    %r12
ffffffff812c0095:	41 5d                	pop    %r13
ffffffff812c0097:	41 5e                	pop    %r14
ffffffff812c0099:	41 5f                	pop    %r15
ffffffff812c009b:	5d                   	pop    %rbp
ffffffff812c009c:	c3                   	retq   

ffffffff812c009d <ept_lookup_gpa.isra.1.constprop.3>:

#define ADDR_TO_IDX(la, n) \
	((((unsigned long) (la)) >> (12 + 9 * (n))) & ((1 << 9) - 1))

	static int
ept_lookup_gpa(struct vmx_vcpu *vcpu, void *gpa, int level,
ffffffff812c009d:	55                   	push   %rbp
		int create, epte_t **epte_out)
{
	int i;
	epte_t paddr;
	epte_t *dir = (epte_t *) __va(vcpu->ept_root);
ffffffff812c009e:	48 b8 00 00 00 00 00 	movabs $0xffff880000000000,%rax
ffffffff812c00a5:	88 ff ff 
	*epte_out = 0;
ffffffff812c00a8:	48 c7 02 00 00 00 00 	movq   $0x0,(%rdx)
ept_lookup_gpa(struct vmx_vcpu *vcpu, void *gpa, int level,
		int create, epte_t **epte_out)
{
	int i;
	epte_t paddr;
	epte_t *dir = (epte_t *) __va(vcpu->ept_root);
ffffffff812c00af:	48 01 c7             	add    %rax,%rdi
	*epte_out = 0;
ffffffff812c00b2:	b9 27 00 00 00       	mov    $0x27,%ecx

	for (i = EPT_LEVELS - 1; i > level; i--) {
ffffffff812c00b7:	41 b8 03 00 00 00    	mov    $0x3,%r8d

#define ADDR_TO_IDX(la, n) \
	((((unsigned long) (la)) >> (12 + 9 * (n))) & ((1 << 9) - 1))

	static int
ept_lookup_gpa(struct vmx_vcpu *vcpu, void *gpa, int level,
ffffffff812c00bd:	48 89 e5             	mov    %rsp,%rbp
				return -EINVAL;
			level = i;
			break;
		}

		dir = (epte_t *) epte_page_vaddr(dir[idx]);
ffffffff812c00c0:	49 89 c1             	mov    %rax,%r9
	*epte_out = 0;

	for (i = EPT_LEVELS - 1; i > level; i--) {
		int idx = ADDR_TO_IDX(gpa, i);

		if (!epte_present(dir[idx])) {
ffffffff812c00c3:	48 89 f0             	mov    %rsi,%rax
ffffffff812c00c6:	48 d3 e8             	shr    %cl,%rax
ffffffff812c00c9:	25 ff 01 00 00       	and    $0x1ff,%eax
ffffffff812c00ce:	48 8b 04 c7          	mov    (%rdi,%rax,8),%rax
ffffffff812c00d2:	a8 07                	test   $0x7,%al
ffffffff812c00d4:	74 3e                	je     ffffffff812c0114 <ept_lookup_gpa.isra.1.constprop.3+0x77>
			else
				dir[idx] = epte_addr(virt_to_phys(page)) |
					__EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT;
		}

		if (epte_big(dir[idx])) {
ffffffff812c00d6:	a8 80                	test   $0x80,%al
ffffffff812c00d8:	74 0d                	je     ffffffff812c00e7 <ept_lookup_gpa.isra.1.constprop.3+0x4a>
			if (i != 1)
ffffffff812c00da:	41 83 f8 01          	cmp    $0x1,%r8d
				return -EINVAL;
ffffffff812c00de:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
				dir[idx] = epte_addr(virt_to_phys(page)) |
					__EPTE_FULL | __EPTE_TYPE(EPTE_TYPE_WB) | __EPTE_IPAT;
		}

		if (epte_big(dir[idx])) {
			if (i != 1)
ffffffff812c00e3:	74 14                	je     ffffffff812c00f9 <ept_lookup_gpa.isra.1.constprop.3+0x5c>
ffffffff812c00e5:	eb 32                	jmp    ffffffff812c0119 <ept_lookup_gpa.isra.1.constprop.3+0x7c>
				return -EINVAL;
			level = i;
			break;
		}

		dir = (epte_t *) epte_page_vaddr(dir[idx]);
ffffffff812c00e7:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
ffffffff812c00ed:	83 e9 09             	sub    $0x9,%ecx
	int i;
	epte_t paddr;
	epte_t *dir = (epte_t *) __va(vcpu->ept_root);
	*epte_out = 0;

	for (i = EPT_LEVELS - 1; i > level; i--) {
ffffffff812c00f0:	41 ff c8             	dec    %r8d
				return -EINVAL;
			level = i;
			break;
		}

		dir = (epte_t *) epte_page_vaddr(dir[idx]);
ffffffff812c00f3:	4a 8d 3c 08          	lea    (%rax,%r9,1),%rdi
	int i;
	epte_t paddr;
	epte_t *dir = (epte_t *) __va(vcpu->ept_root);
	*epte_out = 0;

	for (i = EPT_LEVELS - 1; i > level; i--) {
ffffffff812c00f7:	75 ca                	jne    ffffffff812c00c3 <ept_lookup_gpa.isra.1.constprop.3+0x26>
		}

		dir = (epte_t *) epte_page_vaddr(dir[idx]);
	}

	*epte_out = &dir[ADDR_TO_IDX(gpa, level)];
ffffffff812c00f9:	43 8d 0c c0          	lea    (%r8,%r8,8),%ecx
ffffffff812c00fd:	83 c1 0c             	add    $0xc,%ecx
ffffffff812c0100:	48 d3 ee             	shr    %cl,%rsi
ffffffff812c0103:	81 e6 ff 01 00 00    	and    $0x1ff,%esi
ffffffff812c0109:	48 8d 04 f7          	lea    (%rdi,%rsi,8),%rax
ffffffff812c010d:	48 89 02             	mov    %rax,(%rdx)
	return 0;
ffffffff812c0110:	31 c0                	xor    %eax,%eax
ffffffff812c0112:	eb 05                	jmp    ffffffff812c0119 <ept_lookup_gpa.isra.1.constprop.3+0x7c>

		if (!epte_present(dir[idx])) {
			void *page;

			if (!create)
				return -ENOENT;
ffffffff812c0114:	b8 fe ff ff ff       	mov    $0xfffffffe,%eax
		dir = (epte_t *) epte_page_vaddr(dir[idx]);
	}

	*epte_out = &dir[ADDR_TO_IDX(gpa, level)];
	return 0;
}
ffffffff812c0119:	5d                   	pop    %rbp
ffffffff812c011a:	c3                   	retq   

ffffffff812c011b <hva_to_gpa.isra.0>:
static inline int epte_big(epte_t epte)
{
	return (epte & __EPTE_SZ) > 0;
}

static pte_t hva_to_gpa(struct vmx_vcpu *vcpu,
ffffffff812c011b:	48 89 f0             	mov    %rsi,%rax
ffffffff812c011e:	55                   	push   %rbp
ffffffff812c011f:	48 c1 e8 27          	shr    $0x27,%rax
ffffffff812c0123:	25 ff 01 00 00       	and    $0x1ff,%eax
ffffffff812c0128:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c012b:	48 8b 14 c7          	mov    (%rdi,%rax,8),%rdx

	page = NULL;
     	
	pgd = pgd_offset(mm, addr);
	// pgd_none() returns 1 if the entry does not exist, helps to check if a valid page table is being looked on
     	if (pgd_none(*pgd) || pgd_bad(*pgd))
ffffffff812c012f:	48 85 d2             	test   %rdx,%rdx
ffffffff812c0132:	0f 84 91 00 00 00    	je     ffffffff812c01c9 <hva_to_gpa.isra.0+0xae>
ffffffff812c0138:	48 b9 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rcx
ffffffff812c013f:	c0 ff ff 
ffffffff812c0142:	48 89 d0             	mov    %rdx,%rax
ffffffff812c0145:	48 21 c8             	and    %rcx,%rax
ffffffff812c0148:	48 83 f8 63          	cmp    $0x63,%rax
ffffffff812c014c:	75 7b                	jne    ffffffff812c01c9 <hva_to_gpa.isra.0+0xae>
ffffffff812c014e:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c0151:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
ffffffff812c0158:	3f 00 00 
ffffffff812c015b:	48 c1 ef 1b          	shr    $0x1b,%rdi
ffffffff812c015f:	48 21 c2             	and    %rax,%rdx
ffffffff812c0162:	81 e7 f8 0f 00 00    	and    $0xff8,%edi
ffffffff812c0168:	48 01 fa             	add    %rdi,%rdx
ffffffff812c016b:	48 bf 00 00 00 00 00 	movabs $0xffff880000000000,%rdi
ffffffff812c0172:	88 ff ff 
ffffffff812c0175:	48 8b 14 3a          	mov    (%rdx,%rdi,1),%rdx
                 goto out;

        pud = pud_offset(pgd, addr);
        if (pud_none(*pud) || pud_bad(*pud))
ffffffff812c0179:	48 85 d2             	test   %rdx,%rdx
ffffffff812c017c:	74 4b                	je     ffffffff812c01c9 <hva_to_gpa.isra.0+0xae>
ffffffff812c017e:	49 b8 98 0f 00 00 00 	movabs $0xffffc00000000f98,%r8
ffffffff812c0185:	c0 ff ff 
ffffffff812c0188:	4c 85 c2             	test   %r8,%rdx
ffffffff812c018b:	75 3c                	jne    ffffffff812c01c9 <hva_to_gpa.isra.0+0xae>
ffffffff812c018d:	49 89 f0             	mov    %rsi,%r8
ffffffff812c0190:	48 21 c2             	and    %rax,%rdx
ffffffff812c0193:	49 c1 e8 12          	shr    $0x12,%r8
ffffffff812c0197:	41 81 e0 f8 0f 00 00 	and    $0xff8,%r8d
ffffffff812c019e:	4c 01 c2             	add    %r8,%rdx
ffffffff812c01a1:	48 8b 14 3a          	mov    (%rdx,%rdi,1),%rdx
        	goto out;

        pmd = pmd_offset(pud, addr);
        if (pmd_none(*pmd) || pmd_bad(*pmd))
ffffffff812c01a5:	48 85 d2             	test   %rdx,%rdx
ffffffff812c01a8:	74 1f                	je     ffffffff812c01c9 <hva_to_gpa.isra.0+0xae>
ffffffff812c01aa:	48 21 d1             	and    %rdx,%rcx
ffffffff812c01ad:	48 83 f9 63          	cmp    $0x63,%rcx
ffffffff812c01b1:	75 16                	jne    ffffffff812c01c9 <hva_to_gpa.isra.0+0xae>

        ptep = pte_offset_map(pmd, addr);
	if (!ptep)
		goto out;

        pte = *ptep;
ffffffff812c01b3:	48 c1 ee 09          	shr    $0x9,%rsi
ffffffff812c01b7:	48 21 d0             	and    %rdx,%rax
ffffffff812c01ba:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
ffffffff812c01c0:	48 01 c6             	add    %rax,%rsi
        page = pte_page(pte);
	address = (page_to_pfn(page)) << PAGE_SHIFT;
	return pte;
ffffffff812c01c3:	48 8b 04 3e          	mov    (%rsi,%rdi,1),%rax
ffffffff812c01c7:	eb 02                	jmp    ffffffff812c01cb <hva_to_gpa.isra.0+0xb0>
ffffffff812c01c9:	31 c0                	xor    %eax,%eax
		return GPA_ADDR_INVAL;
	return addr;
#endif


}
ffffffff812c01cb:	5d                   	pop    %rbp
ffffffff812c01cc:	c3                   	retq   

ffffffff812c01cd <ept_invalidate_page>:
 * Returns 1 if the page was removed, 0 otherwise
 */
static int ept_invalidate_page(struct vmx_vcpu *vcpu,
		struct mm_struct *mm,
		unsigned long addr)
{
ffffffff812c01cd:	55                   	push   %rbp
ffffffff812c01ce:	49 89 d1             	mov    %rdx,%r9
ffffffff812c01d1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c01d4:	41 57                	push   %r15
ffffffff812c01d6:	41 56                	push   %r14
ffffffff812c01d8:	41 55                	push   %r13
ffffffff812c01da:	41 54                	push   %r12
ffffffff812c01dc:	49 89 fc             	mov    %rdi,%r12
ffffffff812c01df:	53                   	push   %rbx
ffffffff812c01e0:	48 83 ec 18          	sub    $0x18,%rsp
	int ret;
	epte_t *epte;
	void *gpa = (void *) pte_val(hva_to_gpa(vcpu, mm, (unsigned long) addr));
ffffffff812c01e4:	48 8b 7e 40          	mov    0x40(%rsi),%rdi
ffffffff812c01e8:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c01eb:	e8 2b ff ff ff       	callq  ffffffff812c011b <hva_to_gpa.isra.0>
ffffffff812c01f0:	49 89 c5             	mov    %rax,%r13

	if (gpa == (void *) GPA_ADDR_INVAL) {
ffffffff812c01f3:	48 b8 00 00 00 00 0c 	movabs $0xc00000000,%rax
ffffffff812c01fa:	00 00 00 
ffffffff812c01fd:	49 39 c5             	cmp    %rax,%r13
ffffffff812c0200:	75 13                	jne    ffffffff812c0215 <ept_invalidate_page+0x48>
		printk(KERN_ERR "ept: hva %lx is out of range\n", addr);
ffffffff812c0202:	4c 89 ce             	mov    %r9,%rsi
ffffffff812c0205:	48 c7 c7 e0 60 7b 81 	mov    $0xffffffff817b60e0,%rdi
ffffffff812c020c:	31 c0                	xor    %eax,%eax
ffffffff812c020e:	e8 7f 2d 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c0213:	eb 2a                	jmp    ffffffff812c023f <ept_invalidate_page+0x72>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c0215:	4d 8d 74 24 28       	lea    0x28(%r12),%r14
ffffffff812c021a:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c021d:	e8 0e 84 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
		return 0;
	}

	spin_lock(&vcpu->ept_lock);
	ret = ept_lookup_gpa(vcpu, (void *) gpa, 0, 0, &epte);
ffffffff812c0222:	49 8b 7c 24 40       	mov    0x40(%r12),%rdi
ffffffff812c0227:	48 8d 55 c8          	lea    -0x38(%rbp),%rdx
ffffffff812c022b:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c022e:	e8 6a fe ff ff       	callq  ffffffff812c009d <ept_lookup_gpa.isra.1.constprop.3>
	if (ret) {
ffffffff812c0233:	85 c0                	test   %eax,%eax
ffffffff812c0235:	74 0c                	je     ffffffff812c0243 <ept_invalidate_page+0x76>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c0237:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c023a:	e8 6d 84 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
		spin_unlock(&vcpu->ept_lock);
		return 0;
ffffffff812c023f:	31 db                	xor    %ebx,%ebx
ffffffff812c0241:	eb 3b                	jmp    ffffffff812c027e <ept_invalidate_page+0xb1>
	}

	ret = ept_clear_epte(epte);
ffffffff812c0243:	4c 8b 7d c8          	mov    -0x38(%rbp),%r15
}

static int ept_clear_epte(epte_t *epte)
{
	if (*epte == __EPTE_NONE)
		return 0;
ffffffff812c0247:	31 db                	xor    %ebx,%ebx
	free_page((unsigned long) pgd);
}

static int ept_clear_epte(epte_t *epte)
{
	if (*epte == __EPTE_NONE)
ffffffff812c0249:	49 8b 3f             	mov    (%r15),%rdi
ffffffff812c024c:	48 85 ff             	test   %rdi,%rdi
ffffffff812c024f:	74 11                	je     ffffffff812c0262 <ept_invalidate_page+0x95>
		return 0;

	free_ept_page(*epte);
ffffffff812c0251:	e8 07 fd ff ff       	callq  ffffffff812bff5d <free_ept_page>
	*epte = __EPTE_NONE;

	return 1;
ffffffff812c0256:	bb 01 00 00 00       	mov    $0x1,%ebx
{
	if (*epte == __EPTE_NONE)
		return 0;

	free_ept_page(*epte);
	*epte = __EPTE_NONE;
ffffffff812c025b:	49 c7 07 00 00 00 00 	movq   $0x0,(%r15)
ffffffff812c0262:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c0265:	e8 42 84 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	}

	ret = ept_clear_epte(epte);
	spin_unlock(&vcpu->ept_lock);

	if (ret)
ffffffff812c026a:	85 db                	test   %ebx,%ebx
ffffffff812c026c:	74 10                	je     ffffffff812c027e <ept_invalidate_page+0xb1>
		vmx_ept_sync_individual_addr(vcpu, (gpa_t) gpa);
ffffffff812c026e:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c0271:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0274:	bb 01 00 00 00       	mov    $0x1,%ebx
ffffffff812c0279:	e8 5e e7 ff ff       	callq  ffffffff812be9dc <vmx_ept_sync_individual_addr>

	return ret;
}
ffffffff812c027e:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c0282:	89 d8                	mov    %ebx,%eax
ffffffff812c0284:	5b                   	pop    %rbx
ffffffff812c0285:	41 5c                	pop    %r12
ffffffff812c0287:	41 5d                	pop    %r13
ffffffff812c0289:	41 5e                	pop    %r14
ffffffff812c028b:	41 5f                	pop    %r15
ffffffff812c028d:	5d                   	pop    %rbp
ffffffff812c028e:	c3                   	retq   

ffffffff812c028f <ept_mmu_notifier_invalidate_page>:
}

static void ept_mmu_notifier_invalidate_page(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long address)
{
ffffffff812c028f:	55                   	push   %rbp
	struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);

	pr_debug("ept: invalidate_page addr %lx\n", address);

	ept_invalidate_page(vcpu, mm, address);
ffffffff812c0290:	48 83 ef 10          	sub    $0x10,%rdi
}

static void ept_mmu_notifier_invalidate_page(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long address)
{
ffffffff812c0294:	48 89 e5             	mov    %rsp,%rbp
	struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);

	pr_debug("ept: invalidate_page addr %lx\n", address);

	ept_invalidate_page(vcpu, mm, address);
ffffffff812c0297:	e8 31 ff ff ff       	callq  ffffffff812c01cd <ept_invalidate_page>
}
ffffffff812c029c:	5d                   	pop    %rbp
ffffffff812c029d:	c3                   	retq   

ffffffff812c029e <ept_mmu_notifier_change_pte>:

static void ept_mmu_notifier_change_pte(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long address,
		pte_t pte)
{
ffffffff812c029e:	55                   	push   %rbp
	 * NOTE: Recent linux kernels (seen on 3.7 at least) hold a lock
	 * while calling this notifier, making it impossible to call
	 * get_user_pages_fast(). As a result, we just invalidate the
	 * page so that the mapping can be recreated later during a fault.
	 */
	ept_invalidate_page(vcpu, mm, address);
ffffffff812c029f:	48 83 ef 10          	sub    $0x10,%rdi

static void ept_mmu_notifier_change_pte(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long address,
		pte_t pte)
{
ffffffff812c02a3:	48 89 e5             	mov    %rsp,%rbp
	 * NOTE: Recent linux kernels (seen on 3.7 at least) hold a lock
	 * while calling this notifier, making it impossible to call
	 * get_user_pages_fast(). As a result, we just invalidate the
	 * page so that the mapping can be recreated later during a fault.
	 */
	ept_invalidate_page(vcpu, mm, address);
ffffffff812c02a6:	e8 22 ff ff ff       	callq  ffffffff812c01cd <ept_invalidate_page>
}
ffffffff812c02ab:	5d                   	pop    %rbp
ffffffff812c02ac:	c3                   	retq   

ffffffff812c02ad <ept_check_page_accessed>:
 */
static int ept_check_page_accessed(struct vmx_vcpu *vcpu,
		struct mm_struct *mm,
		unsigned long addr,
		bool flush)
{
ffffffff812c02ad:	55                   	push   %rbp
ffffffff812c02ae:	49 89 d1             	mov    %rdx,%r9
ffffffff812c02b1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c02b4:	41 55                	push   %r13
ffffffff812c02b6:	41 54                	push   %r12
ffffffff812c02b8:	53                   	push   %rbx
ffffffff812c02b9:	49 89 fd             	mov    %rdi,%r13
ffffffff812c02bc:	48 83 ec 18          	sub    $0x18,%rsp
	int ret, accessed;
	epte_t *epte;
	void *gpa = (void *) pte_val(hva_to_gpa(vcpu, mm, (unsigned long) addr));
ffffffff812c02c0:	48 8b 7e 40          	mov    0x40(%rsi),%rdi
ffffffff812c02c4:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c02c7:	e8 4f fe ff ff       	callq  ffffffff812c011b <hva_to_gpa.isra.0>
ffffffff812c02cc:	48 89 c3             	mov    %rax,%rbx

	if (gpa == (void *) GPA_ADDR_INVAL) {
ffffffff812c02cf:	48 b8 00 00 00 00 0c 	movabs $0xc00000000,%rax
ffffffff812c02d6:	00 00 00 
ffffffff812c02d9:	48 39 c3             	cmp    %rax,%rbx
ffffffff812c02dc:	75 13                	jne    ffffffff812c02f1 <ept_check_page_accessed+0x44>
		printk(KERN_ERR "ept: hva %lx is out of range\n", addr);
ffffffff812c02de:	4c 89 ce             	mov    %r9,%rsi
ffffffff812c02e1:	48 c7 c7 e0 60 7b 81 	mov    $0xffffffff817b60e0,%rdi
ffffffff812c02e8:	31 c0                	xor    %eax,%eax
ffffffff812c02ea:	e8 a3 2c 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c02ef:	eb 28                	jmp    ffffffff812c0319 <ept_check_page_accessed+0x6c>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c02f1:	4d 8d 65 28          	lea    0x28(%r13),%r12
ffffffff812c02f5:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c02f8:	e8 33 83 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
		return 0;
	}

	spin_lock(&vcpu->ept_lock);
	ret = ept_lookup_gpa(vcpu, (void *) gpa, 0, 0, &epte);
ffffffff812c02fd:	49 8b 7d 40          	mov    0x40(%r13),%rdi
ffffffff812c0301:	48 8d 55 d8          	lea    -0x28(%rbp),%rdx
ffffffff812c0305:	48 89 de             	mov    %rbx,%rsi
ffffffff812c0308:	e8 90 fd ff ff       	callq  ffffffff812c009d <ept_lookup_gpa.isra.1.constprop.3>
	if (ret) {
ffffffff812c030d:	85 c0                	test   %eax,%eax
ffffffff812c030f:	74 0c                	je     ffffffff812c031d <ept_check_page_accessed+0x70>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c0311:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0314:	e8 93 83 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
		spin_unlock(&vcpu->ept_lock);
		return 0;
ffffffff812c0319:	31 c0                	xor    %eax,%eax
ffffffff812c031b:	eb 19                	jmp    ffffffff812c0336 <ept_check_page_accessed+0x89>
	}

	accessed = (*epte & __EPTE_A);
ffffffff812c031d:	48 8b 45 d8          	mov    -0x28(%rbp),%rax
ffffffff812c0321:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0324:	48 8b 00             	mov    (%rax),%rax
ffffffff812c0327:	89 c3                	mov    %eax,%ebx
ffffffff812c0329:	81 e3 00 01 00 00    	and    $0x100,%ebx
ffffffff812c032f:	e8 78 83 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
ffffffff812c0334:	89 d8                	mov    %ebx,%eax

	if (flush & accessed)
		vmx_ept_sync_individual_addr(vcpu, (gpa_t) gpa);

	return accessed;
}
ffffffff812c0336:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c033a:	5b                   	pop    %rbx
ffffffff812c033b:	41 5c                	pop    %r12
ffffffff812c033d:	41 5d                	pop    %r13
ffffffff812c033f:	5d                   	pop    %rbp
ffffffff812c0340:	c3                   	retq   

ffffffff812c0341 <ept_mmu_notifier_test_young>:
#endif

static int ept_mmu_notifier_test_young(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long address)
{
ffffffff812c0341:	55                   	push   %rbp
ffffffff812c0342:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0345:	41 55                	push   %r13
ffffffff812c0347:	41 54                	push   %r12
ffffffff812c0349:	53                   	push   %rbx
ffffffff812c034a:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c034d:	48 83 ec 28          	sub    $0x28,%rsp
	struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);

	pr_debug("ept: test_young addr %lx\n", address);

	if (!vcpu->ept_ad_enabled)
ffffffff812c0351:	80 7f 40 00          	cmpb   $0x0,0x40(%rdi)
ffffffff812c0355:	75 6a                	jne    ffffffff812c03c1 <ept_mmu_notifier_test_young+0x80>
		struct mm_struct *mm,
		unsigned long addr)
{
	int ret;
	epte_t *epte;
	void *gpa = (void *) pte_val(hva_to_gpa(vcpu, mm, (unsigned long) addr));
ffffffff812c0357:	48 8b 7e 40          	mov    0x40(%rsi),%rdi
ffffffff812c035b:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c035e:	48 89 55 c8          	mov    %rdx,-0x38(%rbp)
ffffffff812c0362:	e8 b4 fd ff ff       	callq  ffffffff812c011b <hva_to_gpa.isra.0>
ffffffff812c0367:	49 89 c5             	mov    %rax,%r13

	if (gpa == (void *) GPA_ADDR_INVAL) {
ffffffff812c036a:	48 b8 00 00 00 00 0c 	movabs $0xc00000000,%rax
ffffffff812c0371:	00 00 00 
ffffffff812c0374:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
ffffffff812c0378:	49 39 c5             	cmp    %rax,%r13
ffffffff812c037b:	75 15                	jne    ffffffff812c0392 <ept_mmu_notifier_test_young+0x51>
		printk(KERN_ERR "ept: hva %lx is out of range\n", addr);
ffffffff812c037d:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c0380:	48 c7 c7 e0 60 7b 81 	mov    $0xffffffff817b60e0,%rdi
ffffffff812c0387:	31 c0                	xor    %eax,%eax
ffffffff812c0389:	e8 04 2c 17 00       	callq  ffffffff81432f92 <printk>
		return 0;
ffffffff812c038e:	31 c0                	xor    %eax,%eax
ffffffff812c0390:	eb 3a                	jmp    ffffffff812c03cc <ept_mmu_notifier_test_young+0x8b>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c0392:	4c 8d 63 18          	lea    0x18(%rbx),%r12
ffffffff812c0396:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0399:	e8 92 82 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	}

	spin_lock(&vcpu->ept_lock);
	ret = ept_lookup_gpa(vcpu, (void *) gpa, 0, 0, &epte);
ffffffff812c039e:	48 8b 7b 30          	mov    0x30(%rbx),%rdi
ffffffff812c03a2:	48 8d 55 d8          	lea    -0x28(%rbp),%rdx
ffffffff812c03a6:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c03a9:	e8 ef fc ff ff       	callq  ffffffff812c009d <ept_lookup_gpa.isra.1.constprop.3>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c03ae:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c03b1:	89 c3                	mov    %eax,%ebx
ffffffff812c03b3:	e8 f4 82 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&vcpu->ept_lock);

	return !ret;
ffffffff812c03b8:	31 c0                	xor    %eax,%eax
ffffffff812c03ba:	85 db                	test   %ebx,%ebx
ffffffff812c03bc:	0f 94 c0             	sete   %al
	struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);

	pr_debug("ept: test_young addr %lx\n", address);

	if (!vcpu->ept_ad_enabled)
		return ept_check_page_mapped(vcpu, mm, address);
ffffffff812c03bf:	eb 0b                	jmp    ffffffff812c03cc <ept_mmu_notifier_test_young+0x8b>
	else
		return ept_check_page_accessed(vcpu, mm, address, false);
ffffffff812c03c1:	48 8d 7f f0          	lea    -0x10(%rdi),%rdi
ffffffff812c03c5:	31 c9                	xor    %ecx,%ecx
ffffffff812c03c7:	e8 e1 fe ff ff       	callq  ffffffff812c02ad <ept_check_page_accessed>
}
ffffffff812c03cc:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c03d0:	5b                   	pop    %rbx
ffffffff812c03d1:	41 5c                	pop    %r12
ffffffff812c03d3:	41 5d                	pop    %r13
ffffffff812c03d5:	5d                   	pop    %rbp
ffffffff812c03d6:	c3                   	retq   

ffffffff812c03d7 <ept_mmu_notifier_clear_flush_young>:

#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 18, 0)
static int ept_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long start, unsigned long end)
{
ffffffff812c03d7:	55                   	push   %rbp
ffffffff812c03d8:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c03db:	41 57                	push   %r15
ffffffff812c03dd:	41 56                	push   %r14
ffffffff812c03df:	41 55                	push   %r13
ffffffff812c03e1:	41 54                	push   %r12
	return accessed;
}

static inline struct vmx_vcpu *mmu_notifier_to_vmx(struct mmu_notifier *mn)
{
	return container_of(mn, struct vmx_vcpu, mmu_notifier);
ffffffff812c03e3:	4c 8d 67 f0          	lea    -0x10(%rdi),%r12

#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 18, 0)
static int ept_mmu_notifier_clear_flush_young(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long start, unsigned long end)
{
ffffffff812c03e7:	53                   	push   %rbx
ffffffff812c03e8:	41 50                	push   %r8
ffffffff812c03ea:	49 89 fd             	mov    %rdi,%r13
ffffffff812c03ed:	49 89 f6             	mov    %rsi,%r14
ffffffff812c03f0:	48 89 d3             	mov    %rdx,%rbx
ffffffff812c03f3:	49 89 cf             	mov    %rcx,%r15
	struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
	unsigned long address;
	int err;

	for (address = start ; address < end ; address += PAGE_SIZE) {
ffffffff812c03f6:	4c 39 fb             	cmp    %r15,%rbx
ffffffff812c03f9:	73 37                	jae    ffffffff812c0432 <ept_mmu_notifier_clear_flush_young+0x5b>
		pr_debug("ept: clear_flush_young addr %lx\n", address);

		if (!vcpu->ept_ad_enabled)
ffffffff812c03fb:	41 80 7d 40 00       	cmpb   $0x0,0x40(%r13)
ffffffff812c0400:	75 10                	jne    ffffffff812c0412 <ept_mmu_notifier_clear_flush_young+0x3b>
			err = ept_invalidate_page(vcpu, mm, address);
ffffffff812c0402:	48 89 da             	mov    %rbx,%rdx
ffffffff812c0405:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c0408:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c040b:	e8 bd fd ff ff       	callq  ffffffff812c01cd <ept_invalidate_page>
ffffffff812c0410:	eb 13                	jmp    ffffffff812c0425 <ept_mmu_notifier_clear_flush_young+0x4e>
		else
			err = ept_check_page_accessed(vcpu, mm, address, true);
ffffffff812c0412:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812c0417:	48 89 da             	mov    %rbx,%rdx
ffffffff812c041a:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c041d:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0420:	e8 88 fe ff ff       	callq  ffffffff812c02ad <ept_check_page_accessed>

		if (err < 0)
ffffffff812c0425:	85 c0                	test   %eax,%eax
ffffffff812c0427:	78 0b                	js     ffffffff812c0434 <ept_mmu_notifier_clear_flush_young+0x5d>
{
	struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
	unsigned long address;
	int err;

	for (address = start ; address < end ; address += PAGE_SIZE) {
ffffffff812c0429:	48 81 c3 00 10 00 00 	add    $0x1000,%rbx
ffffffff812c0430:	eb c4                	jmp    ffffffff812c03f6 <ept_mmu_notifier_clear_flush_young+0x1f>

		if (err < 0)
			return err;
	}

	return 0;
ffffffff812c0432:	31 c0                	xor    %eax,%eax
}
ffffffff812c0434:	5a                   	pop    %rdx
ffffffff812c0435:	5b                   	pop    %rbx
ffffffff812c0436:	41 5c                	pop    %r12
ffffffff812c0438:	41 5d                	pop    %r13
ffffffff812c043a:	41 5e                	pop    %r14
ffffffff812c043c:	41 5f                	pop    %r15
ffffffff812c043e:	5d                   	pop    %rbp
ffffffff812c043f:	c3                   	retq   

ffffffff812c0440 <ept_mmu_notifier_invalidate_range_start>:

static void ept_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long start,
		unsigned long end)
{
ffffffff812c0440:	55                   	push   %rbp
ffffffff812c0441:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0444:	41 57                	push   %r15
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c0446:	4c 8d 7f 18          	lea    0x18(%rdi),%r15
ffffffff812c044a:	41 56                	push   %r14
ffffffff812c044c:	41 55                	push   %r13
	struct vmx_vcpu *vcpu = mmu_notifier_to_vmx(mn);
	int ret;
	epte_t *epte;
	unsigned long pos = start;
	bool sync_needed = false;
ffffffff812c044e:	45 31 f6             	xor    %r14d,%r14d

static void ept_mmu_notifier_invalidate_range_start(struct mmu_notifier *mn,
		struct mm_struct *mm,
		unsigned long start,
		unsigned long end)
{
ffffffff812c0451:	41 54                	push   %r12
ffffffff812c0453:	53                   	push   %rbx
ffffffff812c0454:	49 89 fc             	mov    %rdi,%r12
ffffffff812c0457:	4c 89 ff             	mov    %r15,%rdi
ffffffff812c045a:	49 89 f5             	mov    %rsi,%r13
ffffffff812c045d:	48 89 d3             	mov    %rdx,%rbx
ffffffff812c0460:	48 83 ec 28          	sub    $0x28,%rsp
ffffffff812c0464:	48 89 4d b8          	mov    %rcx,-0x48(%rbp)
ffffffff812c0468:	e8 c3 81 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	bool sync_needed = false;

	pr_debug("ept: invalidate_range_start start %lx end %lx\n", start, end);

	spin_lock(&vcpu->ept_lock);
	while (pos < end) {
ffffffff812c046d:	48 3b 5d b8          	cmp    -0x48(%rbp),%rbx
ffffffff812c0471:	0f 83 b1 00 00 00    	jae    ffffffff812c0528 <ept_mmu_notifier_invalidate_range_start+0xe8>

	static int
ept_lookup(struct vmx_vcpu *vcpu, struct mm_struct *mm,
		void *hva, int level, int create, epte_t **epte_out)
{
	void *gpa = (void *) pte_val(hva_to_gpa(vcpu, mm, (unsigned long) hva));
ffffffff812c0477:	49 8b 7d 40          	mov    0x40(%r13),%rdi
ffffffff812c047b:	48 89 de             	mov    %rbx,%rsi
ffffffff812c047e:	e8 98 fc ff ff       	callq  ffffffff812c011b <hva_to_gpa.isra.0>

	if (gpa == (void *) GPA_ADDR_INVAL) {
ffffffff812c0483:	48 ba 00 00 00 00 0c 	movabs $0xc00000000,%rdx
ffffffff812c048a:	00 00 00 
ffffffff812c048d:	48 39 d0             	cmp    %rdx,%rax
ffffffff812c0490:	75 2c                	jne    ffffffff812c04be <ept_mmu_notifier_invalidate_range_start+0x7e>
		printk(KERN_ERR "ept: hva %p is out of range\n", hva);
ffffffff812c0492:	48 89 de             	mov    %rbx,%rsi
ffffffff812c0495:	48 c7 c7 00 61 7b 81 	mov    $0xffffffff817b6100,%rdi
ffffffff812c049c:	31 c0                	xor    %eax,%eax
ffffffff812c049e:	e8 ef 2a 17 00       	callq  ffffffff81432f92 <printk>
		printk(KERN_ERR "ept: mem_base %lx, stack_start %lx\n",
ffffffff812c04a3:	49 8b 95 48 01 00 00 	mov    0x148(%r13),%rdx
ffffffff812c04aa:	49 8b 75 20          	mov    0x20(%r13),%rsi
ffffffff812c04ae:	48 c7 c7 1f 61 7b 81 	mov    $0xffffffff817b611f,%rdi
ffffffff812c04b5:	31 c0                	xor    %eax,%eax
ffffffff812c04b7:	e8 d6 2a 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c04bc:	eb 5e                	jmp    ffffffff812c051c <ept_mmu_notifier_invalidate_range_start+0xdc>
				mm->mmap_base, mm->start_stack);
		return -EINVAL;
	}

	return ept_lookup_gpa(vcpu, gpa, level, create, epte_out);
ffffffff812c04be:	49 8b 7c 24 30       	mov    0x30(%r12),%rdi
ffffffff812c04c3:	48 8d 55 c8          	lea    -0x38(%rbp),%rdx
ffffffff812c04c7:	48 89 c6             	mov    %rax,%rsi
ffffffff812c04ca:	e8 ce fb ff ff       	callq  ffffffff812c009d <ept_lookup_gpa.isra.1.constprop.3>
	pr_debug("ept: invalidate_range_start start %lx end %lx\n", start, end);

	spin_lock(&vcpu->ept_lock);
	while (pos < end) {
		ret = ept_lookup(vcpu, mm, (void *) pos, 0, 0, &epte);
		if (!ret) {
ffffffff812c04cf:	85 c0                	test   %eax,%eax
ffffffff812c04d1:	75 49                	jne    ffffffff812c051c <ept_mmu_notifier_invalidate_range_start+0xdc>
			pos += epte_big(*epte) ? HUGE_PAGE_SIZE : PAGE_SIZE;
ffffffff812c04d3:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
			ept_clear_epte(epte);
			sync_needed = true;
ffffffff812c04d7:	41 b6 01             	mov    $0x1,%r14b

	spin_lock(&vcpu->ept_lock);
	while (pos < end) {
		ret = ept_lookup(vcpu, mm, (void *) pos, 0, 0, &epte);
		if (!ret) {
			pos += epte_big(*epte) ? HUGE_PAGE_SIZE : PAGE_SIZE;
ffffffff812c04da:	48 8b 3a             	mov    (%rdx),%rdi
ffffffff812c04dd:	48 89 f8             	mov    %rdi,%rax
ffffffff812c04e0:	25 80 00 00 00       	and    $0x80,%eax
ffffffff812c04e5:	48 83 f8 01          	cmp    $0x1,%rax
ffffffff812c04e9:	48 19 c0             	sbb    %rax,%rax
ffffffff812c04ec:	48 25 00 10 e0 ff    	and    $0xffffffffffe01000,%rax
	free_page((unsigned long) pgd);
}

static int ept_clear_epte(epte_t *epte)
{
	if (*epte == __EPTE_NONE)
ffffffff812c04f2:	48 85 ff             	test   %rdi,%rdi

	spin_lock(&vcpu->ept_lock);
	while (pos < end) {
		ret = ept_lookup(vcpu, mm, (void *) pos, 0, 0, &epte);
		if (!ret) {
			pos += epte_big(*epte) ? HUGE_PAGE_SIZE : PAGE_SIZE;
ffffffff812c04f5:	48 8d 9c 18 00 00 20 	lea    0x200000(%rax,%rbx,1),%rbx
ffffffff812c04fc:	00 
	free_page((unsigned long) pgd);
}

static int ept_clear_epte(epte_t *epte)
{
	if (*epte == __EPTE_NONE)
ffffffff812c04fd:	0f 84 6a ff ff ff    	je     ffffffff812c046d <ept_mmu_notifier_invalidate_range_start+0x2d>
ffffffff812c0503:	48 89 55 b0          	mov    %rdx,-0x50(%rbp)
		return 0;

	free_ept_page(*epte);
ffffffff812c0507:	e8 51 fa ff ff       	callq  ffffffff812bff5d <free_ept_page>
	*epte = __EPTE_NONE;
ffffffff812c050c:	48 8b 55 b0          	mov    -0x50(%rbp),%rdx
ffffffff812c0510:	48 c7 02 00 00 00 00 	movq   $0x0,(%rdx)
ffffffff812c0517:	e9 51 ff ff ff       	jmpq   ffffffff812c046d <ept_mmu_notifier_invalidate_range_start+0x2d>
		if (!ret) {
			pos += epte_big(*epte) ? HUGE_PAGE_SIZE : PAGE_SIZE;
			ept_clear_epte(epte);
			sync_needed = true;
		} else
			pos += PAGE_SIZE;
ffffffff812c051c:	48 81 c3 00 10 00 00 	add    $0x1000,%rbx
ffffffff812c0523:	e9 45 ff ff ff       	jmpq   ffffffff812c046d <ept_mmu_notifier_invalidate_range_start+0x2d>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c0528:	4c 89 ff             	mov    %r15,%rdi
ffffffff812c052b:	e8 7c 81 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	}
	spin_unlock(&vcpu->ept_lock);

	if (sync_needed)
ffffffff812c0530:	45 84 f6             	test   %r14b,%r14b
ffffffff812c0533:	74 0a                	je     ffffffff812c053f <ept_mmu_notifier_invalidate_range_start+0xff>
		vmx_ept_sync_vcpu(vcpu);
ffffffff812c0535:	49 8d 7c 24 f0       	lea    -0x10(%r12),%rdi
ffffffff812c053a:	e8 81 e4 ff ff       	callq  ffffffff812be9c0 <vmx_ept_sync_vcpu>
}
ffffffff812c053f:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c0543:	5b                   	pop    %rbx
ffffffff812c0544:	41 5c                	pop    %r12
ffffffff812c0546:	41 5d                	pop    %r13
ffffffff812c0548:	41 5e                	pop    %r14
ffffffff812c054a:	41 5f                	pop    %r15
ffffffff812c054c:	5d                   	pop    %rbp
ffffffff812c054d:	c3                   	retq   

ffffffff812c054e <va_to_pte>:
	.release		= ept_mmu_notifier_release,
};

pte_t va_to_pte(unsigned long pgd_root,
		unsigned long addr)
{
ffffffff812c054e:	55                   	push   %rbp
	pgd_t *pgd;
        pud_t *pud;
        pmd_t *pmd;
        pte_t *ptep, pte;	

     	if(addr >= PAGE_OFFSET)
ffffffff812c054f:	48 b8 ff ff ff ff ff 	movabs $0xffff87ffffffffff,%rax
ffffffff812c0556:	87 ff ff 
ffffffff812c0559:	48 39 c6             	cmp    %rax,%rsi
	.release		= ept_mmu_notifier_release,
};

pte_t va_to_pte(unsigned long pgd_root,
		unsigned long addr)
{
ffffffff812c055c:	48 89 e5             	mov    %rsp,%rbp
	pgd_t *pgd;
        pud_t *pud;
        pmd_t *pmd;
        pte_t *ptep, pte;	

     	if(addr >= PAGE_OFFSET)
ffffffff812c055f:	76 16                	jbe    ffffffff812c0577 <va_to_pte+0x29>
		pgd = pgd_offset_k(addr);
ffffffff812c0561:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c0564:	48 c1 ef 24          	shr    $0x24,%rdi
ffffffff812c0568:	81 e7 f8 0f 00 00    	and    $0xff8,%edi
ffffffff812c056e:	48 03 3d 4b b0 76 00 	add    0x76b04b(%rip),%rdi        # ffffffff81a2b5c0 <init_mm+0x40>
ffffffff812c0575:	eb 0f                	jmp    ffffffff812c0586 <va_to_pte+0x38>
	else
		pgd = pgd_root + (((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1));
ffffffff812c0577:	48 89 f0             	mov    %rsi,%rax
ffffffff812c057a:	48 c1 e8 27          	shr    $0x27,%rax
ffffffff812c057e:	25 ff 01 00 00       	and    $0x1ff,%eax
ffffffff812c0583:	48 01 c7             	add    %rax,%rdi
ffffffff812c0586:	48 8b 17             	mov    (%rdi),%rdx
     	
	if (pgd_none(*pgd) || pgd_bad(*pgd))
ffffffff812c0589:	48 85 d2             	test   %rdx,%rdx
ffffffff812c058c:	0f 84 91 00 00 00    	je     ffffffff812c0623 <va_to_pte+0xd5>
ffffffff812c0592:	48 b9 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rcx
ffffffff812c0599:	c0 ff ff 
ffffffff812c059c:	48 89 d0             	mov    %rdx,%rax
ffffffff812c059f:	48 21 c8             	and    %rcx,%rax
ffffffff812c05a2:	48 83 f8 63          	cmp    $0x63,%rax
ffffffff812c05a6:	75 7b                	jne    ffffffff812c0623 <va_to_pte+0xd5>
ffffffff812c05a8:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c05ab:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
ffffffff812c05b2:	3f 00 00 
ffffffff812c05b5:	48 c1 ef 1b          	shr    $0x1b,%rdi
ffffffff812c05b9:	48 21 c2             	and    %rax,%rdx
ffffffff812c05bc:	81 e7 f8 0f 00 00    	and    $0xff8,%edi
ffffffff812c05c2:	48 01 fa             	add    %rdi,%rdx
ffffffff812c05c5:	48 bf 00 00 00 00 00 	movabs $0xffff880000000000,%rdi
ffffffff812c05cc:	88 ff ff 
ffffffff812c05cf:	48 8b 14 3a          	mov    (%rdx,%rdi,1),%rdx
                 goto out;

        pud = pud_offset(pgd, addr);
        if (pud_none(*pud) || pud_bad(*pud))
ffffffff812c05d3:	48 85 d2             	test   %rdx,%rdx
ffffffff812c05d6:	74 4b                	je     ffffffff812c0623 <va_to_pte+0xd5>
ffffffff812c05d8:	49 b8 98 0f 00 00 00 	movabs $0xffffc00000000f98,%r8
ffffffff812c05df:	c0 ff ff 
ffffffff812c05e2:	4c 85 c2             	test   %r8,%rdx
ffffffff812c05e5:	75 3c                	jne    ffffffff812c0623 <va_to_pte+0xd5>
ffffffff812c05e7:	49 89 f0             	mov    %rsi,%r8
ffffffff812c05ea:	48 21 c2             	and    %rax,%rdx
ffffffff812c05ed:	49 c1 e8 12          	shr    $0x12,%r8
ffffffff812c05f1:	41 81 e0 f8 0f 00 00 	and    $0xff8,%r8d
ffffffff812c05f8:	4c 01 c2             	add    %r8,%rdx
ffffffff812c05fb:	48 8b 14 3a          	mov    (%rdx,%rdi,1),%rdx
        	goto out;

        pmd = pmd_offset(pud, addr);
        if (pmd_none(*pmd) || pmd_bad(*pmd))
ffffffff812c05ff:	48 85 d2             	test   %rdx,%rdx
ffffffff812c0602:	74 1f                	je     ffffffff812c0623 <va_to_pte+0xd5>
ffffffff812c0604:	48 21 d1             	and    %rdx,%rcx
ffffffff812c0607:	48 83 f9 63          	cmp    $0x63,%rcx
ffffffff812c060b:	75 16                	jne    ffffffff812c0623 <va_to_pte+0xd5>

        ptep = pte_offset_map(pmd, addr);
	if (!ptep)
		goto out;

        pte = *ptep;
ffffffff812c060d:	48 c1 ee 09          	shr    $0x9,%rsi
ffffffff812c0611:	48 21 d0             	and    %rdx,%rax
ffffffff812c0614:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
ffffffff812c061a:	48 01 c6             	add    %rax,%rsi
	return pte;
ffffffff812c061d:	48 8b 04 3e          	mov    (%rsi,%rdi,1),%rax
ffffffff812c0621:	eb 02                	jmp    ffffffff812c0625 <va_to_pte+0xd7>
ffffffff812c0623:	31 c0                	xor    %eax,%eax

out:	return __pte(0);
}
ffffffff812c0625:	5d                   	pop    %rbp
ffffffff812c0626:	c3                   	retq   

ffffffff812c0627 <pte_kernel>:
	if(__PAGE_KERNEL_EXEC == pte_flags(pte) & __PAGE_KERNEL_EXEC) 
		r = 1;
	else if (__PAGE_KERNEL == pte_flags(pte) & __PAGE_KERNEL) 
		r = 1;

	return r; 
ffffffff812c0627:	48 b8 ff 0f 00 00 00 	movabs $0x7fffc00000000fff,%rax
ffffffff812c062e:	c0 ff 7f 

out:	return __pte(0);
}

int pte_kernel(pte_t pte)
{
ffffffff812c0631:	55                   	push   %rbp
	if(__PAGE_KERNEL_EXEC == pte_flags(pte) & __PAGE_KERNEL_EXEC) 
		r = 1;
	else if (__PAGE_KERNEL == pte_flags(pte) & __PAGE_KERNEL) 
		r = 1;

	return r; 
ffffffff812c0632:	48 21 c7             	and    %rax,%rdi
ffffffff812c0635:	31 c0                	xor    %eax,%eax
ffffffff812c0637:	48 81 ff 63 01 00 00 	cmp    $0x163,%rdi

out:	return __pte(0);
}

int pte_kernel(pte_t pte)
{
ffffffff812c063e:	48 89 e5             	mov    %rsp,%rbp
	if(__PAGE_KERNEL_EXEC == pte_flags(pte) & __PAGE_KERNEL_EXEC) 
		r = 1;
	else if (__PAGE_KERNEL == pte_flags(pte) & __PAGE_KERNEL) 
		r = 1;

	return r; 
ffffffff812c0641:	0f 94 c0             	sete   %al
}
ffffffff812c0644:	5d                   	pop    %rbp
ffffffff812c0645:	c3                   	retq   

ffffffff812c0646 <ept_set_epte>:

int ept_set_epte(struct vmx_vcpu *vcpu, int make_write,
		unsigned long gpa)
{
ffffffff812c0646:	55                   	push   %rbp
ffffffff812c0647:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c064a:	41 56                	push   %r14
ffffffff812c064c:	41 55                	push   %r13
ffffffff812c064e:	41 54                	push   %r12
ffffffff812c0650:	53                   	push   %rbx
ffffffff812c0651:	49 89 fd             	mov    %rdi,%r13
ffffffff812c0654:	48 89 d3             	mov    %rdx,%rbx
	int ret;
	void *va;
	epte_t *epte;
	struct pgprot prot;
	pte_t pa = va_to_pte(vmcs_readl(GUEST_CR3), __va(gpa));
ffffffff812c0657:	49 bc 00 00 00 00 00 	movabs $0xffff880000000000,%r12
ffffffff812c065e:	88 ff ff 
ffffffff812c0661:	bf 02 68 00 00       	mov    $0x6802,%edi
	return r; 
}

int ept_set_epte(struct vmx_vcpu *vcpu, int make_write,
		unsigned long gpa)
{
ffffffff812c0666:	48 83 ec 10          	sub    $0x10,%rsp
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c066a:	4d 8d 75 28          	lea    0x28(%r13),%r14
ffffffff812c066e:	89 75 dc             	mov    %esi,-0x24(%rbp)
	int ret;
	void *va;
	epte_t *epte;
	struct pgprot prot;
	pte_t pa = va_to_pte(vmcs_readl(GUEST_CR3), __va(gpa));
ffffffff812c0671:	e8 1a e3 ff ff       	callq  ffffffff812be990 <vmcs_readl>
ffffffff812c0676:	4a 8d 34 23          	lea    (%rbx,%r12,1),%rsi
ffffffff812c067a:	48 89 c7             	mov    %rax,%rdi
ffffffff812c067d:	e8 cc fe ff ff       	callq  ffffffff812c054e <va_to_pte>
ffffffff812c0682:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c0685:	48 89 c3             	mov    %rax,%rbx
ffffffff812c0688:	e8 a3 7f 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	return pte.pte;
}

static inline pteval_t pte_flags(pte_t pte)
{
	return native_pte_val(pte) & PTE_FLAGS_MASK;
ffffffff812c068d:	48 b9 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rcx
ffffffff812c0694:	c0 ff ff 
//	pte_t pa = __va(gpa);
	/*Attention: change the flags to EPT flags.*/
//	printk(KERN_ERR "EPT: ept_set_epte:  gpa is %lx\n", gpa);
	spin_lock(&vcpu->ept_lock);	
	ret = add_to_ept(__va(vcpu->ept_root), pte_val(pa) & PTE_PFN_MASK, pte_kernel(pa), !pte_write(pa), pte_exec(pa), make_write);
ffffffff812c0697:	48 89 df             	mov    %rbx,%rdi
ffffffff812c069a:	48 21 d9             	and    %rbx,%rcx
ffffffff812c069d:	e8 85 ff ff ff       	callq  ffffffff812c0627 <pte_kernel>
ffffffff812c06a2:	49 89 c8             	mov    %rcx,%r8
ffffffff812c06a5:	80 e1 02             	and    $0x2,%cl
ffffffff812c06a8:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c06ab:	0f 94 c1             	sete   %cl
ffffffff812c06ae:	49 03 7d 40          	add    0x40(%r13),%rdi
ffffffff812c06b2:	44 8b 4d dc          	mov    -0x24(%rbp),%r9d
ffffffff812c06b6:	89 de                	mov    %ebx,%esi
ffffffff812c06b8:	49 f7 d0             	not    %r8
ffffffff812c06bb:	0f b6 c9             	movzbl %cl,%ecx
ffffffff812c06be:	89 c2                	mov    %eax,%edx
ffffffff812c06c0:	81 e6 00 f0 ff ff    	and    $0xfffff000,%esi
ffffffff812c06c6:	49 c1 e8 3f          	shr    $0x3f,%r8
ffffffff812c06ca:	e8 00 f6 ff ff       	callq  ffffffff812bfccf <add_to_ept>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c06cf:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c06d2:	89 45 dc             	mov    %eax,-0x24(%rbp)
ffffffff812c06d5:	e8 d2 7f 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&vcpu->ept_lock);
	return ret;
}
ffffffff812c06da:	8b 45 dc             	mov    -0x24(%rbp),%eax
ffffffff812c06dd:	5a                   	pop    %rdx
ffffffff812c06de:	59                   	pop    %rcx
ffffffff812c06df:	5b                   	pop    %rbx
ffffffff812c06e0:	41 5c                	pop    %r12
ffffffff812c06e2:	41 5d                	pop    %r13
ffffffff812c06e4:	41 5e                	pop    %r14
ffffffff812c06e6:	5d                   	pop    %rbp
ffffffff812c06e7:	c3                   	retq   

ffffffff812c06e8 <vmx_do_ept_fault>:
 }
 */

int vmx_do_ept_fault(struct vmx_vcpu *vcpu, unsigned long gpa,
		unsigned long gva, int fault_flags)
{
ffffffff812c06e8:	55                   	push   %rbp
	epte_t *epte;
	int ret;
	int make_write = (fault_flags & VMX_EPT_FAULT_WRITE) ? 1 : 0;
ffffffff812c06e9:	89 c8                	mov    %ecx,%eax
 }
 */

int vmx_do_ept_fault(struct vmx_vcpu *vcpu, unsigned long gpa,
		unsigned long gva, int fault_flags)
{
ffffffff812c06eb:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c06ee:	41 56                	push   %r14
ffffffff812c06f0:	41 55                	push   %r13
ffffffff812c06f2:	41 54                	push   %r12
ffffffff812c06f4:	53                   	push   %rbx
	epte_t *epte;
	int ret;
	int make_write = (fault_flags & VMX_EPT_FAULT_WRITE) ? 1 : 0;
ffffffff812c06f5:	45 31 f6             	xor    %r14d,%r14d
 }
 */

int vmx_do_ept_fault(struct vmx_vcpu *vcpu, unsigned long gpa,
		unsigned long gva, int fault_flags)
{
ffffffff812c06f8:	49 89 fd             	mov    %rdi,%r13
ffffffff812c06fb:	49 89 f4             	mov    %rsi,%r12
ffffffff812c06fe:	48 83 ec 20          	sub    $0x20,%rsp
	epte_t *epte;
	int ret;
	int make_write = (fault_flags & VMX_EPT_FAULT_WRITE) ? 1 : 0;
ffffffff812c0702:	83 e0 02             	and    $0x2,%eax
ffffffff812c0705:	41 0f 95 c6          	setne  %r14b

	pr_debug("ept: GPA: 0x%lx, GVA: 0x%lx, flags: %x\n",
			gpa, gva, fault_flags);


	if(make_write && -ENOENT != ept_lookup_gpa(vcpu, (void *) gpa, 0, 0, &epte))
ffffffff812c0709:	0f 84 87 00 00 00    	je     ffffffff812c0796 <vmx_do_ept_fault+0xae>
ffffffff812c070f:	48 8b 7f 40          	mov    0x40(%rdi),%rdi
ffffffff812c0713:	49 89 d2             	mov    %rdx,%r10
ffffffff812c0716:	48 8d 55 d8          	lea    -0x28(%rbp),%rdx
ffffffff812c071a:	89 4d cc             	mov    %ecx,-0x34(%rbp)
ffffffff812c071d:	e8 7b f9 ff ff       	callq  ffffffff812c009d <ept_lookup_gpa.isra.1.constprop.3>
ffffffff812c0722:	83 f8 fe             	cmp    $0xfffffffe,%eax
ffffffff812c0725:	8b 4d cc             	mov    -0x34(%rbp),%ecx
ffffffff812c0728:	74 6c                	je     ffffffff812c0796 <vmx_do_ept_fault+0xae>
	{
		printk(KERN_ERR "EPT: vmx_do_ept_fault: The fault is write protection. gpa is %lx. gva is %lx. flags is %x. Expected virtual is %lx. Fount EPT entry: %lx\n", gpa, gva, fault_flags, phys_to_virt(gpa), *epte);
ffffffff812c072a:	48 8b 45 d8          	mov    -0x28(%rbp),%rax
 *	this function
 */

static inline void *phys_to_virt(phys_addr_t address)
{
	return __va(address);
ffffffff812c072e:	48 bb 00 00 00 00 00 	movabs $0xffff880000000000,%rbx
ffffffff812c0735:	88 ff ff 
ffffffff812c0738:	4c 89 d2             	mov    %r10,%rdx
ffffffff812c073b:	4e 8d 04 23          	lea    (%rbx,%r12,1),%r8
ffffffff812c073f:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c0742:	48 c7 c7 45 61 7b 81 	mov    $0xffffffff817b6145,%rdi
ffffffff812c0749:	4c 8b 08             	mov    (%rax),%r9
ffffffff812c074c:	31 c0                	xor    %eax,%eax
ffffffff812c074e:	e8 3f 28 17 00       	callq  ffffffff81432f92 <printk>
		printk(KERN_ERR "EPT:vmx_do_ept_fault: Wrong permissions for <%p> %pS \n", phys_to_virt(gpa), phys_to_virt(gpa));
ffffffff812c0753:	4a 8d 14 23          	lea    (%rbx,%r12,1),%rdx
ffffffff812c0757:	48 c7 c7 d1 61 7b 81 	mov    $0xffffffff817b61d1,%rdi
ffffffff812c075e:	31 c0                	xor    %eax,%eax
ffffffff812c0760:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c0763:	e8 2a 28 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c0768:	65 48 8b 04 25 00 aa 	mov    %gs:0xaa00,%rax
ffffffff812c076f:	00 00 
ffffffff812c0771:	48 8b 80 88 02 00 00 	mov    0x288(%rax),%rax
		printk(KERN_ERR "EPT:vmx_do_ept_fault: The reverse physical addess is %p", pte_val(hva_to_gpa(vcpu, current->active_mm, phys_to_virt(gpa))));
ffffffff812c0778:	4a 8d 34 23          	lea    (%rbx,%r12,1),%rsi
ffffffff812c077c:	48 8b 78 40          	mov    0x40(%rax),%rdi
ffffffff812c0780:	e8 96 f9 ff ff       	callq  ffffffff812c011b <hva_to_gpa.isra.0>
ffffffff812c0785:	48 c7 c7 0a 62 7b 81 	mov    $0xffffffff817b620a,%rdi
ffffffff812c078c:	48 89 c6             	mov    %rax,%rsi
ffffffff812c078f:	31 c0                	xor    %eax,%eax
ffffffff812c0791:	e8 fc 27 17 00       	callq  ffffffff81432f92 <printk>
	}

	ret = ept_set_epte(vcpu, make_write, gpa);
ffffffff812c0796:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c0799:	44 89 f6             	mov    %r14d,%esi
ffffffff812c079c:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c079f:	e8 a2 fe ff ff       	callq  ffffffff812c0646 <ept_set_epte>

	return ret;
}
ffffffff812c07a4:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812c07a8:	5b                   	pop    %rbx
ffffffff812c07a9:	41 5c                	pop    %r12
ffffffff812c07ab:	41 5d                	pop    %r13
ffffffff812c07ad:	41 5e                	pop    %r14
ffffffff812c07af:	5d                   	pop    %rbp
ffffffff812c07b0:	c3                   	retq   

ffffffff812c07b1 <add_mm>:
	return -ENOMEM;

}

int add_mm(struct mm_struct *mm, struct vmx_vcpu *vcpu, pte_t *epgd)
{
ffffffff812c07b1:	55                   	push   %rbp
ffffffff812c07b2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c07b5:	41 57                	push   %r15
ffffffff812c07b7:	41 56                	push   %r14
ffffffff812c07b9:	41 55                	push   %r13
ffffffff812c07bb:	41 54                	push   %r12
ffffffff812c07bd:	49 89 d5             	mov    %rdx,%r13
ffffffff812c07c0:	53                   	push   %rbx

        //asm("\t movq %%cr3,%0" : "=r"(cr3));

        //printk(KERN_ERR "vmx_init_ept: asm pass \n");
        //pgd = phys_to_virt(cr3 & PTE_PFN_MASK);
        pgd = (pte_t *)mm->pgd;
ffffffff812c07c1:	45 31 f6             	xor    %r14d,%r14d
	return -ENOMEM;

}

int add_mm(struct mm_struct *mm, struct vmx_vcpu *vcpu, pte_t *epgd)
{
ffffffff812c07c4:	48 83 ec 38          	sub    $0x38,%rsp

        //asm("\t movq %%cr3,%0" : "=r"(cr3));

        //printk(KERN_ERR "vmx_init_ept: asm pass \n");
        //pgd = phys_to_virt(cr3 & PTE_PFN_MASK);
        pgd = (pte_t *)mm->pgd;
ffffffff812c07c8:	48 8b 47 40          	mov    0x40(%rdi),%rax
ffffffff812c07cc:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
ffffffff812c07d0:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812c07d4:	4a 8b 34 30          	mov    (%rax,%r14,1),%rsi


        for (i = 0; i < PTRS_PER_PGD; i++) {
                // If pgd entry not present, continue to next
                if (!pte_present(pgd[i]))
ffffffff812c07d8:	f7 c6 01 01 00 00    	test   $0x101,%esi
ffffffff812c07de:	75 14                	jne    ffffffff812c07f4 <add_mm+0x43>
ffffffff812c07e0:	49 83 c6 08          	add    $0x8,%r14
        //printk(KERN_ERR "vmx_init_ept: asm pass \n");
        //pgd = phys_to_virt(cr3 & PTE_PFN_MASK);
        pgd = (pte_t *)mm->pgd;


        for (i = 0; i < PTRS_PER_PGD; i++) {
ffffffff812c07e4:	49 81 fe 00 10 00 00 	cmp    $0x1000,%r14
ffffffff812c07eb:	75 e3                	jne    ffffffff812c07d0 <add_mm+0x1f>
                                }
                        }
                }
        }

        return 0;
ffffffff812c07ed:	31 c0                	xor    %eax,%eax
ffffffff812c07ef:	e9 ab 01 00 00       	jmpq   ffffffff812c099f <add_mm+0x1ee>
                // Get original pud vaddress
                pud = (pte_t *)phys_to_virt(pte_val(pgd[i]) & PTE_PFN_MASK);
#if DEBUG
//                printk("Adding pud root at %lx\n", pte_val(pgd[i]) & PTE_PFN_MASK);
#endif
                if(add_to_ept(epgd, (pte_val(pgd[i]) & PTE_PFN_MASK), pte_val(pgd[i]) & __PAGE_KERNEL, pte_val(pgd[i]) & __PAGE_KERNEL_RO, pte_val(pgd[i]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c07f4:	89 f1                	mov    %esi,%ecx
ffffffff812c07f6:	89 f2                	mov    %esi,%edx
        for (i = 0; i < PTRS_PER_PGD; i++) {
                // If pgd entry not present, continue to next
                if (!pte_present(pgd[i]))
                        continue;
                // Get original pud vaddress
                pud = (pte_t *)phys_to_virt(pte_val(pgd[i]) & PTE_PFN_MASK);
ffffffff812c07f8:	49 bc 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r12
ffffffff812c07ff:	3f 00 00 
#if DEBUG
//                printk("Adding pud root at %lx\n", pte_val(pgd[i]) & PTE_PFN_MASK);
#endif
                if(add_to_ept(epgd, (pte_val(pgd[i]) & PTE_PFN_MASK), pte_val(pgd[i]) & __PAGE_KERNEL, pte_val(pgd[i]) & __PAGE_KERNEL_RO, pte_val(pgd[i]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c0802:	81 e1 61 01 00 00    	and    $0x161,%ecx
        for (i = 0; i < PTRS_PER_PGD; i++) {
                // If pgd entry not present, continue to next
                if (!pte_present(pgd[i]))
                        continue;
                // Get original pud vaddress
                pud = (pte_t *)phys_to_virt(pte_val(pgd[i]) & PTE_PFN_MASK);
ffffffff812c0808:	49 21 f4             	and    %rsi,%r12
#if DEBUG
//                printk("Adding pud root at %lx\n", pte_val(pgd[i]) & PTE_PFN_MASK);
#endif
                if(add_to_ept(epgd, (pte_val(pgd[i]) & PTE_PFN_MASK), pte_val(pgd[i]) & __PAGE_KERNEL, pte_val(pgd[i]) & __PAGE_KERNEL_RO, pte_val(pgd[i]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c080b:	81 e2 63 01 00 00    	and    $0x163,%edx
ffffffff812c0811:	81 e6 00 f0 ff ff    	and    $0xfffff000,%esi
ffffffff812c0817:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c081a:	48 bb 00 00 00 00 00 	movabs $0xffff880000000000,%rbx
ffffffff812c0821:	88 ff ff 
ffffffff812c0824:	41 89 c8             	mov    %ecx,%r8d
ffffffff812c0827:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c082a:	4c 01 e3             	add    %r12,%rbx
ffffffff812c082d:	e8 9d f4 ff ff       	callq  ffffffff812bfccf <add_to_ept>
ffffffff812c0832:	85 c0                	test   %eax,%eax
ffffffff812c0834:	0f 88 52 01 00 00    	js     ffffffff812c098c <add_mm+0x1db>
ffffffff812c083a:	48 b8 00 10 00 00 00 	movabs $0xffff880000001000,%rax
ffffffff812c0841:	88 ff ff 
ffffffff812c0844:	4c 01 e0             	add    %r12,%rax
ffffffff812c0847:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812c084b:	48 8b 33             	mov    (%rbx),%rsi
                        goto ERROR;
                }

                // Traverse thro all pud entries
                for (j = 0; j < PTRS_PER_PUD; j++) {
                        if (!pte_present(pud[j]))
ffffffff812c084e:	f7 c6 01 01 00 00    	test   $0x101,%esi
ffffffff812c0854:	75 0f                	jne    ffffffff812c0865 <add_mm+0xb4>
ffffffff812c0856:	48 83 c3 08          	add    $0x8,%rbx
                if(add_to_ept(epgd, (pte_val(pgd[i]) & PTE_PFN_MASK), pte_val(pgd[i]) & __PAGE_KERNEL, pte_val(pgd[i]) & __PAGE_KERNEL_RO, pte_val(pgd[i]) & __PAGE_KERNEL_RX, 0) < 0){
                        goto ERROR;
                }

                // Traverse thro all pud entries
                for (j = 0; j < PTRS_PER_PUD; j++) {
ffffffff812c085a:	48 3b 5d b8          	cmp    -0x48(%rbp),%rbx
ffffffff812c085e:	75 eb                	jne    ffffffff812c084b <add_mm+0x9a>
ffffffff812c0860:	e9 7b ff ff ff       	jmpq   ffffffff812c07e0 <add_mm+0x2f>
                                continue;
                        pmd = (pte_t *) phys_to_virt(pte_val(pud[j]) & PTE_PFN_MASK);
#if DEBUG
//                        printk("Adding pmd root at %lx\n", pte_val(pud[j]) & PTE_PFN_MASK);
#endif
                        if(add_to_ept(epgd, pte_val(pud[j]) & PTE_PFN_MASK, pte_val(pud[j]) & __PAGE_KERNEL, pte_val(pud[j]) & __PAGE_KERNEL_RO, pte_val(pud[j]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c0865:	89 f1                	mov    %esi,%ecx
ffffffff812c0867:	89 f2                	mov    %esi,%edx

                // Traverse thro all pud entries
                for (j = 0; j < PTRS_PER_PUD; j++) {
                        if (!pte_present(pud[j]))
                                continue;
                        pmd = (pte_t *) phys_to_virt(pte_val(pud[j]) & PTE_PFN_MASK);
ffffffff812c0869:	49 bf 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r15
ffffffff812c0870:	3f 00 00 
#if DEBUG
//                        printk("Adding pmd root at %lx\n", pte_val(pud[j]) & PTE_PFN_MASK);
#endif
                        if(add_to_ept(epgd, pte_val(pud[j]) & PTE_PFN_MASK, pte_val(pud[j]) & __PAGE_KERNEL, pte_val(pud[j]) & __PAGE_KERNEL_RO, pte_val(pud[j]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c0873:	81 e1 61 01 00 00    	and    $0x161,%ecx

                // Traverse thro all pud entries
                for (j = 0; j < PTRS_PER_PUD; j++) {
                        if (!pte_present(pud[j]))
                                continue;
                        pmd = (pte_t *) phys_to_virt(pte_val(pud[j]) & PTE_PFN_MASK);
ffffffff812c0879:	49 21 f7             	and    %rsi,%r15
#if DEBUG
//                        printk("Adding pmd root at %lx\n", pte_val(pud[j]) & PTE_PFN_MASK);
#endif
                        if(add_to_ept(epgd, pte_val(pud[j]) & PTE_PFN_MASK, pte_val(pud[j]) & __PAGE_KERNEL, pte_val(pud[j]) & __PAGE_KERNEL_RO, pte_val(pud[j]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c087c:	81 e2 63 01 00 00    	and    $0x163,%edx
ffffffff812c0882:	81 e6 00 f0 ff ff    	and    $0xfffff000,%esi
ffffffff812c0888:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c088b:	49 bc 00 00 00 00 00 	movabs $0xffff880000000000,%r12
ffffffff812c0892:	88 ff ff 
ffffffff812c0895:	41 89 c8             	mov    %ecx,%r8d
ffffffff812c0898:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c089b:	4d 01 fc             	add    %r15,%r12
ffffffff812c089e:	e8 2c f4 ff ff       	callq  ffffffff812bfccf <add_to_ept>
ffffffff812c08a3:	85 c0                	test   %eax,%eax
ffffffff812c08a5:	0f 88 e1 00 00 00    	js     ffffffff812c098c <add_mm+0x1db>
ffffffff812c08ab:	48 b8 00 10 00 00 00 	movabs $0xffff880000001000,%rax
ffffffff812c08b2:	88 ff ff 
ffffffff812c08b5:	4c 01 f8             	add    %r15,%rax
ffffffff812c08b8:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812c08bc:	49 8b 34 24          	mov    (%r12),%rsi
                                goto ERROR;
                        }

                        // Traverse thro all pmd entries
                        for (k = 0; k < PTRS_PER_PMD; k++) {
                                if (!pte_present(pmd[k]))
ffffffff812c08c0:	f7 c6 01 01 00 00    	test   $0x101,%esi
ffffffff812c08c6:	75 0c                	jne    ffffffff812c08d4 <add_mm+0x123>
ffffffff812c08c8:	49 83 c4 08          	add    $0x8,%r12
                        if(add_to_ept(epgd, pte_val(pud[j]) & PTE_PFN_MASK, pte_val(pud[j]) & __PAGE_KERNEL, pte_val(pud[j]) & __PAGE_KERNEL_RO, pte_val(pud[j]) & __PAGE_KERNEL_RX, 0) < 0){
                                goto ERROR;
                        }

                        // Traverse thro all pmd entries
                        for (k = 0; k < PTRS_PER_PMD; k++) {
ffffffff812c08cc:	4c 3b 65 c0          	cmp    -0x40(%rbp),%r12
ffffffff812c08d0:	75 ea                	jne    ffffffff812c08bc <add_mm+0x10b>
ffffffff812c08d2:	eb 82                	jmp    ffffffff812c0856 <add_mm+0xa5>

                                pte = (pte_t *) phys_to_virt(pte_val(pmd[k]) & PTE_PFN_MASK);
#if DEBUG
//                                printk("Adding pte root at %lx\n", pte_val(pmd[k]) & PTE_PFN_MASK);
#endif
                                if(add_to_ept(epgd, pte_val(pmd[k]) & PTE_PFN_MASK, pte_val(pmd[k]) & __PAGE_KERNEL, pte_val(pmd[k]) & __PAGE_KERNEL_RO, pte_val(pmd[k]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c08d4:	89 f1                	mov    %esi,%ecx
                        // Traverse thro all pmd entries
                        for (k = 0; k < PTRS_PER_PMD; k++) {
                                if (!pte_present(pmd[k]))
                                        continue;

                                pte = (pte_t *) phys_to_virt(pte_val(pmd[k]) & PTE_PFN_MASK);
ffffffff812c08d6:	49 bf 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r15
ffffffff812c08dd:	3f 00 00 
#if DEBUG
//                                printk("Adding pte root at %lx\n", pte_val(pmd[k]) & PTE_PFN_MASK);
#endif
                                if(add_to_ept(epgd, pte_val(pmd[k]) & PTE_PFN_MASK, pte_val(pmd[k]) & __PAGE_KERNEL, pte_val(pmd[k]) & __PAGE_KERNEL_RO, pte_val(pmd[k]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c08e0:	89 f2                	mov    %esi,%edx
                        // Traverse thro all pmd entries
                        for (k = 0; k < PTRS_PER_PMD; k++) {
                                if (!pte_present(pmd[k]))
                                        continue;

                                pte = (pte_t *) phys_to_virt(pte_val(pmd[k]) & PTE_PFN_MASK);
ffffffff812c08e2:	49 21 f7             	and    %rsi,%r15
#if DEBUG
//                                printk("Adding pte root at %lx\n", pte_val(pmd[k]) & PTE_PFN_MASK);
#endif
                                if(add_to_ept(epgd, pte_val(pmd[k]) & PTE_PFN_MASK, pte_val(pmd[k]) & __PAGE_KERNEL, pte_val(pmd[k]) & __PAGE_KERNEL_RO, pte_val(pmd[k]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c08e5:	81 e1 61 01 00 00    	and    $0x161,%ecx
ffffffff812c08eb:	48 b8 00 00 00 00 00 	movabs $0xffff880000000000,%rax
ffffffff812c08f2:	88 ff ff 
ffffffff812c08f5:	4c 01 f8             	add    %r15,%rax
ffffffff812c08f8:	81 e2 63 01 00 00    	and    $0x163,%edx
ffffffff812c08fe:	81 e6 00 f0 ff ff    	and    $0xfffff000,%esi
ffffffff812c0904:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c0907:	41 89 c8             	mov    %ecx,%r8d
ffffffff812c090a:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c090d:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
ffffffff812c0911:	e8 b9 f3 ff ff       	callq  ffffffff812bfccf <add_to_ept>
ffffffff812c0916:	85 c0                	test   %eax,%eax
ffffffff812c0918:	78 72                	js     ffffffff812c098c <add_mm+0x1db>
ffffffff812c091a:	48 b8 00 10 00 00 00 	movabs $0xffff880000001000,%rax
ffffffff812c0921:	88 ff ff 
ffffffff812c0924:	4c 01 f8             	add    %r15,%rax
ffffffff812c0927:	4c 8b 7d b0          	mov    -0x50(%rbp),%r15
ffffffff812c092b:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
ffffffff812c092f:	49 8b 37             	mov    (%r15),%rsi
                                        goto ERROR;
                                }

                                for (l = 0; l < PTRS_PER_PTE; l++) {
                                        if (!pte_present(pte[l]))
ffffffff812c0932:	f7 c6 01 01 00 00    	test   $0x101,%esi
ffffffff812c0938:	75 0c                	jne    ffffffff812c0946 <add_mm+0x195>
ffffffff812c093a:	49 83 c7 08          	add    $0x8,%r15
#endif
                                if(add_to_ept(epgd, pte_val(pmd[k]) & PTE_PFN_MASK, pte_val(pmd[k]) & __PAGE_KERNEL, pte_val(pmd[k]) & __PAGE_KERNEL_RO, pte_val(pmd[k]) & __PAGE_KERNEL_RX, 0) < 0){
                                        goto ERROR;
                                }

                                for (l = 0; l < PTRS_PER_PTE; l++) {
ffffffff812c093e:	4c 39 7d a8          	cmp    %r15,-0x58(%rbp)
ffffffff812c0942:	75 eb                	jne    ffffffff812c092f <add_mm+0x17e>
ffffffff812c0944:	eb 82                	jmp    ffffffff812c08c8 <add_mm+0x117>
                                        if (!pte_present(pte[l]))
                                                continue;
#if DEBUG
                                        printk("Adding pte entry at %lx\n", pte_val(pte[l]) & PTE_PFN_MASK);
ffffffff812c0946:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
ffffffff812c094d:	3f 00 00 
ffffffff812c0950:	48 c7 c7 44 62 7b 81 	mov    $0xffffffff817b6244,%rdi
ffffffff812c0957:	48 21 c6             	and    %rax,%rsi
ffffffff812c095a:	31 c0                	xor    %eax,%eax
ffffffff812c095c:	e8 31 26 17 00       	callq  ffffffff81432f92 <printk>
#endif
                                        if(add_to_ept(epgd, pte_val(pte[l]) & PTE_PFN_MASK, pte_val(pte[l]) & __PAGE_KERNEL, pte_val(pte[l]) & __PAGE_KERNEL_RO, pte_val(pte[l]) & __PAGE_KERNEL_RX, 0) < 0){
ffffffff812c0961:	49 8b 37             	mov    (%r15),%rsi
ffffffff812c0964:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c0967:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c096a:	89 f1                	mov    %esi,%ecx
ffffffff812c096c:	89 f2                	mov    %esi,%edx
ffffffff812c096e:	81 e6 00 f0 ff ff    	and    $0xfffff000,%esi
ffffffff812c0974:	81 e1 61 01 00 00    	and    $0x161,%ecx
ffffffff812c097a:	81 e2 63 01 00 00    	and    $0x163,%edx
ffffffff812c0980:	41 89 c8             	mov    %ecx,%r8d
ffffffff812c0983:	e8 47 f3 ff ff       	callq  ffffffff812bfccf <add_to_ept>
ffffffff812c0988:	85 c0                	test   %eax,%eax
ffffffff812c098a:	79 ae                	jns    ffffffff812c093a <add_mm+0x189>
                }
        }

        return 0;
ERROR:
        printk(KERN_ERR "vmx_init_ept error");
ffffffff812c098c:	48 c7 c7 5d 62 7b 81 	mov    $0xffffffff817b625d,%rdi
ffffffff812c0993:	31 c0                	xor    %eax,%eax
ffffffff812c0995:	e8 f8 25 17 00       	callq  ffffffff81432f92 <printk>
        return -ENOMEM;
ffffffff812c099a:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
}
ffffffff812c099f:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812c09a3:	5b                   	pop    %rbx
ffffffff812c09a4:	41 5c                	pop    %r12
ffffffff812c09a6:	41 5d                	pop    %r13
ffffffff812c09a8:	41 5e                	pop    %r14
ffffffff812c09aa:	41 5f                	pop    %r15
ffffffff812c09ac:	5d                   	pop    %rbp
ffffffff812c09ad:	c3                   	retq   

ffffffff812c09ae <ept_set_stack_perm>:

int ept_set_stack_perm(struct vmx_vcpu *vcpu, pte_t *epgd, unsigned long rsp)
{
ffffffff812c09ae:	55                   	push   %rbp
ffffffff812c09af:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c09b2:	41 54                	push   %r12
ffffffff812c09b4:	49 89 f4             	mov    %rsi,%r12
ffffffff812c09b7:	53                   	push   %rbx
ffffffff812c09b8:	48 89 d3             	mov    %rdx,%rbx
	int r = 0;
	unsigned long curr_rsp, pa_rsp, pa_curr_rsp;
	asm volatile("mov %%rsp,%0\n\t" : "=r" (curr_rsp), "=m" (__force_order));
ffffffff812c09bb:	48 89 e0             	mov    %rsp,%rax
//	pa_rsp = pte_val(va2pa(vcpu, rsp));
//	pa_curr_rsp = pte_val(va2pa(vcpu, curr_rsp));
//	for(i = rsp; i < 0xffffff7fffffffff && r == 0; i+= PAGE_SIZE)
	printk(KERN_ERR "Setting stack pointers %p\n", rsp);
ffffffff812c09be:	48 c7 c7 72 62 7b 81 	mov    $0xffffffff817b6272,%rdi
ffffffff812c09c5:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c09c8:	31 c0                	xor    %eax,%eax
ffffffff812c09ca:	e8 c3 25 17 00       	callq  ffffffff81432f92 <printk>
	r = add_to_ept(epgd, (long unsigned int)__pa(rsp) & PTE_PFN_MASK, 1, 0, 0, 0);
ffffffff812c09cf:	48 89 df             	mov    %rbx,%rdi
ffffffff812c09d2:	e8 df 1e da ff       	callq  ffffffff810628b6 <__phys_addr>
ffffffff812c09d7:	89 c6                	mov    %eax,%esi
ffffffff812c09d9:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c09dc:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c09df:	81 e6 00 f0 ff ff    	and    $0xfffff000,%esi
ffffffff812c09e5:	45 31 c0             	xor    %r8d,%r8d
ffffffff812c09e8:	31 c9                	xor    %ecx,%ecx
ffffffff812c09ea:	ba 01 00 00 00       	mov    $0x1,%edx
ffffffff812c09ef:	e8 db f2 ff ff       	callq  ffffffff812bfccf <add_to_ept>
	return r;
}
ffffffff812c09f4:	5b                   	pop    %rbx
ffffffff812c09f5:	41 5c                	pop    %r12
ffffffff812c09f7:	5d                   	pop    %rbp
ffffffff812c09f8:	c3                   	retq   

ffffffff812c09f9 <ept_set_cr3_perm>:

int ept_set_cr3_perm(struct vmx_vcpu *vcpu, pte_t *epgd, unsigned long pa_cr3)
{
ffffffff812c09f9:	55                   	push   %rbp
	int r = 0;
	printk(KERN_ERR "Setting CR3 perms %p\n", pa_cr3);
ffffffff812c09fa:	48 c7 c7 8f 62 7b 81 	mov    $0xffffffff817b628f,%rdi
ffffffff812c0a01:	31 c0                	xor    %eax,%eax
	r = add_to_ept(epgd, (long unsigned int)__pa(rsp) & PTE_PFN_MASK, 1, 0, 0, 0);
	return r;
}

int ept_set_cr3_perm(struct vmx_vcpu *vcpu, pte_t *epgd, unsigned long pa_cr3)
{
ffffffff812c0a03:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0a06:	41 54                	push   %r12
ffffffff812c0a08:	53                   	push   %rbx
ffffffff812c0a09:	49 89 f4             	mov    %rsi,%r12
ffffffff812c0a0c:	48 89 d3             	mov    %rdx,%rbx
	int r = 0;
	printk(KERN_ERR "Setting CR3 perms %p\n", pa_cr3);
ffffffff812c0a0f:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c0a12:	e8 7b 25 17 00       	callq  ffffffff81432f92 <printk>
	r = add_to_ept(epgd, pa_cr3 & PTE_PFN_MASK, 1, 0, 0, 0);
ffffffff812c0a17:	89 de                	mov    %ebx,%esi
ffffffff812c0a19:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0a1c:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c0a1f:	81 e6 00 f0 ff ff    	and    $0xfffff000,%esi
ffffffff812c0a25:	45 31 c0             	xor    %r8d,%r8d
ffffffff812c0a28:	31 c9                	xor    %ecx,%ecx
ffffffff812c0a2a:	ba 01 00 00 00       	mov    $0x1,%edx
ffffffff812c0a2f:	e8 9b f2 ff ff       	callq  ffffffff812bfccf <add_to_ept>
	return r;
}
ffffffff812c0a34:	5b                   	pop    %rbx
ffffffff812c0a35:	41 5c                	pop    %r12
ffffffff812c0a37:	5d                   	pop    %rbp
ffffffff812c0a38:	c3                   	retq   

ffffffff812c0a39 <vmx_init_ept>:


int vmx_init_ept(struct vmx_vcpu *vcpu, struct dune_config *conf)
{	
ffffffff812c0a39:	55                   	push   %rbp
ffffffff812c0a3a:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0a3d:	41 55                	push   %r13
ffffffff812c0a3f:	41 54                	push   %r12
ffffffff812c0a41:	53                   	push   %rbx
ffffffff812c0a42:	51                   	push   %rcx
ffffffff812c0a43:	49 89 fc             	mov    %rdi,%r12
ffffffff812c0a46:	49 89 f5             	mov    %rsi,%r13
	struct mm_struct *mm = current->active_mm;
	int ret, i;
	pte_t *epgd;
        void *page = (void *) __get_free_page(GFP_AZK);
ffffffff812c0a49:	bf 10 80 00 00       	mov    $0x8010,%edi
ffffffff812c0a4e:	31 f6                	xor    %esi,%esi
ffffffff812c0a50:	e8 6f 96 e1 ff       	callq  ffffffff810da0c4 <__get_free_pages>

        /*Change : changing PTE_PFN_MASK to EPT_PAGE_MASK */

        printk(KERN_ERR "vmx_init_ept: Entering \n");
ffffffff812c0a55:	48 c7 c7 a7 62 7b 81 	mov    $0xffffffff817b62a7,%rdi
int vmx_init_ept(struct vmx_vcpu *vcpu, struct dune_config *conf)
{	
	struct mm_struct *mm = current->active_mm;
	int ret, i;
	pte_t *epgd;
        void *page = (void *) __get_free_page(GFP_AZK);
ffffffff812c0a5c:	48 89 c3             	mov    %rax,%rbx

        /*Change : changing PTE_PFN_MASK to EPT_PAGE_MASK */

        printk(KERN_ERR "vmx_init_ept: Entering \n");
ffffffff812c0a5f:	31 c0                	xor    %eax,%eax
ffffffff812c0a61:	e8 2c 25 17 00       	callq  ffffffff81432f92 <printk>
        if (!page) {
ffffffff812c0a66:	48 85 db             	test   %rbx,%rbx
ffffffff812c0a69:	75 10                	jne    ffffffff812c0a7b <vmx_init_ept+0x42>
                printk(KERN_ERR "vmx_init_ept: Alloc 1 fail \n");
ffffffff812c0a6b:	48 c7 c7 c2 62 7b 81 	mov    $0xffffffff817b62c2,%rdi
ffffffff812c0a72:	31 c0                	xor    %eax,%eax
ffffffff812c0a74:	e8 19 25 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c0a79:	eb 2c                	jmp    ffffffff812c0aa7 <vmx_init_ept+0x6e>
                return -ENOMEM;
        }

        memset(page, 0, PAGE_SIZE);
ffffffff812c0a7b:	31 c0                	xor    %eax,%eax
ffffffff812c0a7d:	b9 00 04 00 00       	mov    $0x400,%ecx
ffffffff812c0a82:	48 89 df             	mov    %rbx,%rdi
ffffffff812c0a85:	f3 ab                	rep stos %eax,%es:(%rdi)
        epgd = (pte_t *)page;
        vcpu->ept_root =  __pa(page);
ffffffff812c0a87:	48 89 df             	mov    %rbx,%rdi
ffffffff812c0a8a:	e8 27 1e da ff       	callq  ffffffff810628b6 <__phys_addr>
ffffffff812c0a8f:	49 89 44 24 40       	mov    %rax,0x40(%r12)
	if(ept_set_stack_perm(vcpu, epgd, conf->rsp))
ffffffff812c0a94:	49 8b 55 08          	mov    0x8(%r13),%rdx
ffffffff812c0a98:	48 89 de             	mov    %rbx,%rsi
ffffffff812c0a9b:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0a9e:	e8 0b ff ff ff       	callq  ffffffff812c09ae <ept_set_stack_perm>
ffffffff812c0aa3:	85 c0                	test   %eax,%eax
ffffffff812c0aa5:	74 07                	je     ffffffff812c0aae <vmx_init_ept+0x75>
                return -ENOMEM;
ffffffff812c0aa7:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
ffffffff812c0aac:	eb 13                	jmp    ffffffff812c0ac1 <vmx_init_ept+0x88>
	if(ept_set_cr3_perm(vcpu, epgd, conf->cr3))
ffffffff812c0aae:	49 8b 55 10          	mov    0x10(%r13),%rdx
ffffffff812c0ab2:	48 89 de             	mov    %rbx,%rsi
ffffffff812c0ab5:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0ab8:	e8 3c ff ff ff       	callq  ffffffff812c09f9 <ept_set_cr3_perm>
ffffffff812c0abd:	85 c0                	test   %eax,%eax
ffffffff812c0abf:	75 e6                	jne    ffffffff812c0aa7 <vmx_init_ept+0x6e>
*/	return 0;

	if((ret = add_mm(mm, vcpu, epgd)))
		return ret;
	return add_mm(azk_init_mm, vcpu, epgd);
}
ffffffff812c0ac1:	5a                   	pop    %rdx
ffffffff812c0ac2:	5b                   	pop    %rbx
ffffffff812c0ac3:	41 5c                	pop    %r12
ffffffff812c0ac5:	41 5d                	pop    %r13
ffffffff812c0ac7:	5d                   	pop    %rbp
ffffffff812c0ac8:	c3                   	retq   

ffffffff812c0ac9 <vmx_create_ept>:

int vmx_create_ept(struct vmx_vcpu *vcpu)
{
ffffffff812c0ac9:	55                   	push   %rbp
ffffffff812c0aca:	65 48 8b 04 25 00 aa 	mov    %gs:0xaa00,%rax
ffffffff812c0ad1:	00 00 
ffffffff812c0ad3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0ad6:	53                   	push   %rbx
ffffffff812c0ad7:	48 89 fb             	mov    %rdi,%rbx
	int ret;

	vcpu->mmu_notifier.ops = &ept_mmu_notifier_ops;
	ret = mmu_notifier_register(&vcpu->mmu_notifier, current->mm);
ffffffff812c0ada:	48 83 c7 10          	add    $0x10,%rdi
		return ret;
	return add_mm(azk_init_mm, vcpu, epgd);
}

int vmx_create_ept(struct vmx_vcpu *vcpu)
{
ffffffff812c0ade:	48 83 ec 18          	sub    $0x18,%rsp
	int ret;

	vcpu->mmu_notifier.ops = &ept_mmu_notifier_ops;
ffffffff812c0ae2:	48 c7 47 10 20 ef 63 	movq   $0xffffffff8163ef20,0x10(%rdi)
ffffffff812c0ae9:	81 
	ret = mmu_notifier_register(&vcpu->mmu_notifier, current->mm);
ffffffff812c0aea:	48 8b b0 80 02 00 00 	mov    0x280(%rax),%rsi
ffffffff812c0af1:	e8 81 ef e3 ff       	callq  ffffffff810ffa77 <mmu_notifier_register>
	if (ret)
ffffffff812c0af6:	85 c0                	test   %eax,%eax
ffffffff812c0af8:	74 0f                	je     ffffffff812c0b09 <vmx_create_ept+0x40>
		goto fail;

	return 0;

fail:
	vmx_free_ept(vcpu->ept_root);
ffffffff812c0afa:	48 8b 7b 40          	mov    0x40(%rbx),%rdi
ffffffff812c0afe:	89 45 ec             	mov    %eax,-0x14(%rbp)
ffffffff812c0b01:	e8 8f f4 ff ff       	callq  ffffffff812bff95 <vmx_free_ept>
ffffffff812c0b06:	8b 45 ec             	mov    -0x14(%rbp),%eax

	return ret;
}
ffffffff812c0b09:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c0b0d:	5b                   	pop    %rbx
ffffffff812c0b0e:	5d                   	pop    %rbp
ffffffff812c0b0f:	c3                   	retq   

ffffffff812c0b10 <vmx_destroy_ept>:

void vmx_destroy_ept(struct vmx_vcpu *vcpu)
{
ffffffff812c0b10:	55                   	push   %rbp
ffffffff812c0b11:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0b14:	53                   	push   %rbx
ffffffff812c0b15:	50                   	push   %rax
ffffffff812c0b16:	65 48 8b 04 25 00 aa 	mov    %gs:0xaa00,%rax
ffffffff812c0b1d:	00 00 
	mmu_notifier_unregister(&vcpu->mmu_notifier, current->mm);
ffffffff812c0b1f:	48 8b b0 80 02 00 00 	mov    0x280(%rax),%rsi

	return ret;
}

void vmx_destroy_ept(struct vmx_vcpu *vcpu)
{
ffffffff812c0b26:	48 89 fb             	mov    %rdi,%rbx
	mmu_notifier_unregister(&vcpu->mmu_notifier, current->mm);
ffffffff812c0b29:	48 8d 7f 10          	lea    0x10(%rdi),%rdi
ffffffff812c0b2d:	e8 74 ef e3 ff       	callq  ffffffff810ffaa6 <mmu_notifier_unregister>
	vmx_free_ept(vcpu->ept_root);
ffffffff812c0b32:	48 8b 7b 40          	mov    0x40(%rbx),%rdi
ffffffff812c0b36:	e8 5a f4 ff ff       	callq  ffffffff812bff95 <vmx_free_ept>
}
ffffffff812c0b3b:	5a                   	pop    %rdx
ffffffff812c0b3c:	5b                   	pop    %rbx
ffffffff812c0b3d:	5d                   	pop    %rbp
ffffffff812c0b3e:	c3                   	retq   

ffffffff812c0b3f <argv_free>:
 * @argv - the argument vector to be freed
 *
 * Frees an argv and the strings it points to.
 */
void argv_free(char **argv)
{
ffffffff812c0b3f:	55                   	push   %rbp
ffffffff812c0b40:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0b43:	53                   	push   %rbx
ffffffff812c0b44:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c0b47:	50                   	push   %rax
	argv--;
	kfree(argv[0]);
ffffffff812c0b48:	48 8b 7f f8          	mov    -0x8(%rdi),%rdi
ffffffff812c0b4c:	e8 38 fe e3 ff       	callq  ffffffff81100989 <kfree>
	kfree(argv);
ffffffff812c0b51:	48 8d 7b f8          	lea    -0x8(%rbx),%rdi
ffffffff812c0b55:	e8 2f fe e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812c0b5a:	5a                   	pop    %rdx
ffffffff812c0b5b:	5b                   	pop    %rbx
ffffffff812c0b5c:	5d                   	pop    %rbp
ffffffff812c0b5d:	c3                   	retq   

ffffffff812c0b5e <argv_split>:
 * The source string at `str' may be undergoing concurrent alteration via
 * userspace sysctl activity (at least).  The argv_split() implementation
 * attempts to handle this gracefully by taking a local copy to work on.
 */
char **argv_split(gfp_t gfp, const char *str, int *argcp)
{
ffffffff812c0b5e:	55                   	push   %rbp
ffffffff812c0b5f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0b62:	41 56                	push   %r14
ffffffff812c0b64:	41 89 fe             	mov    %edi,%r14d
ffffffff812c0b67:	41 55                	push   %r13
ffffffff812c0b69:	41 54                	push   %r12
ffffffff812c0b6b:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c0b6e:	53                   	push   %rbx
ffffffff812c0b6f:	49 89 d4             	mov    %rdx,%r12
	char *argv_str;
	bool was_space;
	char **argv, **argv_ret;
	int argc;

	argv_str = kstrndup(str, KMALLOC_MAX_SIZE - 1, gfp);
ffffffff812c0b72:	be ff ff 3f 00       	mov    $0x3fffff,%esi
ffffffff812c0b77:	44 89 f2             	mov    %r14d,%edx
ffffffff812c0b7a:	e8 65 53 e2 ff       	callq  ffffffff810e5ee4 <kstrndup>
ffffffff812c0b7f:	48 89 c3             	mov    %rax,%rbx
	if (!argv_str)
		return NULL;
ffffffff812c0b82:	31 c0                	xor    %eax,%eax
	bool was_space;
	char **argv, **argv_ret;
	int argc;

	argv_str = kstrndup(str, KMALLOC_MAX_SIZE - 1, gfp);
	if (!argv_str)
ffffffff812c0b84:	48 85 db             	test   %rbx,%rbx
ffffffff812c0b87:	0f 84 99 00 00 00    	je     ffffffff812c0c26 <argv_split+0xc8>
ffffffff812c0b8d:	48 89 da             	mov    %rbx,%rdx
ffffffff812c0b90:	b1 01                	mov    $0x1,%cl
ffffffff812c0b92:	45 31 ed             	xor    %r13d,%r13d
static int count_argc(const char *str)
{
	int count = 0;
	bool was_space;

	for (was_space = true; *str; str++) {
ffffffff812c0b95:	0f b6 02             	movzbl (%rdx),%eax
ffffffff812c0b98:	84 c0                	test   %al,%al
ffffffff812c0b9a:	74 1b                	je     ffffffff812c0bb7 <argv_split+0x59>
		if (isspace(*str)) {
ffffffff812c0b9c:	f6 80 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rax)
ffffffff812c0ba3:	75 0b                	jne    ffffffff812c0bb0 <argv_split+0x52>
			was_space = true;
		} else if (was_space) {
ffffffff812c0ba5:	84 c9                	test   %cl,%cl
ffffffff812c0ba7:	74 09                	je     ffffffff812c0bb2 <argv_split+0x54>
			was_space = false;
			count++;
ffffffff812c0ba9:	41 ff c5             	inc    %r13d

	for (was_space = true; *str; str++) {
		if (isspace(*str)) {
			was_space = true;
		} else if (was_space) {
			was_space = false;
ffffffff812c0bac:	31 c9                	xor    %ecx,%ecx
ffffffff812c0bae:	eb 02                	jmp    ffffffff812c0bb2 <argv_split+0x54>
	int count = 0;
	bool was_space;

	for (was_space = true; *str; str++) {
		if (isspace(*str)) {
			was_space = true;
ffffffff812c0bb0:	b1 01                	mov    $0x1,%cl
static int count_argc(const char *str)
{
	int count = 0;
	bool was_space;

	for (was_space = true; *str; str++) {
ffffffff812c0bb2:	48 ff c2             	inc    %rdx
ffffffff812c0bb5:	eb de                	jmp    ffffffff812c0b95 <argv_split+0x37>
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812c0bb7:	41 8d 7d 02          	lea    0x2(%r13),%edi
ffffffff812c0bbb:	44 89 f6             	mov    %r14d,%esi
ffffffff812c0bbe:	48 63 ff             	movslq %edi,%rdi
ffffffff812c0bc1:	48 c1 e7 03          	shl    $0x3,%rdi
ffffffff812c0bc5:	e8 c1 05 e4 ff       	callq  ffffffff8110118b <__kmalloc>
	if (!argv_str)
		return NULL;

	argc = count_argc(argv_str);
	argv = kmalloc(sizeof(*argv) * (argc + 2), gfp);
	if (!argv) {
ffffffff812c0bca:	48 85 c0             	test   %rax,%rax
ffffffff812c0bcd:	75 0c                	jne    ffffffff812c0bdb <argv_split+0x7d>
		kfree(argv_str);
ffffffff812c0bcf:	48 89 df             	mov    %rbx,%rdi
ffffffff812c0bd2:	e8 b2 fd e3 ff       	callq  ffffffff81100989 <kfree>
		return NULL;
ffffffff812c0bd7:	31 c0                	xor    %eax,%eax
ffffffff812c0bd9:	eb 4b                	jmp    ffffffff812c0c26 <argv_split+0xc8>
	}

	*argv = argv_str;
	argv_ret = ++argv;
ffffffff812c0bdb:	48 8d 50 08          	lea    0x8(%rax),%rdx
	if (!argv) {
		kfree(argv_str);
		return NULL;
	}

	*argv = argv_str;
ffffffff812c0bdf:	48 89 18             	mov    %rbx,(%rax)
	argv_ret = ++argv;
	for (was_space = true; *argv_str; argv_str++) {
ffffffff812c0be2:	40 b6 01             	mov    $0x1,%sil
		kfree(argv_str);
		return NULL;
	}

	*argv = argv_str;
	argv_ret = ++argv;
ffffffff812c0be5:	48 89 d0             	mov    %rdx,%rax
	for (was_space = true; *argv_str; argv_str++) {
ffffffff812c0be8:	0f b6 0b             	movzbl (%rbx),%ecx
ffffffff812c0beb:	84 c9                	test   %cl,%cl
ffffffff812c0bed:	74 24                	je     ffffffff812c0c13 <argv_split+0xb5>
		if (isspace(*argv_str)) {
ffffffff812c0bef:	f6 81 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rcx)
ffffffff812c0bf6:	74 08                	je     ffffffff812c0c00 <argv_split+0xa2>
			was_space = true;
			*argv_str = 0;
ffffffff812c0bf8:	c6 03 00             	movb   $0x0,(%rbx)

	*argv = argv_str;
	argv_ret = ++argv;
	for (was_space = true; *argv_str; argv_str++) {
		if (isspace(*argv_str)) {
			was_space = true;
ffffffff812c0bfb:	40 b6 01             	mov    $0x1,%sil
ffffffff812c0bfe:	eb 0e                	jmp    ffffffff812c0c0e <argv_split+0xb0>
			*argv_str = 0;
		} else if (was_space) {
ffffffff812c0c00:	40 84 f6             	test   %sil,%sil
ffffffff812c0c03:	74 09                	je     ffffffff812c0c0e <argv_split+0xb0>
			was_space = false;
			*argv++ = argv_str;
ffffffff812c0c05:	48 89 18             	mov    %rbx,(%rax)
	for (was_space = true; *argv_str; argv_str++) {
		if (isspace(*argv_str)) {
			was_space = true;
			*argv_str = 0;
		} else if (was_space) {
			was_space = false;
ffffffff812c0c08:	31 f6                	xor    %esi,%esi
			*argv++ = argv_str;
ffffffff812c0c0a:	48 83 c0 08          	add    $0x8,%rax
		return NULL;
	}

	*argv = argv_str;
	argv_ret = ++argv;
	for (was_space = true; *argv_str; argv_str++) {
ffffffff812c0c0e:	48 ff c3             	inc    %rbx
ffffffff812c0c11:	eb d5                	jmp    ffffffff812c0be8 <argv_split+0x8a>
			*argv++ = argv_str;
		}
	}
	*argv = NULL;

	if (argcp)
ffffffff812c0c13:	4d 85 e4             	test   %r12,%r12
		} else if (was_space) {
			was_space = false;
			*argv++ = argv_str;
		}
	}
	*argv = NULL;
ffffffff812c0c16:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)

	if (argcp)
		*argcp = argc;
	return argv_ret;
ffffffff812c0c1d:	48 89 d0             	mov    %rdx,%rax
			*argv++ = argv_str;
		}
	}
	*argv = NULL;

	if (argcp)
ffffffff812c0c20:	74 04                	je     ffffffff812c0c26 <argv_split+0xc8>
		*argcp = argc;
ffffffff812c0c22:	45 89 2c 24          	mov    %r13d,(%r12)
	return argv_ret;
}
ffffffff812c0c26:	5b                   	pop    %rbx
ffffffff812c0c27:	41 5c                	pop    %r12
ffffffff812c0c29:	41 5d                	pop    %r13
ffffffff812c0c2b:	41 5e                	pop    %r14
ffffffff812c0c2d:	5d                   	pop    %rbp
ffffffff812c0c2e:	c3                   	retq   

ffffffff812c0c2f <module_bug_finalize>:
	return bug;
}

void module_bug_finalize(const Elf_Ehdr *hdr, const Elf_Shdr *sechdrs,
			 struct module *mod)
{
ffffffff812c0c2f:	55                   	push   %rbp
ffffffff812c0c30:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0c33:	41 57                	push   %r15
ffffffff812c0c35:	41 56                	push   %r14
ffffffff812c0c37:	41 55                	push   %r13
ffffffff812c0c39:	41 54                	push   %r12
ffffffff812c0c3b:	4c 8d 66 40          	lea    0x40(%rsi),%r12
ffffffff812c0c3f:	53                   	push   %rbx
ffffffff812c0c40:	49 89 fd             	mov    %rdi,%r13
ffffffff812c0c43:	48 89 d3             	mov    %rdx,%rbx
	mod->bug_table = NULL;
	mod->num_bugs = 0;

	/* Find the __bug_table section, if present */
	secstrings = (char *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
	for (i = 1; i < hdr->e_shnum; i++) {
ffffffff812c0c46:	41 bf 01 00 00 00    	mov    $0x1,%r15d
	return bug;
}

void module_bug_finalize(const Elf_Ehdr *hdr, const Elf_Shdr *sechdrs,
			 struct module *mod)
{
ffffffff812c0c4c:	48 83 ec 18          	sub    $0x18,%rsp
	char *secstrings;
	unsigned int i;

	mod->bug_table = NULL;
ffffffff812c0c50:	48 c7 82 98 01 00 00 	movq   $0x0,0x198(%rdx)
ffffffff812c0c57:	00 00 00 00 
	mod->num_bugs = 0;
ffffffff812c0c5b:	c7 82 84 01 00 00 00 	movl   $0x0,0x184(%rdx)
ffffffff812c0c62:	00 00 00 

	/* Find the __bug_table section, if present */
	secstrings = (char *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
ffffffff812c0c65:	0f b7 47 3e          	movzwl 0x3e(%rdi),%eax
ffffffff812c0c69:	48 c1 e0 06          	shl    $0x6,%rax
ffffffff812c0c6d:	48 8b 44 06 18       	mov    0x18(%rsi,%rax,1),%rax
ffffffff812c0c72:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	for (i = 1; i < hdr->e_shnum; i++) {
ffffffff812c0c76:	0f b7 47 3c          	movzwl 0x3c(%rdi),%eax
ffffffff812c0c7a:	89 45 c4             	mov    %eax,-0x3c(%rbp)
ffffffff812c0c7d:	44 3b 7d c4          	cmp    -0x3c(%rbp),%r15d
ffffffff812c0c81:	73 46                	jae    ffffffff812c0cc9 <module_bug_finalize+0x9a>
		if (strcmp(secstrings+sechdrs[i].sh_name, "__bug_table"))
ffffffff812c0c83:	41 8b 3c 24          	mov    (%r12),%edi
ffffffff812c0c87:	48 c7 c6 e1 62 7b 81 	mov    $0xffffffff817b62e1,%rsi
ffffffff812c0c8e:	4d 89 e6             	mov    %r12,%r14
ffffffff812c0c91:	48 03 7d c8          	add    -0x38(%rbp),%rdi
ffffffff812c0c95:	49 83 c4 40          	add    $0x40,%r12
ffffffff812c0c99:	4c 01 ef             	add    %r13,%rdi
ffffffff812c0c9c:	e8 b7 6f 00 00       	callq  ffffffff812c7c58 <strcmp>
ffffffff812c0ca1:	85 c0                	test   %eax,%eax
ffffffff812c0ca3:	74 05                	je     ffffffff812c0caa <module_bug_finalize+0x7b>
	mod->bug_table = NULL;
	mod->num_bugs = 0;

	/* Find the __bug_table section, if present */
	secstrings = (char *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
	for (i = 1; i < hdr->e_shnum; i++) {
ffffffff812c0ca5:	41 ff c7             	inc    %r15d
ffffffff812c0ca8:	eb d3                	jmp    ffffffff812c0c7d <module_bug_finalize+0x4e>
		if (strcmp(secstrings+sechdrs[i].sh_name, "__bug_table"))
			continue;
		mod->bug_table = (void *) sechdrs[i].sh_addr;
ffffffff812c0caa:	49 8b 46 10          	mov    0x10(%r14),%rax
		mod->num_bugs = sechdrs[i].sh_size / sizeof(struct bug_entry);
ffffffff812c0cae:	b9 0c 00 00 00       	mov    $0xc,%ecx
ffffffff812c0cb3:	31 d2                	xor    %edx,%edx
	/* Find the __bug_table section, if present */
	secstrings = (char *)hdr + sechdrs[hdr->e_shstrndx].sh_offset;
	for (i = 1; i < hdr->e_shnum; i++) {
		if (strcmp(secstrings+sechdrs[i].sh_name, "__bug_table"))
			continue;
		mod->bug_table = (void *) sechdrs[i].sh_addr;
ffffffff812c0cb5:	48 89 83 98 01 00 00 	mov    %rax,0x198(%rbx)
		mod->num_bugs = sechdrs[i].sh_size / sizeof(struct bug_entry);
ffffffff812c0cbc:	49 8b 46 20          	mov    0x20(%r14),%rax
ffffffff812c0cc0:	48 f7 f1             	div    %rcx
ffffffff812c0cc3:	89 83 84 01 00 00    	mov    %eax,0x184(%rbx)
 * the _rcu list-traversal primitives, such as
 * list_for_each_entry_rcu().
 */
static inline void list_add_rcu(struct list_head *new, struct list_head *head)
{
	__list_add_rcu(new, head, head->next);
ffffffff812c0cc9:	48 8b 15 00 52 77 00 	mov    0x775200(%rip),%rdx        # ffffffff81a35ed0 <module_bug_list>
	 * traversals, but since we only traverse on BUG()s, a spinlock
	 * could potentially lead to deadlock and thus be counter-productive.
	 * Thus, this uses RCU to safely manipulate the bug list, since BUG
	 * must run in non-interruptive state.
	 */
	list_add_rcu(&mod->bug_list, &module_bug_list);
ffffffff812c0cd0:	48 8d bb 88 01 00 00 	lea    0x188(%rbx),%rdi
ffffffff812c0cd7:	48 c7 c6 d0 5e a3 81 	mov    $0xffffffff81a35ed0,%rsi
ffffffff812c0cde:	e8 ef 42 01 00       	callq  ffffffff812d4fd2 <__list_add_rcu>
}
ffffffff812c0ce3:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c0ce7:	5b                   	pop    %rbx
ffffffff812c0ce8:	41 5c                	pop    %r12
ffffffff812c0cea:	41 5d                	pop    %r13
ffffffff812c0cec:	41 5e                	pop    %r14
ffffffff812c0cee:	41 5f                	pop    %r15
ffffffff812c0cf0:	5d                   	pop    %rbp
ffffffff812c0cf1:	c3                   	retq   

ffffffff812c0cf2 <module_bug_cleanup>:

void module_bug_cleanup(struct module *mod)
{
ffffffff812c0cf2:	55                   	push   %rbp
ffffffff812c0cf3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0cf6:	53                   	push   %rbx
ffffffff812c0cf7:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c0cfa:	50                   	push   %rax
	list_del_rcu(&mod->bug_list);
ffffffff812c0cfb:	48 8d bf 88 01 00 00 	lea    0x188(%rdi),%rdi
 * or call_rcu() must be used to defer freeing until an RCU
 * grace period has elapsed.
 */
static inline void list_del_rcu(struct list_head *entry)
{
	__list_del_entry(entry);
ffffffff812c0d02:	e8 0c 42 01 00       	callq  ffffffff812d4f13 <__list_del_entry>
	entry->prev = LIST_POISON2;
ffffffff812c0d07:	48 b8 00 02 20 00 00 	movabs $0xdead000000200200,%rax
ffffffff812c0d0e:	00 ad de 
ffffffff812c0d11:	48 89 83 90 01 00 00 	mov    %rax,0x190(%rbx)
}
ffffffff812c0d18:	5a                   	pop    %rdx
ffffffff812c0d19:	5b                   	pop    %rbx
ffffffff812c0d1a:	5d                   	pop    %rbp
ffffffff812c0d1b:	c3                   	retq   

ffffffff812c0d1c <find_bug>:

const struct bug_entry *find_bug(unsigned long bugaddr)
{
	const struct bug_entry *bug;

	for (bug = __start___bug_table; bug < __stop___bug_table; ++bug)
ffffffff812c0d1c:	48 c7 c0 f0 3f 7d 81 	mov    $0xffffffff817d3ff0,%rax
ffffffff812c0d23:	48 3d f0 b7 7d 81    	cmp    $0xffffffff817db7f0,%rax
ffffffff812c0d29:	73 11                	jae    ffffffff812c0d3c <find_bug+0x20>
		if (bugaddr == bug_addr(bug))
ffffffff812c0d2b:	48 63 10             	movslq (%rax),%rdx
ffffffff812c0d2e:	48 01 c2             	add    %rax,%rdx
ffffffff812c0d31:	48 39 d7             	cmp    %rdx,%rdi
ffffffff812c0d34:	74 67                	je     ffffffff812c0d9d <find_bug+0x81>

const struct bug_entry *find_bug(unsigned long bugaddr)
{
	const struct bug_entry *bug;

	for (bug = __start___bug_table; bug < __stop___bug_table; ++bug)
ffffffff812c0d36:	48 83 c0 0c          	add    $0xc,%rax
ffffffff812c0d3a:	eb e7                	jmp    ffffffff812c0d23 <find_bug+0x7>
	return NULL;
}
#endif

const struct bug_entry *find_bug(unsigned long bugaddr)
{
ffffffff812c0d3c:	55                   	push   %rbp
ffffffff812c0d3d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0d40:	48 83 ec 10          	sub    $0x10,%rsp
{
	struct module *mod;
	const struct bug_entry *bug = NULL;

	rcu_read_lock();
	list_for_each_entry_rcu(mod, &module_bug_list, bug_list) {
ffffffff812c0d44:	48 8b 05 85 51 77 00 	mov    0x775185(%rip),%rax        # ffffffff81a35ed0 <module_bug_list>
ffffffff812c0d4b:	48 89 45 f8          	mov    %rax,-0x8(%rbp)
ffffffff812c0d4f:	48 8b 55 f8          	mov    -0x8(%rbp),%rdx
ffffffff812c0d53:	48 81 ea 88 01 00 00 	sub    $0x188,%rdx
ffffffff812c0d5a:	48 8d 82 88 01 00 00 	lea    0x188(%rdx),%rax
ffffffff812c0d61:	48 3d d0 5e a3 81    	cmp    $0xffffffff81a35ed0,%rax
ffffffff812c0d67:	74 31                	je     ffffffff812c0d9a <find_bug+0x7e>
		unsigned i;

		bug = mod->bug_table;
ffffffff812c0d69:	48 8b 82 98 01 00 00 	mov    0x198(%rdx),%rax
		for (i = 0; i < mod->num_bugs; ++i, ++bug)
ffffffff812c0d70:	44 8b 82 84 01 00 00 	mov    0x184(%rdx),%r8d
ffffffff812c0d77:	31 c9                	xor    %ecx,%ecx
ffffffff812c0d79:	44 39 c1             	cmp    %r8d,%ecx
ffffffff812c0d7c:	74 13                	je     ffffffff812c0d91 <find_bug+0x75>
			if (bugaddr == bug_addr(bug))
ffffffff812c0d7e:	48 63 30             	movslq (%rax),%rsi
ffffffff812c0d81:	48 01 c6             	add    %rax,%rsi
ffffffff812c0d84:	48 39 f7             	cmp    %rsi,%rdi
ffffffff812c0d87:	74 13                	je     ffffffff812c0d9c <find_bug+0x80>
	rcu_read_lock();
	list_for_each_entry_rcu(mod, &module_bug_list, bug_list) {
		unsigned i;

		bug = mod->bug_table;
		for (i = 0; i < mod->num_bugs; ++i, ++bug)
ffffffff812c0d89:	ff c1                	inc    %ecx
ffffffff812c0d8b:	48 83 c0 0c          	add    $0xc,%rax
ffffffff812c0d8f:	eb e8                	jmp    ffffffff812c0d79 <find_bug+0x5d>
{
	struct module *mod;
	const struct bug_entry *bug = NULL;

	rcu_read_lock();
	list_for_each_entry_rcu(mod, &module_bug_list, bug_list) {
ffffffff812c0d91:	48 8b 82 88 01 00 00 	mov    0x188(%rdx),%rax
ffffffff812c0d98:	eb b1                	jmp    ffffffff812c0d4b <find_bug+0x2f>
		bug = mod->bug_table;
		for (i = 0; i < mod->num_bugs; ++i, ++bug)
			if (bugaddr == bug_addr(bug))
				goto out;
	}
	bug = NULL;
ffffffff812c0d9a:	31 c0                	xor    %eax,%eax
	for (bug = __start___bug_table; bug < __stop___bug_table; ++bug)
		if (bugaddr == bug_addr(bug))
			return bug;

	return module_find_bug(bugaddr);
}
ffffffff812c0d9c:	c9                   	leaveq 
ffffffff812c0d9d:	c3                   	retq   

ffffffff812c0d9e <report_bug>:

enum bug_trap_type report_bug(unsigned long bugaddr, struct pt_regs *regs)
{
ffffffff812c0d9e:	55                   	push   %rbp
ffffffff812c0d9f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0da2:	41 57                	push   %r15
ffffffff812c0da4:	41 56                	push   %r14
ffffffff812c0da6:	41 55                	push   %r13
ffffffff812c0da8:	41 54                	push   %r12
ffffffff812c0daa:	49 89 fd             	mov    %rdi,%r13
ffffffff812c0dad:	53                   	push   %rbx
ffffffff812c0dae:	51                   	push   %rcx
ffffffff812c0daf:	49 89 f6             	mov    %rsi,%r14
	const struct bug_entry *bug;
	const char *file;
	unsigned line, warning;

	if (!is_valid_bugaddr(bugaddr))
ffffffff812c0db2:	e8 c7 1a d7 ff       	callq  ffffffff8103287e <is_valid_bugaddr>
		return BUG_TRAP_TYPE_NONE;
ffffffff812c0db7:	31 d2                	xor    %edx,%edx
{
	const struct bug_entry *bug;
	const char *file;
	unsigned line, warning;

	if (!is_valid_bugaddr(bugaddr))
ffffffff812c0db9:	85 c0                	test   %eax,%eax
ffffffff812c0dbb:	0f 84 c6 00 00 00    	je     ffffffff812c0e87 <report_bug+0xe9>
		return BUG_TRAP_TYPE_NONE;

	bug = find_bug(bugaddr);
ffffffff812c0dc1:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c0dc4:	e8 53 ff ff ff       	callq  ffffffff812c0d1c <find_bug>

	file = NULL;
	line = 0;
	warning = 0;

	if (bug) {
ffffffff812c0dc9:	48 85 c0             	test   %rax,%rax
	unsigned line, warning;

	if (!is_valid_bugaddr(bugaddr))
		return BUG_TRAP_TYPE_NONE;

	bug = find_bug(bugaddr);
ffffffff812c0dcc:	49 89 c4             	mov    %rax,%r12

	file = NULL;
	line = 0;
	warning = 0;

	if (bug) {
ffffffff812c0dcf:	74 72                	je     ffffffff812c0e43 <report_bug+0xa5>
#ifdef CONFIG_DEBUG_BUGVERBOSE
#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
		file = bug->file;
#else
		file = (const char *)bug + bug->file_disp;
ffffffff812c0dd1:	48 63 58 04          	movslq 0x4(%rax),%rbx
#endif
		line = bug->line;
ffffffff812c0dd5:	44 0f b7 78 08       	movzwl 0x8(%rax),%r15d
	if (bug) {
#ifdef CONFIG_DEBUG_BUGVERBOSE
#ifndef CONFIG_GENERIC_BUG_RELATIVE_POINTERS
		file = bug->file;
#else
		file = (const char *)bug + bug->file_disp;
ffffffff812c0dda:	48 01 c3             	add    %rax,%rbx
		line = bug->line;
#endif
		warning = (bug->flags & BUGFLAG_WARNING) != 0;
	}

	if (warning) {
ffffffff812c0ddd:	f6 40 0a 01          	testb  $0x1,0xa(%rax)
ffffffff812c0de1:	74 65                	je     ffffffff812c0e48 <report_bug+0xaa>
		/* this is a WARN_ON rather than BUG/BUG_ON */
		pr_warn("------------[ cut here ]------------\n");
ffffffff812c0de3:	31 c0                	xor    %eax,%eax
ffffffff812c0de5:	48 c7 c7 23 0e 79 81 	mov    $0xffffffff81790e23,%rdi
ffffffff812c0dec:	e8 a1 21 17 00       	callq  ffffffff81432f92 <printk>

		if (file)
ffffffff812c0df1:	48 85 db             	test   %rbx,%rbx
ffffffff812c0df4:	74 16                	je     ffffffff812c0e0c <report_bug+0x6e>
			pr_warn("WARNING: at %s:%u\n", file, line);
ffffffff812c0df6:	44 89 fa             	mov    %r15d,%edx
ffffffff812c0df9:	48 89 de             	mov    %rbx,%rsi
ffffffff812c0dfc:	48 c7 c7 ed 62 7b 81 	mov    $0xffffffff817b62ed,%rdi
ffffffff812c0e03:	31 c0                	xor    %eax,%eax
ffffffff812c0e05:	e8 88 21 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c0e0a:	eb 11                	jmp    ffffffff812c0e1d <report_bug+0x7f>
		else
			pr_warn("WARNING: at %p [verbose debug info unavailable]\n",
ffffffff812c0e0c:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c0e0f:	48 c7 c7 02 63 7b 81 	mov    $0xffffffff817b6302,%rdi
ffffffff812c0e16:	31 c0                	xor    %eax,%eax
ffffffff812c0e18:	e8 75 21 17 00       	callq  ffffffff81432f92 <printk>
				(void *)bugaddr);

		print_modules();
ffffffff812c0e1d:	e8 17 f7 de ff       	callq  ffffffff810b0539 <print_modules>
		show_regs(regs);
ffffffff812c0e22:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c0e25:	e8 c6 18 d7 ff       	callq  ffffffff810326f0 <show_regs>
		print_oops_end_marker();
ffffffff812c0e2a:	e8 36 54 da ff       	callq  ffffffff81066265 <print_oops_end_marker>
		/* Just a warning, don't kill lockdep. */
		add_taint(BUG_GET_TAINT(bug), LOCKDEP_STILL_OK);
ffffffff812c0e2f:	41 0f b6 7c 24 0b    	movzbl 0xb(%r12),%edi
ffffffff812c0e35:	31 f6                	xor    %esi,%esi
ffffffff812c0e37:	e8 34 52 da ff       	callq  ffffffff81066070 <add_taint>
		return BUG_TRAP_TYPE_WARN;
ffffffff812c0e3c:	ba 01 00 00 00       	mov    $0x1,%edx
ffffffff812c0e41:	eb 44                	jmp    ffffffff812c0e87 <report_bug+0xe9>
		return BUG_TRAP_TYPE_NONE;

	bug = find_bug(bugaddr);

	file = NULL;
	line = 0;
ffffffff812c0e43:	45 31 ff             	xor    %r15d,%r15d
	if (!is_valid_bugaddr(bugaddr))
		return BUG_TRAP_TYPE_NONE;

	bug = find_bug(bugaddr);

	file = NULL;
ffffffff812c0e46:	31 db                	xor    %ebx,%ebx
		/* Just a warning, don't kill lockdep. */
		add_taint(BUG_GET_TAINT(bug), LOCKDEP_STILL_OK);
		return BUG_TRAP_TYPE_WARN;
	}

	printk(KERN_DEFAULT "------------[ cut here ]------------\n");
ffffffff812c0e48:	31 c0                	xor    %eax,%eax
ffffffff812c0e4a:	48 c7 c7 35 63 7b 81 	mov    $0xffffffff817b6335,%rdi
ffffffff812c0e51:	e8 3c 21 17 00       	callq  ffffffff81432f92 <printk>

	if (file)
ffffffff812c0e56:	48 85 db             	test   %rbx,%rbx
ffffffff812c0e59:	74 16                	je     ffffffff812c0e71 <report_bug+0xd3>
		pr_crit("kernel BUG at %s:%u!\n", file, line);
ffffffff812c0e5b:	44 89 fa             	mov    %r15d,%edx
ffffffff812c0e5e:	48 89 de             	mov    %rbx,%rsi
ffffffff812c0e61:	48 c7 c7 5d 63 7b 81 	mov    $0xffffffff817b635d,%rdi
ffffffff812c0e68:	31 c0                	xor    %eax,%eax
ffffffff812c0e6a:	e8 23 21 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c0e6f:	eb 11                	jmp    ffffffff812c0e82 <report_bug+0xe4>
	else
		pr_crit("Kernel BUG at %p [verbose debug info unavailable]\n",
ffffffff812c0e71:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c0e74:	48 c7 c7 75 63 7b 81 	mov    $0xffffffff817b6375,%rdi
ffffffff812c0e7b:	31 c0                	xor    %eax,%eax
ffffffff812c0e7d:	e8 10 21 17 00       	callq  ffffffff81432f92 <printk>
			(void *)bugaddr);

	return BUG_TRAP_TYPE_BUG;
ffffffff812c0e82:	ba 02 00 00 00       	mov    $0x2,%edx
}
ffffffff812c0e87:	89 d0                	mov    %edx,%eax
ffffffff812c0e89:	5a                   	pop    %rdx
ffffffff812c0e8a:	5b                   	pop    %rbx
ffffffff812c0e8b:	41 5c                	pop    %r12
ffffffff812c0e8d:	41 5d                	pop    %r13
ffffffff812c0e8f:	41 5e                	pop    %r14
ffffffff812c0e91:	41 5f                	pop    %r15
ffffffff812c0e93:	5d                   	pop    %rbp
ffffffff812c0e94:	c3                   	retq   

ffffffff812c0e95 <get_option>:
 *	2 - int found including a subsequent comma
 *	3 - hyphen found to denote a range
 */

int get_option(char **str, int *pint)
{
ffffffff812c0e95:	55                   	push   %rbp
ffffffff812c0e96:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0e99:	41 55                	push   %r13
ffffffff812c0e9b:	41 54                	push   %r12
ffffffff812c0e9d:	53                   	push   %rbx
ffffffff812c0e9e:	51                   	push   %rcx
	char *cur = *str;
ffffffff812c0e9f:	4c 8b 27             	mov    (%rdi),%r12

	if (!cur || !(*cur))
ffffffff812c0ea2:	4d 85 e4             	test   %r12,%r12
ffffffff812c0ea5:	75 04                	jne    ffffffff812c0eab <get_option+0x16>
		return 0;
ffffffff812c0ea7:	31 c0                	xor    %eax,%eax
ffffffff812c0ea9:	eb 46                	jmp    ffffffff812c0ef1 <get_option+0x5c>

int get_option(char **str, int *pint)
{
	char *cur = *str;

	if (!cur || !(*cur))
ffffffff812c0eab:	41 80 3c 24 00       	cmpb   $0x0,(%r12)
ffffffff812c0eb0:	74 f5                	je     ffffffff812c0ea7 <get_option+0x12>
ffffffff812c0eb2:	49 89 f5             	mov    %rsi,%r13
ffffffff812c0eb5:	48 89 fb             	mov    %rdi,%rbx
		return 0;
	*pint = simple_strtol(cur, str, 0);
ffffffff812c0eb8:	31 d2                	xor    %edx,%edx
ffffffff812c0eba:	48 89 fe             	mov    %rdi,%rsi
ffffffff812c0ebd:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c0ec0:	e8 56 7f 00 00       	callq  ffffffff812c8e1b <simple_strtol>
ffffffff812c0ec5:	41 89 45 00          	mov    %eax,0x0(%r13)
	if (cur == *str)
ffffffff812c0ec9:	48 8b 03             	mov    (%rbx),%rax
ffffffff812c0ecc:	49 39 c4             	cmp    %rax,%r12
ffffffff812c0ecf:	74 d6                	je     ffffffff812c0ea7 <get_option+0x12>
		return 0;
	if (**str == ',') {
ffffffff812c0ed1:	8a 10                	mov    (%rax),%dl
ffffffff812c0ed3:	80 fa 2c             	cmp    $0x2c,%dl
ffffffff812c0ed6:	75 0d                	jne    ffffffff812c0ee5 <get_option+0x50>
		(*str)++;
ffffffff812c0ed8:	48 ff c0             	inc    %rax
ffffffff812c0edb:	48 89 03             	mov    %rax,(%rbx)
		return 2;
ffffffff812c0ede:	b8 02 00 00 00       	mov    $0x2,%eax
ffffffff812c0ee3:	eb 0c                	jmp    ffffffff812c0ef1 <get_option+0x5c>
	}
	if (**str == '-')
		return 3;

	return 1;
ffffffff812c0ee5:	31 c0                	xor    %eax,%eax
ffffffff812c0ee7:	80 fa 2d             	cmp    $0x2d,%dl
ffffffff812c0eea:	0f 94 c0             	sete   %al
ffffffff812c0eed:	8d 44 00 01          	lea    0x1(%rax,%rax,1),%eax
}
ffffffff812c0ef1:	5a                   	pop    %rdx
ffffffff812c0ef2:	5b                   	pop    %rbx
ffffffff812c0ef3:	41 5c                	pop    %r12
ffffffff812c0ef5:	41 5d                	pop    %r13
ffffffff812c0ef7:	5d                   	pop    %rbp
ffffffff812c0ef8:	c3                   	retq   

ffffffff812c0ef9 <get_options>:
 *	the parse to end (typically a null terminator, if @str is
 *	completely parseable).
 */

char *get_options(const char *str, int nints, int *ints)
{
ffffffff812c0ef9:	55                   	push   %rbp
ffffffff812c0efa:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0efd:	41 57                	push   %r15
ffffffff812c0eff:	41 56                	push   %r14
ffffffff812c0f01:	41 55                	push   %r13
ffffffff812c0f03:	41 54                	push   %r12
ffffffff812c0f05:	41 89 f5             	mov    %esi,%r13d
ffffffff812c0f08:	53                   	push   %rbx
ffffffff812c0f09:	49 89 d4             	mov    %rdx,%r12
	int res, i = 1;
ffffffff812c0f0c:	bb 01 00 00 00       	mov    $0x1,%ebx
 *	the parse to end (typically a null terminator, if @str is
 *	completely parseable).
 */

char *get_options(const char *str, int nints, int *ints)
{
ffffffff812c0f11:	48 83 ec 18          	sub    $0x18,%rsp
ffffffff812c0f15:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)
	int res, i = 1;

	while (i < nints) {
ffffffff812c0f19:	44 39 eb             	cmp    %r13d,%ebx
ffffffff812c0f1c:	7d 5a                	jge    ffffffff812c0f78 <get_options+0x7f>
		res = get_option((char **)&str, ints + i);
ffffffff812c0f1e:	48 63 c3             	movslq %ebx,%rax
ffffffff812c0f21:	48 8d 7d c8          	lea    -0x38(%rbp),%rdi
ffffffff812c0f25:	4d 8d 34 84          	lea    (%r12,%rax,4),%r14
ffffffff812c0f29:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c0f2c:	e8 64 ff ff ff       	callq  ffffffff812c0e95 <get_option>
		if (res == 0)
ffffffff812c0f31:	85 c0                	test   %eax,%eax
char *get_options(const char *str, int nints, int *ints)
{
	int res, i = 1;

	while (i < nints) {
		res = get_option((char **)&str, ints + i);
ffffffff812c0f33:	41 89 c7             	mov    %eax,%r15d
		if (res == 0)
ffffffff812c0f36:	74 40                	je     ffffffff812c0f78 <get_options+0x7f>
			break;
		if (res == 3) {
ffffffff812c0f38:	83 f8 03             	cmp    $0x3,%eax
ffffffff812c0f3b:	75 34                	jne    ffffffff812c0f71 <get_options+0x78>

static int get_range(char **str, int *pint)
{
	int x, inc_counter, upper_range;

	(*str)++;
ffffffff812c0f3d:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
	upper_range = simple_strtol((*str), NULL, 0);
ffffffff812c0f41:	31 d2                	xor    %edx,%edx
ffffffff812c0f43:	31 f6                	xor    %esi,%esi

static int get_range(char **str, int *pint)
{
	int x, inc_counter, upper_range;

	(*str)++;
ffffffff812c0f45:	48 8d 78 01          	lea    0x1(%rax),%rdi
ffffffff812c0f49:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)
	upper_range = simple_strtol((*str), NULL, 0);
ffffffff812c0f4d:	e8 c9 7e 00 00       	callq  ffffffff812c8e1b <simple_strtol>
	inc_counter = upper_range - *pint;
ffffffff812c0f52:	41 8b 16             	mov    (%r14),%edx
static int get_range(char **str, int *pint)
{
	int x, inc_counter, upper_range;

	(*str)++;
	upper_range = simple_strtol((*str), NULL, 0);
ffffffff812c0f55:	89 c1                	mov    %eax,%ecx
	inc_counter = upper_range - *pint;
ffffffff812c0f57:	29 d0                	sub    %edx,%eax
	for (x = *pint; x < upper_range; x++)
ffffffff812c0f59:	39 d1                	cmp    %edx,%ecx
ffffffff812c0f5b:	7e 0c                	jle    ffffffff812c0f69 <get_options+0x70>
		*pint++ = x;
ffffffff812c0f5d:	49 83 c6 04          	add    $0x4,%r14
ffffffff812c0f61:	41 89 56 fc          	mov    %edx,-0x4(%r14)
	int x, inc_counter, upper_range;

	(*str)++;
	upper_range = simple_strtol((*str), NULL, 0);
	inc_counter = upper_range - *pint;
	for (x = *pint; x < upper_range; x++)
ffffffff812c0f65:	ff c2                	inc    %edx
ffffffff812c0f67:	eb f0                	jmp    ffffffff812c0f59 <get_options+0x60>
		if (res == 0)
			break;
		if (res == 3) {
			int range_nums;
			range_nums = get_range((char **)&str, ints + i);
			if (range_nums < 0)
ffffffff812c0f69:	85 c0                	test   %eax,%eax
ffffffff812c0f6b:	78 0b                	js     ffffffff812c0f78 <get_options+0x7f>
			/*
			 * Decrement the result by one to leave out the
			 * last number in the range.  The next iteration
			 * will handle the upper number in the range
			 */
			i += (range_nums - 1);
ffffffff812c0f6d:	8d 5c 03 ff          	lea    -0x1(%rbx,%rax,1),%ebx
		}
		i++;
ffffffff812c0f71:	ff c3                	inc    %ebx
		if (res == 1)
ffffffff812c0f73:	41 ff cf             	dec    %r15d
ffffffff812c0f76:	75 a1                	jne    ffffffff812c0f19 <get_options+0x20>
			break;
	}
	ints[0] = i - 1;
ffffffff812c0f78:	ff cb                	dec    %ebx
	return (char *)str;
}
ffffffff812c0f7a:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
		}
		i++;
		if (res == 1)
			break;
	}
	ints[0] = i - 1;
ffffffff812c0f7e:	41 89 1c 24          	mov    %ebx,(%r12)
	return (char *)str;
}
ffffffff812c0f82:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c0f86:	5b                   	pop    %rbx
ffffffff812c0f87:	41 5c                	pop    %r12
ffffffff812c0f89:	41 5d                	pop    %r13
ffffffff812c0f8b:	41 5e                	pop    %r14
ffffffff812c0f8d:	41 5f                	pop    %r15
ffffffff812c0f8f:	5d                   	pop    %rbp
ffffffff812c0f90:	c3                   	retq   

ffffffff812c0f91 <memparse>:
 *	Parses a string into a number.  The number stored at @ptr is
 *	potentially suffixed with K, M, G, T, P, E.
 */

unsigned long long memparse(const char *ptr, char **retptr)
{
ffffffff812c0f91:	55                   	push   %rbp
	char *endptr;	/* local pointer to end of parsed string */

	unsigned long long ret = simple_strtoull(ptr, &endptr, 0);
ffffffff812c0f92:	31 d2                	xor    %edx,%edx
 *	Parses a string into a number.  The number stored at @ptr is
 *	potentially suffixed with K, M, G, T, P, E.
 */

unsigned long long memparse(const char *ptr, char **retptr)
{
ffffffff812c0f94:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c0f97:	53                   	push   %rbx
ffffffff812c0f98:	48 89 f3             	mov    %rsi,%rbx
	char *endptr;	/* local pointer to end of parsed string */

	unsigned long long ret = simple_strtoull(ptr, &endptr, 0);
ffffffff812c0f9b:	48 8d 75 e8          	lea    -0x18(%rbp),%rsi
 *	Parses a string into a number.  The number stored at @ptr is
 *	potentially suffixed with K, M, G, T, P, E.
 */

unsigned long long memparse(const char *ptr, char **retptr)
{
ffffffff812c0f9f:	48 83 ec 18          	sub    $0x18,%rsp
	char *endptr;	/* local pointer to end of parsed string */

	unsigned long long ret = simple_strtoull(ptr, &endptr, 0);
ffffffff812c0fa3:	e8 db 75 00 00       	callq  ffffffff812c8583 <simple_strtoull>

	switch (*endptr) {
ffffffff812c0fa8:	48 8b 4d e8          	mov    -0x18(%rbp),%rcx
ffffffff812c0fac:	8a 11                	mov    (%rcx),%dl
ffffffff812c0fae:	80 fa 54             	cmp    $0x54,%dl
ffffffff812c0fb1:	74 4b                	je     ffffffff812c0ffe <memparse+0x6d>
ffffffff812c0fb3:	7f 1d                	jg     ffffffff812c0fd2 <memparse+0x41>
ffffffff812c0fb5:	80 fa 4b             	cmp    $0x4b,%dl
ffffffff812c0fb8:	74 50                	je     ffffffff812c100a <memparse+0x79>
ffffffff812c0fba:	7f 0a                	jg     ffffffff812c0fc6 <memparse+0x35>
ffffffff812c0fbc:	80 fa 45             	cmp    $0x45,%dl
ffffffff812c0fbf:	74 35                	je     ffffffff812c0ff6 <memparse+0x65>
ffffffff812c0fc1:	80 fa 47             	cmp    $0x47,%dl
ffffffff812c0fc4:	eb 1b                	jmp    ffffffff812c0fe1 <memparse+0x50>
ffffffff812c0fc6:	80 fa 4d             	cmp    $0x4d,%dl
ffffffff812c0fc9:	74 3b                	je     ffffffff812c1006 <memparse+0x75>
ffffffff812c0fcb:	80 fa 50             	cmp    $0x50,%dl
ffffffff812c0fce:	74 2a                	je     ffffffff812c0ffa <memparse+0x69>
ffffffff812c0fd0:	eb 43                	jmp    ffffffff812c1015 <memparse+0x84>
ffffffff812c0fd2:	80 fa 6b             	cmp    $0x6b,%dl
ffffffff812c0fd5:	74 33                	je     ffffffff812c100a <memparse+0x79>
ffffffff812c0fd7:	7f 0c                	jg     ffffffff812c0fe5 <memparse+0x54>
ffffffff812c0fd9:	80 fa 65             	cmp    $0x65,%dl
ffffffff812c0fdc:	74 18                	je     ffffffff812c0ff6 <memparse+0x65>
ffffffff812c0fde:	80 fa 67             	cmp    $0x67,%dl
ffffffff812c0fe1:	74 1f                	je     ffffffff812c1002 <memparse+0x71>
ffffffff812c0fe3:	eb 30                	jmp    ffffffff812c1015 <memparse+0x84>
ffffffff812c0fe5:	80 fa 70             	cmp    $0x70,%dl
ffffffff812c0fe8:	74 10                	je     ffffffff812c0ffa <memparse+0x69>
ffffffff812c0fea:	80 fa 74             	cmp    $0x74,%dl
ffffffff812c0fed:	74 0f                	je     ffffffff812c0ffe <memparse+0x6d>
ffffffff812c0fef:	80 fa 6d             	cmp    $0x6d,%dl
ffffffff812c0ff2:	75 21                	jne    ffffffff812c1015 <memparse+0x84>
ffffffff812c0ff4:	eb 10                	jmp    ffffffff812c1006 <memparse+0x75>
	case 'E':
	case 'e':
		ret <<= 10;
ffffffff812c0ff6:	48 c1 e0 0a          	shl    $0xa,%rax
	case 'P':
	case 'p':
		ret <<= 10;
ffffffff812c0ffa:	48 c1 e0 0a          	shl    $0xa,%rax
	case 'T':
	case 't':
		ret <<= 10;
ffffffff812c0ffe:	48 c1 e0 0a          	shl    $0xa,%rax
	case 'G':
	case 'g':
		ret <<= 10;
ffffffff812c1002:	48 c1 e0 0a          	shl    $0xa,%rax
	case 'M':
	case 'm':
		ret <<= 10;
ffffffff812c1006:	48 c1 e0 0a          	shl    $0xa,%rax
	case 'K':
	case 'k':
		ret <<= 10;
		endptr++;
ffffffff812c100a:	48 ff c1             	inc    %rcx
	case 'M':
	case 'm':
		ret <<= 10;
	case 'K':
	case 'k':
		ret <<= 10;
ffffffff812c100d:	48 c1 e0 0a          	shl    $0xa,%rax
		endptr++;
ffffffff812c1011:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
	default:
		break;
	}

	if (retptr)
ffffffff812c1015:	48 85 db             	test   %rbx,%rbx
ffffffff812c1018:	74 07                	je     ffffffff812c1021 <memparse+0x90>
		*retptr = endptr;
ffffffff812c101a:	48 8b 55 e8          	mov    -0x18(%rbp),%rdx
ffffffff812c101e:	48 89 13             	mov    %rdx,(%rbx)

	return ret;
}
ffffffff812c1021:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c1025:	5b                   	pop    %rbx
ffffffff812c1026:	5d                   	pop    %rbp
ffffffff812c1027:	c3                   	retq   

ffffffff812c1028 <parse_option_str>:
 *	strings like a=b,c.
 *
 *	Return true if there's such option in the string, or return false.
 */
bool parse_option_str(const char *str, const char *option)
{
ffffffff812c1028:	55                   	push   %rbp
ffffffff812c1029:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c102c:	41 57                	push   %r15
ffffffff812c102e:	41 56                	push   %r14
ffffffff812c1030:	41 55                	push   %r13
ffffffff812c1032:	41 54                	push   %r12
ffffffff812c1034:	49 89 f5             	mov    %rsi,%r13
ffffffff812c1037:	53                   	push   %rbx
ffffffff812c1038:	51                   	push   %rcx
ffffffff812c1039:	48 89 fb             	mov    %rdi,%rbx
	while (*str) {
		if (!strncmp(str, option, strlen(option))) {
ffffffff812c103c:	49 83 ce ff          	or     $0xffffffffffffffff,%r14
ffffffff812c1040:	45 31 ff             	xor    %r15d,%r15d
 *
 *	Return true if there's such option in the string, or return false.
 */
bool parse_option_str(const char *str, const char *option)
{
	while (*str) {
ffffffff812c1043:	80 3b 00             	cmpb   $0x0,(%rbx)
ffffffff812c1046:	74 53                	je     ffffffff812c109b <parse_option_str+0x73>
		if (!strncmp(str, option, strlen(option))) {
ffffffff812c1048:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c104b:	44 88 f8             	mov    %r15b,%al
ffffffff812c104e:	4c 89 f1             	mov    %r14,%rcx
ffffffff812c1051:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c1053:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c1056:	48 89 df             	mov    %rbx,%rdi
ffffffff812c1059:	48 f7 d1             	not    %rcx
ffffffff812c105c:	4c 8d 61 ff          	lea    -0x1(%rcx),%r12
ffffffff812c1060:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c1063:	e8 10 6c 00 00       	callq  ffffffff812c7c78 <strncmp>
ffffffff812c1068:	85 c0                	test   %eax,%eax
ffffffff812c106a:	75 1c                	jne    ffffffff812c1088 <parse_option_str+0x60>
			str += strlen(option);
ffffffff812c106c:	4c 01 e3             	add    %r12,%rbx
			if (!*str || *str == ',')
ffffffff812c106f:	8a 03                	mov    (%rbx),%al
ffffffff812c1071:	84 c0                	test   %al,%al
ffffffff812c1073:	0f 94 c2             	sete   %dl
ffffffff812c1076:	3c 2c                	cmp    $0x2c,%al
ffffffff812c1078:	0f 94 c0             	sete   %al
ffffffff812c107b:	08 d0                	or     %dl,%al
ffffffff812c107d:	74 09                	je     ffffffff812c1088 <parse_option_str+0x60>
ffffffff812c107f:	eb 1c                	jmp    ffffffff812c109d <parse_option_str+0x75>
				return true;
		}

		while (*str && *str != ',')
ffffffff812c1081:	3c 2c                	cmp    $0x2c,%al
ffffffff812c1083:	74 09                	je     ffffffff812c108e <parse_option_str+0x66>
			str++;
ffffffff812c1085:	48 ff c3             	inc    %rbx
			str += strlen(option);
			if (!*str || *str == ',')
				return true;
		}

		while (*str && *str != ',')
ffffffff812c1088:	8a 03                	mov    (%rbx),%al
ffffffff812c108a:	84 c0                	test   %al,%al
ffffffff812c108c:	75 f3                	jne    ffffffff812c1081 <parse_option_str+0x59>
			str++;

		if (*str == ',')
ffffffff812c108e:	3c 2c                	cmp    $0x2c,%al
			str++;
ffffffff812c1090:	0f 94 c0             	sete   %al
ffffffff812c1093:	0f b6 c0             	movzbl %al,%eax
ffffffff812c1096:	48 01 c3             	add    %rax,%rbx
ffffffff812c1099:	eb a8                	jmp    ffffffff812c1043 <parse_option_str+0x1b>
	}

	return false;
ffffffff812c109b:	31 c0                	xor    %eax,%eax
}
ffffffff812c109d:	5a                   	pop    %rdx
ffffffff812c109e:	5b                   	pop    %rbx
ffffffff812c109f:	41 5c                	pop    %r12
ffffffff812c10a1:	41 5d                	pop    %r13
ffffffff812c10a3:	41 5e                	pop    %r14
ffffffff812c10a5:	41 5f                	pop    %r15
ffffffff812c10a7:	5d                   	pop    %rbp
ffffffff812c10a8:	c3                   	retq   

ffffffff812c10a9 <cpumask_next_and>:
 *
 * Returns >= nr_cpu_ids if no further cpus set in both.
 */
int cpumask_next_and(int n, const struct cpumask *src1p,
		     const struct cpumask *src2p)
{
ffffffff812c10a9:	55                   	push   %rbp
ffffffff812c10aa:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c10ad:	41 54                	push   %r12
ffffffff812c10af:	49 89 d4             	mov    %rdx,%r12
ffffffff812c10b2:	53                   	push   %rbx
ffffffff812c10b3:	48 89 f3             	mov    %rsi,%rbx
ffffffff812c10b6:	ff c7                	inc    %edi
ffffffff812c10b8:	be 20 00 00 00       	mov    $0x20,%esi
ffffffff812c10bd:	48 63 d7             	movslq %edi,%rdx
ffffffff812c10c0:	48 89 df             	mov    %rbx,%rdi
ffffffff812c10c3:	e8 50 fc 00 00       	callq  ffffffff812d0d18 <find_next_bit>
	while ((n = cpumask_next(n, src1p)) < nr_cpu_ids)
ffffffff812c10c8:	3b 05 1e 82 79 00    	cmp    0x79821e(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c10ce:	89 c7                	mov    %eax,%edi
ffffffff812c10d0:	7d 0d                	jge    ffffffff812c10df <cpumask_next_and+0x36>

static inline int variable_test_bit(long nr, volatile const unsigned long *addr)
{
	int oldbit;

	asm volatile("bt %2,%1\n\t"
ffffffff812c10d2:	89 c0                	mov    %eax,%eax
ffffffff812c10d4:	49 0f a3 04 24       	bt     %rax,(%r12)
ffffffff812c10d9:	19 c0                	sbb    %eax,%eax
		if (cpumask_test_cpu(n, src2p))
ffffffff812c10db:	85 c0                	test   %eax,%eax
ffffffff812c10dd:	74 d7                	je     ffffffff812c10b6 <cpumask_next_and+0xd>
			break;
	return n;
}
ffffffff812c10df:	5b                   	pop    %rbx
ffffffff812c10e0:	89 f8                	mov    %edi,%eax
ffffffff812c10e2:	41 5c                	pop    %r12
ffffffff812c10e4:	5d                   	pop    %rbp
ffffffff812c10e5:	c3                   	retq   

ffffffff812c10e6 <cpumask_local_spread>:
 * wraps around.
 *
 * It's not very efficient, but useful for setup.
 */
unsigned int cpumask_local_spread(unsigned int i, int node)
{
ffffffff812c10e6:	55                   	push   %rbp
	int cpu;

	/* Wrap: we always want a cpu. */
	i %= num_online_cpus();
ffffffff812c10e7:	31 d2                	xor    %edx,%edx
 * wraps around.
 *
 * It's not very efficient, but useful for setup.
 */
unsigned int cpumask_local_spread(unsigned int i, int node)
{
ffffffff812c10e9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c10ec:	41 54                	push   %r12
	int cpu;

	/* Wrap: we always want a cpu. */
	i %= num_online_cpus();
ffffffff812c10ee:	4c 8b 25 33 b1 35 00 	mov    0x35b133(%rip),%r12        # ffffffff8161c228 <cpu_online_mask>
 * wraps around.
 *
 * It's not very efficient, but useful for setup.
 */
unsigned int cpumask_local_spread(unsigned int i, int node)
{
ffffffff812c10f5:	53                   	push   %rbx
ffffffff812c10f6:	89 fb                	mov    %edi,%ebx

#ifdef CONFIG_X86_32
	return  __arch_hweight32((u32)w) +
		__arch_hweight32((u32)(w >> 32));
#else
	asm (ALTERNATIVE("call __sw_hweight64", POPCNT64, X86_FEATURE_POPCNT)
ffffffff812c10f8:	41 8b 3c 24          	mov    (%r12),%edi
ffffffff812c10fc:	e8 06 3d 01 00       	callq  ffffffff812d4e07 <__sw_hweight64>
ffffffff812c1101:	48 89 c7             	mov    %rax,%rdi
	int cpu;

	/* Wrap: we always want a cpu. */
	i %= num_online_cpus();
ffffffff812c1104:	89 d8                	mov    %ebx,%eax
ffffffff812c1106:	f7 f7                	div    %edi
ffffffff812c1108:	83 c8 ff             	or     $0xffffffff,%eax

	if (node == -1) {
ffffffff812c110b:	83 fe ff             	cmp    $0xffffffff,%esi
unsigned int cpumask_local_spread(unsigned int i, int node)
{
	int cpu;

	/* Wrap: we always want a cpu. */
	i %= num_online_cpus();
ffffffff812c110e:	89 d3                	mov    %edx,%ebx

	if (node == -1) {
ffffffff812c1110:	75 30                	jne    ffffffff812c1142 <cpumask_local_spread+0x5c>
ffffffff812c1112:	ff c6                	inc    %esi
ffffffff812c1114:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c1117:	48 63 d6             	movslq %esi,%rdx
ffffffff812c111a:	be 20 00 00 00       	mov    $0x20,%esi
ffffffff812c111f:	e8 f4 fb 00 00       	callq  ffffffff812d0d18 <find_next_bit>
		for_each_cpu(cpu, cpu_online_mask)
ffffffff812c1124:	3b 05 c2 81 79 00    	cmp    0x7981c2(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c112a:	89 c6                	mov    %eax,%esi
ffffffff812c112c:	7d 60                	jge    ffffffff812c118e <cpumask_local_spread+0xa8>
			if (i-- == 0)
ffffffff812c112e:	85 db                	test   %ebx,%ebx
ffffffff812c1130:	8d 53 ff             	lea    -0x1(%rbx),%edx
ffffffff812c1133:	74 5b                	je     ffffffff812c1190 <cpumask_local_spread+0xaa>
ffffffff812c1135:	89 d3                	mov    %edx,%ebx
ffffffff812c1137:	eb d9                	jmp    ffffffff812c1112 <cpumask_local_spread+0x2c>
				return cpu;
	} else {
		/* NUMA first. */
		for_each_cpu_and(cpu, cpumask_of_node(node), cpu_online_mask)
			if (i-- == 0)
ffffffff812c1139:	85 db                	test   %ebx,%ebx
ffffffff812c113b:	8d 53 ff             	lea    -0x1(%rbx),%edx
ffffffff812c113e:	74 50                	je     ffffffff812c1190 <cpumask_local_spread+0xaa>
ffffffff812c1140:	89 d3                	mov    %edx,%ebx
		for_each_cpu(cpu, cpu_online_mask)
			if (i-- == 0)
				return cpu;
	} else {
		/* NUMA first. */
		for_each_cpu_and(cpu, cpumask_of_node(node), cpu_online_mask)
ffffffff812c1142:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c1145:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c1148:	89 c7                	mov    %eax,%edi
ffffffff812c114a:	e8 5a ff ff ff       	callq  ffffffff812c10a9 <cpumask_next_and>
ffffffff812c114f:	3b 05 97 81 79 00    	cmp    0x798197(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c1155:	7c e2                	jl     ffffffff812c1139 <cpumask_local_spread+0x53>
ffffffff812c1157:	83 ca ff             	or     $0xffffffff,%edx
ffffffff812c115a:	ff c2                	inc    %edx
ffffffff812c115c:	be 20 00 00 00       	mov    $0x20,%esi
ffffffff812c1161:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c1164:	48 63 d2             	movslq %edx,%rdx
ffffffff812c1167:	e8 ac fb 00 00       	callq  ffffffff812d0d18 <find_next_bit>
			if (i-- == 0)
				return cpu;

		for_each_cpu(cpu, cpu_online_mask) {
ffffffff812c116c:	3b 05 7a 81 79 00    	cmp    0x79817a(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c1172:	89 c2                	mov    %eax,%edx
ffffffff812c1174:	7d 18                	jge    ffffffff812c118e <cpumask_local_spread+0xa8>
ffffffff812c1176:	89 c1                	mov    %eax,%ecx
ffffffff812c1178:	49 0f a3 0c 24       	bt     %rcx,(%r12)
ffffffff812c117d:	19 c9                	sbb    %ecx,%ecx
			/* Skip NUMA nodes, done above. */
			if (cpumask_test_cpu(cpu, cpumask_of_node(node)))
ffffffff812c117f:	85 c9                	test   %ecx,%ecx
ffffffff812c1181:	75 d7                	jne    ffffffff812c115a <cpumask_local_spread+0x74>
				continue;

			if (i-- == 0)
ffffffff812c1183:	85 db                	test   %ebx,%ebx
ffffffff812c1185:	8d 4b ff             	lea    -0x1(%rbx),%ecx
ffffffff812c1188:	74 06                	je     ffffffff812c1190 <cpumask_local_spread+0xaa>
ffffffff812c118a:	89 cb                	mov    %ecx,%ebx
ffffffff812c118c:	eb cc                	jmp    ffffffff812c115a <cpumask_local_spread+0x74>
				return cpu;
		}
	}
	BUG();
ffffffff812c118e:	0f 0b                	ud2    
}
ffffffff812c1190:	5b                   	pop    %rbx
ffffffff812c1191:	41 5c                	pop    %r12
ffffffff812c1193:	5d                   	pop    %rbp
ffffffff812c1194:	c3                   	retq   

ffffffff812c1195 <cpumask_any_but>:
 *
 * Often used to find any cpu but smp_processor_id() in a mask.
 * Returns >= nr_cpu_ids if no cpus set.
 */
int cpumask_any_but(const struct cpumask *mask, unsigned int cpu)
{
ffffffff812c1195:	55                   	push   %rbp
	unsigned int i;

	cpumask_check(cpu);
	for_each_cpu(i, mask)
ffffffff812c1196:	83 ca ff             	or     $0xffffffff,%edx
 *
 * Often used to find any cpu but smp_processor_id() in a mask.
 * Returns >= nr_cpu_ids if no cpus set.
 */
int cpumask_any_but(const struct cpumask *mask, unsigned int cpu)
{
ffffffff812c1199:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c119c:	41 54                	push   %r12
ffffffff812c119e:	49 89 fc             	mov    %rdi,%r12
ffffffff812c11a1:	53                   	push   %rbx
ffffffff812c11a2:	89 f3                	mov    %esi,%ebx
ffffffff812c11a4:	ff c2                	inc    %edx
ffffffff812c11a6:	be 20 00 00 00       	mov    $0x20,%esi
ffffffff812c11ab:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c11ae:	48 63 d2             	movslq %edx,%rdx
ffffffff812c11b1:	e8 62 fb 00 00       	callq  ffffffff812d0d18 <find_next_bit>
	unsigned int i;

	cpumask_check(cpu);
	for_each_cpu(i, mask)
		if (i != cpu)
ffffffff812c11b6:	39 05 30 81 79 00    	cmp    %eax,0x798130(%rip)        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c11bc:	89 c2                	mov    %eax,%edx
ffffffff812c11be:	76 04                	jbe    ffffffff812c11c4 <cpumask_any_but+0x2f>
ffffffff812c11c0:	39 c3                	cmp    %eax,%ebx
ffffffff812c11c2:	74 e0                	je     ffffffff812c11a4 <cpumask_any_but+0xf>
			break;
	return i;
}
ffffffff812c11c4:	5b                   	pop    %rbx
ffffffff812c11c5:	41 5c                	pop    %r12
ffffffff812c11c7:	5d                   	pop    %rbp
ffffffff812c11c8:	c3                   	retq   

ffffffff812c11c9 <_atomic_dec_and_lock>:
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return ACCESS_ONCE((v)->counter);
ffffffff812c11c9:	8b 17                	mov    (%rdi),%edx
static inline int __atomic_add_unless(atomic_t *v, int a, int u)
{
	int c, old;
	c = atomic_read(v);
	for (;;) {
		if (unlikely(c == (u)))
ffffffff812c11cb:	83 fa 01             	cmp    $0x1,%edx
ffffffff812c11ce:	74 14                	je     ffffffff812c11e4 <_atomic_dec_and_lock+0x1b>
			break;
		old = atomic_cmpxchg((v), c, c + (a));
ffffffff812c11d0:	8d 4a ff             	lea    -0x1(%rdx),%ecx
#define atomic_inc_return(v)  (atomic_add_return(1, v))
#define atomic_dec_return(v)  (atomic_sub_return(1, v))

static inline int atomic_cmpxchg(atomic_t *v, int old, int new)
{
	return cmpxchg(&v->counter, old, new);
ffffffff812c11d3:	89 d0                	mov    %edx,%eax
ffffffff812c11d5:	f0 0f b1 0f          	lock cmpxchg %ecx,(%rdi)
	c = atomic_read(v);
	for (;;) {
		if (unlikely(c == (u)))
			break;
		old = atomic_cmpxchg((v), c, c + (a));
		if (likely(old == c))
ffffffff812c11d9:	39 c2                	cmp    %eax,%edx
ffffffff812c11db:	74 04                	je     ffffffff812c11e1 <_atomic_dec_and_lock+0x18>
ffffffff812c11dd:	89 c2                	mov    %eax,%edx
ffffffff812c11df:	eb ea                	jmp    ffffffff812c11cb <_atomic_dec_and_lock+0x2>
 */
int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock)
{
	/* Subtract 1 from counter unless that drops it to 0 (ie. it was 1) */
	if (atomic_add_unless(atomic, -1, 1))
		return 0;
ffffffff812c11e1:	31 c0                	xor    %eax,%eax
	spin_lock(lock);
	if (atomic_dec_and_test(atomic))
		return 1;
	spin_unlock(lock);
	return 0;
}
ffffffff812c11e3:	c3                   	retq   
 *
 * because the spin-lock and the decrement must be
 * "atomic".
 */
int _atomic_dec_and_lock(atomic_t *atomic, spinlock_t *lock)
{
ffffffff812c11e4:	55                   	push   %rbp
ffffffff812c11e5:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c11e8:	41 54                	push   %r12
ffffffff812c11ea:	53                   	push   %rbx
ffffffff812c11eb:	48 89 fb             	mov    %rdi,%rbx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c11ee:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c11f1:	49 89 f4             	mov    %rsi,%r12
ffffffff812c11f4:	e8 37 74 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
 * returns true if the result is 0, or false for all other
 * cases.
 */
static inline int atomic_dec_and_test(atomic_t *v)
{
	GEN_UNARY_RMWcc(LOCK_PREFIX "decl", v->counter, "%0", "e");
ffffffff812c11f9:	f0 ff 0b             	lock decl (%rbx)
ffffffff812c11fc:	74 0c                	je     ffffffff812c120a <_atomic_dec_and_lock+0x41>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c11fe:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c1201:	e8 a6 74 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	/* Otherwise do it the slow way */
	spin_lock(lock);
	if (atomic_dec_and_test(atomic))
		return 1;
	spin_unlock(lock);
	return 0;
ffffffff812c1206:	31 c0                	xor    %eax,%eax
ffffffff812c1208:	eb 05                	jmp    ffffffff812c120f <_atomic_dec_and_lock+0x46>
		return 0;

	/* Otherwise do it the slow way */
	spin_lock(lock);
	if (atomic_dec_and_test(atomic))
		return 1;
ffffffff812c120a:	b8 01 00 00 00       	mov    $0x1,%eax
	spin_unlock(lock);
	return 0;
}
ffffffff812c120f:	5b                   	pop    %rbx
ffffffff812c1210:	41 5c                	pop    %r12
ffffffff812c1212:	5d                   	pop    %rbp
ffffffff812c1213:	c3                   	retq   

ffffffff812c1214 <find_cpio_data>:
 *              the match returned an empty filename string.
 */

struct cpio_data find_cpio_data(const char *path, void *data,
				size_t len,  long *nextoff)
{
ffffffff812c1214:	55                   	push   %rbp
	const size_t cpio_header_len = 8*C_NFIELDS - 2;
	struct cpio_data cd = { NULL, 0, "" };
ffffffff812c1215:	31 c0                	xor    %eax,%eax
 *              the match returned an empty filename string.
 */

struct cpio_data find_cpio_data(const char *path, void *data,
				size_t len,  long *nextoff)
{
ffffffff812c1217:	49 89 d2             	mov    %rdx,%r10
ffffffff812c121a:	48 89 ca             	mov    %rcx,%rdx
	const size_t cpio_header_len = 8*C_NFIELDS - 2;
	struct cpio_data cd = { NULL, 0, "" };
ffffffff812c121d:	b9 0a 00 00 00       	mov    $0xa,%ecx
 *              the match returned an empty filename string.
 */

struct cpio_data find_cpio_data(const char *path, void *data,
				size_t len,  long *nextoff)
{
ffffffff812c1222:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1225:	41 57                	push   %r15
ffffffff812c1227:	41 56                	push   %r14
ffffffff812c1229:	41 55                	push   %r13
ffffffff812c122b:	41 54                	push   %r12
ffffffff812c122d:	53                   	push   %rbx
	unsigned int ch[C_NFIELDS], *chp, v;
	unsigned char c, x;
	size_t mypathsize = strlen(path);
	int i, j;

	p = data;
ffffffff812c122e:	4c 89 d3             	mov    %r10,%rbx
 *              the match returned an empty filename string.
 */

struct cpio_data find_cpio_data(const char *path, void *data,
				size_t len,  long *nextoff)
{
ffffffff812c1231:	48 81 ec a8 00 00 00 	sub    $0xa8,%rsp
ffffffff812c1238:	48 89 bd 60 ff ff ff 	mov    %rdi,-0xa0(%rbp)
	const size_t cpio_header_len = 8*C_NFIELDS - 2;
	struct cpio_data cd = { NULL, 0, "" };
ffffffff812c123f:	48 8d 7d 88          	lea    -0x78(%rbp),%rdi
 *              the match returned an empty filename string.
 */

struct cpio_data find_cpio_data(const char *path, void *data,
				size_t len,  long *nextoff)
{
ffffffff812c1243:	48 89 b5 58 ff ff ff 	mov    %rsi,-0xa8(%rbp)
	const size_t cpio_header_len = 8*C_NFIELDS - 2;
	struct cpio_data cd = { NULL, 0, "" };
ffffffff812c124a:	48 c7 85 70 ff ff ff 	movq   $0x0,-0x90(%rbp)
ffffffff812c1251:	00 00 00 00 
ffffffff812c1255:	f3 aa                	rep stos %al,%es:(%rdi)
ffffffff812c1257:	48 c7 85 78 ff ff ff 	movq   $0x0,-0x88(%rbp)
ffffffff812c125e:	00 00 00 00 
ffffffff812c1262:	48 c7 45 80 00 00 00 	movq   $0x0,-0x80(%rbp)
ffffffff812c1269:	00 
	const char *p, *dptr, *nptr;
	unsigned int ch[C_NFIELDS], *chp, v;
	unsigned char c, x;
	size_t mypathsize = strlen(path);
ffffffff812c126a:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812c126e:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c1271:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c1273:	48 f7 d1             	not    %rcx
ffffffff812c1276:	48 8d 41 ff          	lea    -0x1(%rcx),%rax
ffffffff812c127a:	48 89 85 68 ff ff ff 	mov    %rax,-0x98(%rbp)
	int i, j;

	p = data;

	while (len > cpio_header_len) {
ffffffff812c1281:	48 83 fa 6e          	cmp    $0x6e,%rdx
ffffffff812c1285:	76 51                	jbe    ffffffff812c12d8 <find_cpio_data+0xc4>
		if (!*p) {
ffffffff812c1287:	80 3b 00             	cmpb   $0x0,(%rbx)
ffffffff812c128a:	75 64                	jne    ffffffff812c12f0 <find_cpio_data+0xdc>
			/* All cpio headers need to be 4-byte aligned */
			p += 4;
ffffffff812c128c:	48 83 c3 04          	add    $0x4,%rbx
			len -= 4;
ffffffff812c1290:	48 83 ea 04          	sub    $0x4,%rdx
			continue;
ffffffff812c1294:	eb eb                	jmp    ffffffff812c1281 <find_cpio_data+0x6d>
		j = 6;		/* The magic field is only 6 characters */
		chp = ch;
		for (i = C_NFIELDS; i; i--) {
			v = 0;
			while (j--) {
				v <<= 4;
ffffffff812c1296:	c1 e0 04             	shl    $0x4,%eax
				c = *p++;
ffffffff812c1299:	48 ff c3             	inc    %rbx
		j = 6;		/* The magic field is only 6 characters */
		chp = ch;
		for (i = C_NFIELDS; i; i--) {
			v = 0;
			while (j--) {
				v <<= 4;
ffffffff812c129c:	89 c6                	mov    %eax,%esi
				c = *p++;
ffffffff812c129e:	8a 43 ff             	mov    -0x1(%rbx),%al

				x = c - '0';
ffffffff812c12a1:	8d 78 d0             	lea    -0x30(%rax),%edi
				if (x < 10) {
ffffffff812c12a4:	40 80 ff 09          	cmp    $0x9,%dil
ffffffff812c12a8:	77 55                	ja     ffffffff812c12ff <find_cpio_data+0xeb>
					v += x;
ffffffff812c12aa:	40 0f b6 c7          	movzbl %dil,%eax
ffffffff812c12ae:	01 f0                	add    %esi,%eax

		j = 6;		/* The magic field is only 6 characters */
		chp = ch;
		for (i = C_NFIELDS; i; i--) {
			v = 0;
			while (j--) {
ffffffff812c12b0:	4c 39 e3             	cmp    %r12,%rbx
ffffffff812c12b3:	75 e1                	jne    ffffffff812c1296 <find_cpio_data+0x82>
					continue;
				}

				goto quit; /* Invalid hexadecimal */
			}
			*chp++ = v;
ffffffff812c12b5:	89 44 0d 98          	mov    %eax,-0x68(%rbp,%rcx,1)
ffffffff812c12b9:	48 83 c1 04          	add    $0x4,%rcx
ffffffff812c12bd:	4c 89 e3             	mov    %r12,%rbx
			continue;
		}

		j = 6;		/* The magic field is only 6 characters */
		chp = ch;
		for (i = C_NFIELDS; i; i--) {
ffffffff812c12c0:	48 83 f9 38          	cmp    $0x38,%rcx
				}

				goto quit; /* Invalid hexadecimal */
			}
			*chp++ = v;
			j = 8;	/* All other fields are 8 characters */
ffffffff812c12c4:	be 08 00 00 00       	mov    $0x8,%esi
			continue;
		}

		j = 6;		/* The magic field is only 6 characters */
		chp = ch;
		for (i = C_NFIELDS; i; i--) {
ffffffff812c12c9:	75 2c                	jne    ffffffff812c12f7 <find_cpio_data+0xe3>
			}
			*chp++ = v;
			j = 8;	/* All other fields are 8 characters */
		}

		if ((ch[C_MAGIC] - 0x070701) > 1)
ffffffff812c12cb:	8b 45 98             	mov    -0x68(%rbp),%eax
ffffffff812c12ce:	2d 01 07 07 00       	sub    $0x70701,%eax
ffffffff812c12d3:	83 f8 01             	cmp    $0x1,%eax
ffffffff812c12d6:	76 3a                	jbe    ffffffff812c1312 <find_cpio_data+0xfe>
		len -= (nptr - p);
		p = nptr;
	}

quit:
	return cd;
ffffffff812c12d8:	48 8d b5 70 ff ff ff 	lea    -0x90(%rbp),%rsi
ffffffff812c12df:	b9 0a 00 00 00       	mov    $0xa,%ecx
ffffffff812c12e4:	48 8b bd 60 ff ff ff 	mov    -0xa0(%rbp),%rdi
ffffffff812c12eb:	e9 2b 01 00 00       	jmpq   ffffffff812c141b <find_cpio_data+0x207>
ffffffff812c12f0:	31 c9                	xor    %ecx,%ecx
ffffffff812c12f2:	be 06 00 00 00       	mov    $0x6,%esi
ffffffff812c12f7:	4c 8d 24 33          	lea    (%rbx,%rsi,1),%r12
 *              the match returned an empty filename string.
 */

struct cpio_data find_cpio_data(const char *path, void *data,
				size_t len,  long *nextoff)
{
ffffffff812c12fb:	31 c0                	xor    %eax,%eax
ffffffff812c12fd:	eb b1                	jmp    ffffffff812c12b0 <find_cpio_data+0x9c>
				if (x < 10) {
					v += x;
					continue;
				}

				x = (c | 0x20) - 'a';
ffffffff812c12ff:	83 c8 20             	or     $0x20,%eax
ffffffff812c1302:	83 e8 61             	sub    $0x61,%eax
				if (x < 6) {
ffffffff812c1305:	3c 05                	cmp    $0x5,%al
ffffffff812c1307:	77 cf                	ja     ffffffff812c12d8 <find_cpio_data+0xc4>
					v += x + 10;
ffffffff812c1309:	0f b6 c0             	movzbl %al,%eax
ffffffff812c130c:	8d 44 06 0a          	lea    0xa(%rsi,%rax,1),%eax
					continue;
ffffffff812c1310:	eb 9e                	jmp    ffffffff812c12b0 <find_cpio_data+0x9c>
		if ((ch[C_MAGIC] - 0x070701) > 1)
			goto quit; /* Invalid magic */

		len -= cpio_header_len;

		dptr = PTR_ALIGN(p + ch[C_NAMESIZE], 4);
ffffffff812c1312:	44 8b 5d c8          	mov    -0x38(%rbp),%r11d
		nptr = PTR_ALIGN(dptr + ch[C_FILESIZE], 4);
ffffffff812c1316:	44 8b 7d b4          	mov    -0x4c(%rbp),%r15d
		}

		if ((ch[C_MAGIC] - 0x070701) > 1)
			goto quit; /* Invalid magic */

		len -= cpio_header_len;
ffffffff812c131a:	48 8d 4a 92          	lea    -0x6e(%rdx),%rcx

		dptr = PTR_ALIGN(p + ch[C_NAMESIZE], 4);
		nptr = PTR_ALIGN(dptr + ch[C_FILESIZE], 4);

		if (nptr > p + len || dptr < p || nptr < dptr)
ffffffff812c131e:	49 8d 04 0c          	lea    (%r12,%rcx,1),%rax
		if ((ch[C_MAGIC] - 0x070701) > 1)
			goto quit; /* Invalid magic */

		len -= cpio_header_len;

		dptr = PTR_ALIGN(p + ch[C_NAMESIZE], 4);
ffffffff812c1322:	4f 8d 74 1c 03       	lea    0x3(%r12,%r11,1),%r14
ffffffff812c1327:	49 83 e6 fc          	and    $0xfffffffffffffffc,%r14
		nptr = PTR_ALIGN(dptr + ch[C_FILESIZE], 4);
ffffffff812c132b:	4f 8d 6c 3e 03       	lea    0x3(%r14,%r15,1),%r13
ffffffff812c1330:	49 83 e5 fc          	and    $0xfffffffffffffffc,%r13

		if (nptr > p + len || dptr < p || nptr < dptr)
ffffffff812c1334:	49 39 c5             	cmp    %rax,%r13
			goto quit; /* Invalid magic */

		len -= cpio_header_len;

		dptr = PTR_ALIGN(p + ch[C_NAMESIZE], 4);
		nptr = PTR_ALIGN(dptr + ch[C_FILESIZE], 4);
ffffffff812c1337:	4c 89 eb             	mov    %r13,%rbx

		if (nptr > p + len || dptr < p || nptr < dptr)
ffffffff812c133a:	77 9c                	ja     ffffffff812c12d8 <find_cpio_data+0xc4>
ffffffff812c133c:	4d 39 e6             	cmp    %r12,%r14
ffffffff812c133f:	72 97                	jb     ffffffff812c12d8 <find_cpio_data+0xc4>
ffffffff812c1341:	4d 39 ee             	cmp    %r13,%r14
ffffffff812c1344:	77 92                	ja     ffffffff812c12d8 <find_cpio_data+0xc4>
			goto quit; /* Buffer overrun */

		if ((ch[C_MODE] & 0170000) == 0100000 &&
ffffffff812c1346:	8b 45 a0             	mov    -0x60(%rbp),%eax
ffffffff812c1349:	25 00 f0 00 00       	and    $0xf000,%eax
ffffffff812c134e:	3d 00 80 00 00       	cmp    $0x8000,%eax
ffffffff812c1353:	0f 85 dd 00 00 00    	jne    ffffffff812c1436 <find_cpio_data+0x222>
ffffffff812c1359:	4c 39 9d 68 ff ff ff 	cmp    %r11,-0x98(%rbp)
ffffffff812c1360:	4c 89 9d 38 ff ff ff 	mov    %r11,-0xc8(%rbp)
ffffffff812c1367:	0f 87 c9 00 00 00    	ja     ffffffff812c1436 <find_cpio_data+0x222>
		    ch[C_NAMESIZE] >= mypathsize &&
		    !memcmp(p, path, mypathsize)) {
ffffffff812c136d:	48 8b 95 68 ff ff ff 	mov    -0x98(%rbp),%rdx
ffffffff812c1374:	48 8b b5 58 ff ff ff 	mov    -0xa8(%rbp),%rsi
ffffffff812c137b:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c137e:	4c 89 85 40 ff ff ff 	mov    %r8,-0xc0(%rbp)
ffffffff812c1385:	4c 89 95 48 ff ff ff 	mov    %r10,-0xb8(%rbp)
ffffffff812c138c:	48 89 8d 50 ff ff ff 	mov    %rcx,-0xb0(%rbp)
ffffffff812c1393:	e8 21 6b 00 00       	callq  ffffffff812c7eb9 <memcmp>

		if (nptr > p + len || dptr < p || nptr < dptr)
			goto quit; /* Buffer overrun */

		if ((ch[C_MODE] & 0170000) == 0100000 &&
		    ch[C_NAMESIZE] >= mypathsize &&
ffffffff812c1398:	85 c0                	test   %eax,%eax
ffffffff812c139a:	48 8b 8d 50 ff ff ff 	mov    -0xb0(%rbp),%rcx
ffffffff812c13a1:	4c 8b 95 48 ff ff ff 	mov    -0xb8(%rbp),%r10
ffffffff812c13a8:	4c 8b 85 40 ff ff ff 	mov    -0xc0(%rbp),%r8
ffffffff812c13af:	4c 8b 9d 38 ff ff ff 	mov    -0xc8(%rbp),%r11
ffffffff812c13b6:	75 7e                	jne    ffffffff812c1436 <find_cpio_data+0x222>
		    !memcmp(p, path, mypathsize)) {
			*nextoff = (long)nptr - (long)data;
			if (ch[C_NAMESIZE] - mypathsize >= MAX_CPIO_FILE_NAME) {
ffffffff812c13b8:	4c 2b 9d 68 ff ff ff 	sub    -0x98(%rbp),%r11
			goto quit; /* Buffer overrun */

		if ((ch[C_MODE] & 0170000) == 0100000 &&
		    ch[C_NAMESIZE] >= mypathsize &&
		    !memcmp(p, path, mypathsize)) {
			*nextoff = (long)nptr - (long)data;
ffffffff812c13bf:	4c 29 d3             	sub    %r10,%rbx
ffffffff812c13c2:	49 89 18             	mov    %rbx,(%r8)
			if (ch[C_NAMESIZE] - mypathsize >= MAX_CPIO_FILE_NAME) {
ffffffff812c13c5:	49 83 fb 11          	cmp    $0x11,%r11
ffffffff812c13c9:	76 14                	jbe    ffffffff812c13df <find_cpio_data+0x1cb>
				pr_warn(
ffffffff812c13cb:	ba 12 00 00 00       	mov    $0x12,%edx
ffffffff812c13d0:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c13d3:	48 c7 c7 9d 64 7b 81 	mov    $0xffffffff817b649d,%rdi
ffffffff812c13da:	e8 b3 1b 17 00       	callq  ffffffff81432f92 <printk>
				"File %s exceeding MAX_CPIO_FILE_NAME [%d]\n",
				p, MAX_CPIO_FILE_NAME);
			}
			strlcpy(cd.name, p + mypathsize, MAX_CPIO_FILE_NAME);
ffffffff812c13df:	48 8b b5 68 ff ff ff 	mov    -0x98(%rbp),%rsi
ffffffff812c13e6:	48 8d 9d 70 ff ff ff 	lea    -0x90(%rbp),%rbx
ffffffff812c13ed:	ba 12 00 00 00       	mov    $0x12,%edx
ffffffff812c13f2:	48 8d 7b 10          	lea    0x10(%rbx),%rdi
ffffffff812c13f6:	4c 01 e6             	add    %r12,%rsi
ffffffff812c13f9:	e8 fb 6b 00 00       	callq  ffffffff812c7ff9 <strlcpy>

			cd.data = (void *)dptr;
			cd.size = ch[C_FILESIZE];
			return cd; /* Found it! */
ffffffff812c13fe:	48 8b bd 60 ff ff ff 	mov    -0xa0(%rbp),%rdi
				"File %s exceeding MAX_CPIO_FILE_NAME [%d]\n",
				p, MAX_CPIO_FILE_NAME);
			}
			strlcpy(cd.name, p + mypathsize, MAX_CPIO_FILE_NAME);

			cd.data = (void *)dptr;
ffffffff812c1405:	4c 89 b5 70 ff ff ff 	mov    %r14,-0x90(%rbp)
			cd.size = ch[C_FILESIZE];
			return cd; /* Found it! */
ffffffff812c140c:	b9 0a 00 00 00       	mov    $0xa,%ecx
				p, MAX_CPIO_FILE_NAME);
			}
			strlcpy(cd.name, p + mypathsize, MAX_CPIO_FILE_NAME);

			cd.data = (void *)dptr;
			cd.size = ch[C_FILESIZE];
ffffffff812c1411:	4c 89 bd 78 ff ff ff 	mov    %r15,-0x88(%rbp)
			return cd; /* Found it! */
ffffffff812c1418:	48 89 de             	mov    %rbx,%rsi
ffffffff812c141b:	f3 a5                	rep movsl %ds:(%rsi),%es:(%rdi)
		p = nptr;
	}

quit:
	return cd;
}
ffffffff812c141d:	48 8b 85 60 ff ff ff 	mov    -0xa0(%rbp),%rax
ffffffff812c1424:	48 81 c4 a8 00 00 00 	add    $0xa8,%rsp
ffffffff812c142b:	5b                   	pop    %rbx
ffffffff812c142c:	41 5c                	pop    %r12
ffffffff812c142e:	41 5d                	pop    %r13
ffffffff812c1430:	41 5e                	pop    %r14
ffffffff812c1432:	41 5f                	pop    %r15
ffffffff812c1434:	5d                   	pop    %rbp
ffffffff812c1435:	c3                   	retq   

			cd.data = (void *)dptr;
			cd.size = ch[C_FILESIZE];
			return cd; /* Found it! */
		}
		len -= (nptr - p);
ffffffff812c1436:	4d 29 e5             	sub    %r12,%r13
ffffffff812c1439:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c143c:	4c 29 ea             	sub    %r13,%rdx
ffffffff812c143f:	e9 3d fe ff ff       	jmpq   ffffffff812c1281 <find_cpio_data+0x6d>

ffffffff812c1444 <fprop_reflect_period_single.isra.4>:
{
	unsigned int period = p->period;
	unsigned long flags;

	/* Fast path - period didn't change */
	if (pl->period == period)
ffffffff812c1444:	39 7e 08             	cmp    %edi,0x8(%rsi)
ffffffff812c1447:	74 50                	je     ffffffff812c1499 <fprop_reflect_period_single.isra.4+0x55>

void fprop_local_destroy_single(struct fprop_local_single *pl)
{
}

static void fprop_reflect_period_single(struct fprop_global *p,
ffffffff812c1449:	55                   	push   %rbp
ffffffff812c144a:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c144d:	41 55                	push   %r13
	unsigned long flags;

	/* Fast path - period didn't change */
	if (pl->period == period)
		return;
	raw_spin_lock_irqsave(&pl->lock, flags);
ffffffff812c144f:	4c 8d 6e 10          	lea    0x10(%rsi),%r13

void fprop_local_destroy_single(struct fprop_local_single *pl)
{
}

static void fprop_reflect_period_single(struct fprop_global *p,
ffffffff812c1453:	41 54                	push   %r12
ffffffff812c1455:	53                   	push   %rbx
ffffffff812c1456:	41 89 fc             	mov    %edi,%r12d
ffffffff812c1459:	52                   	push   %rdx
ffffffff812c145a:	48 89 f3             	mov    %rsi,%rbx
	unsigned long flags;

	/* Fast path - period didn't change */
	if (pl->period == period)
		return;
	raw_spin_lock_irqsave(&pl->lock, flags);
ffffffff812c145d:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c1460:	e8 c3 73 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
ffffffff812c1465:	48 89 c6             	mov    %rax,%rsi
	/* Someone updated pl->period while we were spinning? */
	if (pl->period >= period) {
ffffffff812c1468:	8b 43 08             	mov    0x8(%rbx),%eax
ffffffff812c146b:	44 39 e0             	cmp    %r12d,%eax
ffffffff812c146e:	73 1a                	jae    ffffffff812c148a <fprop_reflect_period_single.isra.4+0x46>
		raw_spin_unlock_irqrestore(&pl->lock, flags);
		return;
	}
	/* Aging zeroed our fraction? */
	if (period - pl->period < BITS_PER_LONG)
ffffffff812c1470:	44 89 e1             	mov    %r12d,%ecx
ffffffff812c1473:	29 c1                	sub    %eax,%ecx
ffffffff812c1475:	83 f9 3f             	cmp    $0x3f,%ecx
ffffffff812c1478:	77 05                	ja     ffffffff812c147f <fprop_reflect_period_single.isra.4+0x3b>
		pl->events >>= period - pl->period;
ffffffff812c147a:	48 d3 2b             	shrq   %cl,(%rbx)
ffffffff812c147d:	eb 07                	jmp    ffffffff812c1486 <fprop_reflect_period_single.isra.4+0x42>
	else
		pl->events = 0;
ffffffff812c147f:	48 c7 03 00 00 00 00 	movq   $0x0,(%rbx)
	pl->period = period;
ffffffff812c1486:	44 89 63 08          	mov    %r12d,0x8(%rbx)
	raw_spin_unlock_irqrestore(&pl->lock, flags);
ffffffff812c148a:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c148d:	e8 4a 72 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
}
ffffffff812c1492:	58                   	pop    %rax
ffffffff812c1493:	5b                   	pop    %rbx
ffffffff812c1494:	41 5c                	pop    %r12
ffffffff812c1496:	41 5d                	pop    %r13
ffffffff812c1498:	5d                   	pop    %rbp
ffffffff812c1499:	c3                   	retq   

ffffffff812c149a <fprop_reflect_period_percpu.isra.5>:
{
	unsigned int period = p->period;
	unsigned long flags;

	/* Fast path - period didn't change */
	if (pl->period == period)
ffffffff812c149a:	39 7e 38             	cmp    %edi,0x38(%rsi)
ffffffff812c149d:	0f 84 a7 00 00 00    	je     ffffffff812c154a <fprop_reflect_period_percpu.isra.5+0xb0>
void fprop_local_destroy_percpu(struct fprop_local_percpu *pl)
{
	percpu_counter_destroy(&pl->events);
}

static void fprop_reflect_period_percpu(struct fprop_global *p,
ffffffff812c14a3:	55                   	push   %rbp
ffffffff812c14a4:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c14a7:	41 56                	push   %r14
ffffffff812c14a9:	41 55                	push   %r13
	unsigned long flags;

	/* Fast path - period didn't change */
	if (pl->period == period)
		return;
	raw_spin_lock_irqsave(&pl->lock, flags);
ffffffff812c14ab:	4c 8d 6e 40          	lea    0x40(%rsi),%r13
void fprop_local_destroy_percpu(struct fprop_local_percpu *pl)
{
	percpu_counter_destroy(&pl->events);
}

static void fprop_reflect_period_percpu(struct fprop_global *p,
ffffffff812c14af:	41 54                	push   %r12
ffffffff812c14b1:	53                   	push   %rbx
ffffffff812c14b2:	41 89 fc             	mov    %edi,%r12d
ffffffff812c14b5:	48 89 f3             	mov    %rsi,%rbx
	unsigned long flags;

	/* Fast path - period didn't change */
	if (pl->period == period)
		return;
	raw_spin_lock_irqsave(&pl->lock, flags);
ffffffff812c14b8:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c14bb:	e8 68 73 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
ffffffff812c14c0:	49 89 c6             	mov    %rax,%r14
	/* Someone updated pl->period while we were spinning? */
	if (pl->period >= period) {
ffffffff812c14c3:	8b 43 38             	mov    0x38(%rbx),%eax
ffffffff812c14c6:	44 39 e0             	cmp    %r12d,%eax
ffffffff812c14c9:	73 6c                	jae    ffffffff812c1537 <fprop_reflect_period_percpu.isra.5+0x9d>
		raw_spin_unlock_irqrestore(&pl->lock, flags);
		return;
	}
	/* Aging zeroed our fraction? */
	if (period - pl->period < BITS_PER_LONG) {
ffffffff812c14cb:	44 89 e1             	mov    %r12d,%ecx
ffffffff812c14ce:	29 c1                	sub    %eax,%ecx
ffffffff812c14d0:	83 f9 3f             	cmp    $0x3f,%ecx
ffffffff812c14d3:	77 54                	ja     ffffffff812c1529 <fprop_reflect_period_percpu.isra.5+0x8f>
		s64 val = percpu_counter_read(&pl->events);

		if (val < (nr_cpu_ids * PROP_BATCH))
ffffffff812c14d5:	8b 15 11 7e 79 00    	mov    0x797e11(%rip),%edx        # ffffffff81a592ec <nr_cpu_ids>
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
	asm("bsrl %1,%0"
ffffffff812c14db:	83 c9 ff             	or     $0xffffffff,%ecx
ffffffff812c14de:	48 8b 43 18          	mov    0x18(%rbx),%rax
ffffffff812c14e2:	0f bd ca             	bsr    %edx,%ecx
ffffffff812c14e5:	8d 0c cd 08 00 00 00 	lea    0x8(,%rcx,8),%ecx
ffffffff812c14ec:	0f af d1             	imul   %ecx,%edx
ffffffff812c14ef:	48 63 d2             	movslq %edx,%rdx
ffffffff812c14f2:	48 39 d0             	cmp    %rdx,%rax
ffffffff812c14f5:	7d 08                	jge    ffffffff812c14ff <fprop_reflect_period_percpu.isra.5+0x65>
	return ret < 0 ? 0 : ret;
}

static inline s64 percpu_counter_sum(struct percpu_counter *fbc)
{
	return __percpu_counter_sum(fbc);
ffffffff812c14f7:	48 89 df             	mov    %rbx,%rdi
ffffffff812c14fa:	e8 bd bc 01 00       	callq  ffffffff812dd1bc <__percpu_counter_sum>
			val = percpu_counter_sum(&pl->events);

		__percpu_counter_add(&pl->events,
ffffffff812c14ff:	44 89 e1             	mov    %r12d,%ecx
ffffffff812c1502:	2b 4b 38             	sub    0x38(%rbx),%ecx
ffffffff812c1505:	48 89 c6             	mov    %rax,%rsi
ffffffff812c1508:	83 ca ff             	or     $0xffffffff,%edx
ffffffff812c150b:	48 89 df             	mov    %rbx,%rdi
ffffffff812c150e:	0f bd 15 d7 7d 79 00 	bsr    0x797dd7(%rip),%edx        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c1515:	8d 14 d5 08 00 00 00 	lea    0x8(,%rdx,8),%edx
ffffffff812c151c:	48 d3 fe             	sar    %cl,%rsi
ffffffff812c151f:	48 29 c6             	sub    %rax,%rsi
ffffffff812c1522:	e8 fe bc 01 00       	callq  ffffffff812dd225 <__percpu_counter_add>
ffffffff812c1527:	eb 0a                	jmp    ffffffff812c1533 <fprop_reflect_period_percpu.isra.5+0x99>
			-val + (val >> (period-pl->period)), PROP_BATCH);
	} else
		percpu_counter_set(&pl->events, 0);
ffffffff812c1529:	31 f6                	xor    %esi,%esi
ffffffff812c152b:	48 89 df             	mov    %rbx,%rdi
ffffffff812c152e:	e8 9e be 01 00       	callq  ffffffff812dd3d1 <percpu_counter_set>
	pl->period = period;
ffffffff812c1533:	44 89 63 38          	mov    %r12d,0x38(%rbx)
	raw_spin_unlock_irqrestore(&pl->lock, flags);
ffffffff812c1537:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c153a:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c153d:	e8 9a 71 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
}
ffffffff812c1542:	5b                   	pop    %rbx
ffffffff812c1543:	41 5c                	pop    %r12
ffffffff812c1545:	41 5d                	pop    %r13
ffffffff812c1547:	41 5e                	pop    %r14
ffffffff812c1549:	5d                   	pop    %rbp
ffffffff812c154a:	c3                   	retq   

ffffffff812c154b <fprop_global_init>:
 * which something happened with proportion of type j.
 */
#include <linux/flex_proportions.h>

int fprop_global_init(struct fprop_global *p, gfp_t gfp)
{
ffffffff812c154b:	55                   	push   %rbp
ffffffff812c154c:	89 f2                	mov    %esi,%edx
	int err;

	p->period = 0;
	/* Use 1 to avoid dealing with periods with 0 events... */
	err = percpu_counter_init(&p->events, 1, gfp);
ffffffff812c154e:	be 01 00 00 00       	mov    $0x1,%esi
 * which something happened with proportion of type j.
 */
#include <linux/flex_proportions.h>

int fprop_global_init(struct fprop_global *p, gfp_t gfp)
{
ffffffff812c1553:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1556:	53                   	push   %rbx
ffffffff812c1557:	51                   	push   %rcx
	int err;

	p->period = 0;
ffffffff812c1558:	c7 47 38 00 00 00 00 	movl   $0x0,0x38(%rdi)
	/* Use 1 to avoid dealing with periods with 0 events... */
	err = percpu_counter_init(&p->events, 1, gfp);
ffffffff812c155f:	48 c7 c1 88 17 bb 81 	mov    $0xffffffff81bb1788,%rcx
 * which something happened with proportion of type j.
 */
#include <linux/flex_proportions.h>

int fprop_global_init(struct fprop_global *p, gfp_t gfp)
{
ffffffff812c1566:	48 89 fb             	mov    %rdi,%rbx
	int err;

	p->period = 0;
	/* Use 1 to avoid dealing with periods with 0 events... */
	err = percpu_counter_init(&p->events, 1, gfp);
ffffffff812c1569:	e8 cd bd 01 00       	callq  ffffffff812dd33b <__percpu_counter_init>
	if (err)
ffffffff812c156e:	85 c0                	test   %eax,%eax
ffffffff812c1570:	75 07                	jne    ffffffff812c1579 <fprop_global_init+0x2e>
{
	/*
	 * Make sure we are not reinitializing a held lock:
	 */
	lockdep_init_map(&s->dep_map, name, key, 0);
	s->sequence = 0;
ffffffff812c1572:	c7 43 3c 00 00 00 00 	movl   $0x0,0x3c(%rbx)
		return err;
	seqcount_init(&p->sequence);
	return 0;
}
ffffffff812c1579:	5a                   	pop    %rdx
ffffffff812c157a:	5b                   	pop    %rbx
ffffffff812c157b:	5d                   	pop    %rbp
ffffffff812c157c:	c3                   	retq   

ffffffff812c157d <fprop_global_destroy>:

void fprop_global_destroy(struct fprop_global *p)
{
ffffffff812c157d:	55                   	push   %rbp
ffffffff812c157e:	48 89 e5             	mov    %rsp,%rbp
	percpu_counter_destroy(&p->events);
ffffffff812c1581:	e8 06 bd 01 00       	callq  ffffffff812dd28c <percpu_counter_destroy>
}
ffffffff812c1586:	5d                   	pop    %rbp
ffffffff812c1587:	c3                   	retq   

ffffffff812c1588 <fprop_new_period>:
 * The function returns true if the proportions are still defined and false
 * if aging zeroed out all events. This can be used to detect whether declaring
 * further periods has any effect.
 */
bool fprop_new_period(struct fprop_global *p, int periods)
{
ffffffff812c1588:	55                   	push   %rbp
ffffffff812c1589:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c158c:	41 55                	push   %r13
ffffffff812c158e:	41 54                	push   %r12
ffffffff812c1590:	41 89 f4             	mov    %esi,%r12d
ffffffff812c1593:	53                   	push   %rbx
ffffffff812c1594:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c1597:	51                   	push   %rcx
ffffffff812c1598:	9c                   	pushfq 
ffffffff812c1599:	41 5d                	pop    %r13
		     :"memory", "cc");
}

static inline void native_irq_disable(void)
{
	asm volatile("cli": : :"memory");
ffffffff812c159b:	fa                   	cli    
ffffffff812c159c:	e8 1b bc 01 00       	callq  ffffffff812dd1bc <__percpu_counter_sum>
	local_irq_save(flags);
	events = percpu_counter_sum(&p->events);
	/*
	 * Don't do anything if there are no events.
	 */
	if (events <= 1) {
ffffffff812c15a1:	48 83 f8 01          	cmp    $0x1,%rax
ffffffff812c15a5:	7f 07                	jg     ffffffff812c15ae <fprop_new_period+0x26>
	return flags;
}

static inline void native_restore_fl(unsigned long flags)
{
	asm volatile("push %0 ; popf"
ffffffff812c15a7:	41 55                	push   %r13
ffffffff812c15a9:	9d                   	popfq  
		local_irq_restore(flags);
		return false;
ffffffff812c15aa:	31 c0                	xor    %eax,%eax
ffffffff812c15ac:	eb 35                	jmp    ffffffff812c15e3 <fprop_new_period+0x5b>



static inline void raw_write_seqcount_begin(seqcount_t *s)
{
	s->sequence++;
ffffffff812c15ae:	ff 43 3c             	incl   0x3c(%rbx)
	}
	write_seqcount_begin(&p->sequence);
	if (periods < 64)
ffffffff812c15b1:	41 83 fc 3f          	cmp    $0x3f,%r12d
ffffffff812c15b5:	7f 0c                	jg     ffffffff812c15c3 <fprop_new_period+0x3b>
		events -= events >> periods;
ffffffff812c15b7:	48 89 c2             	mov    %rax,%rdx
ffffffff812c15ba:	44 88 e1             	mov    %r12b,%cl
ffffffff812c15bd:	48 d3 fa             	sar    %cl,%rdx
ffffffff812c15c0:	48 29 d0             	sub    %rdx,%rax
	return __percpu_counter_compare(fbc, rhs, percpu_counter_batch);
}

static inline void percpu_counter_add(struct percpu_counter *fbc, s64 amount)
{
	__percpu_counter_add(fbc, amount, percpu_counter_batch);
ffffffff812c15c3:	8b 15 3f 82 79 00    	mov    0x79823f(%rip),%edx        # ffffffff81a59808 <percpu_counter_batch>
ffffffff812c15c9:	48 f7 d8             	neg    %rax
ffffffff812c15cc:	48 89 df             	mov    %rbx,%rdi
ffffffff812c15cf:	48 89 c6             	mov    %rax,%rsi
ffffffff812c15d2:	e8 4e bc 01 00       	callq  ffffffff812dd225 <__percpu_counter_add>
	/* Use addition to avoid losing events happening between sum and set */
	percpu_counter_add(&p->events, -events);
	p->period += periods;
ffffffff812c15d7:	44 01 63 38          	add    %r12d,0x38(%rbx)
}

static inline void raw_write_seqcount_end(seqcount_t *s)
{
	smp_wmb();
	s->sequence++;
ffffffff812c15db:	ff 43 3c             	incl   0x3c(%rbx)
ffffffff812c15de:	41 55                	push   %r13
ffffffff812c15e0:	9d                   	popfq  
	write_seqcount_end(&p->sequence);
	local_irq_restore(flags);

	return true;
ffffffff812c15e1:	b0 01                	mov    $0x1,%al
}
ffffffff812c15e3:	5a                   	pop    %rdx
ffffffff812c15e4:	5b                   	pop    %rbx
ffffffff812c15e5:	41 5c                	pop    %r12
ffffffff812c15e7:	41 5d                	pop    %r13
ffffffff812c15e9:	5d                   	pop    %rbp
ffffffff812c15ea:	c3                   	retq   

ffffffff812c15eb <fprop_local_init_single>:
/*
 * ---- SINGLE ----
 */

int fprop_local_init_single(struct fprop_local_single *pl)
{
ffffffff812c15eb:	55                   	push   %rbp
	pl->events = 0;
	pl->period = 0;
	raw_spin_lock_init(&pl->lock);
ffffffff812c15ec:	48 83 c7 10          	add    $0x10,%rdi
 * ---- SINGLE ----
 */

int fprop_local_init_single(struct fprop_local_single *pl)
{
	pl->events = 0;
ffffffff812c15f0:	48 c7 47 f0 00 00 00 	movq   $0x0,-0x10(%rdi)
ffffffff812c15f7:	00 
	pl->period = 0;
ffffffff812c15f8:	c7 47 f8 00 00 00 00 	movl   $0x0,-0x8(%rdi)
	raw_spin_lock_init(&pl->lock);
ffffffff812c15ff:	48 c7 c2 88 17 bb 81 	mov    $0xffffffff81bb1788,%rdx
ffffffff812c1606:	48 c7 c6 ca 64 7b 81 	mov    $0xffffffff817b64ca,%rsi
/*
 * ---- SINGLE ----
 */

int fprop_local_init_single(struct fprop_local_single *pl)
{
ffffffff812c160d:	48 89 e5             	mov    %rsp,%rbp
	pl->events = 0;
	pl->period = 0;
	raw_spin_lock_init(&pl->lock);
ffffffff812c1610:	e8 5e e7 dc ff       	callq  ffffffff8108fd73 <__raw_spin_lock_init>
	return 0;
}
ffffffff812c1615:	31 c0                	xor    %eax,%eax
ffffffff812c1617:	5d                   	pop    %rbp
ffffffff812c1618:	c3                   	retq   

ffffffff812c1619 <fprop_local_destroy_single>:

void fprop_local_destroy_single(struct fprop_local_single *pl)
{
ffffffff812c1619:	55                   	push   %rbp
ffffffff812c161a:	48 89 e5             	mov    %rsp,%rbp
}
ffffffff812c161d:	5d                   	pop    %rbp
ffffffff812c161e:	c3                   	retq   

ffffffff812c161f <__fprop_inc_single>:
	raw_spin_unlock_irqrestore(&pl->lock, flags);
}

/* Event of type pl happened */
void __fprop_inc_single(struct fprop_global *p, struct fprop_local_single *pl)
{
ffffffff812c161f:	55                   	push   %rbp
ffffffff812c1620:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1623:	41 54                	push   %r12
ffffffff812c1625:	49 89 fc             	mov    %rdi,%r12
ffffffff812c1628:	53                   	push   %rbx
	fprop_reflect_period_single(p, pl);
ffffffff812c1629:	8b 7f 38             	mov    0x38(%rdi),%edi
	raw_spin_unlock_irqrestore(&pl->lock, flags);
}

/* Event of type pl happened */
void __fprop_inc_single(struct fprop_global *p, struct fprop_local_single *pl)
{
ffffffff812c162c:	48 89 f3             	mov    %rsi,%rbx
	fprop_reflect_period_single(p, pl);
ffffffff812c162f:	e8 10 fe ff ff       	callq  ffffffff812c1444 <fprop_reflect_period_single.isra.4>
	pl->events++;
ffffffff812c1634:	48 ff 03             	incq   (%rbx)
ffffffff812c1637:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c163a:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c163f:	8b 15 c3 81 79 00    	mov    0x7981c3(%rip),%edx        # ffffffff81a59808 <percpu_counter_batch>
ffffffff812c1645:	e8 db bb 01 00       	callq  ffffffff812dd225 <__percpu_counter_add>
	percpu_counter_add(&p->events, 1);
}
ffffffff812c164a:	5b                   	pop    %rbx
ffffffff812c164b:	41 5c                	pop    %r12
ffffffff812c164d:	5d                   	pop    %rbp
ffffffff812c164e:	c3                   	retq   

ffffffff812c164f <fprop_fraction_single>:

/* Return fraction of events of type pl */
void fprop_fraction_single(struct fprop_global *p,
			   struct fprop_local_single *pl,
			   unsigned long *numerator, unsigned long *denominator)
{
ffffffff812c164f:	55                   	push   %rbp
ffffffff812c1650:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1653:	41 57                	push   %r15
ffffffff812c1655:	41 56                	push   %r14
ffffffff812c1657:	41 55                	push   %r13
ffffffff812c1659:	41 54                	push   %r12
ffffffff812c165b:	49 89 f5             	mov    %rsi,%r13
ffffffff812c165e:	53                   	push   %rbx
ffffffff812c165f:	41 50                	push   %r8
ffffffff812c1661:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c1664:	49 89 d6             	mov    %rdx,%r14
ffffffff812c1667:	49 89 cf             	mov    %rcx,%r15
static __always_inline void __read_once_size(const volatile void *p, void *res, int size)
{
	switch (size) {
	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;
	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;
	case 4: *(__u32 *)res = *(volatile __u32 *)p; break;
ffffffff812c166a:	44 8b 63 3c          	mov    0x3c(%rbx),%r12d
{
	unsigned ret;

repeat:
	ret = READ_ONCE(s->sequence);
	if (unlikely(ret & 1)) {
ffffffff812c166e:	41 f6 c4 01          	test   $0x1,%r12b
ffffffff812c1672:	74 04                	je     ffffffff812c1678 <fprop_fraction_single+0x29>
}

/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */
static inline void rep_nop(void)
{
	asm volatile("rep; nop" ::: "memory");
ffffffff812c1674:	f3 90                	pause  
ffffffff812c1676:	eb f2                	jmp    ffffffff812c166a <fprop_fraction_single+0x1b>
	unsigned int seq;
	s64 num, den;

	do {
		seq = read_seqcount_begin(&p->sequence);
		fprop_reflect_period_single(p, pl);
ffffffff812c1678:	8b 7b 38             	mov    0x38(%rbx),%edi
ffffffff812c167b:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c167e:	e8 c1 fd ff ff       	callq  ffffffff812c1444 <fprop_reflect_period_single.isra.4>
		num = pl->events;
ffffffff812c1683:	49 8b 75 00          	mov    0x0(%r13),%rsi
ffffffff812c1687:	48 8b 43 18          	mov    0x18(%rbx),%rax
		den = percpu_counter_read_positive(&p->events);
	} while (read_seqcount_retry(&p->sequence, seq));
ffffffff812c168b:	44 39 63 3c          	cmp    %r12d,0x3c(%rbx)
ffffffff812c168f:	75 d9                	jne    ffffffff812c166a <fprop_fraction_single+0x1b>
ffffffff812c1691:	48 85 c0             	test   %rax,%rax
ffffffff812c1694:	ba 00 00 00 00       	mov    $0x0,%edx
ffffffff812c1699:	48 0f 48 c2          	cmovs  %rdx,%rax

	/*
	 * Make fraction <= 1 and denominator > 0 even in presence of percpu
	 * counter errors
	 */
	if (den <= num) {
ffffffff812c169d:	48 39 f0             	cmp    %rsi,%rax
ffffffff812c16a0:	7f 0c                	jg     ffffffff812c16ae <fprop_fraction_single+0x5f>
		if (num)
ffffffff812c16a2:	48 85 f6             	test   %rsi,%rsi
ffffffff812c16a5:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c16aa:	48 0f 45 c6          	cmovne %rsi,%rax
			den = num;
		else
			den = 1;
	}
	*denominator = den;
ffffffff812c16ae:	49 89 07             	mov    %rax,(%r15)
	*numerator = num;
ffffffff812c16b1:	49 89 36             	mov    %rsi,(%r14)
}
ffffffff812c16b4:	58                   	pop    %rax
ffffffff812c16b5:	5b                   	pop    %rbx
ffffffff812c16b6:	41 5c                	pop    %r12
ffffffff812c16b8:	41 5d                	pop    %r13
ffffffff812c16ba:	41 5e                	pop    %r14
ffffffff812c16bc:	41 5f                	pop    %r15
ffffffff812c16be:	5d                   	pop    %rbp
ffffffff812c16bf:	c3                   	retq   

ffffffff812c16c0 <fprop_local_init_percpu>:
 * ---- PERCPU ----
 */
#define PROP_BATCH (8*(1+ilog2(nr_cpu_ids)))

int fprop_local_init_percpu(struct fprop_local_percpu *pl, gfp_t gfp)
{
ffffffff812c16c0:	55                   	push   %rbp
ffffffff812c16c1:	89 f2                	mov    %esi,%edx
	int err;

	err = percpu_counter_init(&pl->events, 0, gfp);
ffffffff812c16c3:	48 c7 c1 88 17 bb 81 	mov    $0xffffffff81bb1788,%rcx
ffffffff812c16ca:	31 f6                	xor    %esi,%esi
 * ---- PERCPU ----
 */
#define PROP_BATCH (8*(1+ilog2(nr_cpu_ids)))

int fprop_local_init_percpu(struct fprop_local_percpu *pl, gfp_t gfp)
{
ffffffff812c16cc:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c16cf:	53                   	push   %rbx
ffffffff812c16d0:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c16d3:	48 83 ec 18          	sub    $0x18,%rsp
	int err;

	err = percpu_counter_init(&pl->events, 0, gfp);
ffffffff812c16d7:	e8 5f bc 01 00       	callq  ffffffff812dd33b <__percpu_counter_init>
	if (err)
ffffffff812c16dc:	85 c0                	test   %eax,%eax
ffffffff812c16de:	75 24                	jne    ffffffff812c1704 <fprop_local_init_percpu+0x44>
		return err;
	pl->period = 0;
	raw_spin_lock_init(&pl->lock);
ffffffff812c16e0:	48 8d 7b 40          	lea    0x40(%rbx),%rdi
	int err;

	err = percpu_counter_init(&pl->events, 0, gfp);
	if (err)
		return err;
	pl->period = 0;
ffffffff812c16e4:	c7 43 38 00 00 00 00 	movl   $0x0,0x38(%rbx)
	raw_spin_lock_init(&pl->lock);
ffffffff812c16eb:	48 c7 c2 88 17 bb 81 	mov    $0xffffffff81bb1788,%rdx
ffffffff812c16f2:	48 c7 c6 ca 64 7b 81 	mov    $0xffffffff817b64ca,%rsi
ffffffff812c16f9:	89 45 ec             	mov    %eax,-0x14(%rbp)
ffffffff812c16fc:	e8 72 e6 dc ff       	callq  ffffffff8108fd73 <__raw_spin_lock_init>
ffffffff812c1701:	8b 45 ec             	mov    -0x14(%rbp),%eax
	return 0;
}
ffffffff812c1704:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c1708:	5b                   	pop    %rbx
ffffffff812c1709:	5d                   	pop    %rbp
ffffffff812c170a:	c3                   	retq   

ffffffff812c170b <fprop_local_destroy_percpu>:

void fprop_local_destroy_percpu(struct fprop_local_percpu *pl)
{
ffffffff812c170b:	55                   	push   %rbp
ffffffff812c170c:	48 89 e5             	mov    %rsp,%rbp
	percpu_counter_destroy(&pl->events);
ffffffff812c170f:	e8 78 bb 01 00       	callq  ffffffff812dd28c <percpu_counter_destroy>
}
ffffffff812c1714:	5d                   	pop    %rbp
ffffffff812c1715:	c3                   	retq   

ffffffff812c1716 <__fprop_inc_percpu>:
	raw_spin_unlock_irqrestore(&pl->lock, flags);
}

/* Event of type pl happened */
void __fprop_inc_percpu(struct fprop_global *p, struct fprop_local_percpu *pl)
{
ffffffff812c1716:	55                   	push   %rbp
ffffffff812c1717:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c171a:	41 54                	push   %r12
ffffffff812c171c:	53                   	push   %rbx
ffffffff812c171d:	48 89 fb             	mov    %rdi,%rbx
	fprop_reflect_period_percpu(p, pl);
ffffffff812c1720:	8b 7f 38             	mov    0x38(%rdi),%edi
	raw_spin_unlock_irqrestore(&pl->lock, flags);
}

/* Event of type pl happened */
void __fprop_inc_percpu(struct fprop_global *p, struct fprop_local_percpu *pl)
{
ffffffff812c1723:	49 89 f4             	mov    %rsi,%r12
	fprop_reflect_period_percpu(p, pl);
ffffffff812c1726:	e8 6f fd ff ff       	callq  ffffffff812c149a <fprop_reflect_period_percpu.isra.5>
ffffffff812c172b:	83 c8 ff             	or     $0xffffffff,%eax
	__percpu_counter_add(&pl->events, 1, PROP_BATCH);
ffffffff812c172e:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c1731:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c1736:	0f bd 05 af 7b 79 00 	bsr    0x797baf(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c173d:	8d 14 c5 08 00 00 00 	lea    0x8(,%rax,8),%edx
ffffffff812c1744:	e8 dc ba 01 00       	callq  ffffffff812dd225 <__percpu_counter_add>
ffffffff812c1749:	8b 15 b9 80 79 00    	mov    0x7980b9(%rip),%edx        # ffffffff81a59808 <percpu_counter_batch>
ffffffff812c174f:	48 89 df             	mov    %rbx,%rdi
ffffffff812c1752:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c1757:	e8 c9 ba 01 00       	callq  ffffffff812dd225 <__percpu_counter_add>
	percpu_counter_add(&p->events, 1);
}
ffffffff812c175c:	5b                   	pop    %rbx
ffffffff812c175d:	41 5c                	pop    %r12
ffffffff812c175f:	5d                   	pop    %rbp
ffffffff812c1760:	c3                   	retq   

ffffffff812c1761 <fprop_fraction_percpu>:

void fprop_fraction_percpu(struct fprop_global *p,
			   struct fprop_local_percpu *pl,
			   unsigned long *numerator, unsigned long *denominator)
{
ffffffff812c1761:	55                   	push   %rbp
ffffffff812c1762:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1765:	41 57                	push   %r15
ffffffff812c1767:	41 56                	push   %r14
ffffffff812c1769:	41 55                	push   %r13
ffffffff812c176b:	41 54                	push   %r12
ffffffff812c176d:	49 89 f5             	mov    %rsi,%r13
ffffffff812c1770:	53                   	push   %rbx
ffffffff812c1771:	41 50                	push   %r8
ffffffff812c1773:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c1776:	49 89 d6             	mov    %rdx,%r14
ffffffff812c1779:	49 89 cf             	mov    %rcx,%r15
ffffffff812c177c:	44 8b 63 3c          	mov    0x3c(%rbx),%r12d
ffffffff812c1780:	41 f6 c4 01          	test   $0x1,%r12b
ffffffff812c1784:	74 04                	je     ffffffff812c178a <fprop_fraction_percpu+0x29>
ffffffff812c1786:	f3 90                	pause  
ffffffff812c1788:	eb f2                	jmp    ffffffff812c177c <fprop_fraction_percpu+0x1b>
	unsigned int seq;
	s64 num, den;

	do {
		seq = read_seqcount_begin(&p->sequence);
		fprop_reflect_period_percpu(p, pl);
ffffffff812c178a:	8b 7b 38             	mov    0x38(%rbx),%edi
ffffffff812c178d:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c1790:	e8 05 fd ff ff       	callq  ffffffff812c149a <fprop_reflect_period_percpu.isra.5>
ffffffff812c1795:	49 8b 75 18          	mov    0x18(%r13),%rsi
 */
static inline s64 percpu_counter_read_positive(struct percpu_counter *fbc)
{
	s64 ret = fbc->count;

	barrier();		/* Prevent reloads of fbc->count */
ffffffff812c1799:	48 8b 43 18          	mov    0x18(%rbx),%rax
		num = percpu_counter_read_positive(&pl->events);
		den = percpu_counter_read_positive(&p->events);
	} while (read_seqcount_retry(&p->sequence, seq));
ffffffff812c179d:	44 39 63 3c          	cmp    %r12d,0x3c(%rbx)
ffffffff812c17a1:	75 d9                	jne    ffffffff812c177c <fprop_fraction_percpu+0x1b>
ffffffff812c17a3:	31 ff                	xor    %edi,%edi
ffffffff812c17a5:	48 85 f6             	test   %rsi,%rsi
ffffffff812c17a8:	48 0f 48 f7          	cmovs  %rdi,%rsi
ffffffff812c17ac:	48 85 c0             	test   %rax,%rax
ffffffff812c17af:	48 0f 48 c7          	cmovs  %rdi,%rax

	/*
	 * Make fraction <= 1 and denominator > 0 even in presence of percpu
	 * counter errors
	 */
	if (den <= num) {
ffffffff812c17b3:	48 39 c6             	cmp    %rax,%rsi
ffffffff812c17b6:	7c 0c                	jl     ffffffff812c17c4 <fprop_fraction_percpu+0x63>
		if (num)
ffffffff812c17b8:	48 85 f6             	test   %rsi,%rsi
ffffffff812c17bb:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c17c0:	48 0f 45 c6          	cmovne %rsi,%rax
			den = num;
		else
			den = 1;
	}
	*denominator = den;
ffffffff812c17c4:	49 89 07             	mov    %rax,(%r15)
	*numerator = num;
ffffffff812c17c7:	49 89 36             	mov    %rsi,(%r14)
}
ffffffff812c17ca:	58                   	pop    %rax
ffffffff812c17cb:	5b                   	pop    %rbx
ffffffff812c17cc:	41 5c                	pop    %r12
ffffffff812c17ce:	41 5d                	pop    %r13
ffffffff812c17d0:	41 5e                	pop    %r14
ffffffff812c17d2:	41 5f                	pop    %r15
ffffffff812c17d4:	5d                   	pop    %rbp
ffffffff812c17d5:	c3                   	retq   

ffffffff812c17d6 <__fprop_inc_percpu_max>:
 * Like __fprop_inc_percpu() except that event is counted only if the given
 * type has fraction smaller than @max_frac/FPROP_FRAC_BASE
 */
void __fprop_inc_percpu_max(struct fprop_global *p,
			    struct fprop_local_percpu *pl, int max_frac)
{
ffffffff812c17d6:	55                   	push   %rbp
ffffffff812c17d7:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c17da:	41 55                	push   %r13
ffffffff812c17dc:	41 54                	push   %r12
ffffffff812c17de:	53                   	push   %rbx
ffffffff812c17df:	48 63 da             	movslq %edx,%rbx
ffffffff812c17e2:	49 89 fc             	mov    %rdi,%r12
ffffffff812c17e5:	49 89 f5             	mov    %rsi,%r13
ffffffff812c17e8:	48 83 ec 18          	sub    $0x18,%rsp
	if (unlikely(max_frac < FPROP_FRAC_BASE)) {
ffffffff812c17ec:	81 fb ff 03 00 00    	cmp    $0x3ff,%ebx
ffffffff812c17f2:	77 1e                	ja     ffffffff812c1812 <__fprop_inc_percpu_max+0x3c>
		unsigned long numerator, denominator;

		fprop_fraction_percpu(p, pl, &numerator, &denominator);
ffffffff812c17f4:	48 8d 4d d8          	lea    -0x28(%rbp),%rcx
ffffffff812c17f8:	48 8d 55 d0          	lea    -0x30(%rbp),%rdx
ffffffff812c17fc:	e8 60 ff ff ff       	callq  ffffffff812c1761 <fprop_fraction_percpu>
		if (numerator >
ffffffff812c1801:	48 0f af 5d d8       	imul   -0x28(%rbp),%rbx
ffffffff812c1806:	48 c1 eb 0a          	shr    $0xa,%rbx
ffffffff812c180a:	48 3b 5d d0          	cmp    -0x30(%rbp),%rbx
ffffffff812c180e:	73 0a                	jae    ffffffff812c181a <__fprop_inc_percpu_max+0x44>
ffffffff812c1810:	eb 39                	jmp    ffffffff812c184b <__fprop_inc_percpu_max+0x75>
		    (((u64)denominator) * max_frac) >> FPROP_FRAC_SHIFT)
			return;
	} else
		fprop_reflect_period_percpu(p, pl);
ffffffff812c1812:	8b 7f 38             	mov    0x38(%rdi),%edi
ffffffff812c1815:	e8 80 fc ff ff       	callq  ffffffff812c149a <fprop_reflect_period_percpu.isra.5>
ffffffff812c181a:	83 c8 ff             	or     $0xffffffff,%eax
	__percpu_counter_add(&pl->events, 1, PROP_BATCH);
ffffffff812c181d:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c1822:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c1825:	0f bd 05 c0 7a 79 00 	bsr    0x797ac0(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812c182c:	8d 14 c5 08 00 00 00 	lea    0x8(,%rax,8),%edx
ffffffff812c1833:	e8 ed b9 01 00       	callq  ffffffff812dd225 <__percpu_counter_add>
	return __percpu_counter_compare(fbc, rhs, percpu_counter_batch);
}

static inline void percpu_counter_add(struct percpu_counter *fbc, s64 amount)
{
	__percpu_counter_add(fbc, amount, percpu_counter_batch);
ffffffff812c1838:	8b 15 ca 7f 79 00    	mov    0x797fca(%rip),%edx        # ffffffff81a59808 <percpu_counter_batch>
ffffffff812c183e:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c1843:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c1846:	e8 da b9 01 00       	callq  ffffffff812dd225 <__percpu_counter_add>
	percpu_counter_add(&p->events, 1);
}
ffffffff812c184b:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c184f:	5b                   	pop    %rbx
ffffffff812c1850:	41 5c                	pop    %r12
ffffffff812c1852:	41 5d                	pop    %r13
ffffffff812c1854:	5d                   	pop    %rbp
ffffffff812c1855:	c3                   	retq   

ffffffff812c1856 <idr_max>:
/* the maximum ID which can be allocated given idr->layers */
static int idr_max(int layers)
{
	int bits = min_t(int, layers * IDR_BITS, MAX_IDR_SHIFT);

	return (1 << bits) - 1;
ffffffff812c1856:	8d 0c fd 00 00 00 00 	lea    0x0(,%rdi,8),%ecx
ffffffff812c185d:	b8 1f 00 00 00       	mov    $0x1f,%eax
static DEFINE_PER_CPU(int, idr_preload_cnt);
static DEFINE_SPINLOCK(simple_ida_lock);

/* the maximum ID which can be allocated given idr->layers */
static int idr_max(int layers)
{
ffffffff812c1862:	55                   	push   %rbp
	int bits = min_t(int, layers * IDR_BITS, MAX_IDR_SHIFT);

	return (1 << bits) - 1;
ffffffff812c1863:	83 f9 1f             	cmp    $0x1f,%ecx
static DEFINE_PER_CPU(int, idr_preload_cnt);
static DEFINE_SPINLOCK(simple_ida_lock);

/* the maximum ID which can be allocated given idr->layers */
static int idr_max(int layers)
{
ffffffff812c1866:	48 89 e5             	mov    %rsp,%rbp
	int bits = min_t(int, layers * IDR_BITS, MAX_IDR_SHIFT);

	return (1 << bits) - 1;
ffffffff812c1869:	0f 4f c8             	cmovg  %eax,%ecx
ffffffff812c186c:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c1871:	d3 e0                	shl    %cl,%eax
ffffffff812c1873:	ff c8                	dec    %eax
}
ffffffff812c1875:	5d                   	pop    %rbp
ffffffff812c1876:	c3                   	retq   

ffffffff812c1877 <get_from_free_list>:
{
	return ~idr_max(layer + 1);
}

static struct idr_layer *get_from_free_list(struct idr *idp)
{
ffffffff812c1877:	55                   	push   %rbp
ffffffff812c1878:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c187b:	41 55                	push   %r13
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &lock->rlock;
ffffffff812c187d:	4c 8d 6f 18          	lea    0x18(%rdi),%r13
ffffffff812c1881:	41 54                	push   %r12
ffffffff812c1883:	53                   	push   %rbx
ffffffff812c1884:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c1887:	51                   	push   %rcx
	struct idr_layer *p;
	unsigned long flags;

	spin_lock_irqsave(&idp->lock, flags);
ffffffff812c1888:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c188b:	e8 98 6f 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
	if ((p = idp->id_free)) {
ffffffff812c1890:	4c 8b 63 38          	mov    0x38(%rbx),%r12
ffffffff812c1894:	4d 85 e4             	test   %r12,%r12
ffffffff812c1897:	74 15                	je     ffffffff812c18ae <get_from_free_list+0x37>
		idp->id_free = p->ary[0];
ffffffff812c1899:	49 8b 54 24 08       	mov    0x8(%r12),%rdx
		idp->id_free_cnt--;
ffffffff812c189e:	ff 4b 30             	decl   0x30(%rbx)
	struct idr_layer *p;
	unsigned long flags;

	spin_lock_irqsave(&idp->lock, flags);
	if ((p = idp->id_free)) {
		idp->id_free = p->ary[0];
ffffffff812c18a1:	48 89 53 38          	mov    %rdx,0x38(%rbx)
		idp->id_free_cnt--;
		p->ary[0] = NULL;
ffffffff812c18a5:	49 c7 44 24 08 00 00 	movq   $0x0,0x8(%r12)
ffffffff812c18ac:	00 00 
	raw_spin_unlock_irq(&lock->rlock);
}

static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	raw_spin_unlock_irqrestore(&lock->rlock, flags);
ffffffff812c18ae:	48 89 c6             	mov    %rax,%rsi
ffffffff812c18b1:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c18b4:	e8 23 6e 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
	}
	spin_unlock_irqrestore(&idp->lock, flags);
	return(p);
}
ffffffff812c18b9:	5a                   	pop    %rdx
ffffffff812c18ba:	4c 89 e0             	mov    %r12,%rax
ffffffff812c18bd:	5b                   	pop    %rbx
ffffffff812c18be:	41 5c                	pop    %r12
ffffffff812c18c0:	41 5d                	pop    %r13
ffffffff812c18c2:	5d                   	pop    %rbp
ffffffff812c18c3:	c3                   	retq   

ffffffff812c18c4 <idr_for_each>:
 *
 * The caller must serialize idr_for_each() vs idr_get_new() and idr_remove().
 */
int idr_for_each(struct idr *idp,
		 int (*fn)(int id, void *p, void *data), void *data)
{
ffffffff812c18c4:	55                   	push   %rbp
ffffffff812c18c5:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c18c8:	41 57                	push   %r15
ffffffff812c18ca:	41 56                	push   %r14
ffffffff812c18cc:	41 55                	push   %r13
ffffffff812c18ce:	41 54                	push   %r12
	int n, id, max, error = 0;
	struct idr_layer *p;
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];
ffffffff812c18d0:	4c 8d 75 a8          	lea    -0x58(%rbp),%r14
 *
 * The caller must serialize idr_for_each() vs idr_get_new() and idr_remove().
 */
int idr_for_each(struct idr *idp,
		 int (*fn)(int id, void *p, void *data), void *data)
{
ffffffff812c18d4:	53                   	push   %rbx
ffffffff812c18d5:	49 89 f5             	mov    %rsi,%r13

	n = idp->layers * IDR_BITS;
	*paa = rcu_dereference_raw(idp->top);
	max = idr_max(idp->layers);

	id = 0;
ffffffff812c18d8:	45 31 e4             	xor    %r12d,%r12d
ffffffff812c18db:	41 83 cf ff          	or     $0xffffffff,%r15d
 *
 * The caller must serialize idr_for_each() vs idr_get_new() and idr_remove().
 */
int idr_for_each(struct idr *idp,
		 int (*fn)(int id, void *p, void *data), void *data)
{
ffffffff812c18df:	48 83 ec 48          	sub    $0x48,%rsp
	int n, id, max, error = 0;
	struct idr_layer *p;
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];

	n = idp->layers * IDR_BITS;
ffffffff812c18e3:	8b 47 10             	mov    0x10(%rdi),%eax
 *
 * The caller must serialize idr_for_each() vs idr_get_new() and idr_remove().
 */
int idr_for_each(struct idr *idp,
		 int (*fn)(int id, void *p, void *data), void *data)
{
ffffffff812c18e6:	48 89 55 90          	mov    %rdx,-0x70(%rbp)
	struct idr_layer *p;
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];

	n = idp->layers * IDR_BITS;
	*paa = rcu_dereference_raw(idp->top);
ffffffff812c18ea:	48 8b 57 08          	mov    0x8(%rdi),%rdx
	max = idr_max(idp->layers);
ffffffff812c18ee:	89 c7                	mov    %eax,%edi
	int n, id, max, error = 0;
	struct idr_layer *p;
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];

	n = idp->layers * IDR_BITS;
ffffffff812c18f0:	8d 1c c5 00 00 00 00 	lea    0x0(,%rax,8),%ebx
	*paa = rcu_dereference_raw(idp->top);
ffffffff812c18f7:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
	max = idr_max(idp->layers);
ffffffff812c18fb:	e8 56 ff ff ff       	callq  ffffffff812c1856 <idr_max>
ffffffff812c1900:	89 45 9c             	mov    %eax,-0x64(%rbp)

	id = 0;
	while (id >= 0 && id <= max) {
ffffffff812c1903:	44 89 e0             	mov    %r12d,%eax
ffffffff812c1906:	83 e8 00             	sub    $0x0,%eax
ffffffff812c1909:	78 66                	js     ffffffff812c1971 <idr_for_each+0xad>
ffffffff812c190b:	44 3b 65 9c          	cmp    -0x64(%rbp),%r12d
ffffffff812c190f:	7f 60                	jg     ffffffff812c1971 <idr_for_each+0xad>
		p = *paa;
ffffffff812c1911:	49 8b 36             	mov    (%r14),%rsi
		while (n > 0 && p) {
ffffffff812c1914:	85 db                	test   %ebx,%ebx
ffffffff812c1916:	7e 20                	jle    ffffffff812c1938 <idr_for_each+0x74>
ffffffff812c1918:	48 85 f6             	test   %rsi,%rsi
ffffffff812c191b:	74 1b                	je     ffffffff812c1938 <idr_for_each+0x74>
			n -= IDR_BITS;
ffffffff812c191d:	83 eb 08             	sub    $0x8,%ebx
			p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
ffffffff812c1920:	44 89 e0             	mov    %r12d,%eax
			*++paa = p;
ffffffff812c1923:	49 83 c6 08          	add    $0x8,%r14
	id = 0;
	while (id >= 0 && id <= max) {
		p = *paa;
		while (n > 0 && p) {
			n -= IDR_BITS;
			p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
ffffffff812c1927:	88 d9                	mov    %bl,%cl
ffffffff812c1929:	d3 f8                	sar    %cl,%eax
ffffffff812c192b:	0f b6 c0             	movzbl %al,%eax
ffffffff812c192e:	48 8b 74 c6 08       	mov    0x8(%rsi,%rax,8),%rsi
			*++paa = p;
ffffffff812c1933:	49 89 36             	mov    %rsi,(%r14)
ffffffff812c1936:	eb dc                	jmp    ffffffff812c1914 <idr_for_each+0x50>
		}

		if (p) {
ffffffff812c1938:	48 85 f6             	test   %rsi,%rsi
ffffffff812c193b:	75 17                	jne    ffffffff812c1954 <idr_for_each+0x90>
			error = fn(id, (void *)p, data);
			if (error)
				break;
		}

		id += 1 << n;
ffffffff812c193d:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c1942:	88 d9                	mov    %bl,%cl
ffffffff812c1944:	d3 e0                	shl    %cl,%eax
ffffffff812c1946:	41 01 c4             	add    %eax,%r12d
ffffffff812c1949:	44 89 f8             	mov    %r15d,%eax
ffffffff812c194c:	41 0f bd c4          	bsr    %r12d,%eax
		while (n < fls(id)) {
ffffffff812c1950:	ff c0                	inc    %eax
ffffffff812c1952:	eb 10                	jmp    ffffffff812c1964 <idr_for_each+0xa0>
			p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
			*++paa = p;
		}

		if (p) {
			error = fn(id, (void *)p, data);
ffffffff812c1954:	48 8b 55 90          	mov    -0x70(%rbp),%rdx
ffffffff812c1958:	44 89 e7             	mov    %r12d,%edi
ffffffff812c195b:	41 ff d5             	callq  *%r13
			if (error)
ffffffff812c195e:	85 c0                	test   %eax,%eax
ffffffff812c1960:	74 db                	je     ffffffff812c193d <idr_for_each+0x79>
ffffffff812c1962:	eb 0f                	jmp    ffffffff812c1973 <idr_for_each+0xaf>
				break;
		}

		id += 1 << n;
		while (n < fls(id)) {
ffffffff812c1964:	39 c3                	cmp    %eax,%ebx
ffffffff812c1966:	7d 9b                	jge    ffffffff812c1903 <idr_for_each+0x3f>
			n += IDR_BITS;
ffffffff812c1968:	83 c3 08             	add    $0x8,%ebx
			--paa;
ffffffff812c196b:	49 83 ee 08          	sub    $0x8,%r14
ffffffff812c196f:	eb f3                	jmp    ffffffff812c1964 <idr_for_each+0xa0>
ffffffff812c1971:	31 c0                	xor    %eax,%eax
		}
	}

	return error;
}
ffffffff812c1973:	48 83 c4 48          	add    $0x48,%rsp
ffffffff812c1977:	5b                   	pop    %rbx
ffffffff812c1978:	41 5c                	pop    %r12
ffffffff812c197a:	41 5d                	pop    %r13
ffffffff812c197c:	41 5e                	pop    %r14
ffffffff812c197e:	41 5f                	pop    %r15
ffffffff812c1980:	5d                   	pop    %rbp
ffffffff812c1981:	c3                   	retq   

ffffffff812c1982 <idr_get_next>:
 *
 * This function can be called under rcu_read_lock(), given that the leaf
 * pointers lifetimes are correctly managed.
 */
void *idr_get_next(struct idr *idp, int *nextidp)
{
ffffffff812c1982:	55                   	push   %rbp
ffffffff812c1983:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1986:	48 83 ec 38          	sub    $0x38,%rsp
	struct idr_layer **paa = &pa[0];
	int id = *nextidp;
	int n, max;

	/* find first ent */
	p = *paa = rcu_dereference_raw(idp->top);
ffffffff812c198a:	48 8b 47 08          	mov    0x8(%rdi),%rax
 */
void *idr_get_next(struct idr *idp, int *nextidp)
{
	struct idr_layer *p, *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];
	int id = *nextidp;
ffffffff812c198e:	44 8b 06             	mov    (%rsi),%r8d
	int n, max;

	/* find first ent */
	p = *paa = rcu_dereference_raw(idp->top);
	if (!p)
ffffffff812c1991:	48 85 c0             	test   %rax,%rax
	struct idr_layer **paa = &pa[0];
	int id = *nextidp;
	int n, max;

	/* find first ent */
	p = *paa = rcu_dereference_raw(idp->top);
ffffffff812c1994:	48 89 45 d8          	mov    %rax,-0x28(%rbp)
	if (!p)
ffffffff812c1998:	75 07                	jne    ffffffff812c19a1 <idr_get_next+0x1f>
		return NULL;
ffffffff812c199a:	31 c0                	xor    %eax,%eax
ffffffff812c199c:	e9 89 00 00 00       	jmpq   ffffffff812c1a2a <idr_get_next+0xa8>
	n = (p->layer + 1) * IDR_BITS;
ffffffff812c19a1:	8b 78 04             	mov    0x4(%rax),%edi
ffffffff812c19a4:	ff c7                	inc    %edi
ffffffff812c19a6:	8d 0c fd 00 00 00 00 	lea    0x0(,%rdi,8),%ecx
ffffffff812c19ad:	89 4d cc             	mov    %ecx,-0x34(%rbp)
	max = idr_max(p->layer + 1);
ffffffff812c19b0:	e8 a1 fe ff ff       	callq  ffffffff812c1856 <idr_max>

	while (id >= 0 && id <= max) {
ffffffff812c19b5:	8b 4d cc             	mov    -0x34(%rbp),%ecx
 * pointers lifetimes are correctly managed.
 */
void *idr_get_next(struct idr *idp, int *nextidp)
{
	struct idr_layer *p, *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];
ffffffff812c19b8:	48 8d 7d d8          	lea    -0x28(%rbp),%rdi
		 * idr_for_each(), @id isn't guaranteed to be aligned to
		 * layer boundary at this point and adding 1 << n may
		 * incorrectly skip IDs.  Make sure we jump to the
		 * beginning of the next layer using round_up().
		 */
		id = round_up(id + 1, 1 << n);
ffffffff812c19bc:	41 b9 01 00 00 00    	mov    $0x1,%r9d
ffffffff812c19c2:	41 83 ca ff          	or     $0xffffffff,%r10d
	if (!p)
		return NULL;
	n = (p->layer + 1) * IDR_BITS;
	max = idr_max(p->layer + 1);

	while (id >= 0 && id <= max) {
ffffffff812c19c6:	44 89 c2             	mov    %r8d,%edx
ffffffff812c19c9:	83 ea 00             	sub    $0x0,%edx
ffffffff812c19cc:	78 cc                	js     ffffffff812c199a <idr_get_next+0x18>
ffffffff812c19ce:	41 39 c0             	cmp    %eax,%r8d
ffffffff812c19d1:	7f c7                	jg     ffffffff812c199a <idr_get_next+0x18>
		p = *paa;
ffffffff812c19d3:	48 8b 17             	mov    (%rdi),%rdx
		while (n > 0 && p) {
ffffffff812c19d6:	85 c9                	test   %ecx,%ecx
ffffffff812c19d8:	7e 20                	jle    ffffffff812c19fa <idr_get_next+0x78>
ffffffff812c19da:	48 85 d2             	test   %rdx,%rdx
ffffffff812c19dd:	74 1b                	je     ffffffff812c19fa <idr_get_next+0x78>
			n -= IDR_BITS;
ffffffff812c19df:	83 e9 08             	sub    $0x8,%ecx
			p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
ffffffff812c19e2:	45 89 c3             	mov    %r8d,%r11d
			*++paa = p;
ffffffff812c19e5:	48 83 c7 08          	add    $0x8,%rdi

	while (id >= 0 && id <= max) {
		p = *paa;
		while (n > 0 && p) {
			n -= IDR_BITS;
			p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
ffffffff812c19e9:	41 d3 fb             	sar    %cl,%r11d
ffffffff812c19ec:	45 0f b6 db          	movzbl %r11b,%r11d
ffffffff812c19f0:	4a 8b 54 da 08       	mov    0x8(%rdx,%r11,8),%rdx
			*++paa = p;
ffffffff812c19f5:	48 89 17             	mov    %rdx,(%rdi)
ffffffff812c19f8:	eb dc                	jmp    ffffffff812c19d6 <idr_get_next+0x54>
		}

		if (p) {
ffffffff812c19fa:	48 85 d2             	test   %rdx,%rdx
ffffffff812c19fd:	74 08                	je     ffffffff812c1a07 <idr_get_next+0x85>
			*nextidp = id;
ffffffff812c19ff:	44 89 06             	mov    %r8d,(%rsi)
			return p;
ffffffff812c1a02:	48 89 d0             	mov    %rdx,%rax
ffffffff812c1a05:	eb 23                	jmp    ffffffff812c1a2a <idr_get_next+0xa8>
		 * idr_for_each(), @id isn't guaranteed to be aligned to
		 * layer boundary at this point and adding 1 << n may
		 * incorrectly skip IDs.  Make sure we jump to the
		 * beginning of the next layer using round_up().
		 */
		id = round_up(id + 1, 1 << n);
ffffffff812c1a07:	44 89 ca             	mov    %r9d,%edx
ffffffff812c1a0a:	d3 e2                	shl    %cl,%edx
ffffffff812c1a0c:	ff ca                	dec    %edx
ffffffff812c1a0e:	41 09 d0             	or     %edx,%r8d
ffffffff812c1a11:	44 89 d2             	mov    %r10d,%edx
ffffffff812c1a14:	41 ff c0             	inc    %r8d
ffffffff812c1a17:	41 0f bd d0          	bsr    %r8d,%edx
		while (n < fls(id)) {
ffffffff812c1a1b:	ff c2                	inc    %edx
ffffffff812c1a1d:	39 d1                	cmp    %edx,%ecx
ffffffff812c1a1f:	7d a5                	jge    ffffffff812c19c6 <idr_get_next+0x44>
			n += IDR_BITS;
ffffffff812c1a21:	83 c1 08             	add    $0x8,%ecx
			--paa;
ffffffff812c1a24:	48 83 ef 08          	sub    $0x8,%rdi
ffffffff812c1a28:	eb f3                	jmp    ffffffff812c1a1d <idr_get_next+0x9b>
		}
	}
	return NULL;
}
ffffffff812c1a2a:	c9                   	leaveq 
ffffffff812c1a2b:	c3                   	retq   

ffffffff812c1a2c <idr_has_entry>:
	spin_lock_init(&idp->lock);
}
EXPORT_SYMBOL(idr_init);

static int idr_has_entry(int id, void *p, void *data)
{
ffffffff812c1a2c:	55                   	push   %rbp
	return 1;
}
ffffffff812c1a2d:	b8 01 00 00 00       	mov    $0x1,%eax
	spin_lock_init(&idp->lock);
}
EXPORT_SYMBOL(idr_init);

static int idr_has_entry(int id, void *p, void *data)
{
ffffffff812c1a32:	48 89 e5             	mov    %rsp,%rbp
	return 1;
}
ffffffff812c1a35:	5d                   	pop    %rbp
ffffffff812c1a36:	c3                   	retq   

ffffffff812c1a37 <idr_is_empty>:

bool idr_is_empty(struct idr *idp)
{
ffffffff812c1a37:	55                   	push   %rbp
	return !idr_for_each(idp, idr_has_entry, NULL);
ffffffff812c1a38:	31 d2                	xor    %edx,%edx
ffffffff812c1a3a:	48 c7 c6 2c 1a 2c 81 	mov    $0xffffffff812c1a2c,%rsi
{
	return 1;
}

bool idr_is_empty(struct idr *idp)
{
ffffffff812c1a41:	48 89 e5             	mov    %rsp,%rbp
	return !idr_for_each(idp, idr_has_entry, NULL);
ffffffff812c1a44:	e8 7b fe ff ff       	callq  ffffffff812c18c4 <idr_for_each>
ffffffff812c1a49:	85 c0                	test   %eax,%eax
ffffffff812c1a4b:	0f 94 c0             	sete   %al
}
ffffffff812c1a4e:	5d                   	pop    %rbp
ffffffff812c1a4f:	c3                   	retq   

ffffffff812c1a50 <idr_replace>:
void *idr_replace(struct idr *idp, void *ptr, int id)
{
	int n;
	struct idr_layer *p, *old_p;

	if (id < 0)
ffffffff812c1a50:	85 d2                	test   %edx,%edx
		return ERR_PTR(-EINVAL);
ffffffff812c1a52:	49 c7 c0 ea ff ff ff 	mov    $0xffffffffffffffea,%r8
void *idr_replace(struct idr *idp, void *ptr, int id)
{
	int n;
	struct idr_layer *p, *old_p;

	if (id < 0)
ffffffff812c1a59:	78 7c                	js     ffffffff812c1ad7 <idr_replace+0x87>
		return ERR_PTR(-EINVAL);

	p = idp->top;
ffffffff812c1a5b:	4c 8b 4f 08          	mov    0x8(%rdi),%r9
	if (!p)
		return ERR_PTR(-ENOENT);
ffffffff812c1a5f:	49 c7 c0 fe ff ff ff 	mov    $0xfffffffffffffffe,%r8

	if (id < 0)
		return ERR_PTR(-EINVAL);

	p = idp->top;
	if (!p)
ffffffff812c1a66:	4d 85 c9             	test   %r9,%r9
ffffffff812c1a69:	74 6c                	je     ffffffff812c1ad7 <idr_replace+0x87>
 * A %-EINVAL return indicates that @id was not within valid constraints.
 *
 * The caller must serialize with writers.
 */
void *idr_replace(struct idr *idp, void *ptr, int id)
{
ffffffff812c1a6b:	55                   	push   %rbp

	p = idp->top;
	if (!p)
		return ERR_PTR(-ENOENT);

	if (id > idr_max(p->layer + 1))
ffffffff812c1a6c:	45 8b 51 04          	mov    0x4(%r9),%r10d
 * A %-EINVAL return indicates that @id was not within valid constraints.
 *
 * The caller must serialize with writers.
 */
void *idr_replace(struct idr *idp, void *ptr, int id)
{
ffffffff812c1a70:	48 89 e5             	mov    %rsp,%rbp

	p = idp->top;
	if (!p)
		return ERR_PTR(-ENOENT);

	if (id > idr_max(p->layer + 1))
ffffffff812c1a73:	41 8d 7a 01          	lea    0x1(%r10),%edi
ffffffff812c1a77:	e8 da fd ff ff       	callq  ffffffff812c1856 <idr_max>
ffffffff812c1a7c:	39 c2                	cmp    %eax,%edx
	if (id < 0)
		return ERR_PTR(-EINVAL);

	p = idp->top;
	if (!p)
		return ERR_PTR(-ENOENT);
ffffffff812c1a7e:	49 c7 c0 fe ff ff ff 	mov    $0xfffffffffffffffe,%r8

	if (id > idr_max(p->layer + 1))
		return ERR_PTR(-ENOENT);

	n = p->layer * IDR_BITS;
ffffffff812c1a85:	42 8d 0c d5 00 00 00 	lea    0x0(,%r10,8),%ecx
ffffffff812c1a8c:	00 

	p = idp->top;
	if (!p)
		return ERR_PTR(-ENOENT);

	if (id > idr_max(p->layer + 1))
ffffffff812c1a8d:	7f 43                	jg     ffffffff812c1ad2 <idr_replace+0x82>
		return ERR_PTR(-ENOENT);

	n = p->layer * IDR_BITS;
	while ((n > 0) && p) {
ffffffff812c1a8f:	85 c9                	test   %ecx,%ecx
ffffffff812c1a91:	7e 16                	jle    ffffffff812c1aa9 <idr_replace+0x59>
ffffffff812c1a93:	4d 85 c9             	test   %r9,%r9
ffffffff812c1a96:	74 11                	je     ffffffff812c1aa9 <idr_replace+0x59>
		p = p->ary[(id >> n) & IDR_MASK];
ffffffff812c1a98:	89 d0                	mov    %edx,%eax
ffffffff812c1a9a:	d3 f8                	sar    %cl,%eax
		n -= IDR_BITS;
ffffffff812c1a9c:	83 e9 08             	sub    $0x8,%ecx
	if (id > idr_max(p->layer + 1))
		return ERR_PTR(-ENOENT);

	n = p->layer * IDR_BITS;
	while ((n > 0) && p) {
		p = p->ary[(id >> n) & IDR_MASK];
ffffffff812c1a9f:	0f b6 c0             	movzbl %al,%eax
ffffffff812c1aa2:	4d 8b 4c c1 08       	mov    0x8(%r9,%rax,8),%r9
ffffffff812c1aa7:	eb e6                	jmp    ffffffff812c1a8f <idr_replace+0x3f>
		n -= IDR_BITS;
	}

	n = id & IDR_MASK;
	if (unlikely(p == NULL || !test_bit(n, p->bitmap)))
ffffffff812c1aa9:	4d 85 c9             	test   %r9,%r9
	while ((n > 0) && p) {
		p = p->ary[(id >> n) & IDR_MASK];
		n -= IDR_BITS;
	}

	n = id & IDR_MASK;
ffffffff812c1aac:	0f b6 d2             	movzbl %dl,%edx
	if (id < 0)
		return ERR_PTR(-EINVAL);

	p = idp->top;
	if (!p)
		return ERR_PTR(-ENOENT);
ffffffff812c1aaf:	49 c7 c0 fe ff ff ff 	mov    $0xfffffffffffffffe,%r8
		p = p->ary[(id >> n) & IDR_MASK];
		n -= IDR_BITS;
	}

	n = id & IDR_MASK;
	if (unlikely(p == NULL || !test_bit(n, p->bitmap)))
ffffffff812c1ab6:	74 1a                	je     ffffffff812c1ad2 <idr_replace+0x82>

static inline int variable_test_bit(long nr, volatile const unsigned long *addr)
{
	int oldbit;

	asm volatile("bt %2,%1\n\t"
ffffffff812c1ab8:	49 0f a3 91 10 08 00 	bt     %rdx,0x810(%r9)
ffffffff812c1abf:	00 
ffffffff812c1ac0:	19 c0                	sbb    %eax,%eax
ffffffff812c1ac2:	85 c0                	test   %eax,%eax
ffffffff812c1ac4:	74 0c                	je     ffffffff812c1ad2 <idr_replace+0x82>
ffffffff812c1ac6:	49 8d 04 d1          	lea    (%r9,%rdx,8),%rax
		return ERR_PTR(-ENOENT);

	old_p = p->ary[n];
ffffffff812c1aca:	4c 8b 40 08          	mov    0x8(%rax),%r8
	rcu_assign_pointer(p->ary[n], ptr);
ffffffff812c1ace:	48 89 70 08          	mov    %rsi,0x8(%rax)

	return old_p;
}
ffffffff812c1ad2:	4c 89 c0             	mov    %r8,%rax
ffffffff812c1ad5:	5d                   	pop    %rbp
ffffffff812c1ad6:	c3                   	retq   
ffffffff812c1ad7:	4c 89 c0             	mov    %r8,%rax
ffffffff812c1ada:	c3                   	retq   

ffffffff812c1adb <idr_layer_rcu_free>:
	 */
	return kmem_cache_zalloc(idr_layer_cache, gfp_mask);
}

static void idr_layer_rcu_free(struct rcu_head *head)
{
ffffffff812c1adb:	55                   	push   %rbp
	struct idr_layer *layer;

	layer = container_of(head, struct idr_layer, rcu_head);
	kmem_cache_free(idr_layer_cache, layer);
ffffffff812c1adc:	48 8d b7 f0 f7 ff ff 	lea    -0x810(%rdi),%rsi
ffffffff812c1ae3:	48 8b 3d 9e fc 8e 00 	mov    0x8efc9e(%rip),%rdi        # ffffffff81bb1788 <__key.11630>
	 */
	return kmem_cache_zalloc(idr_layer_cache, gfp_mask);
}

static void idr_layer_rcu_free(struct rcu_head *head)
{
ffffffff812c1aea:	48 89 e5             	mov    %rsp,%rbp
	struct idr_layer *layer;

	layer = container_of(head, struct idr_layer, rcu_head);
	kmem_cache_free(idr_layer_cache, layer);
ffffffff812c1aed:	e8 80 ea e3 ff       	callq  ffffffff81100572 <kmem_cache_free>
}
ffffffff812c1af2:	5d                   	pop    %rbp
ffffffff812c1af3:	c3                   	retq   

ffffffff812c1af4 <idr_init>:
 * This function is use to set up the handle (@idp) that you will pass
 * to the rest of the functions.
 */
void idr_init(struct idr *idp)
{
	memset(idp, 0, sizeof(struct idr));
ffffffff812c1af4:	31 c0                	xor    %eax,%eax
ffffffff812c1af6:	b9 10 00 00 00       	mov    $0x10,%ecx
 *
 * This function is use to set up the handle (@idp) that you will pass
 * to the rest of the functions.
 */
void idr_init(struct idr *idp)
{
ffffffff812c1afb:	55                   	push   %rbp
ffffffff812c1afc:	48 89 fa             	mov    %rdi,%rdx
	memset(idp, 0, sizeof(struct idr));
	spin_lock_init(&idp->lock);
ffffffff812c1aff:	48 c7 c6 d4 64 7b 81 	mov    $0xffffffff817b64d4,%rsi
 * This function is use to set up the handle (@idp) that you will pass
 * to the rest of the functions.
 */
void idr_init(struct idr *idp)
{
	memset(idp, 0, sizeof(struct idr));
ffffffff812c1b06:	f3 ab                	rep stos %eax,%es:(%rdi)
	spin_lock_init(&idp->lock);
ffffffff812c1b08:	48 8d 7a 18          	lea    0x18(%rdx),%rdi
 *
 * This function is use to set up the handle (@idp) that you will pass
 * to the rest of the functions.
 */
void idr_init(struct idr *idp)
{
ffffffff812c1b0c:	48 89 e5             	mov    %rsp,%rbp
	memset(idp, 0, sizeof(struct idr));
	spin_lock_init(&idp->lock);
ffffffff812c1b0f:	48 c7 c2 88 17 bb 81 	mov    $0xffffffff81bb1788,%rdx
ffffffff812c1b16:	e8 58 e2 dc ff       	callq  ffffffff8108fd73 <__raw_spin_lock_init>
}
ffffffff812c1b1b:	5d                   	pop    %rbp
ffffffff812c1b1c:	c3                   	retq   

ffffffff812c1b1d <ida_init>:
 * This function is use to set up the handle (@ida) that you will pass
 * to the rest of the functions.
 */
void ida_init(struct ida *ida)
{
	memset(ida, 0, sizeof(struct ida));
ffffffff812c1b1d:	31 c0                	xor    %eax,%eax
ffffffff812c1b1f:	b9 12 00 00 00       	mov    $0x12,%ecx
 *
 * This function is use to set up the handle (@ida) that you will pass
 * to the rest of the functions.
 */
void ida_init(struct ida *ida)
{
ffffffff812c1b24:	55                   	push   %rbp
ffffffff812c1b25:	48 89 fa             	mov    %rdi,%rdx
	memset(ida, 0, sizeof(struct ida));
ffffffff812c1b28:	f3 ab                	rep stos %eax,%es:(%rdi)
 *
 * This function is use to set up the handle (@ida) that you will pass
 * to the rest of the functions.
 */
void ida_init(struct ida *ida)
{
ffffffff812c1b2a:	48 89 e5             	mov    %rsp,%rbp
	memset(ida, 0, sizeof(struct ida));
	idr_init(&ida->idr);
ffffffff812c1b2d:	48 89 d7             	mov    %rdx,%rdi
ffffffff812c1b30:	e8 bf ff ff ff       	callq  ffffffff812c1af4 <idr_init>

}
ffffffff812c1b35:	5d                   	pop    %rbp
ffffffff812c1b36:	c3                   	retq   

ffffffff812c1b37 <free_bitmap>:
 *
 * 2007-04-25  written by Tejun Heo <htejun@gmail.com>
 */

static void free_bitmap(struct ida *ida, struct ida_bitmap *bitmap)
{
ffffffff812c1b37:	55                   	push   %rbp
ffffffff812c1b38:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1b3b:	41 55                	push   %r13
ffffffff812c1b3d:	41 54                	push   %r12
ffffffff812c1b3f:	53                   	push   %rbx
ffffffff812c1b40:	52                   	push   %rdx
ffffffff812c1b41:	49 89 f4             	mov    %rsi,%r12
	unsigned long flags;

	if (!ida->free_bitmap) {
ffffffff812c1b44:	48 83 7f 40 00       	cmpq   $0x0,0x40(%rdi)
ffffffff812c1b49:	75 28                	jne    ffffffff812c1b73 <free_bitmap+0x3c>
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &lock->rlock;
ffffffff812c1b4b:	4c 8d 6f 18          	lea    0x18(%rdi),%r13
ffffffff812c1b4f:	48 89 fb             	mov    %rdi,%rbx
		spin_lock_irqsave(&ida->idr.lock, flags);
ffffffff812c1b52:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c1b55:	e8 ce 6c 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
		if (!ida->free_bitmap) {
ffffffff812c1b5a:	48 83 7b 40 00       	cmpq   $0x0,0x40(%rbx)
ffffffff812c1b5f:	75 07                	jne    ffffffff812c1b68 <free_bitmap+0x31>
			ida->free_bitmap = bitmap;
ffffffff812c1b61:	4c 89 63 40          	mov    %r12,0x40(%rbx)
			bitmap = NULL;
ffffffff812c1b65:	45 31 e4             	xor    %r12d,%r12d
	raw_spin_unlock_irq(&lock->rlock);
}

static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	raw_spin_unlock_irqrestore(&lock->rlock, flags);
ffffffff812c1b68:	48 89 c6             	mov    %rax,%rsi
ffffffff812c1b6b:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c1b6e:	e8 69 6b 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
		}
		spin_unlock_irqrestore(&ida->idr.lock, flags);
	}

	kfree(bitmap);
ffffffff812c1b73:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c1b76:	e8 0e ee e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812c1b7b:	58                   	pop    %rax
ffffffff812c1b7c:	5b                   	pop    %rbx
ffffffff812c1b7d:	41 5c                	pop    %r12
ffffffff812c1b7f:	41 5d                	pop    %r13
ffffffff812c1b81:	5d                   	pop    %rbp
ffffffff812c1b82:	c3                   	retq   

ffffffff812c1b83 <idr_find_slowpath>:
void *idr_find_slowpath(struct idr *idp, int id)
{
	int n;
	struct idr_layer *p;

	if (id < 0)
ffffffff812c1b83:	85 f6                	test   %esi,%esi
ffffffff812c1b85:	78 63                	js     ffffffff812c1bea <idr_find_slowpath+0x67>
	}
}
EXPORT_SYMBOL(idr_destroy);

void *idr_find_slowpath(struct idr *idp, int id)
{
ffffffff812c1b87:	55                   	push   %rbp
ffffffff812c1b88:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1b8b:	41 57                	push   %r15
ffffffff812c1b8d:	41 56                	push   %r14
ffffffff812c1b8f:	53                   	push   %rbx
	struct idr_layer *p;

	if (id < 0)
		return NULL;

	p = rcu_dereference_raw(idp->top);
ffffffff812c1b90:	48 8b 5f 08          	mov    0x8(%rdi),%rbx
	if (!p)
ffffffff812c1b94:	48 85 db             	test   %rbx,%rbx
ffffffff812c1b97:	74 54                	je     ffffffff812c1bed <idr_find_slowpath+0x6a>
		return NULL;
	n = (p->layer+1) * IDR_BITS;
ffffffff812c1b99:	8b 43 04             	mov    0x4(%rbx),%eax
ffffffff812c1b9c:	41 89 f7             	mov    %esi,%r15d
ffffffff812c1b9f:	8d 78 01             	lea    0x1(%rax),%edi
ffffffff812c1ba2:	44 8d 34 fd 00 00 00 	lea    0x0(,%rdi,8),%r14d
ffffffff812c1ba9:	00 

	if (id > idr_max(p->layer + 1))
ffffffff812c1baa:	e8 a7 fc ff ff       	callq  ffffffff812c1856 <idr_max>
ffffffff812c1baf:	39 c6                	cmp    %eax,%esi
ffffffff812c1bb1:	7f 3a                	jg     ffffffff812c1bed <idr_find_slowpath+0x6a>
		return NULL;
	BUG_ON(n == 0);
ffffffff812c1bb3:	45 85 f6             	test   %r14d,%r14d
ffffffff812c1bb6:	75 12                	jne    ffffffff812c1bca <idr_find_slowpath+0x47>
ffffffff812c1bb8:	0f 0b                	ud2    

	while (n > 0 && p) {
		n -= IDR_BITS;
		BUG_ON(n != p->layer*IDR_BITS);
		p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
ffffffff812c1bba:	44 89 f8             	mov    %r15d,%eax
ffffffff812c1bbd:	44 88 f1             	mov    %r14b,%cl
ffffffff812c1bc0:	d3 f8                	sar    %cl,%eax
ffffffff812c1bc2:	0f b6 c0             	movzbl %al,%eax
ffffffff812c1bc5:	48 8b 5c c3 08       	mov    0x8(%rbx,%rax,8),%rbx

	if (id > idr_max(p->layer + 1))
		return NULL;
	BUG_ON(n == 0);

	while (n > 0 && p) {
ffffffff812c1bca:	45 85 f6             	test   %r14d,%r14d
ffffffff812c1bcd:	7e 16                	jle    ffffffff812c1be5 <idr_find_slowpath+0x62>
ffffffff812c1bcf:	48 85 db             	test   %rbx,%rbx
ffffffff812c1bd2:	74 11                	je     ffffffff812c1be5 <idr_find_slowpath+0x62>
		n -= IDR_BITS;
		BUG_ON(n != p->layer*IDR_BITS);
ffffffff812c1bd4:	8b 43 04             	mov    0x4(%rbx),%eax
	if (id > idr_max(p->layer + 1))
		return NULL;
	BUG_ON(n == 0);

	while (n > 0 && p) {
		n -= IDR_BITS;
ffffffff812c1bd7:	41 83 ee 08          	sub    $0x8,%r14d
		BUG_ON(n != p->layer*IDR_BITS);
ffffffff812c1bdb:	c1 e0 03             	shl    $0x3,%eax
ffffffff812c1bde:	41 39 c6             	cmp    %eax,%r14d
ffffffff812c1be1:	74 d7                	je     ffffffff812c1bba <idr_find_slowpath+0x37>
ffffffff812c1be3:	0f 0b                	ud2    
ffffffff812c1be5:	48 89 d8             	mov    %rbx,%rax
ffffffff812c1be8:	eb 05                	jmp    ffffffff812c1bef <idr_find_slowpath+0x6c>
{
	int n;
	struct idr_layer *p;

	if (id < 0)
		return NULL;
ffffffff812c1bea:	31 c0                	xor    %eax,%eax
		n -= IDR_BITS;
		BUG_ON(n != p->layer*IDR_BITS);
		p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
	}
	return((void *)p);
}
ffffffff812c1bec:	c3                   	retq   
{
	int n;
	struct idr_layer *p;

	if (id < 0)
		return NULL;
ffffffff812c1bed:	31 c0                	xor    %eax,%eax
		n -= IDR_BITS;
		BUG_ON(n != p->layer*IDR_BITS);
		p = rcu_dereference_raw(p->ary[(id >> n) & IDR_MASK]);
	}
	return((void *)p);
}
ffffffff812c1bef:	5b                   	pop    %rbx
ffffffff812c1bf0:	41 5e                	pop    %r14
ffffffff812c1bf2:	41 5f                	pop    %r15
ffffffff812c1bf4:	5d                   	pop    %rbp
ffffffff812c1bf5:	c3                   	retq   

ffffffff812c1bf6 <free_layer.isra.4>:

	layer = container_of(head, struct idr_layer, rcu_head);
	kmem_cache_free(idr_layer_cache, layer);
}

static inline void free_layer(struct idr *idr, struct idr_layer *p)
ffffffff812c1bf6:	55                   	push   %rbp
{
	if (idr->hint == p)
ffffffff812c1bf7:	48 39 37             	cmp    %rsi,(%rdi)

	layer = container_of(head, struct idr_layer, rcu_head);
	kmem_cache_free(idr_layer_cache, layer);
}

static inline void free_layer(struct idr *idr, struct idr_layer *p)
ffffffff812c1bfa:	48 89 e5             	mov    %rsp,%rbp
{
	if (idr->hint == p)
ffffffff812c1bfd:	75 07                	jne    ffffffff812c1c06 <free_layer.isra.4+0x10>
		RCU_INIT_POINTER(idr->hint, NULL);
ffffffff812c1bff:	48 c7 07 00 00 00 00 	movq   $0x0,(%rdi)
	call_rcu(&p->rcu_head, idr_layer_rcu_free);
ffffffff812c1c06:	48 8d be 10 08 00 00 	lea    0x810(%rsi),%rdi
ffffffff812c1c0d:	48 c7 c6 db 1a 2c 81 	mov    $0xffffffff812c1adb,%rsi
ffffffff812c1c14:	e8 9a aa dd ff       	callq  ffffffff8109c6b3 <call_rcu_sched>
}
ffffffff812c1c19:	5d                   	pop    %rbp
ffffffff812c1c1a:	c3                   	retq   

ffffffff812c1c1b <idr_destroy>:
 * A typical clean-up sequence for objects stored in an idr tree will use
 * idr_for_each() to free all objects, if necessary, then idr_destroy() to
 * free up the id mappings and cached idr_layers.
 */
void idr_destroy(struct idr *idp)
{
ffffffff812c1c1b:	55                   	push   %rbp
ffffffff812c1c1c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1c1f:	41 57                	push   %r15
ffffffff812c1c21:	41 56                	push   %r14
ffffffff812c1c23:	41 55                	push   %r13
ffffffff812c1c25:	41 54                	push   %r12
ffffffff812c1c27:	49 89 fd             	mov    %rdi,%r13
ffffffff812c1c2a:	53                   	push   %rbx
{
	int n, id, max;
	int bt_mask;
	struct idr_layer *p;
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];
ffffffff812c1c2b:	4c 8d 75 a8          	lea    -0x58(%rbp),%r14
 * A typical clean-up sequence for objects stored in an idr tree will use
 * idr_for_each() to free all objects, if necessary, then idr_destroy() to
 * free up the id mappings and cached idr_layers.
 */
void idr_destroy(struct idr *idp)
{
ffffffff812c1c2f:	48 83 ec 48          	sub    $0x48,%rsp
	int bt_mask;
	struct idr_layer *p;
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];

	n = idp->layers * IDR_BITS;
ffffffff812c1c33:	8b 7f 10             	mov    0x10(%rdi),%edi
	*paa = idp->top;
ffffffff812c1c36:	49 8b 45 08          	mov    0x8(%r13),%rax
	RCU_INIT_POINTER(idp->top, NULL);
ffffffff812c1c3a:	49 c7 45 08 00 00 00 	movq   $0x0,0x8(%r13)
ffffffff812c1c41:	00 
	int bt_mask;
	struct idr_layer *p;
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct idr_layer **paa = &pa[0];

	n = idp->layers * IDR_BITS;
ffffffff812c1c42:	8d 1c fd 00 00 00 00 	lea    0x0(,%rdi,8),%ebx
	*paa = idp->top;
ffffffff812c1c49:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
	RCU_INIT_POINTER(idp->top, NULL);
	max = idr_max(idp->layers);
ffffffff812c1c4d:	e8 04 fc ff ff       	callq  ffffffff812c1856 <idr_max>

	id = 0;
ffffffff812c1c52:	31 d2                	xor    %edx,%edx
	while (id >= 0 && id <= max) {
ffffffff812c1c54:	89 d1                	mov    %edx,%ecx
ffffffff812c1c56:	83 e9 00             	sub    $0x0,%ecx
ffffffff812c1c59:	78 71                	js     ffffffff812c1ccc <idr_destroy+0xb1>
ffffffff812c1c5b:	39 d0                	cmp    %edx,%eax
ffffffff812c1c5d:	7c 6d                	jl     ffffffff812c1ccc <idr_destroy+0xb1>
		p = *paa;
ffffffff812c1c5f:	49 8b 36             	mov    (%r14),%rsi
		while (n > IDR_BITS && p) {
ffffffff812c1c62:	83 fb 08             	cmp    $0x8,%ebx
ffffffff812c1c65:	7e 20                	jle    ffffffff812c1c87 <idr_destroy+0x6c>
ffffffff812c1c67:	48 85 f6             	test   %rsi,%rsi
ffffffff812c1c6a:	74 1b                	je     ffffffff812c1c87 <idr_destroy+0x6c>
			n -= IDR_BITS;
ffffffff812c1c6c:	83 eb 08             	sub    $0x8,%ebx
			p = p->ary[(id >> n) & IDR_MASK];
ffffffff812c1c6f:	89 d7                	mov    %edx,%edi
			*++paa = p;
ffffffff812c1c71:	49 83 c6 08          	add    $0x8,%r14
	id = 0;
	while (id >= 0 && id <= max) {
		p = *paa;
		while (n > IDR_BITS && p) {
			n -= IDR_BITS;
			p = p->ary[(id >> n) & IDR_MASK];
ffffffff812c1c75:	88 d9                	mov    %bl,%cl
ffffffff812c1c77:	d3 ff                	sar    %cl,%edi
ffffffff812c1c79:	40 0f b6 cf          	movzbl %dil,%ecx
ffffffff812c1c7d:	48 8b 74 ce 08       	mov    0x8(%rsi,%rcx,8),%rsi
			*++paa = p;
ffffffff812c1c82:	49 89 36             	mov    %rsi,(%r14)
ffffffff812c1c85:	eb db                	jmp    ffffffff812c1c62 <idr_destroy+0x47>
		}

		bt_mask = id;
		id += 1 << n;
ffffffff812c1c87:	41 bc 01 00 00 00    	mov    $0x1,%r12d
ffffffff812c1c8d:	88 d9                	mov    %bl,%cl
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
	asm("bsrl %1,%0"
ffffffff812c1c8f:	41 83 cf ff          	or     $0xffffffff,%r15d
ffffffff812c1c93:	41 d3 e4             	shl    %cl,%r12d
ffffffff812c1c96:	41 01 d4             	add    %edx,%r12d
ffffffff812c1c99:	44 31 e2             	xor    %r12d,%edx
ffffffff812c1c9c:	44 0f bd fa          	bsr    %edx,%r15d
		/* Get the highest bit that the above add changed from 0->1. */
		while (n < fls(id ^ bt_mask)) {
ffffffff812c1ca0:	41 ff c7             	inc    %r15d
ffffffff812c1ca3:	44 39 fb             	cmp    %r15d,%ebx
ffffffff812c1ca6:	7d 1f                	jge    ffffffff812c1cc7 <idr_destroy+0xac>
			if (*paa)
ffffffff812c1ca8:	49 8b 36             	mov    (%r14),%rsi
ffffffff812c1cab:	48 85 f6             	test   %rsi,%rsi
ffffffff812c1cae:	74 0e                	je     ffffffff812c1cbe <idr_destroy+0xa3>
				free_layer(idp, *paa);
ffffffff812c1cb0:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c1cb3:	89 45 9c             	mov    %eax,-0x64(%rbp)
ffffffff812c1cb6:	e8 3b ff ff ff       	callq  ffffffff812c1bf6 <free_layer.isra.4>
ffffffff812c1cbb:	8b 45 9c             	mov    -0x64(%rbp),%eax
			n += IDR_BITS;
ffffffff812c1cbe:	83 c3 08             	add    $0x8,%ebx
			--paa;
ffffffff812c1cc1:	49 83 ee 08          	sub    $0x8,%r14
ffffffff812c1cc5:	eb dc                	jmp    ffffffff812c1ca3 <idr_destroy+0x88>
			p = p->ary[(id >> n) & IDR_MASK];
			*++paa = p;
		}

		bt_mask = id;
		id += 1 << n;
ffffffff812c1cc7:	44 89 e2             	mov    %r12d,%edx
ffffffff812c1cca:	eb 88                	jmp    ffffffff812c1c54 <idr_destroy+0x39>
				free_layer(idp, *paa);
			n += IDR_BITS;
			--paa;
		}
	}
	idp->layers = 0;
ffffffff812c1ccc:	41 c7 45 10 00 00 00 	movl   $0x0,0x10(%r13)
ffffffff812c1cd3:	00 
 */
void idr_destroy(struct idr *idp)
{
	__idr_remove_all(idp);

	while (idp->id_free_cnt) {
ffffffff812c1cd4:	41 83 7d 30 00       	cmpl   $0x0,0x30(%r13)
ffffffff812c1cd9:	74 19                	je     ffffffff812c1cf4 <idr_destroy+0xd9>
		struct idr_layer *p = get_from_free_list(idp);
ffffffff812c1cdb:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c1cde:	e8 94 fb ff ff       	callq  ffffffff812c1877 <get_from_free_list>
		kmem_cache_free(idr_layer_cache, p);
ffffffff812c1ce3:	48 8b 3d 9e fa 8e 00 	mov    0x8efa9e(%rip),%rdi        # ffffffff81bb1788 <__key.11630>
ffffffff812c1cea:	48 89 c6             	mov    %rax,%rsi
ffffffff812c1ced:	e8 80 e8 e3 ff       	callq  ffffffff81100572 <kmem_cache_free>
ffffffff812c1cf2:	eb e0                	jmp    ffffffff812c1cd4 <idr_destroy+0xb9>
	}
}
ffffffff812c1cf4:	48 83 c4 48          	add    $0x48,%rsp
ffffffff812c1cf8:	5b                   	pop    %rbx
ffffffff812c1cf9:	41 5c                	pop    %r12
ffffffff812c1cfb:	41 5d                	pop    %r13
ffffffff812c1cfd:	41 5e                	pop    %r14
ffffffff812c1cff:	41 5f                	pop    %r15
ffffffff812c1d01:	5d                   	pop    %rbp
ffffffff812c1d02:	c3                   	retq   

ffffffff812c1d03 <ida_destroy>:
/**
 * ida_destroy - release all cached layers within an ida tree
 * @ida:		ida handle
 */
void ida_destroy(struct ida *ida)
{
ffffffff812c1d03:	55                   	push   %rbp
ffffffff812c1d04:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1d07:	53                   	push   %rbx
ffffffff812c1d08:	50                   	push   %rax
ffffffff812c1d09:	48 89 fb             	mov    %rdi,%rbx
	idr_destroy(&ida->idr);
ffffffff812c1d0c:	e8 0a ff ff ff       	callq  ffffffff812c1c1b <idr_destroy>
	kfree(ida->free_bitmap);
ffffffff812c1d11:	48 8b 7b 40          	mov    0x40(%rbx),%rdi
ffffffff812c1d15:	e8 6f ec e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812c1d1a:	5a                   	pop    %rdx
ffffffff812c1d1b:	5b                   	pop    %rbx
ffffffff812c1d1c:	5d                   	pop    %rbp
ffffffff812c1d1d:	c3                   	retq   

ffffffff812c1d1e <ida_pre_get>:
 *
 * If the system is REALLY out of memory this function returns %0,
 * otherwise %1.
 */
int ida_pre_get(struct ida *ida, gfp_t gfp_mask)
{
ffffffff812c1d1e:	55                   	push   %rbp
ffffffff812c1d1f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1d22:	41 57                	push   %r15
/*
 * Shortcuts
 */
static inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
{
	return kmem_cache_alloc(k, flags | __GFP_ZERO);
ffffffff812c1d24:	41 89 f7             	mov    %esi,%r15d
ffffffff812c1d27:	41 56                	push   %r14
ffffffff812c1d29:	41 55                	push   %r13
ffffffff812c1d2b:	41 81 cf 00 80 00 00 	or     $0x8000,%r15d
ffffffff812c1d32:	41 54                	push   %r12
ffffffff812c1d34:	53                   	push   %rbx
ffffffff812c1d35:	41 89 f4             	mov    %esi,%r12d
ffffffff812c1d38:	51                   	push   %rcx
ffffffff812c1d39:	48 89 fb             	mov    %rdi,%rbx
	}
}

static int __idr_pre_get(struct idr *idp, gfp_t gfp_mask)
{
	while (idp->id_free_cnt < MAX_IDR_FREE) {
ffffffff812c1d3c:	83 7b 30 07          	cmpl   $0x7,0x30(%rbx)
ffffffff812c1d40:	77 43                	ja     ffffffff812c1d85 <ida_pre_get+0x67>
ffffffff812c1d42:	48 8b 3d 3f fa 8e 00 	mov    0x8efa3f(%rip),%rdi        # ffffffff81bb1788 <__key.11630>
ffffffff812c1d49:	44 89 fe             	mov    %r15d,%esi
ffffffff812c1d4c:	e8 80 f3 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
		struct idr_layer *new;
		new = kmem_cache_zalloc(idr_layer_cache, gfp_mask);
		if (new == NULL)
ffffffff812c1d51:	48 85 c0             	test   %rax,%rax
ffffffff812c1d54:	49 89 c5             	mov    %rax,%r13
ffffffff812c1d57:	75 04                	jne    ffffffff812c1d5d <ida_pre_get+0x3f>
 */
int ida_pre_get(struct ida *ida, gfp_t gfp_mask)
{
	/* allocate idr_layers */
	if (!__idr_pre_get(&ida->idr, gfp_mask))
		return 0;
ffffffff812c1d59:	31 c0                	xor    %eax,%eax
ffffffff812c1d5b:	eb 65                	jmp    ffffffff812c1dc2 <ida_pre_get+0xa4>
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &lock->rlock;
ffffffff812c1d5d:	4c 8d 73 18          	lea    0x18(%rbx),%r14
	unsigned long flags;

	/*
	 * Depends on the return element being zeroed.
	 */
	spin_lock_irqsave(&idp->lock, flags);
ffffffff812c1d61:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c1d64:	e8 bf 6a 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
}

/* only called when idp->lock is held */
static void __move_to_free_list(struct idr *idp, struct idr_layer *p)
{
	p->ary[0] = idp->id_free;
ffffffff812c1d69:	48 8b 53 38          	mov    0x38(%rbx),%rdx
	raw_spin_unlock_irq(&lock->rlock);
}

static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	raw_spin_unlock_irqrestore(&lock->rlock, flags);
ffffffff812c1d6d:	48 89 c6             	mov    %rax,%rsi
ffffffff812c1d70:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c1d73:	49 89 55 08          	mov    %rdx,0x8(%r13)
	idp->id_free = p;
	idp->id_free_cnt++;
ffffffff812c1d77:	ff 43 30             	incl   0x30(%rbx)

/* only called when idp->lock is held */
static void __move_to_free_list(struct idr *idp, struct idr_layer *p)
{
	p->ary[0] = idp->id_free;
	idp->id_free = p;
ffffffff812c1d7a:	4c 89 6b 38          	mov    %r13,0x38(%rbx)
ffffffff812c1d7e:	e8 59 69 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
ffffffff812c1d83:	eb b7                	jmp    ffffffff812c1d3c <ida_pre_get+0x1e>
	/* allocate idr_layers */
	if (!__idr_pre_get(&ida->idr, gfp_mask))
		return 0;

	/* allocate free_bitmap */
	if (!ida->free_bitmap) {
ffffffff812c1d85:	48 83 7b 40 00       	cmpq   $0x0,0x40(%rbx)
ffffffff812c1d8a:	75 31                	jne    ffffffff812c1dbd <ida_pre_get+0x9f>

#else /* CONFIG_TRACING */
static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
		gfp_t flags, size_t size)
{
	void *ret = kmem_cache_alloc(s, flags);
ffffffff812c1d8c:	44 89 e6             	mov    %r12d,%esi
#endif
			return kmalloc_container(nsproxy, size, flags);
		}
#endif

		if (!(flags & GFP_DMA)) {
ffffffff812c1d8f:	41 80 e4 01          	and    $0x1,%r12b
ffffffff812c1d93:	75 0e                	jne    ffffffff812c1da3 <ida_pre_get+0x85>

#else /* CONFIG_TRACING */
static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
		gfp_t flags, size_t size)
{
	void *ret = kmem_cache_alloc(s, flags);
ffffffff812c1d95:	48 8b 3d dc 08 8e 00 	mov    0x8e08dc(%rip),%rdi        # ffffffff81ba2678 <kmalloc_caches+0x38>
ffffffff812c1d9c:	e8 30 f3 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
ffffffff812c1da1:	eb 0a                	jmp    ffffffff812c1dad <ida_pre_get+0x8f>
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812c1da3:	bf 80 00 00 00       	mov    $0x80,%edi
ffffffff812c1da8:	e8 de f3 e3 ff       	callq  ffffffff8110118b <__kmalloc>
		struct ida_bitmap *bitmap;

		bitmap = kmalloc(sizeof(struct ida_bitmap), gfp_mask);
		if (!bitmap)
ffffffff812c1dad:	48 85 c0             	test   %rax,%rax
ffffffff812c1db0:	74 a7                	je     ffffffff812c1d59 <ida_pre_get+0x3b>
			return 0;

		free_bitmap(ida, bitmap);
ffffffff812c1db2:	48 89 c6             	mov    %rax,%rsi
ffffffff812c1db5:	48 89 df             	mov    %rbx,%rdi
ffffffff812c1db8:	e8 7a fd ff ff       	callq  ffffffff812c1b37 <free_bitmap>
	}

	return 1;
ffffffff812c1dbd:	b8 01 00 00 00       	mov    $0x1,%eax
}
ffffffff812c1dc2:	5a                   	pop    %rdx
ffffffff812c1dc3:	5b                   	pop    %rbx
ffffffff812c1dc4:	41 5c                	pop    %r12
ffffffff812c1dc6:	41 5d                	pop    %r13
ffffffff812c1dc8:	41 5e                	pop    %r14
ffffffff812c1dca:	41 5f                	pop    %r15
ffffffff812c1dcc:	5d                   	pop    %rbp
ffffffff812c1dcd:	c3                   	retq   

ffffffff812c1dce <idr_mark_full>:
	__move_to_free_list(idp, p);
	spin_unlock_irqrestore(&idp->lock, flags);
}

static void idr_mark_full(struct idr_layer **pa, int id)
{
ffffffff812c1dce:	55                   	push   %rbp
	struct idr_layer *p = pa[0];
	int l = 0;

	__set_bit(id & IDR_MASK, p->bitmap);
ffffffff812c1dcf:	40 0f b6 c6          	movzbl %sil,%eax
	__move_to_free_list(idp, p);
	spin_unlock_irqrestore(&idp->lock, flags);
}

static void idr_mark_full(struct idr_layer **pa, int id)
{
ffffffff812c1dd3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1dd6:	41 55                	push   %r13
ffffffff812c1dd8:	49 89 fd             	mov    %rdi,%r13
ffffffff812c1ddb:	41 54                	push   %r12
ffffffff812c1ddd:	53                   	push   %rbx
ffffffff812c1dde:	89 f3                	mov    %esi,%ebx
ffffffff812c1de0:	52                   	push   %rdx
	struct idr_layer *p = pa[0];
ffffffff812c1de1:	48 8b 3f             	mov    (%rdi),%rdi
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
ffffffff812c1de4:	48 0f ab 87 10 08 00 	bts    %rax,0x810(%rdi)
ffffffff812c1deb:	00 
	int l = 0;
ffffffff812c1dec:	45 31 e4             	xor    %r12d,%r12d
	 * If this layer is full mark the bit in the layer above to
	 * show that this part of the radix tree is full.  This may
	 * complete the layer above and require walking up the radix
	 * tree.
	 */
	while (bitmap_full(p->bitmap, IDR_SIZE)) {
ffffffff812c1def:	48 81 c7 10 08 00 00 	add    $0x810,%rdi
static inline int bitmap_full(const unsigned long *src, unsigned int nbits)
{
	if (small_const_nbits(nbits))
		return ! (~(*src) & BITMAP_LAST_WORD_MASK(nbits));

	return find_first_zero_bit(src, nbits) == nbits;
ffffffff812c1df6:	be 00 01 00 00       	mov    $0x100,%esi
ffffffff812c1dfb:	e8 82 ee 00 00       	callq  ffffffff812d0c82 <find_first_zero_bit>
ffffffff812c1e00:	48 3d 00 01 00 00    	cmp    $0x100,%rax
ffffffff812c1e06:	75 20                	jne    ffffffff812c1e28 <idr_mark_full+0x5a>
		if (!(p = pa[++l]))
ffffffff812c1e08:	41 ff c4             	inc    %r12d
ffffffff812c1e0b:	49 63 c4             	movslq %r12d,%rax
ffffffff812c1e0e:	49 8b 7c c5 00       	mov    0x0(%r13,%rax,8),%rdi
ffffffff812c1e13:	48 85 ff             	test   %rdi,%rdi
ffffffff812c1e16:	74 10                	je     ffffffff812c1e28 <idr_mark_full+0x5a>
			break;
		id = id >> IDR_BITS;
ffffffff812c1e18:	c1 fb 08             	sar    $0x8,%ebx
		__set_bit((id & IDR_MASK), p->bitmap);
ffffffff812c1e1b:	0f b6 c3             	movzbl %bl,%eax
ffffffff812c1e1e:	48 0f ab 87 10 08 00 	bts    %rax,0x810(%rdi)
ffffffff812c1e25:	00 
ffffffff812c1e26:	eb c7                	jmp    ffffffff812c1def <idr_mark_full+0x21>
	}
}
ffffffff812c1e28:	58                   	pop    %rax
ffffffff812c1e29:	5b                   	pop    %rbx
ffffffff812c1e2a:	41 5c                	pop    %r12
ffffffff812c1e2c:	41 5d                	pop    %r13
ffffffff812c1e2e:	5d                   	pop    %rbp
ffffffff812c1e2f:	c3                   	retq   

ffffffff812c1e30 <idr_preload>:
 *	idr_preload_end();
 *	if (id < 0)
 *		error;
 */
void idr_preload(gfp_t gfp_mask)
{
ffffffff812c1e30:	55                   	push   %rbp
ffffffff812c1e31:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1e34:	53                   	push   %rbx
ffffffff812c1e35:	52                   	push   %rdx
 * We mask the PREEMPT_NEED_RESCHED bit so as not to confuse all current users
 * that think a non-zero value indicates we cannot preempt.
 */
static __always_inline int preempt_count(void)
{
	return raw_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
ffffffff812c1e36:	65 8b 05 5b 8b d4 7e 	mov    %gs:0x7ed48b5b(%rip),%eax        # a998 <__preempt_count>
	/*
	 * Consuming preload buffer from non-process context breaks preload
	 * allocation guarantee.  Disallow usage from those contexts.
	 */
	WARN_ON_ONCE(in_interrupt());
ffffffff812c1e3d:	a9 00 ff 1f 00       	test   $0x1fff00,%eax
 *	idr_preload_end();
 *	if (id < 0)
 *		error;
 */
void idr_preload(gfp_t gfp_mask)
{
ffffffff812c1e42:	89 fb                	mov    %edi,%ebx
	/*
	 * Consuming preload buffer from non-process context breaks preload
	 * allocation guarantee.  Disallow usage from those contexts.
	 */
	WARN_ON_ONCE(in_interrupt());
ffffffff812c1e44:	74 21                	je     ffffffff812c1e67 <idr_preload+0x37>
ffffffff812c1e46:	80 3d 97 54 79 00 00 	cmpb   $0x0,0x795497(%rip)        # ffffffff81a572e4 <__warned.16218>
ffffffff812c1e4d:	75 18                	jne    ffffffff812c1e67 <idr_preload+0x37>
ffffffff812c1e4f:	be 91 01 00 00       	mov    $0x191,%esi
ffffffff812c1e54:	48 c7 c7 e9 64 7b 81 	mov    $0xffffffff817b64e9,%rdi
ffffffff812c1e5b:	e8 55 45 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c1e60:	c6 05 7d 54 79 00 01 	movb   $0x1,0x79547d(%rip)        # ffffffff81a572e4 <__warned.16218>
/*
 * Shortcuts
 */
static inline void *kmem_cache_zalloc(struct kmem_cache *k, gfp_t flags)
{
	return kmem_cache_alloc(k, flags | __GFP_ZERO);
ffffffff812c1e67:	80 cf 80             	or     $0x80,%bh
	 * return value from idr_alloc() needs to be checked for failure
	 * anyway.  Silently give up if allocation fails.  The caller can
	 * treat failures from idr_alloc() as if idr_alloc() were called
	 * with @gfp_mask which should be enough.
	 */
	while (__this_cpu_read(idr_preload_cnt) < MAX_IDR_FREE) {
ffffffff812c1e6a:	65 8b 05 d7 ea d4 7e 	mov    %gs:0x7ed4ead7(%rip),%eax        # 10948 <idr_preload_cnt>
ffffffff812c1e71:	83 f8 07             	cmp    $0x7,%eax
ffffffff812c1e74:	77 30                	ja     ffffffff812c1ea6 <idr_preload+0x76>
ffffffff812c1e76:	48 8b 3d 0b f9 8e 00 	mov    0x8ef90b(%rip),%rdi        # ffffffff81bb1788 <__key.11630>
ffffffff812c1e7d:	89 de                	mov    %ebx,%esi
ffffffff812c1e7f:	e8 4d f2 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
		struct idr_layer *new;

		preempt_enable();
		new = kmem_cache_zalloc(idr_layer_cache, gfp_mask);
		preempt_disable();
		if (!new)
ffffffff812c1e84:	48 85 c0             	test   %rax,%rax
ffffffff812c1e87:	74 1d                	je     ffffffff812c1ea6 <idr_preload+0x76>
			break;

		/* link the new one to per-cpu preload list */
		new->ary[0] = __this_cpu_read(idr_preload_head);
ffffffff812c1e89:	65 48 8b 15 bf ea d4 	mov    %gs:0x7ed4eabf(%rip),%rdx        # 10950 <idr_preload_head>
ffffffff812c1e90:	7e 
		__this_cpu_write(idr_preload_head, new);
		__this_cpu_inc(idr_preload_cnt);
ffffffff812c1e91:	65 ff 05 b0 ea d4 7e 	incl   %gs:0x7ed4eab0(%rip)        # 10948 <idr_preload_cnt>
		preempt_disable();
		if (!new)
			break;

		/* link the new one to per-cpu preload list */
		new->ary[0] = __this_cpu_read(idr_preload_head);
ffffffff812c1e98:	48 89 50 08          	mov    %rdx,0x8(%rax)
		__this_cpu_write(idr_preload_head, new);
ffffffff812c1e9c:	65 48 89 05 ac ea d4 	mov    %rax,%gs:0x7ed4eaac(%rip)        # 10950 <idr_preload_head>
ffffffff812c1ea3:	7e 
		__this_cpu_inc(idr_preload_cnt);
ffffffff812c1ea4:	eb c4                	jmp    ffffffff812c1e6a <idr_preload+0x3a>
	}
}
ffffffff812c1ea6:	58                   	pop    %rax
ffffffff812c1ea7:	5b                   	pop    %rbx
ffffffff812c1ea8:	5d                   	pop    %rbp
ffffffff812c1ea9:	c3                   	retq   

ffffffff812c1eaa <idr_layer_alloc>:
 * @layer_idr is to maintain backward compatibility with the old alloc
 * interface - idr_pre_get() and idr_get_new*() - and will be removed
 * together with per-pool preload buffer.
 */
static struct idr_layer *idr_layer_alloc(gfp_t gfp_mask, struct idr *layer_idr)
{
ffffffff812c1eaa:	55                   	push   %rbp
	struct idr_layer *new;

	/* this is the old path, bypass to get_from_free_list() */
	if (layer_idr)
ffffffff812c1eab:	48 85 f6             	test   %rsi,%rsi
 * @layer_idr is to maintain backward compatibility with the old alloc
 * interface - idr_pre_get() and idr_get_new*() - and will be removed
 * together with per-pool preload buffer.
 */
static struct idr_layer *idr_layer_alloc(gfp_t gfp_mask, struct idr *layer_idr)
{
ffffffff812c1eae:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1eb1:	53                   	push   %rbx
ffffffff812c1eb2:	51                   	push   %rcx
	struct idr_layer *new;

	/* this is the old path, bypass to get_from_free_list() */
	if (layer_idr)
ffffffff812c1eb3:	74 0a                	je     ffffffff812c1ebf <idr_layer_alloc+0x15>
		return get_from_free_list(layer_idr);
ffffffff812c1eb5:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c1eb8:	e8 ba f9 ff ff       	callq  ffffffff812c1877 <get_from_free_list>
ffffffff812c1ebd:	eb 69                	jmp    ffffffff812c1f28 <idr_layer_alloc+0x7e>
ffffffff812c1ebf:	89 fe                	mov    %edi,%esi
ffffffff812c1ec1:	89 fb                	mov    %edi,%ebx
ffffffff812c1ec3:	48 8b 3d be f8 8e 00 	mov    0x8ef8be(%rip),%rdi        # ffffffff81bb1788 <__key.11630>
ffffffff812c1eca:	81 ce 00 82 00 00    	or     $0x8200,%esi
ffffffff812c1ed0:	e8 fc f1 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
	 * users will end up taking advantage of preloading ones.  As the
	 * following is allowed to fail for preloaded cases, suppress
	 * warning this time.
	 */
	new = kmem_cache_zalloc(idr_layer_cache, gfp_mask | __GFP_NOWARN);
	if (new)
ffffffff812c1ed5:	48 85 c0             	test   %rax,%rax
ffffffff812c1ed8:	75 4e                	jne    ffffffff812c1f28 <idr_layer_alloc+0x7e>
ffffffff812c1eda:	65 8b 05 b7 8a d4 7e 	mov    %gs:0x7ed48ab7(%rip),%eax        # a998 <__preempt_count>

	/*
	 * Try to fetch one from the per-cpu preload buffer if in process
	 * context.  See idr_preload() for details.
	 */
	if (!in_interrupt()) {
ffffffff812c1ee1:	a9 00 ff 1f 00       	test   $0x1fff00,%eax
ffffffff812c1ee6:	74 13                	je     ffffffff812c1efb <idr_layer_alloc+0x51>
ffffffff812c1ee8:	48 8b 3d 99 f8 8e 00 	mov    0x8ef899(%rip),%rdi        # ffffffff81bb1788 <__key.11630>
ffffffff812c1eef:	80 cf 80             	or     $0x80,%bh
ffffffff812c1ef2:	89 de                	mov    %ebx,%esi
ffffffff812c1ef4:	e8 d8 f1 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
ffffffff812c1ef9:	eb 2d                	jmp    ffffffff812c1f28 <idr_layer_alloc+0x7e>
		preempt_disable();
		new = __this_cpu_read(idr_preload_head);
ffffffff812c1efb:	65 48 8b 05 4d ea d4 	mov    %gs:0x7ed4ea4d(%rip),%rax        # 10950 <idr_preload_head>
ffffffff812c1f02:	7e 
		if (new) {
ffffffff812c1f03:	48 85 c0             	test   %rax,%rax
ffffffff812c1f06:	74 1b                	je     ffffffff812c1f23 <idr_layer_alloc+0x79>
			__this_cpu_write(idr_preload_head, new->ary[0]);
ffffffff812c1f08:	48 8b 50 08          	mov    0x8(%rax),%rdx
			__this_cpu_dec(idr_preload_cnt);
			new->ary[0] = NULL;
ffffffff812c1f0c:	48 c7 40 08 00 00 00 	movq   $0x0,0x8(%rax)
ffffffff812c1f13:	00 
	 */
	if (!in_interrupt()) {
		preempt_disable();
		new = __this_cpu_read(idr_preload_head);
		if (new) {
			__this_cpu_write(idr_preload_head, new->ary[0]);
ffffffff812c1f14:	65 48 89 15 34 ea d4 	mov    %rdx,%gs:0x7ed4ea34(%rip)        # 10950 <idr_preload_head>
ffffffff812c1f1b:	7e 
			__this_cpu_dec(idr_preload_cnt);
ffffffff812c1f1c:	65 ff 0d 25 ea d4 7e 	decl   %gs:0x7ed4ea25(%rip)        # 10948 <idr_preload_cnt>
			new->ary[0] = NULL;
		}
		preempt_enable();
		if (new)
ffffffff812c1f23:	48 85 c0             	test   %rax,%rax
ffffffff812c1f26:	74 c0                	je     ffffffff812c1ee8 <idr_layer_alloc+0x3e>
	/*
	 * Both failed.  Try kmem_cache again w/o adding __GFP_NOWARN so
	 * that memory allocation failure warning is printed as intended.
	 */
	return kmem_cache_zalloc(idr_layer_cache, gfp_mask);
}
ffffffff812c1f28:	5a                   	pop    %rdx
ffffffff812c1f29:	5b                   	pop    %rbx
ffffffff812c1f2a:	5d                   	pop    %rbp
ffffffff812c1f2b:	c3                   	retq   

ffffffff812c1f2c <idr_get_empty_slot>:
}

static int idr_get_empty_slot(struct idr *idp, int starting_id,
			      struct idr_layer **pa, gfp_t gfp_mask,
			      struct idr *layer_idr)
{
ffffffff812c1f2c:	55                   	push   %rbp
ffffffff812c1f2d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c1f30:	41 57                	push   %r15
ffffffff812c1f32:	41 56                	push   %r14
ffffffff812c1f34:	41 55                	push   %r13
ffffffff812c1f36:	41 54                	push   %r12
	struct idr_layer *p, *new;
	int layers, v, id;
	unsigned long flags;

	id = starting_id;
ffffffff812c1f38:	41 89 f5             	mov    %esi,%r13d
}

static int idr_get_empty_slot(struct idr *idp, int starting_id,
			      struct idr_layer **pa, gfp_t gfp_mask,
			      struct idr *layer_idr)
{
ffffffff812c1f3b:	53                   	push   %rbx
ffffffff812c1f3c:	49 89 fc             	mov    %rdi,%r12
ffffffff812c1f3f:	48 83 ec 28          	sub    $0x28,%rsp
ffffffff812c1f43:	48 89 55 c0          	mov    %rdx,-0x40(%rbp)
ffffffff812c1f47:	89 4d c8             	mov    %ecx,-0x38(%rbp)
ffffffff812c1f4a:	4c 89 45 b8          	mov    %r8,-0x48(%rbp)
	int layers, v, id;
	unsigned long flags;

	id = starting_id;
build_up:
	p = idp->top;
ffffffff812c1f4e:	4d 8b 74 24 08       	mov    0x8(%r12),%r14
	layers = idp->layers;
ffffffff812c1f53:	45 8b 7c 24 10       	mov    0x10(%r12),%r15d
	if (unlikely(!p)) {
ffffffff812c1f58:	4d 85 f6             	test   %r14,%r14
ffffffff812c1f5b:	75 46                	jne    ffffffff812c1fa3 <idr_get_empty_slot+0x77>
		if (!(p = idr_layer_alloc(gfp_mask, layer_idr)))
ffffffff812c1f5d:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812c1f61:	8b 7d c8             	mov    -0x38(%rbp),%edi
ffffffff812c1f64:	e8 41 ff ff ff       	callq  ffffffff812c1eaa <idr_layer_alloc>
ffffffff812c1f69:	48 85 c0             	test   %rax,%rax
ffffffff812c1f6c:	49 89 c6             	mov    %rax,%r14
ffffffff812c1f6f:	0f 84 6f 02 00 00    	je     ffffffff812c21e4 <idr_get_empty_slot+0x2b8>
			return -ENOMEM;
		p->layer = 0;
ffffffff812c1f75:	c7 40 04 00 00 00 00 	movl   $0x0,0x4(%rax)
		layers = 1;
ffffffff812c1f7c:	41 bf 01 00 00 00    	mov    $0x1,%r15d
ffffffff812c1f82:	eb 1f                	jmp    ffffffff812c1fa3 <idr_get_empty_slot+0x77>
	 * Add a new layer to the top of the tree if the requested
	 * id is larger than the currently allocated space.
	 */
	while (id > idr_max(layers)) {
		layers++;
		if (!p->count) {
ffffffff812c1f84:	41 83 be 08 08 00 00 	cmpl   $0x0,0x808(%r14)
ffffffff812c1f8b:	00 
	/*
	 * Add a new layer to the top of the tree if the requested
	 * id is larger than the currently allocated space.
	 */
	while (id > idr_max(layers)) {
		layers++;
ffffffff812c1f8c:	41 8d 47 01          	lea    0x1(%r15),%eax
ffffffff812c1f90:	89 45 cc             	mov    %eax,-0x34(%rbp)
		if (!p->count) {
ffffffff812c1f93:	75 43                	jne    ffffffff812c1fd8 <idr_get_empty_slot+0xac>
			/* special case: if the tree is currently empty,
			 * then we grow the tree by moving the top node
			 * upwards.
			 */
			p->layer++;
ffffffff812c1f95:	41 ff 46 04          	incl   0x4(%r14)
			WARN_ON_ONCE(p->prefix);
ffffffff812c1f99:	41 83 3e 00          	cmpl   $0x0,(%r14)
ffffffff812c1f9d:	75 16                	jne    ffffffff812c1fb5 <idr_get_empty_slot+0x89>
ffffffff812c1f9f:	44 8b 7d cc          	mov    -0x34(%rbp),%r15d
	}
	/*
	 * Add a new layer to the top of the tree if the requested
	 * id is larger than the currently allocated space.
	 */
	while (id > idr_max(layers)) {
ffffffff812c1fa3:	44 89 ff             	mov    %r15d,%edi
ffffffff812c1fa6:	e8 ab f8 ff ff       	callq  ffffffff812c1856 <idr_max>
ffffffff812c1fab:	44 39 e8             	cmp    %r13d,%eax
ffffffff812c1fae:	7c d4                	jl     ffffffff812c1f84 <idr_get_empty_slot+0x58>
ffffffff812c1fb0:	e9 f3 00 00 00       	jmpq   ffffffff812c20a8 <idr_get_empty_slot+0x17c>
			/* special case: if the tree is currently empty,
			 * then we grow the tree by moving the top node
			 * upwards.
			 */
			p->layer++;
			WARN_ON_ONCE(p->prefix);
ffffffff812c1fb5:	80 3d 29 53 79 00 00 	cmpb   $0x0,0x795329(%rip)        # ffffffff81a572e5 <__warned.16177>
ffffffff812c1fbc:	75 e1                	jne    ffffffff812c1f9f <idr_get_empty_slot+0x73>
ffffffff812c1fbe:	be 3f 01 00 00       	mov    $0x13f,%esi
ffffffff812c1fc3:	48 c7 c7 e9 64 7b 81 	mov    $0xffffffff817b64e9,%rdi
ffffffff812c1fca:	e8 e6 43 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c1fcf:	c6 05 0f 53 79 00 01 	movb   $0x1,0x79530f(%rip)        # ffffffff81a572e5 <__warned.16177>
ffffffff812c1fd6:	eb c7                	jmp    ffffffff812c1f9f <idr_get_empty_slot+0x73>
			continue;
		}
		if (!(new = idr_layer_alloc(gfp_mask, layer_idr))) {
ffffffff812c1fd8:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812c1fdc:	8b 7d c8             	mov    -0x38(%rbp),%edi
ffffffff812c1fdf:	e8 c6 fe ff ff       	callq  ffffffff812c1eaa <idr_layer_alloc>
ffffffff812c1fe4:	48 85 c0             	test   %rax,%rax
ffffffff812c1fe7:	48 89 c3             	mov    %rax,%rbx
ffffffff812c1fea:	75 6e                	jne    ffffffff812c205a <idr_get_empty_slot+0x12e>
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &lock->rlock;
ffffffff812c1fec:	49 8d 5c 24 18       	lea    0x18(%r12),%rbx
			/*
			 * The allocation failed.  If we built part of
			 * the structure tear it down.
			 */
			spin_lock_irqsave(&idp->lock, flags);
ffffffff812c1ff1:	48 89 df             	mov    %rbx,%rdi
ffffffff812c1ff4:	e8 2f 68 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
ffffffff812c1ff9:	49 89 c7             	mov    %rax,%r15
			for (new = p; p && p != idp->top; new = p) {
ffffffff812c1ffc:	4d 39 74 24 08       	cmp    %r14,0x8(%r12)
ffffffff812c2001:	74 42                	je     ffffffff812c2045 <idr_get_empty_slot+0x119>
				p = p->ary[0];
				new->ary[0] = NULL;
				new->count = 0;
				bitmap_clear(new->bitmap, 0, IDR_SIZE);
ffffffff812c2003:	49 8d be 10 08 00 00 	lea    0x810(%r14),%rdi
			 * The allocation failed.  If we built part of
			 * the structure tear it down.
			 */
			spin_lock_irqsave(&idp->lock, flags);
			for (new = p; p && p != idp->top; new = p) {
				p = p->ary[0];
ffffffff812c200a:	4d 8b 6e 08          	mov    0x8(%r14),%r13
				new->ary[0] = NULL;
				new->count = 0;
				bitmap_clear(new->bitmap, 0, IDR_SIZE);
ffffffff812c200e:	31 f6                	xor    %esi,%esi
			 * the structure tear it down.
			 */
			spin_lock_irqsave(&idp->lock, flags);
			for (new = p; p && p != idp->top; new = p) {
				p = p->ary[0];
				new->ary[0] = NULL;
ffffffff812c2010:	49 c7 46 08 00 00 00 	movq   $0x0,0x8(%r14)
ffffffff812c2017:	00 
				new->count = 0;
ffffffff812c2018:	41 c7 86 08 08 00 00 	movl   $0x0,0x808(%r14)
ffffffff812c201f:	00 00 00 00 
				bitmap_clear(new->bitmap, 0, IDR_SIZE);
ffffffff812c2023:	ba 00 01 00 00       	mov    $0x100,%edx
ffffffff812c2028:	e8 67 b0 00 00       	callq  ffffffff812cd094 <bitmap_clear>
}

/* only called when idp->lock is held */
static void __move_to_free_list(struct idr *idp, struct idr_layer *p)
{
	p->ary[0] = idp->id_free;
ffffffff812c202d:	49 8b 44 24 38       	mov    0x38(%r12),%rax
ffffffff812c2032:	49 89 46 08          	mov    %rax,0x8(%r14)
	idp->id_free = p;
	idp->id_free_cnt++;
ffffffff812c2036:	41 ff 44 24 30       	incl   0x30(%r12)
			/*
			 * The allocation failed.  If we built part of
			 * the structure tear it down.
			 */
			spin_lock_irqsave(&idp->lock, flags);
			for (new = p; p && p != idp->top; new = p) {
ffffffff812c203b:	4d 85 ed             	test   %r13,%r13

/* only called when idp->lock is held */
static void __move_to_free_list(struct idr *idp, struct idr_layer *p)
{
	p->ary[0] = idp->id_free;
	idp->id_free = p;
ffffffff812c203e:	4d 89 74 24 38       	mov    %r14,0x38(%r12)
			/*
			 * The allocation failed.  If we built part of
			 * the structure tear it down.
			 */
			spin_lock_irqsave(&idp->lock, flags);
			for (new = p; p && p != idp->top; new = p) {
ffffffff812c2043:	75 10                	jne    ffffffff812c2055 <idr_get_empty_slot+0x129>
	raw_spin_unlock_irq(&lock->rlock);
}

static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	raw_spin_unlock_irqrestore(&lock->rlock, flags);
ffffffff812c2045:	4c 89 fe             	mov    %r15,%rsi
ffffffff812c2048:	48 89 df             	mov    %rbx,%rdi
ffffffff812c204b:	e8 8c 66 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
ffffffff812c2050:	e9 8f 01 00 00       	jmpq   ffffffff812c21e4 <idr_get_empty_slot+0x2b8>
ffffffff812c2055:	4d 89 ee             	mov    %r13,%r14
ffffffff812c2058:	eb a2                	jmp    ffffffff812c1ffc <idr_get_empty_slot+0xd0>
 * all bits except for the lower IDR_BITS.  For layer 1, 2 * IDR_BITS, and
 * so on.
 */
static int idr_layer_prefix_mask(int layer)
{
	return ~idr_max(layer + 1);
ffffffff812c205a:	8b 7d cc             	mov    -0x34(%rbp),%edi
				__move_to_free_list(idp, new);
			}
			spin_unlock_irqrestore(&idp->lock, flags);
			return -ENOMEM;
		}
		new->ary[0] = p;
ffffffff812c205d:	4c 89 70 08          	mov    %r14,0x8(%rax)
		new->count = 1;
ffffffff812c2061:	c7 80 08 08 00 00 01 	movl   $0x1,0x808(%rax)
ffffffff812c2068:	00 00 00 
		new->layer = layers-1;
ffffffff812c206b:	44 89 78 04          	mov    %r15d,0x4(%rax)
 * all bits except for the lower IDR_BITS.  For layer 1, 2 * IDR_BITS, and
 * so on.
 */
static int idr_layer_prefix_mask(int layer)
{
	return ~idr_max(layer + 1);
ffffffff812c206f:	e8 e2 f7 ff ff       	callq  ffffffff812c1856 <idr_max>
			return -ENOMEM;
		}
		new->ary[0] = p;
		new->count = 1;
		new->layer = layers-1;
		new->prefix = id & idr_layer_prefix_mask(new->layer);
ffffffff812c2074:	f7 d0                	not    %eax
		if (bitmap_full(p->bitmap, IDR_SIZE))
ffffffff812c2076:	49 8d be 10 08 00 00 	lea    0x810(%r14),%rdi
ffffffff812c207d:	be 00 01 00 00       	mov    $0x100,%esi
			return -ENOMEM;
		}
		new->ary[0] = p;
		new->count = 1;
		new->layer = layers-1;
		new->prefix = id & idr_layer_prefix_mask(new->layer);
ffffffff812c2082:	44 21 e8             	and    %r13d,%eax
ffffffff812c2085:	49 89 de             	mov    %rbx,%r14
ffffffff812c2088:	89 03                	mov    %eax,(%rbx)
ffffffff812c208a:	e8 f3 eb 00 00       	callq  ffffffff812d0c82 <find_first_zero_bit>
		if (bitmap_full(p->bitmap, IDR_SIZE))
ffffffff812c208f:	48 3d 00 01 00 00    	cmp    $0x100,%rax
ffffffff812c2095:	0f 85 04 ff ff ff    	jne    ffffffff812c1f9f <idr_get_empty_slot+0x73>
ffffffff812c209b:	0f ba ab 10 08 00 00 	btsl   $0x0,0x810(%rbx)
ffffffff812c20a2:	00 
ffffffff812c20a3:	e9 f7 fe ff ff       	jmpq   ffffffff812c1f9f <idr_get_empty_slot+0x73>
			__set_bit(0, new->bitmap);
		p = new;
	}
	rcu_assign_pointer(idp->top, p);
ffffffff812c20a8:	4d 89 74 24 08       	mov    %r14,0x8(%r12)
	idp->layers = layers;
ffffffff812c20ad:	45 89 7c 24 10       	mov    %r15d,0x10(%r12)
	int l, id, oid;

	id = *starting_id;
 restart:
	p = idp->top;
	l = idp->layers;
ffffffff812c20b2:	49 63 44 24 10       	movslq 0x10(%r12),%rax
	pa[l--] = NULL;
ffffffff812c20b7:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
	struct idr_layer *p, *new;
	int l, id, oid;

	id = *starting_id;
 restart:
	p = idp->top;
ffffffff812c20bb:	4d 8b 74 24 08       	mov    0x8(%r12),%r14
	l = idp->layers;
	pa[l--] = NULL;
ffffffff812c20c0:	44 8d 78 ff          	lea    -0x1(%rax),%r15d
ffffffff812c20c4:	48 c7 04 c2 00 00 00 	movq   $0x0,(%rdx,%rax,8)
ffffffff812c20cb:	00 
	while (1) {
		/*
		 * We run around this while until we reach the leaf node...
		 */
		n = (id >> (IDR_BITS*l)) & IDR_MASK;
ffffffff812c20cc:	42 8d 04 fd 00 00 00 	lea    0x0(,%r15,8),%eax
ffffffff812c20d3:	00 
		m = find_next_zero_bit(p->bitmap, IDR_SIZE, n);
ffffffff812c20d4:	49 8d be 10 08 00 00 	lea    0x810(%r14),%rdi
ffffffff812c20db:	be 00 01 00 00       	mov    $0x100,%esi
			new->layer = l-1;
			new->prefix = id & idr_layer_prefix_mask(new->layer);
			rcu_assign_pointer(p->ary[m], new);
			p->count++;
		}
		pa[l--] = p;
ffffffff812c20e0:	44 89 eb             	mov    %r13d,%ebx
	pa[l--] = NULL;
	while (1) {
		/*
		 * We run around this while until we reach the leaf node...
		 */
		n = (id >> (IDR_BITS*l)) & IDR_MASK;
ffffffff812c20e3:	89 45 cc             	mov    %eax,-0x34(%rbp)
ffffffff812c20e6:	8a 4d cc             	mov    -0x34(%rbp),%cl
ffffffff812c20e9:	44 89 e8             	mov    %r13d,%eax
ffffffff812c20ec:	d3 f8                	sar    %cl,%eax
ffffffff812c20ee:	89 45 b4             	mov    %eax,-0x4c(%rbp)
ffffffff812c20f1:	0f b6 55 b4          	movzbl -0x4c(%rbp),%edx
		m = find_next_zero_bit(p->bitmap, IDR_SIZE, n);
ffffffff812c20f5:	89 55 b0             	mov    %edx,-0x50(%rbp)
ffffffff812c20f8:	e8 78 ec 00 00       	callq  ffffffff812d0d75 <find_next_zero_bit>
		if (m == IDR_SIZE) {
ffffffff812c20fd:	3d 00 01 00 00       	cmp    $0x100,%eax
ffffffff812c2102:	8b 4d b0             	mov    -0x50(%rbp),%ecx
ffffffff812c2105:	75 57                	jne    ffffffff812c215e <idr_get_empty_slot+0x232>
			/* no space available go back to previous layer. */
			l++;
			oid = id;
			id = (id | ((1 << (IDR_BITS * l)) - 1)) + 1;
ffffffff812c2107:	8b 45 cc             	mov    -0x34(%rbp),%eax
ffffffff812c210a:	be 01 00 00 00       	mov    $0x1,%esi

			/* if already at the top layer, we need to grow */
			if (id > idr_max(idp->layers)) {
ffffffff812c210f:	41 8b 7c 24 10       	mov    0x10(%r12),%edi
		 */
		n = (id >> (IDR_BITS*l)) & IDR_MASK;
		m = find_next_zero_bit(p->bitmap, IDR_SIZE, n);
		if (m == IDR_SIZE) {
			/* no space available go back to previous layer. */
			l++;
ffffffff812c2114:	41 ff c7             	inc    %r15d
			oid = id;
			id = (id | ((1 << (IDR_BITS * l)) - 1)) + 1;
ffffffff812c2117:	8d 48 08             	lea    0x8(%rax),%ecx
ffffffff812c211a:	d3 e6                	shl    %cl,%esi
ffffffff812c211c:	ff ce                	dec    %esi
ffffffff812c211e:	41 09 f5             	or     %esi,%r13d
ffffffff812c2121:	41 ff c5             	inc    %r13d

			/* if already at the top layer, we need to grow */
			if (id > idr_max(idp->layers)) {
ffffffff812c2124:	e8 2d f7 ff ff       	callq  ffffffff812c1856 <idr_max>
ffffffff812c2129:	41 39 c5             	cmp    %eax,%r13d
ffffffff812c212c:	0f 8f 1c fe ff ff    	jg     ffffffff812c1f4e <idr_get_empty_slot+0x22>
				*starting_id = id;
				return -EAGAIN;
			}
			p = pa[l];
ffffffff812c2132:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
ffffffff812c2136:	49 63 c7             	movslq %r15d,%rax
ffffffff812c2139:	4c 8b 34 c2          	mov    (%rdx,%rax,8),%r14
			BUG_ON(!p);
ffffffff812c213d:	4d 85 f6             	test   %r14,%r14
ffffffff812c2140:	75 02                	jne    ffffffff812c2144 <idr_get_empty_slot+0x218>
ffffffff812c2142:	0f 0b                	ud2    

			/* If we need to go up one layer, continue the
			 * loop; otherwise, restart from the top.
			 */
			sh = IDR_BITS * (l + 1);
ffffffff812c2144:	8b 4d cc             	mov    -0x34(%rbp),%ecx
			if (oid >> sh == id >> sh)
ffffffff812c2147:	44 89 ea             	mov    %r13d,%edx
			BUG_ON(!p);

			/* If we need to go up one layer, continue the
			 * loop; otherwise, restart from the top.
			 */
			sh = IDR_BITS * (l + 1);
ffffffff812c214a:	83 c1 10             	add    $0x10,%ecx
			if (oid >> sh == id >> sh)
ffffffff812c214d:	d3 fb                	sar    %cl,%ebx
ffffffff812c214f:	d3 fa                	sar    %cl,%edx
ffffffff812c2151:	39 d3                	cmp    %edx,%ebx
ffffffff812c2153:	0f 85 59 ff ff ff    	jne    ffffffff812c20b2 <idr_get_empty_slot+0x186>
ffffffff812c2159:	e9 6e ff ff ff       	jmpq   ffffffff812c20cc <idr_get_empty_slot+0x1a0>
				continue;
			else
				goto restart;
		}
		if (m != n) {
ffffffff812c215e:	39 c1                	cmp    %eax,%ecx
ffffffff812c2160:	74 0c                	je     ffffffff812c216e <idr_get_empty_slot+0x242>
			sh = IDR_BITS*l;
			id = ((id >> sh) ^ n ^ m) << sh;
ffffffff812c2162:	8b 5d b4             	mov    -0x4c(%rbp),%ebx
ffffffff812c2165:	31 c3                	xor    %eax,%ebx
ffffffff812c2167:	31 cb                	xor    %ecx,%ebx
ffffffff812c2169:	8a 4d cc             	mov    -0x34(%rbp),%cl
ffffffff812c216c:	d3 e3                	shl    %cl,%ebx
		}
		if ((id >= MAX_IDR_BIT) || (id < 0))
ffffffff812c216e:	85 db                	test   %ebx,%ebx
ffffffff812c2170:	78 6b                	js     ffffffff812c21dd <idr_get_empty_slot+0x2b1>
			return -ENOSPC;
		if (l == 0)
ffffffff812c2172:	45 85 ff             	test   %r15d,%r15d
ffffffff812c2175:	74 5b                	je     ffffffff812c21d2 <idr_get_empty_slot+0x2a6>
ffffffff812c2177:	48 98                	cltq   
ffffffff812c2179:	4d 8d 2c c6          	lea    (%r14,%rax,8),%r13
			break;
		/*
		 * Create the layer below if it is missing.
		 */
		if (!p->ary[m]) {
ffffffff812c217d:	49 83 7d 08 00       	cmpq   $0x0,0x8(%r13)
ffffffff812c2182:	75 34                	jne    ffffffff812c21b8 <idr_get_empty_slot+0x28c>
			new = idr_layer_alloc(gfp_mask, layer_idr);
ffffffff812c2184:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812c2188:	8b 7d c8             	mov    -0x38(%rbp),%edi
ffffffff812c218b:	e8 1a fd ff ff       	callq  ffffffff812c1eaa <idr_layer_alloc>
			if (!new)
ffffffff812c2190:	48 85 c0             	test   %rax,%rax
			break;
		/*
		 * Create the layer below if it is missing.
		 */
		if (!p->ary[m]) {
			new = idr_layer_alloc(gfp_mask, layer_idr);
ffffffff812c2193:	48 89 c2             	mov    %rax,%rdx
			if (!new)
ffffffff812c2196:	74 4c                	je     ffffffff812c21e4 <idr_get_empty_slot+0x2b8>
				return -ENOMEM;
			new->layer = l-1;
ffffffff812c2198:	41 8d 47 ff          	lea    -0x1(%r15),%eax
 * all bits except for the lower IDR_BITS.  For layer 1, 2 * IDR_BITS, and
 * so on.
 */
static int idr_layer_prefix_mask(int layer)
{
	return ~idr_max(layer + 1);
ffffffff812c219c:	44 89 ff             	mov    %r15d,%edi
		 */
		if (!p->ary[m]) {
			new = idr_layer_alloc(gfp_mask, layer_idr);
			if (!new)
				return -ENOMEM;
			new->layer = l-1;
ffffffff812c219f:	89 42 04             	mov    %eax,0x4(%rdx)
 * all bits except for the lower IDR_BITS.  For layer 1, 2 * IDR_BITS, and
 * so on.
 */
static int idr_layer_prefix_mask(int layer)
{
	return ~idr_max(layer + 1);
ffffffff812c21a2:	e8 af f6 ff ff       	callq  ffffffff812c1856 <idr_max>
		if (!p->ary[m]) {
			new = idr_layer_alloc(gfp_mask, layer_idr);
			if (!new)
				return -ENOMEM;
			new->layer = l-1;
			new->prefix = id & idr_layer_prefix_mask(new->layer);
ffffffff812c21a7:	f7 d0                	not    %eax
ffffffff812c21a9:	21 d8                	and    %ebx,%eax
ffffffff812c21ab:	89 02                	mov    %eax,(%rdx)
			rcu_assign_pointer(p->ary[m], new);
ffffffff812c21ad:	49 89 55 08          	mov    %rdx,0x8(%r13)
			p->count++;
ffffffff812c21b1:	41 ff 86 08 08 00 00 	incl   0x808(%r14)
		}
		pa[l--] = p;
ffffffff812c21b8:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
ffffffff812c21bc:	49 63 c7             	movslq %r15d,%rax
ffffffff812c21bf:	41 ff cf             	dec    %r15d
ffffffff812c21c2:	4c 89 34 c6          	mov    %r14,(%rsi,%rax,8)
		p = p->ary[m];
ffffffff812c21c6:	4d 8b 75 08          	mov    0x8(%r13),%r14
ffffffff812c21ca:	41 89 dd             	mov    %ebx,%r13d
ffffffff812c21cd:	e9 fa fe ff ff       	jmpq   ffffffff812c20cc <idr_get_empty_slot+0x1a0>
	}

	pa[l] = p;
ffffffff812c21d2:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
ffffffff812c21d6:	4c 89 30             	mov    %r14,(%rax)
ffffffff812c21d9:	89 d8                	mov    %ebx,%eax
ffffffff812c21db:	eb 0c                	jmp    ffffffff812c21e9 <idr_get_empty_slot+0x2bd>
		if (m != n) {
			sh = IDR_BITS*l;
			id = ((id >> sh) ^ n ^ m) << sh;
		}
		if ((id >= MAX_IDR_BIT) || (id < 0))
			return -ENOSPC;
ffffffff812c21dd:	b8 e4 ff ff ff       	mov    $0xffffffe4,%eax
ffffffff812c21e2:	eb 05                	jmp    ffffffff812c21e9 <idr_get_empty_slot+0x2bd>
		 * Create the layer below if it is missing.
		 */
		if (!p->ary[m]) {
			new = idr_layer_alloc(gfp_mask, layer_idr);
			if (!new)
				return -ENOMEM;
ffffffff812c21e4:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
	idp->layers = layers;
	v = sub_alloc(idp, &id, pa, gfp_mask, layer_idr);
	if (v == -EAGAIN)
		goto build_up;
	return(v);
}
ffffffff812c21e9:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c21ed:	5b                   	pop    %rbx
ffffffff812c21ee:	41 5c                	pop    %r12
ffffffff812c21f0:	41 5d                	pop    %r13
ffffffff812c21f2:	41 5e                	pop    %r14
ffffffff812c21f4:	41 5f                	pop    %r15
ffffffff812c21f6:	5d                   	pop    %rbp
ffffffff812c21f7:	c3                   	retq   

ffffffff812c21f8 <idr_alloc>:
 * which may modify @idr.  However, read-only accesses such as idr_find()
 * or iteration can be performed under RCU read lock provided the user
 * destroys @ptr in RCU-safe way after removal from idr.
 */
int idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)
{
ffffffff812c21f8:	55                   	push   %rbp
ffffffff812c21f9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c21fc:	41 57                	push   %r15
ffffffff812c21fe:	41 56                	push   %r14
ffffffff812c2200:	41 55                	push   %r13
ffffffff812c2202:	41 54                	push   %r12
ffffffff812c2204:	49 89 f7             	mov    %rsi,%r15
ffffffff812c2207:	53                   	push   %rbx
ffffffff812c2208:	89 cb                	mov    %ecx,%ebx
ffffffff812c220a:	49 89 fe             	mov    %rdi,%r14
ffffffff812c220d:	89 d6                	mov    %edx,%esi
ffffffff812c220f:	44 89 c1             	mov    %r8d,%ecx
ffffffff812c2212:	48 83 ec 38          	sub    $0x38,%rsp
	int max = end > 0 ? end - 1 : INT_MAX;	/* inclusive upper limit */
ffffffff812c2216:	85 db                	test   %ebx,%ebx
ffffffff812c2218:	7e 3b                	jle    ffffffff812c2255 <idr_alloc+0x5d>
ffffffff812c221a:	ff cb                	dec    %ebx
	int id;

	might_sleep_if(gfp_mask & __GFP_WAIT);

	/* sanity checks */
	if (WARN_ON_ONCE(start < 0))
ffffffff812c221c:	85 d2                	test   %edx,%edx
ffffffff812c221e:	79 29                	jns    ffffffff812c2249 <idr_alloc+0x51>
ffffffff812c2220:	80 3d bc 50 79 00 00 	cmpb   $0x0,0x7950bc(%rip)        # ffffffff81a572e3 <__warned.16410>
		return -EINVAL;
ffffffff812c2227:	41 bd ea ff ff ff    	mov    $0xffffffea,%r13d
	int id;

	might_sleep_if(gfp_mask & __GFP_WAIT);

	/* sanity checks */
	if (WARN_ON_ONCE(start < 0))
ffffffff812c222d:	75 72                	jne    ffffffff812c22a1 <idr_alloc+0xa9>
ffffffff812c222f:	be cb 01 00 00       	mov    $0x1cb,%esi
ffffffff812c2234:	48 c7 c7 e9 64 7b 81 	mov    $0xffffffff817b64e9,%rdi
ffffffff812c223b:	e8 75 41 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c2240:	c6 05 9c 50 79 00 01 	movb   $0x1,0x79509c(%rip)        # ffffffff81a572e3 <__warned.16410>
ffffffff812c2247:	eb 58                	jmp    ffffffff812c22a1 <idr_alloc+0xa9>
		return -EINVAL;
	if (unlikely(max < start))
ffffffff812c2249:	39 d3                	cmp    %edx,%ebx
ffffffff812c224b:	7d 11                	jge    ffffffff812c225e <idr_alloc+0x66>
		return -ENOSPC;
ffffffff812c224d:	41 bd e4 ff ff ff    	mov    $0xffffffe4,%r13d
ffffffff812c2253:	eb 4c                	jmp    ffffffff812c22a1 <idr_alloc+0xa9>
	int id;

	might_sleep_if(gfp_mask & __GFP_WAIT);

	/* sanity checks */
	if (WARN_ON_ONCE(start < 0))
ffffffff812c2255:	85 d2                	test   %edx,%edx
 * or iteration can be performed under RCU read lock provided the user
 * destroys @ptr in RCU-safe way after removal from idr.
 */
int idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)
{
	int max = end > 0 ? end - 1 : INT_MAX;	/* inclusive upper limit */
ffffffff812c2257:	bb ff ff ff 7f       	mov    $0x7fffffff,%ebx
	int id;

	might_sleep_if(gfp_mask & __GFP_WAIT);

	/* sanity checks */
	if (WARN_ON_ONCE(start < 0))
ffffffff812c225c:	78 c2                	js     ffffffff812c2220 <idr_alloc+0x28>
		return -EINVAL;
	if (unlikely(max < start))
		return -ENOSPC;

	/* allocate id */
	id = idr_get_empty_slot(idr, start, pa, gfp_mask, NULL);
ffffffff812c225e:	48 8d 55 a8          	lea    -0x58(%rbp),%rdx
ffffffff812c2262:	45 31 c0             	xor    %r8d,%r8d
ffffffff812c2265:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c2268:	e8 bf fc ff ff       	callq  ffffffff812c1f2c <idr_get_empty_slot>
	if (unlikely(id < 0))
ffffffff812c226d:	85 c0                	test   %eax,%eax
		return -EINVAL;
	if (unlikely(max < start))
		return -ENOSPC;

	/* allocate id */
	id = idr_get_empty_slot(idr, start, pa, gfp_mask, NULL);
ffffffff812c226f:	41 89 c4             	mov    %eax,%r12d
ffffffff812c2272:	41 89 c5             	mov    %eax,%r13d
	if (unlikely(id < 0))
ffffffff812c2275:	78 2a                	js     ffffffff812c22a1 <idr_alloc+0xa9>
		return id;
	if (unlikely(id > max))
ffffffff812c2277:	39 c3                	cmp    %eax,%ebx
ffffffff812c2279:	7c d2                	jl     ffffffff812c224d <idr_alloc+0x55>
 */
static void idr_fill_slot(struct idr *idr, void *ptr, int id,
			  struct idr_layer **pa)
{
	/* update hint used for lookup, cleared from free_layer() */
	rcu_assign_pointer(idr->hint, pa[0]);
ffffffff812c227b:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
ffffffff812c227f:	49 89 06             	mov    %rax,(%r14)

	rcu_assign_pointer(pa[0]->ary[id & IDR_MASK], (struct idr_layer *)ptr);
ffffffff812c2282:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
ffffffff812c2286:	41 0f b6 d4          	movzbl %r12b,%edx
	pa[0]->count++;
	idr_mark_full(pa, id);
ffffffff812c228a:	48 8d 7d a8          	lea    -0x58(%rbp),%rdi
ffffffff812c228e:	44 89 e6             	mov    %r12d,%esi
			  struct idr_layer **pa)
{
	/* update hint used for lookup, cleared from free_layer() */
	rcu_assign_pointer(idr->hint, pa[0]);

	rcu_assign_pointer(pa[0]->ary[id & IDR_MASK], (struct idr_layer *)ptr);
ffffffff812c2291:	4c 89 7c d0 08       	mov    %r15,0x8(%rax,%rdx,8)
	pa[0]->count++;
ffffffff812c2296:	ff 80 08 08 00 00    	incl   0x808(%rax)
	idr_mark_full(pa, id);
ffffffff812c229c:	e8 2d fb ff ff       	callq  ffffffff812c1dce <idr_mark_full>
	if (unlikely(id > max))
		return -ENOSPC;

	idr_fill_slot(idr, ptr, id, pa);
	return id;
}
ffffffff812c22a1:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812c22a5:	44 89 e8             	mov    %r13d,%eax
ffffffff812c22a8:	5b                   	pop    %rbx
ffffffff812c22a9:	41 5c                	pop    %r12
ffffffff812c22ab:	41 5d                	pop    %r13
ffffffff812c22ad:	41 5e                	pop    %r14
ffffffff812c22af:	41 5f                	pop    %r15
ffffffff812c22b1:	5d                   	pop    %rbp
ffffffff812c22b2:	c3                   	retq   

ffffffff812c22b3 <idr_alloc_cyclic>:
 * higher ids if it can. If the "cur" counter wraps, then it will start again
 * at the "start" end of the range and allocate one that has already been used.
 */
int idr_alloc_cyclic(struct idr *idr, void *ptr, int start, int end,
			gfp_t gfp_mask)
{
ffffffff812c22b3:	55                   	push   %rbp
ffffffff812c22b4:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c22b7:	41 57                	push   %r15
ffffffff812c22b9:	41 56                	push   %r14
ffffffff812c22bb:	41 55                	push   %r13
ffffffff812c22bd:	41 54                	push   %r12
ffffffff812c22bf:	41 89 d4             	mov    %edx,%r12d
ffffffff812c22c2:	53                   	push   %rbx
ffffffff812c22c3:	41 51                	push   %r9
ffffffff812c22c5:	48 89 fb             	mov    %rdi,%rbx
	int id;

	id = idr_alloc(idr, ptr, max(start, idr->cur), end, gfp_mask);
ffffffff812c22c8:	39 57 14             	cmp    %edx,0x14(%rdi)
 * higher ids if it can. If the "cur" counter wraps, then it will start again
 * at the "start" end of the range and allocate one that has already been used.
 */
int idr_alloc_cyclic(struct idr *idr, void *ptr, int start, int end,
			gfp_t gfp_mask)
{
ffffffff812c22cb:	49 89 f5             	mov    %rsi,%r13
ffffffff812c22ce:	41 89 ce             	mov    %ecx,%r14d
	int id;

	id = idr_alloc(idr, ptr, max(start, idr->cur), end, gfp_mask);
ffffffff812c22d1:	0f 4d 57 14          	cmovge 0x14(%rdi),%edx
 * higher ids if it can. If the "cur" counter wraps, then it will start again
 * at the "start" end of the range and allocate one that has already been used.
 */
int idr_alloc_cyclic(struct idr *idr, void *ptr, int start, int end,
			gfp_t gfp_mask)
{
ffffffff812c22d5:	45 89 c7             	mov    %r8d,%r15d
	int id;

	id = idr_alloc(idr, ptr, max(start, idr->cur), end, gfp_mask);
ffffffff812c22d8:	e8 1b ff ff ff       	callq  ffffffff812c21f8 <idr_alloc>
	if (id == -ENOSPC)
ffffffff812c22dd:	83 f8 e4             	cmp    $0xffffffe4,%eax
ffffffff812c22e0:	75 14                	jne    ffffffff812c22f6 <idr_alloc_cyclic+0x43>
		id = idr_alloc(idr, ptr, start, end, gfp_mask);
ffffffff812c22e2:	45 89 f8             	mov    %r15d,%r8d
ffffffff812c22e5:	44 89 f1             	mov    %r14d,%ecx
ffffffff812c22e8:	44 89 e2             	mov    %r12d,%edx
ffffffff812c22eb:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c22ee:	48 89 df             	mov    %rbx,%rdi
ffffffff812c22f1:	e8 02 ff ff ff       	callq  ffffffff812c21f8 <idr_alloc>

	if (likely(id >= 0))
ffffffff812c22f6:	85 c0                	test   %eax,%eax
ffffffff812c22f8:	78 06                	js     ffffffff812c2300 <idr_alloc_cyclic+0x4d>
		idr->cur = id + 1;
ffffffff812c22fa:	8d 50 01             	lea    0x1(%rax),%edx
ffffffff812c22fd:	89 53 14             	mov    %edx,0x14(%rbx)
	return id;
}
ffffffff812c2300:	5a                   	pop    %rdx
ffffffff812c2301:	5b                   	pop    %rbx
ffffffff812c2302:	41 5c                	pop    %r12
ffffffff812c2304:	41 5d                	pop    %r13
ffffffff812c2306:	41 5e                	pop    %r14
ffffffff812c2308:	41 5f                	pop    %r15
ffffffff812c230a:	5d                   	pop    %rbp
ffffffff812c230b:	c3                   	retq   

ffffffff812c230c <ida_get_new_above>:
 * return %-ENOSPC.
 *
 * @p_id returns a value in the range @starting_id ... %0x7fffffff.
 */
int ida_get_new_above(struct ida *ida, int starting_id, int *p_id)
{
ffffffff812c230c:	55                   	push   %rbp
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct ida_bitmap *bitmap;
	unsigned long flags;
	int idr_id = starting_id / IDA_BITMAP_BITS;
ffffffff812c230d:	48 63 c6             	movslq %esi,%rax
ffffffff812c2310:	b9 c0 03 00 00       	mov    $0x3c0,%ecx
 * return %-ENOSPC.
 *
 * @p_id returns a value in the range @starting_id ... %0x7fffffff.
 */
int ida_get_new_above(struct ida *ida, int starting_id, int *p_id)
{
ffffffff812c2315:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2318:	41 57                	push   %r15
ffffffff812c231a:	41 56                	push   %r14
ffffffff812c231c:	41 55                	push   %r13
ffffffff812c231e:	41 54                	push   %r12
ffffffff812c2320:	53                   	push   %rbx
ffffffff812c2321:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c2324:	48 83 ec 58          	sub    $0x58,%rsp
ffffffff812c2328:	48 89 55 90          	mov    %rdx,-0x70(%rbp)
	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
	struct ida_bitmap *bitmap;
	unsigned long flags;
	int idr_id = starting_id / IDA_BITMAP_BITS;
ffffffff812c232c:	31 d2                	xor    %edx,%edx
ffffffff812c232e:	48 f7 f1             	div    %rcx
ffffffff812c2331:	49 89 c5             	mov    %rax,%r13
	int offset = starting_id % IDA_BITMAP_BITS;
ffffffff812c2334:	41 89 d4             	mov    %edx,%r12d
	int t, id;

 restart:
	/* get vacant slot */
	t = idr_get_empty_slot(&ida->idr, idr_id, pa, 0, &ida->idr);
ffffffff812c2337:	48 8d 55 a8          	lea    -0x58(%rbp),%rdx
ffffffff812c233b:	31 c9                	xor    %ecx,%ecx
ffffffff812c233d:	49 89 d8             	mov    %rbx,%r8
ffffffff812c2340:	44 89 ee             	mov    %r13d,%esi
ffffffff812c2343:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2346:	e8 e1 fb ff ff       	callq  ffffffff812c1f2c <idr_get_empty_slot>
	if (t < 0)
ffffffff812c234b:	85 c0                	test   %eax,%eax
	int offset = starting_id % IDA_BITMAP_BITS;
	int t, id;

 restart:
	/* get vacant slot */
	t = idr_get_empty_slot(&ida->idr, idr_id, pa, 0, &ida->idr);
ffffffff812c234d:	41 89 c7             	mov    %eax,%r15d
	if (t < 0)
ffffffff812c2350:	79 13                	jns    ffffffff812c2365 <ida_get_new_above+0x59>
		return t == -ENOMEM ? -EAGAIN : t;
ffffffff812c2352:	83 f8 f4             	cmp    $0xfffffff4,%eax
ffffffff812c2355:	0f 85 21 01 00 00    	jne    ffffffff812c247c <ida_get_new_above+0x170>
ffffffff812c235b:	b8 f5 ff ff ff       	mov    $0xfffffff5,%eax
ffffffff812c2360:	e9 17 01 00 00       	jmpq   ffffffff812c247c <ida_get_new_above+0x170>

	if (t * IDA_BITMAP_BITS >= MAX_IDR_BIT)
ffffffff812c2365:	48 98                	cltq   
ffffffff812c2367:	48 89 45 98          	mov    %rax,-0x68(%rbp)
ffffffff812c236b:	48 69 c0 c0 03 00 00 	imul   $0x3c0,%rax,%rax
ffffffff812c2372:	48 3d ff ff ff 7f    	cmp    $0x7fffffff,%rax
ffffffff812c2378:	76 0a                	jbe    ffffffff812c2384 <ida_get_new_above+0x78>
		return -ENOSPC;
ffffffff812c237a:	b8 e4 ff ff ff       	mov    $0xffffffe4,%eax
ffffffff812c237f:	e9 f8 00 00 00       	jmpq   ffffffff812c247c <ida_get_new_above+0x170>

	if (t != idr_id)
		offset = 0;
ffffffff812c2384:	45 39 fd             	cmp    %r15d,%r13d
ffffffff812c2387:	b8 00 00 00 00       	mov    $0x0,%eax
	idr_id = t;

	/* if bitmap isn't there, create a new one */
	bitmap = (void *)pa[0]->ary[idr_id & IDR_MASK];
ffffffff812c238c:	45 0f b6 f7          	movzbl %r15b,%r14d

	if (t * IDA_BITMAP_BITS >= MAX_IDR_BIT)
		return -ENOSPC;

	if (t != idr_id)
		offset = 0;
ffffffff812c2390:	44 0f 45 e0          	cmovne %eax,%r12d
	idr_id = t;

	/* if bitmap isn't there, create a new one */
	bitmap = (void *)pa[0]->ary[idr_id & IDR_MASK];
ffffffff812c2394:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
ffffffff812c2398:	4e 8b 6c f0 08       	mov    0x8(%rax,%r14,8),%r13
	if (!bitmap) {
ffffffff812c239d:	4d 85 ed             	test   %r13,%r13
ffffffff812c23a0:	75 4b                	jne    ffffffff812c23ed <ida_get_new_above+0xe1>
 * Map the spin_lock functions to the raw variants for PREEMPT_RT=n
 */

static inline raw_spinlock_t *spinlock_check(spinlock_t *lock)
{
	return &lock->rlock;
ffffffff812c23a2:	48 8d 4b 18          	lea    0x18(%rbx),%rcx
		spin_lock_irqsave(&ida->idr.lock, flags);
ffffffff812c23a6:	48 89 cf             	mov    %rcx,%rdi
ffffffff812c23a9:	48 89 4d 88          	mov    %rcx,-0x78(%rbp)
ffffffff812c23ad:	e8 76 64 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
	raw_spin_unlock_irq(&lock->rlock);
}

static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	raw_spin_unlock_irqrestore(&lock->rlock, flags);
ffffffff812c23b2:	48 8b 4d 88          	mov    -0x78(%rbp),%rcx
		bitmap = ida->free_bitmap;
ffffffff812c23b6:	4c 8b 6b 40          	mov    0x40(%rbx),%r13
ffffffff812c23ba:	48 89 c6             	mov    %rax,%rsi
		ida->free_bitmap = NULL;
ffffffff812c23bd:	48 c7 43 40 00 00 00 	movq   $0x0,0x40(%rbx)
ffffffff812c23c4:	00 
ffffffff812c23c5:	48 89 cf             	mov    %rcx,%rdi
ffffffff812c23c8:	e8 0f 63 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
		spin_unlock_irqrestore(&ida->idr.lock, flags);

		if (!bitmap)
ffffffff812c23cd:	4d 85 ed             	test   %r13,%r13
ffffffff812c23d0:	74 89                	je     ffffffff812c235b <ida_get_new_above+0x4f>
			return -EAGAIN;

		memset(bitmap, 0, sizeof(struct ida_bitmap));
ffffffff812c23d2:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c23d5:	31 c0                	xor    %eax,%eax
ffffffff812c23d7:	b9 20 00 00 00       	mov    $0x20,%ecx
ffffffff812c23dc:	f3 ab                	rep stos %eax,%es:(%rdi)
		rcu_assign_pointer(pa[0]->ary[idr_id & IDR_MASK],
ffffffff812c23de:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
ffffffff812c23e2:	4e 89 6c f0 08       	mov    %r13,0x8(%rax,%r14,8)
				(void *)bitmap);
		pa[0]->count++;
ffffffff812c23e7:	ff 80 08 08 00 00    	incl   0x808(%rax)
	}

	/* lookup for empty slot */
	t = find_next_zero_bit(bitmap->bitmap, IDA_BITMAP_BITS, offset);
ffffffff812c23ed:	4d 8d 75 08          	lea    0x8(%r13),%r14
ffffffff812c23f1:	49 63 d4             	movslq %r12d,%rdx
ffffffff812c23f4:	be c0 03 00 00       	mov    $0x3c0,%esi
ffffffff812c23f9:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c23fc:	e8 74 e9 00 00       	callq  ffffffff812d0d75 <find_next_zero_bit>
	if (t == IDA_BITMAP_BITS) {
ffffffff812c2401:	3d c0 03 00 00       	cmp    $0x3c0,%eax
ffffffff812c2406:	75 0c                	jne    ffffffff812c2414 <ida_get_new_above+0x108>
		/* no empty slot after offset, continue to the next chunk */
		idr_id++;
ffffffff812c2408:	45 8d 6f 01          	lea    0x1(%r15),%r13d
		offset = 0;
ffffffff812c240c:	45 31 e4             	xor    %r12d,%r12d
		goto restart;
ffffffff812c240f:	e9 23 ff ff ff       	jmpq   ffffffff812c2337 <ida_get_new_above+0x2b>
	}

	id = idr_id * IDA_BITMAP_BITS + t;
ffffffff812c2414:	44 69 65 98 c0 03 00 	imul   $0x3c0,-0x68(%rbp),%r12d
ffffffff812c241b:	00 
	if (id >= MAX_IDR_BIT)
ffffffff812c241c:	41 01 c4             	add    %eax,%r12d
ffffffff812c241f:	0f 88 55 ff ff ff    	js     ffffffff812c237a <ida_get_new_above+0x6e>
		return -ENOSPC;

	__set_bit(t, bitmap->bitmap);
ffffffff812c2425:	48 98                	cltq   
ffffffff812c2427:	49 0f ab 06          	bts    %rax,(%r14)
	if (++bitmap->nr_busy == IDA_BITMAP_BITS)
ffffffff812c242b:	49 8b 45 00          	mov    0x0(%r13),%rax
ffffffff812c242f:	48 ff c0             	inc    %rax
ffffffff812c2432:	48 3d c0 03 00 00    	cmp    $0x3c0,%rax
ffffffff812c2438:	49 89 45 00          	mov    %rax,0x0(%r13)
ffffffff812c243c:	75 0c                	jne    ffffffff812c244a <ida_get_new_above+0x13e>
		idr_mark_full(pa, idr_id);
ffffffff812c243e:	48 8d 7d a8          	lea    -0x58(%rbp),%rdi
ffffffff812c2442:	44 89 fe             	mov    %r15d,%esi
ffffffff812c2445:	e8 84 f9 ff ff       	callq  ffffffff812c1dce <idr_mark_full>

	*p_id = id;
ffffffff812c244a:	48 8b 45 90          	mov    -0x70(%rbp),%rax
ffffffff812c244e:	44 89 20             	mov    %r12d,(%rax)
	/* Each leaf node can handle nearly a thousand slots and the
	 * whole idea of ida is to have small memory foot print.
	 * Throw away extra resources one by one after each successful
	 * allocation.
	 */
	if (ida->idr.id_free_cnt || ida->free_bitmap) {
ffffffff812c2451:	83 7b 30 00          	cmpl   $0x0,0x30(%rbx)
ffffffff812c2455:	75 07                	jne    ffffffff812c245e <ida_get_new_above+0x152>
ffffffff812c2457:	48 83 7b 40 00       	cmpq   $0x0,0x40(%rbx)
ffffffff812c245c:	74 1c                	je     ffffffff812c247a <ida_get_new_above+0x16e>
		struct idr_layer *p = get_from_free_list(&ida->idr);
ffffffff812c245e:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2461:	e8 11 f4 ff ff       	callq  ffffffff812c1877 <get_from_free_list>
		if (p)
ffffffff812c2466:	48 85 c0             	test   %rax,%rax
ffffffff812c2469:	74 0f                	je     ffffffff812c247a <ida_get_new_above+0x16e>
			kmem_cache_free(idr_layer_cache, p);
ffffffff812c246b:	48 8b 3d 16 f3 8e 00 	mov    0x8ef316(%rip),%rdi        # ffffffff81bb1788 <__key.11630>
ffffffff812c2472:	48 89 c6             	mov    %rax,%rsi
ffffffff812c2475:	e8 f8 e0 e3 ff       	callq  ffffffff81100572 <kmem_cache_free>
	}

	return 0;
ffffffff812c247a:	31 c0                	xor    %eax,%eax
}
ffffffff812c247c:	48 83 c4 58          	add    $0x58,%rsp
ffffffff812c2480:	5b                   	pop    %rbx
ffffffff812c2481:	41 5c                	pop    %r12
ffffffff812c2483:	41 5d                	pop    %r13
ffffffff812c2485:	41 5e                	pop    %r14
ffffffff812c2487:	41 5f                	pop    %r15
ffffffff812c2489:	5d                   	pop    %rbp
ffffffff812c248a:	c3                   	retq   

ffffffff812c248b <idr_remove>:
void idr_remove(struct idr *idp, int id)
{
	struct idr_layer *p;
	struct idr_layer *to_free;

	if (id < 0)
ffffffff812c248b:	85 f6                	test   %esi,%esi
ffffffff812c248d:	0f 88 85 01 00 00    	js     ffffffff812c2618 <idr_remove+0x18d>
 * idr_remove - remove the given id and free its slot
 * @idp: idr handle
 * @id: unique key
 */
void idr_remove(struct idr *idp, int id)
{
ffffffff812c2493:	55                   	push   %rbp
ffffffff812c2494:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2497:	41 55                	push   %r13
ffffffff812c2499:	41 54                	push   %r12
ffffffff812c249b:	53                   	push   %rbx
ffffffff812c249c:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c249f:	41 89 f5             	mov    %esi,%r13d
ffffffff812c24a2:	48 83 ec 38          	sub    $0x38,%rsp
	struct idr_layer *to_free;

	if (id < 0)
		return;

	if (id > idr_max(idp->layers)) {
ffffffff812c24a6:	44 8b 67 10          	mov    0x10(%rdi),%r12d
ffffffff812c24aa:	44 89 e7             	mov    %r12d,%edi
ffffffff812c24ad:	e8 a4 f3 ff ff       	callq  ffffffff812c1856 <idr_max>
ffffffff812c24b2:	39 c6                	cmp    %eax,%esi
ffffffff812c24b4:	7e 21                	jle    ffffffff812c24d7 <idr_remove+0x4c>
}
EXPORT_SYMBOL(idr_alloc_cyclic);

static void idr_remove_warning(int id)
{
	WARN(1, "idr_remove called for id=%d which is not allocated.\n", id);
ffffffff812c24b6:	89 f1                	mov    %esi,%ecx
ffffffff812c24b8:	48 c7 c2 f3 64 7b 81 	mov    $0xffffffff817b64f3,%rdx
ffffffff812c24bf:	be f9 01 00 00       	mov    $0x1f9,%esi
ffffffff812c24c4:	48 c7 c7 e9 64 7b 81 	mov    $0xffffffff817b64e9,%rdi
ffffffff812c24cb:	31 c0                	xor    %eax,%eax
ffffffff812c24cd:	e8 64 3e da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
ffffffff812c24d2:	e9 37 01 00 00       	jmpq   ffffffff812c260e <idr_remove+0x183>
	struct idr_layer ***paa = &pa[0];
	struct idr_layer *to_free;
	int n;

	*paa = NULL;
	*++paa = &idp->top;
ffffffff812c24d7:	48 8d 53 08          	lea    0x8(%rbx),%rdx
	WARN(1, "idr_remove called for id=%d which is not allocated.\n", id);
}

static void sub_remove(struct idr *idp, int shift, int id)
{
	struct idr_layer *p = idp->top;
ffffffff812c24db:	48 8b 43 08          	mov    0x8(%rbx),%rax
	if (id > idr_max(idp->layers)) {
		idr_remove_warning(id);
		return;
	}

	sub_remove(idp, (idp->layers - 1) * IDR_BITS, id);
ffffffff812c24df:	42 8d 0c e5 f8 ff ff 	lea    -0x8(,%r12,8),%ecx
ffffffff812c24e6:	ff 
	struct idr_layer ***paa = &pa[0];
	struct idr_layer *to_free;
	int n;

	*paa = NULL;
	*++paa = &idp->top;
ffffffff812c24e7:	4c 8d 65 c0          	lea    -0x40(%rbp),%r12
	struct idr_layer **pa[MAX_IDR_LEVEL + 1];
	struct idr_layer ***paa = &pa[0];
	struct idr_layer *to_free;
	int n;

	*paa = NULL;
ffffffff812c24eb:	48 c7 45 b8 00 00 00 	movq   $0x0,-0x48(%rbp)
ffffffff812c24f2:	00 
	*++paa = &idp->top;
ffffffff812c24f3:	48 89 55 c0          	mov    %rdx,-0x40(%rbp)

	while ((shift > 0) && p) {
ffffffff812c24f7:	85 c9                	test   %ecx,%ecx
ffffffff812c24f9:	7e 2c                	jle    ffffffff812c2527 <idr_remove+0x9c>
ffffffff812c24fb:	48 85 c0             	test   %rax,%rax
ffffffff812c24fe:	74 27                	je     ffffffff812c2527 <idr_remove+0x9c>
		n = (id >> shift) & IDR_MASK;
ffffffff812c2500:	44 89 ea             	mov    %r13d,%edx
ffffffff812c2503:	d3 fa                	sar    %cl,%edx
		__clear_bit(n, p->bitmap);
ffffffff812c2505:	0f b6 d2             	movzbl %dl,%edx
	clear_bit(nr, addr);
}

static inline void __clear_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("btr %1,%0" : ADDR : "Ir" (nr));
ffffffff812c2508:	48 0f b3 90 10 08 00 	btr    %rdx,0x810(%rax)
ffffffff812c250f:	00 
		*++paa = &p->ary[n];
ffffffff812c2510:	48 8d 7c d0 08       	lea    0x8(%rax,%rdx,8),%rdi
ffffffff812c2515:	49 83 c4 08          	add    $0x8,%r12
		p = p->ary[n];
ffffffff812c2519:	48 8b 44 d0 08       	mov    0x8(%rax,%rdx,8),%rax
		shift -= IDR_BITS;
ffffffff812c251e:	83 e9 08             	sub    $0x8,%ecx
	*++paa = &idp->top;

	while ((shift > 0) && p) {
		n = (id >> shift) & IDR_MASK;
		__clear_bit(n, p->bitmap);
		*++paa = &p->ary[n];
ffffffff812c2521:	49 89 3c 24          	mov    %rdi,(%r12)
ffffffff812c2525:	eb d0                	jmp    ffffffff812c24f7 <idr_remove+0x6c>
		p = p->ary[n];
		shift -= IDR_BITS;
	}
	n = id & IDR_MASK;
	if (likely(p != NULL && test_bit(n, p->bitmap))) {
ffffffff812c2527:	48 85 c0             	test   %rax,%rax
		__clear_bit(n, p->bitmap);
		*++paa = &p->ary[n];
		p = p->ary[n];
		shift -= IDR_BITS;
	}
	n = id & IDR_MASK;
ffffffff812c252a:	41 0f b6 d5          	movzbl %r13b,%edx
	if (likely(p != NULL && test_bit(n, p->bitmap))) {
ffffffff812c252e:	74 6b                	je     ffffffff812c259b <idr_remove+0x110>

static inline int variable_test_bit(long nr, volatile const unsigned long *addr)
{
	int oldbit;

	asm volatile("bt %2,%1\n\t"
ffffffff812c2530:	48 0f a3 90 10 08 00 	bt     %rdx,0x810(%rax)
ffffffff812c2537:	00 
ffffffff812c2538:	19 c9                	sbb    %ecx,%ecx
ffffffff812c253a:	85 c9                	test   %ecx,%ecx
ffffffff812c253c:	74 5d                	je     ffffffff812c259b <idr_remove+0x110>
	clear_bit(nr, addr);
}

static inline void __clear_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("btr %1,%0" : ADDR : "Ir" (nr));
ffffffff812c253e:	48 0f b3 90 10 08 00 	btr    %rdx,0x810(%rax)
ffffffff812c2545:	00 
		__clear_bit(n, p->bitmap);
		RCU_INIT_POINTER(p->ary[n], NULL);
		to_free = NULL;
ffffffff812c2546:	31 f6                	xor    %esi,%esi
		shift -= IDR_BITS;
	}
	n = id & IDR_MASK;
	if (likely(p != NULL && test_bit(n, p->bitmap))) {
		__clear_bit(n, p->bitmap);
		RCU_INIT_POINTER(p->ary[n], NULL);
ffffffff812c2548:	48 c7 44 d0 08 00 00 	movq   $0x0,0x8(%rax,%rdx,8)
ffffffff812c254f:	00 00 
		to_free = NULL;
		while(*paa && ! --((**paa)->count)){
ffffffff812c2551:	4d 8b 2c 24          	mov    (%r12),%r13
ffffffff812c2555:	4d 85 ed             	test   %r13,%r13
ffffffff812c2558:	74 2b                	je     ffffffff812c2585 <idr_remove+0xfa>
ffffffff812c255a:	49 8b 55 00          	mov    0x0(%r13),%rdx
ffffffff812c255e:	ff 8a 08 08 00 00    	decl   0x808(%rdx)
ffffffff812c2564:	75 26                	jne    ffffffff812c258c <idr_remove+0x101>
			if (to_free)
ffffffff812c2566:	48 85 f6             	test   %rsi,%rsi
ffffffff812c2569:	74 08                	je     ffffffff812c2573 <idr_remove+0xe8>
				free_layer(idp, to_free);
ffffffff812c256b:	48 89 df             	mov    %rbx,%rdi
ffffffff812c256e:	e8 83 f6 ff ff       	callq  ffffffff812c1bf6 <free_layer.isra.4>
			to_free = **paa;
ffffffff812c2573:	49 8b 75 00          	mov    0x0(%r13),%rsi
			**paa-- = NULL;
ffffffff812c2577:	49 83 ec 08          	sub    $0x8,%r12
ffffffff812c257b:	49 c7 45 00 00 00 00 	movq   $0x0,0x0(%r13)
ffffffff812c2582:	00 
ffffffff812c2583:	eb cc                	jmp    ffffffff812c2551 <idr_remove+0xc6>
		}
		if (!*paa)
			idp->layers = 0;
ffffffff812c2585:	c7 43 10 00 00 00 00 	movl   $0x0,0x10(%rbx)
		if (to_free)
ffffffff812c258c:	48 85 f6             	test   %rsi,%rsi
ffffffff812c258f:	74 27                	je     ffffffff812c25b8 <idr_remove+0x12d>
			free_layer(idp, to_free);
ffffffff812c2591:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2594:	e8 5d f6 ff ff       	callq  ffffffff812c1bf6 <free_layer.isra.4>
ffffffff812c2599:	eb 1d                	jmp    ffffffff812c25b8 <idr_remove+0x12d>
}
EXPORT_SYMBOL(idr_alloc_cyclic);

static void idr_remove_warning(int id)
{
	WARN(1, "idr_remove called for id=%d which is not allocated.\n", id);
ffffffff812c259b:	44 89 e9             	mov    %r13d,%ecx
ffffffff812c259e:	48 c7 c2 f3 64 7b 81 	mov    $0xffffffff817b64f3,%rdx
ffffffff812c25a5:	be f9 01 00 00       	mov    $0x1f9,%esi
ffffffff812c25aa:	48 c7 c7 e9 64 7b 81 	mov    $0xffffffff817b64e9,%rdi
ffffffff812c25b1:	31 c0                	xor    %eax,%eax
ffffffff812c25b3:	e8 7e 3d da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
		idr_remove_warning(id);
		return;
	}

	sub_remove(idp, (idp->layers - 1) * IDR_BITS, id);
	if (idp->top && idp->top->count == 1 && (idp->layers > 1) &&
ffffffff812c25b8:	4c 8b 63 08          	mov    0x8(%rbx),%r12
ffffffff812c25bc:	4d 85 e4             	test   %r12,%r12
ffffffff812c25bf:	74 4d                	je     ffffffff812c260e <idr_remove+0x183>
ffffffff812c25c1:	41 83 bc 24 08 08 00 	cmpl   $0x1,0x808(%r12)
ffffffff812c25c8:	00 01 
ffffffff812c25ca:	75 42                	jne    ffffffff812c260e <idr_remove+0x183>
ffffffff812c25cc:	83 7b 10 01          	cmpl   $0x1,0x10(%rbx)
ffffffff812c25d0:	7e 3c                	jle    ffffffff812c260e <idr_remove+0x183>
	    idp->top->ary[0]) {
ffffffff812c25d2:	49 8b 44 24 08       	mov    0x8(%r12),%rax
		idr_remove_warning(id);
		return;
	}

	sub_remove(idp, (idp->layers - 1) * IDR_BITS, id);
	if (idp->top && idp->top->count == 1 && (idp->layers > 1) &&
ffffffff812c25d7:	48 85 c0             	test   %rax,%rax
ffffffff812c25da:	74 32                	je     ffffffff812c260e <idr_remove+0x183>
		 * tree.
		 */
		to_free = idp->top;
		p = idp->top->ary[0];
		rcu_assign_pointer(idp->top, p);
		--idp->layers;
ffffffff812c25dc:	ff 4b 10             	decl   0x10(%rbx)
		to_free->count = 0;
		bitmap_clear(to_free->bitmap, 0, IDR_SIZE);
ffffffff812c25df:	49 8d bc 24 10 08 00 	lea    0x810(%r12),%rdi
ffffffff812c25e6:	00 
		 * inserted, they are inserted at the top of the existing
		 * tree.
		 */
		to_free = idp->top;
		p = idp->top->ary[0];
		rcu_assign_pointer(idp->top, p);
ffffffff812c25e7:	48 89 43 08          	mov    %rax,0x8(%rbx)
		--idp->layers;
		to_free->count = 0;
		bitmap_clear(to_free->bitmap, 0, IDR_SIZE);
ffffffff812c25eb:	31 f6                	xor    %esi,%esi
		 */
		to_free = idp->top;
		p = idp->top->ary[0];
		rcu_assign_pointer(idp->top, p);
		--idp->layers;
		to_free->count = 0;
ffffffff812c25ed:	41 c7 84 24 08 08 00 	movl   $0x0,0x808(%r12)
ffffffff812c25f4:	00 00 00 00 00 
		bitmap_clear(to_free->bitmap, 0, IDR_SIZE);
ffffffff812c25f9:	ba 00 01 00 00       	mov    $0x100,%edx
ffffffff812c25fe:	e8 91 aa 00 00       	callq  ffffffff812cd094 <bitmap_clear>
		free_layer(idp, to_free);
ffffffff812c2603:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c2606:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2609:	e8 e8 f5 ff ff       	callq  ffffffff812c1bf6 <free_layer.isra.4>
	}
}
ffffffff812c260e:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812c2612:	5b                   	pop    %rbx
ffffffff812c2613:	41 5c                	pop    %r12
ffffffff812c2615:	41 5d                	pop    %r13
ffffffff812c2617:	5d                   	pop    %rbp
ffffffff812c2618:	c3                   	retq   

ffffffff812c2619 <ida_remove>:
 * ida_remove - remove the given ID
 * @ida:	ida handle
 * @id:		ID to free
 */
void ida_remove(struct ida *ida, int id)
{
ffffffff812c2619:	55                   	push   %rbp
	struct idr_layer *p = ida->idr.top;
	int shift = (ida->idr.layers - 1) * IDR_BITS;
	int idr_id = id / IDA_BITMAP_BITS;
ffffffff812c261a:	b9 c0 03 00 00       	mov    $0x3c0,%ecx
ffffffff812c261f:	31 d2                	xor    %edx,%edx
 * ida_remove - remove the given ID
 * @ida:	ida handle
 * @id:		ID to free
 */
void ida_remove(struct ida *ida, int id)
{
ffffffff812c2621:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2624:	41 57                	push   %r15
ffffffff812c2626:	4c 63 fe             	movslq %esi,%r15
	struct idr_layer *p = ida->idr.top;
	int shift = (ida->idr.layers - 1) * IDR_BITS;
	int idr_id = id / IDA_BITMAP_BITS;
ffffffff812c2629:	4c 89 f8             	mov    %r15,%rax
 * ida_remove - remove the given ID
 * @ida:	ida handle
 * @id:		ID to free
 */
void ida_remove(struct ida *ida, int id)
{
ffffffff812c262c:	41 56                	push   %r14
ffffffff812c262e:	41 55                	push   %r13
	struct idr_layer *p = ida->idr.top;
	int shift = (ida->idr.layers - 1) * IDR_BITS;
	int idr_id = id / IDA_BITMAP_BITS;
ffffffff812c2630:	48 f7 f1             	div    %rcx
 * ida_remove - remove the given ID
 * @ida:	ida handle
 * @id:		ID to free
 */
void ida_remove(struct ida *ida, int id)
{
ffffffff812c2633:	41 54                	push   %r12
ffffffff812c2635:	53                   	push   %rbx
ffffffff812c2636:	49 89 fc             	mov    %rdi,%r12
ffffffff812c2639:	4d 89 fd             	mov    %r15,%r13
ffffffff812c263c:	48 83 ec 18          	sub    $0x18,%rsp
	struct idr_layer *p = ida->idr.top;
	int shift = (ida->idr.layers - 1) * IDR_BITS;
ffffffff812c2640:	8b 5f 10             	mov    0x10(%rdi),%ebx
	int idr_id = id / IDA_BITMAP_BITS;
ffffffff812c2643:	49 89 c6             	mov    %rax,%r14
	int offset = id % IDA_BITMAP_BITS;
	int n;
	struct ida_bitmap *bitmap;

	if (idr_id > idr_max(ida->idr.layers))
ffffffff812c2646:	89 df                	mov    %ebx,%edi
 */
void ida_remove(struct ida *ida, int id)
{
	struct idr_layer *p = ida->idr.top;
	int shift = (ida->idr.layers - 1) * IDR_BITS;
	int idr_id = id / IDA_BITMAP_BITS;
ffffffff812c2648:	89 45 cc             	mov    %eax,-0x34(%rbp)
	int offset = id % IDA_BITMAP_BITS;
	int n;
	struct ida_bitmap *bitmap;

	if (idr_id > idr_max(ida->idr.layers))
ffffffff812c264b:	e8 06 f2 ff ff       	callq  ffffffff812c1856 <idr_max>
ffffffff812c2650:	41 39 c6             	cmp    %eax,%r14d
ffffffff812c2653:	0f 8f 8f 00 00 00    	jg     ffffffff812c26e8 <ida_remove+0xcf>
 * @ida:	ida handle
 * @id:		ID to free
 */
void ida_remove(struct ida *ida, int id)
{
	struct idr_layer *p = ida->idr.top;
ffffffff812c2659:	49 8b 7c 24 08       	mov    0x8(%r12),%rdi
	int shift = (ida->idr.layers - 1) * IDR_BITS;
ffffffff812c265e:	8d 0c dd f8 ff ff ff 	lea    -0x8(,%rbx,8),%ecx

	if (idr_id > idr_max(ida->idr.layers))
		goto err;

	/* clear full bits while looking up the leaf idr_layer */
	while ((shift > 0) && p) {
ffffffff812c2665:	85 c9                	test   %ecx,%ecx
ffffffff812c2667:	7e 1f                	jle    ffffffff812c2688 <ida_remove+0x6f>
ffffffff812c2669:	48 85 ff             	test   %rdi,%rdi
ffffffff812c266c:	74 1a                	je     ffffffff812c2688 <ida_remove+0x6f>
		n = (idr_id >> shift) & IDR_MASK;
ffffffff812c266e:	8b 45 cc             	mov    -0x34(%rbp),%eax
ffffffff812c2671:	d3 f8                	sar    %cl,%eax
		__clear_bit(n, p->bitmap);
ffffffff812c2673:	0f b6 c0             	movzbl %al,%eax
ffffffff812c2676:	48 0f b3 87 10 08 00 	btr    %rax,0x810(%rdi)
ffffffff812c267d:	00 
		p = p->ary[n];
ffffffff812c267e:	48 8b 7c c7 08       	mov    0x8(%rdi,%rax,8),%rdi
		shift -= IDR_BITS;
ffffffff812c2683:	83 e9 08             	sub    $0x8,%ecx
ffffffff812c2686:	eb dd                	jmp    ffffffff812c2665 <ida_remove+0x4c>
	}

	if (p == NULL)
ffffffff812c2688:	48 85 ff             	test   %rdi,%rdi
ffffffff812c268b:	74 5b                	je     ffffffff812c26e8 <ida_remove+0xcf>
		goto err;

	n = idr_id & IDR_MASK;
	__clear_bit(n, p->bitmap);
ffffffff812c268d:	4c 8d 97 10 08 00 00 	lea    0x810(%rdi),%r10
ffffffff812c2694:	41 0f b6 ce          	movzbl %r14b,%ecx
ffffffff812c2698:	48 0f b3 8f 10 08 00 	btr    %rcx,0x810(%rdi)
ffffffff812c269f:	00 

	bitmap = (void *)p->ary[n];
ffffffff812c26a0:	48 8b 5c cf 08       	mov    0x8(%rdi,%rcx,8),%rbx
	if (!bitmap || !test_bit(offset, bitmap->bitmap))
ffffffff812c26a5:	48 85 db             	test   %rbx,%rbx
ffffffff812c26a8:	74 3e                	je     ffffffff812c26e8 <ida_remove+0xcf>
ffffffff812c26aa:	bf c0 03 00 00       	mov    $0x3c0,%edi
ffffffff812c26af:	4c 89 f8             	mov    %r15,%rax
ffffffff812c26b2:	31 d2                	xor    %edx,%edx
ffffffff812c26b4:	48 f7 f7             	div    %rdi

static inline int variable_test_bit(long nr, volatile const unsigned long *addr)
{
	int oldbit;

	asm volatile("bt %2,%1\n\t"
ffffffff812c26b7:	48 0f a3 53 08       	bt     %rdx,0x8(%rbx)
ffffffff812c26bc:	19 c0                	sbb    %eax,%eax
ffffffff812c26be:	85 c0                	test   %eax,%eax
ffffffff812c26c0:	74 26                	je     ffffffff812c26e8 <ida_remove+0xcf>
	clear_bit(nr, addr);
}

static inline void __clear_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("btr %1,%0" : ADDR : "Ir" (nr));
ffffffff812c26c2:	48 0f b3 53 08       	btr    %rdx,0x8(%rbx)
		goto err;

	/* update bitmap and remove it if empty */
	__clear_bit(offset, bitmap->bitmap);
	if (--bitmap->nr_busy == 0) {
ffffffff812c26c7:	48 ff 0b             	decq   (%rbx)
ffffffff812c26ca:	75 39                	jne    ffffffff812c2705 <ida_remove+0xec>
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
ffffffff812c26cc:	49 0f ab 0a          	bts    %rcx,(%r10)
		__set_bit(n, p->bitmap);	/* to please idr_remove() */
		idr_remove(&ida->idr, idr_id);
ffffffff812c26d0:	44 89 f6             	mov    %r14d,%esi
ffffffff812c26d3:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c26d6:	e8 b0 fd ff ff       	callq  ffffffff812c248b <idr_remove>
		free_bitmap(ida, bitmap);
ffffffff812c26db:	48 89 de             	mov    %rbx,%rsi
ffffffff812c26de:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c26e1:	e8 51 f4 ff ff       	callq  ffffffff812c1b37 <free_bitmap>
ffffffff812c26e6:	eb 1d                	jmp    ffffffff812c2705 <ida_remove+0xec>
	}

	return;

 err:
	WARN(1, "ida_remove called for id=%d which is not allocated.\n", id);
ffffffff812c26e8:	44 89 e9             	mov    %r13d,%ecx
ffffffff812c26eb:	48 c7 c2 28 65 7b 81 	mov    $0xffffffff817b6528,%rdx
ffffffff812c26f2:	be 1b 04 00 00       	mov    $0x41b,%esi
ffffffff812c26f7:	48 c7 c7 e9 64 7b 81 	mov    $0xffffffff817b64e9,%rdi
ffffffff812c26fe:	31 c0                	xor    %eax,%eax
ffffffff812c2700:	e8 31 3c da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
}
ffffffff812c2705:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c2709:	5b                   	pop    %rbx
ffffffff812c270a:	41 5c                	pop    %r12
ffffffff812c270c:	41 5d                	pop    %r13
ffffffff812c270e:	41 5e                	pop    %r14
ffffffff812c2710:	41 5f                	pop    %r15
ffffffff812c2712:	5d                   	pop    %rbp
ffffffff812c2713:	c3                   	retq   

ffffffff812c2714 <ida_simple_remove>:
 */
void ida_simple_remove(struct ida *ida, unsigned int id)
{
	unsigned long flags;

	BUG_ON((int)id < 0);
ffffffff812c2714:	85 f6                	test   %esi,%esi
ffffffff812c2716:	79 02                	jns    ffffffff812c271a <ida_simple_remove+0x6>
ffffffff812c2718:	0f 0b                	ud2    
 * ida_simple_remove - remove an allocated id.
 * @ida: the (initialized) ida.
 * @id: the id returned by ida_simple_get.
 */
void ida_simple_remove(struct ida *ida, unsigned int id)
{
ffffffff812c271a:	55                   	push   %rbp
ffffffff812c271b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c271e:	41 55                	push   %r13
ffffffff812c2720:	41 54                	push   %r12
ffffffff812c2722:	53                   	push   %rbx
ffffffff812c2723:	50                   	push   %rax
ffffffff812c2724:	41 89 f5             	mov    %esi,%r13d
ffffffff812c2727:	48 89 fb             	mov    %rdi,%rbx
	unsigned long flags;

	BUG_ON((int)id < 0);
	spin_lock_irqsave(&simple_ida_lock, flags);
ffffffff812c272a:	48 c7 c7 f0 5e a3 81 	mov    $0xffffffff81a35ef0,%rdi
ffffffff812c2731:	e8 f2 60 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
	ida_remove(ida, id);
ffffffff812c2736:	44 89 ee             	mov    %r13d,%esi
void ida_simple_remove(struct ida *ida, unsigned int id)
{
	unsigned long flags;

	BUG_ON((int)id < 0);
	spin_lock_irqsave(&simple_ida_lock, flags);
ffffffff812c2739:	49 89 c4             	mov    %rax,%r12
	ida_remove(ida, id);
ffffffff812c273c:	48 89 df             	mov    %rbx,%rdi
ffffffff812c273f:	e8 d5 fe ff ff       	callq  ffffffff812c2619 <ida_remove>
ffffffff812c2744:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c2747:	48 c7 c7 f0 5e a3 81 	mov    $0xffffffff81a35ef0,%rdi
ffffffff812c274e:	e8 89 5f 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
	spin_unlock_irqrestore(&simple_ida_lock, flags);
}
ffffffff812c2753:	5a                   	pop    %rdx
ffffffff812c2754:	5b                   	pop    %rbx
ffffffff812c2755:	41 5c                	pop    %r12
ffffffff812c2757:	41 5d                	pop    %r13
ffffffff812c2759:	5d                   	pop    %rbp
ffffffff812c275a:	c3                   	retq   

ffffffff812c275b <ida_simple_get>:
 *
 * Use ida_simple_remove() to get rid of an id.
 */
int ida_simple_get(struct ida *ida, unsigned int start, unsigned int end,
		   gfp_t gfp_mask)
{
ffffffff812c275b:	55                   	push   %rbp
ffffffff812c275c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c275f:	41 57                	push   %r15
ffffffff812c2761:	41 56                	push   %r14
ffffffff812c2763:	41 55                	push   %r13
ffffffff812c2765:	41 54                	push   %r12
ffffffff812c2767:	53                   	push   %rbx
ffffffff812c2768:	48 83 ec 28          	sub    $0x28,%rsp
	int ret, id;
	unsigned int max;
	unsigned long flags;

	BUG_ON((int)start < 0);
ffffffff812c276c:	85 f6                	test   %esi,%esi
ffffffff812c276e:	79 02                	jns    ffffffff812c2772 <ida_simple_get+0x17>
ffffffff812c2770:	0f 0b                	ud2    
	BUG_ON((int)end < 0);
ffffffff812c2772:	85 d2                	test   %edx,%edx
ffffffff812c2774:	79 02                	jns    ffffffff812c2778 <ida_simple_get+0x1d>
ffffffff812c2776:	0f 0b                	ud2    

	if (end == 0)
ffffffff812c2778:	74 0a                	je     ffffffff812c2784 <ida_simple_get+0x29>
		max = 0x80000000;
	else {
		BUG_ON(end < start);
ffffffff812c277a:	39 d6                	cmp    %edx,%esi
		max = end - 1;
ffffffff812c277c:	44 8d 72 ff          	lea    -0x1(%rdx),%r14d
	BUG_ON((int)end < 0);

	if (end == 0)
		max = 0x80000000;
	else {
		BUG_ON(end < start);
ffffffff812c2780:	76 08                	jbe    ffffffff812c278a <ida_simple_get+0x2f>
ffffffff812c2782:	0f 0b                	ud2    

	BUG_ON((int)start < 0);
	BUG_ON((int)end < 0);

	if (end == 0)
		max = 0x80000000;
ffffffff812c2784:	41 be 00 00 00 80    	mov    $0x80000000,%r14d
ffffffff812c278a:	41 89 cd             	mov    %ecx,%r13d
ffffffff812c278d:	41 89 f4             	mov    %esi,%r12d
ffffffff812c2790:	48 89 fb             	mov    %rdi,%rbx
		BUG_ON(end < start);
		max = end - 1;
	}

again:
	if (!ida_pre_get(ida, gfp_mask))
ffffffff812c2793:	44 89 ee             	mov    %r13d,%esi
ffffffff812c2796:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2799:	e8 80 f5 ff ff       	callq  ffffffff812c1d1e <ida_pre_get>
ffffffff812c279e:	85 c0                	test   %eax,%eax
ffffffff812c27a0:	74 55                	je     ffffffff812c27f7 <ida_simple_get+0x9c>
		return -ENOMEM;

	spin_lock_irqsave(&simple_ida_lock, flags);
ffffffff812c27a2:	48 c7 c7 f0 5e a3 81 	mov    $0xffffffff81a35ef0,%rdi
ffffffff812c27a9:	e8 7a 60 17 00       	callq  ffffffff81438828 <_raw_spin_lock_irqsave>
	ret = ida_get_new_above(ida, start, &id);
ffffffff812c27ae:	48 8d 55 cc          	lea    -0x34(%rbp),%rdx
ffffffff812c27b2:	44 89 e6             	mov    %r12d,%esi
ffffffff812c27b5:	48 89 df             	mov    %rbx,%rdi

again:
	if (!ida_pre_get(ida, gfp_mask))
		return -ENOMEM;

	spin_lock_irqsave(&simple_ida_lock, flags);
ffffffff812c27b8:	49 89 c7             	mov    %rax,%r15
	ret = ida_get_new_above(ida, start, &id);
ffffffff812c27bb:	e8 4c fb ff ff       	callq  ffffffff812c230c <ida_get_new_above>
	if (!ret) {
ffffffff812c27c0:	85 c0                	test   %eax,%eax
ffffffff812c27c2:	75 17                	jne    ffffffff812c27db <ida_simple_get+0x80>
		if (id > max) {
ffffffff812c27c4:	8b 45 cc             	mov    -0x34(%rbp),%eax
ffffffff812c27c7:	41 39 c6             	cmp    %eax,%r14d
ffffffff812c27ca:	73 0f                	jae    ffffffff812c27db <ida_simple_get+0x80>
			ida_remove(ida, id);
ffffffff812c27cc:	89 c6                	mov    %eax,%esi
ffffffff812c27ce:	48 89 df             	mov    %rbx,%rdi
ffffffff812c27d1:	e8 43 fe ff ff       	callq  ffffffff812c2619 <ida_remove>
			ret = -ENOSPC;
ffffffff812c27d6:	b8 e4 ff ff ff       	mov    $0xffffffe4,%eax
ffffffff812c27db:	4c 89 fe             	mov    %r15,%rsi
ffffffff812c27de:	48 c7 c7 f0 5e a3 81 	mov    $0xffffffff81a35ef0,%rdi
ffffffff812c27e5:	89 45 bc             	mov    %eax,-0x44(%rbp)
ffffffff812c27e8:	e8 ef 5e 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
			ret = id;
		}
	}
	spin_unlock_irqrestore(&simple_ida_lock, flags);

	if (unlikely(ret == -EAGAIN))
ffffffff812c27ed:	8b 45 bc             	mov    -0x44(%rbp),%eax
ffffffff812c27f0:	83 f8 f5             	cmp    $0xfffffff5,%eax
ffffffff812c27f3:	75 07                	jne    ffffffff812c27fc <ida_simple_get+0xa1>
ffffffff812c27f5:	eb 9c                	jmp    ffffffff812c2793 <ida_simple_get+0x38>
		max = end - 1;
	}

again:
	if (!ida_pre_get(ida, gfp_mask))
		return -ENOMEM;
ffffffff812c27f7:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax

	if (unlikely(ret == -EAGAIN))
		goto again;

	return ret;
}
ffffffff812c27fc:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c2800:	5b                   	pop    %rbx
ffffffff812c2801:	41 5c                	pop    %r12
ffffffff812c2803:	41 5d                	pop    %r13
ffffffff812c2805:	41 5e                	pop    %r14
ffffffff812c2807:	41 5f                	pop    %r15
ffffffff812c2809:	5d                   	pop    %rbp
ffffffff812c280a:	c3                   	retq   

ffffffff812c280b <int_sqrt>:
 * @x: integer of which to calculate the sqrt
 *
 * A very rough approximation to the sqrt() function.
 */
unsigned long int_sqrt(unsigned long x)
{
ffffffff812c280b:	55                   	push   %rbp
	unsigned long b, m, y = 0;

	if (x <= 1)
ffffffff812c280c:	48 83 ff 01          	cmp    $0x1,%rdi
ffffffff812c2810:	48 89 f8             	mov    %rdi,%rax
 * @x: integer of which to calculate the sqrt
 *
 * A very rough approximation to the sqrt() function.
 */
unsigned long int_sqrt(unsigned long x)
{
ffffffff812c2813:	48 89 e5             	mov    %rsp,%rbp
	unsigned long b, m, y = 0;

	if (x <= 1)
ffffffff812c2816:	76 2b                	jbe    ffffffff812c2843 <int_sqrt+0x38>
ffffffff812c2818:	b9 20 00 00 00       	mov    $0x20,%ecx
ffffffff812c281d:	31 c0                	xor    %eax,%eax
ffffffff812c281f:	48 ba 00 00 00 00 00 	movabs $0x4000000000000000,%rdx
ffffffff812c2826:	00 00 40 
		return x;

	m = 1UL << (BITS_PER_LONG - 2);
	while (m != 0) {
		b = y + m;
ffffffff812c2829:	48 8d 34 10          	lea    (%rax,%rdx,1),%rsi
		y >>= 1;
ffffffff812c282d:	48 d1 e8             	shr    %rax

		if (x >= b) {
ffffffff812c2830:	48 39 fe             	cmp    %rdi,%rsi
ffffffff812c2833:	77 06                	ja     ffffffff812c283b <int_sqrt+0x30>
			x -= b;
ffffffff812c2835:	48 29 f7             	sub    %rsi,%rdi
			y += m;
ffffffff812c2838:	48 01 d0             	add    %rdx,%rax
		}
		m >>= 2;
ffffffff812c283b:	48 c1 ea 02          	shr    $0x2,%rdx

	if (x <= 1)
		return x;

	m = 1UL << (BITS_PER_LONG - 2);
	while (m != 0) {
ffffffff812c283f:	ff c9                	dec    %ecx
ffffffff812c2841:	75 e6                	jne    ffffffff812c2829 <int_sqrt+0x1e>
		}
		m >>= 2;
	}

	return y;
}
ffffffff812c2843:	5d                   	pop    %rbp
ffffffff812c2844:	c3                   	retq   

ffffffff812c2845 <ioremap_page_range>:
	return 0;
}

int ioremap_page_range(unsigned long addr,
		       unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
{
ffffffff812c2845:	55                   	push   %rbp
ffffffff812c2846:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2849:	41 57                	push   %r15
ffffffff812c284b:	41 56                	push   %r14
ffffffff812c284d:	41 55                	push   %r13
ffffffff812c284f:	41 54                	push   %r12
ffffffff812c2851:	53                   	push   %rbx
ffffffff812c2852:	48 83 ec 58          	sub    $0x58,%rsp
	pgd_t *pgd;
	unsigned long start;
	unsigned long next;
	int err;

	BUG_ON(addr >= end);
ffffffff812c2856:	48 39 f7             	cmp    %rsi,%rdi
	return 0;
}

int ioremap_page_range(unsigned long addr,
		       unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
{
ffffffff812c2859:	48 89 75 c0          	mov    %rsi,-0x40(%rbp)
ffffffff812c285d:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
	pgd_t *pgd;
	unsigned long start;
	unsigned long next;
	int err;

	BUG_ON(addr >= end);
ffffffff812c2861:	72 02                	jb     ffffffff812c2865 <ioremap_page_range+0x20>
ffffffff812c2863:	0f 0b                	ud2    

	start = addr;
	phys_addr -= addr;
	pgd = pgd_offset_k(addr);
ffffffff812c2865:	49 89 fe             	mov    %rdi,%r14
	do {
		next = pgd_addr_end(addr, end);
ffffffff812c2868:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
	int err;

	BUG_ON(addr >= end);

	start = addr;
	phys_addr -= addr;
ffffffff812c286c:	48 29 fa             	sub    %rdi,%rdx
	pgd = pgd_offset_k(addr);
ffffffff812c286f:	49 c1 ee 24          	shr    $0x24,%r14
ffffffff812c2873:	49 89 ff             	mov    %rdi,%r15
	int err;

	BUG_ON(addr >= end);

	start = addr;
	phys_addr -= addr;
ffffffff812c2876:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
	pgd = pgd_offset_k(addr);
ffffffff812c287a:	41 81 e6 f8 0f 00 00 	and    $0xff8,%r14d
ffffffff812c2881:	4c 03 35 38 8d 76 00 	add    0x768d38(%rip),%r14        # ffffffff81a2b5c0 <init_mm+0x40>
	do {
		next = pgd_addr_end(addr, end);
ffffffff812c2888:	48 ff c8             	dec    %rax
ffffffff812c288b:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
ffffffff812c288f:	49 bc 00 00 00 00 80 	movabs $0x8000000000,%r12
ffffffff812c2896:	00 00 00 
ffffffff812c2899:	4b 8d 04 27          	lea    (%r15,%r12,1),%rax
ffffffff812c289d:	49 bc 00 00 00 00 80 	movabs $0xffffff8000000000,%r12
ffffffff812c28a4:	ff ff ff 
ffffffff812c28a7:	49 21 c4             	and    %rax,%r12
ffffffff812c28aa:	49 8d 44 24 ff       	lea    -0x1(%r12),%rax
ffffffff812c28af:	48 3b 45 a0          	cmp    -0x60(%rbp),%rax
ffffffff812c28b3:	4c 0f 43 65 c0       	cmovae -0x40(%rbp),%r12
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
		NULL: pud_offset(pgd, address);
ffffffff812c28b8:	49 83 3e 00          	cmpq   $0x0,(%r14)
ffffffff812c28bc:	74 3b                	je     ffffffff812c28f9 <ioremap_page_range+0xb4>
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
ffffffff812c28be:	4d 89 fd             	mov    %r15,%r13
ffffffff812c28c1:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
ffffffff812c28c8:	88 ff ff 
ffffffff812c28cb:	49 c1 ed 1b          	shr    $0x1b,%r13
ffffffff812c28cf:	4c 89 e9             	mov    %r13,%rcx
ffffffff812c28d2:	49 bd 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r13
ffffffff812c28d9:	3f 00 00 
ffffffff812c28dc:	4d 23 2e             	and    (%r14),%r13
ffffffff812c28df:	81 e1 f8 0f 00 00    	and    $0xff8,%ecx
ffffffff812c28e5:	49 8d 44 15 00       	lea    0x0(%r13,%rdx,1),%rax
ffffffff812c28ea:	4c 8d 2c 01          	lea    (%rcx,%rax,1),%r13
	phys_addr -= addr;
	pud = pud_alloc(&init_mm, pgd, addr);
	if (!pud)
		return -ENOMEM;
	do {
		next = pud_addr_end(addr, end);
ffffffff812c28ee:	49 8d 44 24 ff       	lea    -0x1(%r12),%rax
ffffffff812c28f3:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
ffffffff812c28f7:	eb 59                	jmp    ffffffff812c2952 <ioremap_page_range+0x10d>
 * Remove it when 4level-fixup.h has been removed.
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
ffffffff812c28f9:	4c 89 fa             	mov    %r15,%rdx
ffffffff812c28fc:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c28ff:	48 c7 c7 80 b5 a2 81 	mov    $0xffffffff81a2b580,%rdi
ffffffff812c2906:	e8 9f c1 e2 ff       	callq  ffffffff810eeaaa <__pud_alloc>
ffffffff812c290b:	85 c0                	test   %eax,%eax
ffffffff812c290d:	74 af                	je     ffffffff812c28be <ioremap_page_range+0x79>
ffffffff812c290f:	e9 fa 01 00 00       	jmpq   ffffffff812c2b0e <ioremap_page_range+0x2c9>

		if (ioremap_pud_enabled() &&
ffffffff812c2914:	48 89 d8             	mov    %rbx,%rax
ffffffff812c2917:	4c 29 f8             	sub    %r15,%rax
ffffffff812c291a:	48 3d 00 00 00 40    	cmp    $0x40000000,%rax
ffffffff812c2920:	75 53                	jne    ffffffff812c2975 <ioremap_page_range+0x130>
		    ((next - addr) == PUD_SIZE) &&
		    IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {
ffffffff812c2922:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812c2926:	4a 8d 34 38          	lea    (%rax,%r15,1),%rsi
		return -ENOMEM;
	do {
		next = pud_addr_end(addr, end);

		if (ioremap_pud_enabled() &&
		    ((next - addr) == PUD_SIZE) &&
ffffffff812c292a:	f7 c6 ff ff ff 3f    	test   $0x3fffffff,%esi
ffffffff812c2930:	75 43                	jne    ffffffff812c2975 <ioremap_page_range+0x130>
		    IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {
			if (pud_set_huge(pud, phys_addr + addr, prot))
ffffffff812c2932:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
ffffffff812c2936:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c2939:	e8 da fd d9 ff       	callq  ffffffff81062718 <pud_set_huge>
ffffffff812c293e:	85 c0                	test   %eax,%eax
ffffffff812c2940:	74 33                	je     ffffffff812c2975 <ioremap_page_range+0x130>
				continue;
		}

		if (ioremap_pmd_range(pud, addr, next, phys_addr + addr, prot))
			return -ENOMEM;
	} while (pud++, addr = next, addr != end);
ffffffff812c2942:	49 83 c5 08          	add    $0x8,%r13
ffffffff812c2946:	49 39 dc             	cmp    %rbx,%r12
ffffffff812c2949:	0f 84 a9 01 00 00    	je     ffffffff812c2af8 <ioremap_page_range+0x2b3>
ffffffff812c294f:	49 89 df             	mov    %rbx,%r15
	phys_addr -= addr;
	pud = pud_alloc(&init_mm, pgd, addr);
	if (!pud)
		return -ENOMEM;
	do {
		next = pud_addr_end(addr, end);
ffffffff812c2952:	49 8d 9f 00 00 00 40 	lea    0x40000000(%r15),%rbx
ffffffff812c2959:	48 81 e3 00 00 00 c0 	and    $0xffffffffc0000000,%rbx
ffffffff812c2960:	48 8d 43 ff          	lea    -0x1(%rbx),%rax
ffffffff812c2964:	48 3b 45 a8          	cmp    -0x58(%rbp),%rax
ffffffff812c2968:	49 0f 43 dc          	cmovae %r12,%rbx

		if (ioremap_pud_enabled() &&
ffffffff812c296c:	83 3d 29 6e 79 00 00 	cmpl   $0x0,0x796e29(%rip)        # ffffffff81a5979c <ioremap_pud_capable>
ffffffff812c2973:	75 9f                	jne    ffffffff812c2914 <ioremap_page_range+0xcf>
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
		NULL: pmd_offset(pud, address);
ffffffff812c2975:	49 83 7d 00 00       	cmpq   $0x0,0x0(%r13)
ffffffff812c297a:	75 1a                	jne    ffffffff812c2996 <ioremap_page_range+0x151>
		NULL: pud_offset(pgd, address);
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
ffffffff812c297c:	4c 89 fa             	mov    %r15,%rdx
ffffffff812c297f:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c2982:	48 c7 c7 80 b5 a2 81 	mov    $0xffffffff81a2b580,%rdi
ffffffff812c2989:	e8 7c c5 e2 ff       	callq  ffffffff810eef0a <__pmd_alloc>
ffffffff812c298e:	85 c0                	test   %eax,%eax
ffffffff812c2990:	0f 85 78 01 00 00    	jne    ffffffff812c2b0e <ioremap_page_range+0x2c9>
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
ffffffff812c2996:	4c 89 f9             	mov    %r15,%rcx
ffffffff812c2999:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
ffffffff812c29a0:	88 ff ff 
ffffffff812c29a3:	48 c1 e9 12          	shr    $0x12,%rcx
ffffffff812c29a7:	48 89 ce             	mov    %rcx,%rsi
ffffffff812c29aa:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
ffffffff812c29b1:	3f 00 00 
ffffffff812c29b4:	49 23 4d 00          	and    0x0(%r13),%rcx
ffffffff812c29b8:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
ffffffff812c29be:	48 8d 04 11          	lea    (%rcx,%rdx,1),%rax
ffffffff812c29c2:	48 8d 0c 06          	lea    (%rsi,%rax,1),%rcx
	phys_addr -= addr;
	pmd = pmd_alloc(&init_mm, pud, addr);
	if (!pmd)
		return -ENOMEM;
	do {
		next = pmd_addr_end(addr, end);
ffffffff812c29c6:	48 8d 43 ff          	lea    -0x1(%rbx),%rax
ffffffff812c29ca:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
ffffffff812c29ce:	eb 52                	jmp    ffffffff812c2a22 <ioremap_page_range+0x1dd>

		if (ioremap_pmd_enabled() &&
ffffffff812c29d0:	4c 89 c0             	mov    %r8,%rax
ffffffff812c29d3:	4c 29 f8             	sub    %r15,%rax
ffffffff812c29d6:	48 3d 00 00 20 00    	cmp    $0x200000,%rax
ffffffff812c29dc:	75 72                	jne    ffffffff812c2a50 <ioremap_page_range+0x20b>
		    ((next - addr) == PMD_SIZE) &&
ffffffff812c29de:	41 f7 c2 ff ff 1f 00 	test   $0x1fffff,%r10d
ffffffff812c29e5:	75 69                	jne    ffffffff812c2a50 <ioremap_page_range+0x20b>
		    IS_ALIGNED(phys_addr + addr, PMD_SIZE)) {
			if (pmd_set_huge(pmd, phys_addr + addr, prot))
ffffffff812c29e7:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
ffffffff812c29eb:	4c 89 d6             	mov    %r10,%rsi
ffffffff812c29ee:	48 89 cf             	mov    %rcx,%rdi
ffffffff812c29f1:	4c 89 45 88          	mov    %r8,-0x78(%rbp)
ffffffff812c29f5:	4c 89 55 90          	mov    %r10,-0x70(%rbp)
ffffffff812c29f9:	48 89 4d 98          	mov    %rcx,-0x68(%rbp)
ffffffff812c29fd:	e8 7b fd d9 ff       	callq  ffffffff8106277d <pmd_set_huge>
ffffffff812c2a02:	85 c0                	test   %eax,%eax
ffffffff812c2a04:	48 8b 4d 98          	mov    -0x68(%rbp),%rcx
ffffffff812c2a08:	4c 8b 55 90          	mov    -0x70(%rbp),%r10
ffffffff812c2a0c:	4c 8b 45 88          	mov    -0x78(%rbp),%r8
ffffffff812c2a10:	74 3e                	je     ffffffff812c2a50 <ioremap_page_range+0x20b>
				continue;
		}

		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
			return -ENOMEM;
	} while (pmd++, addr = next, addr != end);
ffffffff812c2a12:	48 83 c1 08          	add    $0x8,%rcx
ffffffff812c2a16:	4c 39 c3             	cmp    %r8,%rbx
ffffffff812c2a19:	0f 84 23 ff ff ff    	je     ffffffff812c2942 <ioremap_page_range+0xfd>
ffffffff812c2a1f:	4d 89 c7             	mov    %r8,%r15
	phys_addr -= addr;
	pmd = pmd_alloc(&init_mm, pud, addr);
	if (!pmd)
		return -ENOMEM;
	do {
		next = pmd_addr_end(addr, end);
ffffffff812c2a22:	4d 8d 8f 00 00 20 00 	lea    0x200000(%r15),%r9
ffffffff812c2a29:	49 81 e1 00 00 e0 ff 	and    $0xffffffffffe00000,%r9
ffffffff812c2a30:	49 8d 41 ff          	lea    -0x1(%r9),%rax
ffffffff812c2a34:	48 3b 45 b0          	cmp    -0x50(%rbp),%rax
ffffffff812c2a38:	4d 89 c8             	mov    %r9,%r8
ffffffff812c2a3b:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812c2a3f:	4c 0f 43 c3          	cmovae %rbx,%r8

		if (ioremap_pmd_enabled() &&
ffffffff812c2a43:	83 3d 4e 6d 79 00 00 	cmpl   $0x0,0x796d4e(%rip)        # ffffffff81a59798 <ioremap_pmd_capable>
ffffffff812c2a4a:	4e 8d 14 38          	lea    (%rax,%r15,1),%r10
ffffffff812c2a4e:	75 80                	jne    ffffffff812c29d0 <ioremap_page_range+0x18b>
		unsigned long end, phys_addr_t phys_addr, pgprot_t prot)
{
	pte_t *pte;
	u64 pfn;

	pfn = phys_addr >> PAGE_SHIFT;
ffffffff812c2a50:	49 c1 ea 0c          	shr    $0xc,%r10
	pte = pte_alloc_kernel(pmd, addr);
ffffffff812c2a54:	48 83 39 00          	cmpq   $0x0,(%rcx)
ffffffff812c2a58:	75 2b                	jne    ffffffff812c2a85 <ioremap_page_range+0x240>
ffffffff812c2a5a:	48 89 cf             	mov    %rcx,%rdi
ffffffff812c2a5d:	4c 89 fe             	mov    %r15,%rsi
ffffffff812c2a60:	4c 89 55 88          	mov    %r10,-0x78(%rbp)
ffffffff812c2a64:	4c 89 45 90          	mov    %r8,-0x70(%rbp)
ffffffff812c2a68:	48 89 4d 98          	mov    %rcx,-0x68(%rbp)
ffffffff812c2a6c:	e8 f0 b3 e2 ff       	callq  ffffffff810ede61 <__pte_alloc_kernel>
ffffffff812c2a71:	85 c0                	test   %eax,%eax
ffffffff812c2a73:	48 8b 4d 98          	mov    -0x68(%rbp),%rcx
ffffffff812c2a77:	4c 8b 45 90          	mov    -0x70(%rbp),%r8
ffffffff812c2a7b:	4c 8b 55 88          	mov    -0x78(%rbp),%r10
ffffffff812c2a7f:	0f 85 89 00 00 00    	jne    ffffffff812c2b0e <ioremap_page_range+0x2c9>
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
ffffffff812c2a85:	4c 89 f8             	mov    %r15,%rax
ffffffff812c2a88:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
ffffffff812c2a8f:	88 ff ff 
ffffffff812c2a92:	48 c1 e8 09          	shr    $0x9,%rax
ffffffff812c2a96:	25 f8 0f 00 00       	and    $0xff8,%eax
ffffffff812c2a9b:	48 89 c7             	mov    %rax,%rdi
ffffffff812c2a9e:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
ffffffff812c2aa5:	3f 00 00 
ffffffff812c2aa8:	48 23 01             	and    (%rcx),%rax
ffffffff812c2aab:	48 8d 14 30          	lea    (%rax,%rsi,1),%rdx
ffffffff812c2aaf:	48 8d 04 17          	lea    (%rdi,%rdx,1),%rax
 */
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
ffffffff812c2ab3:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
ffffffff812c2ab7:	83 e7 01             	and    $0x1,%edi
ffffffff812c2aba:	eb 34                	jmp    ffffffff812c2af0 <ioremap_page_range+0x2ab>
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
ffffffff812c2abc:	4c 89 d6             	mov    %r10,%rsi
 */
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
ffffffff812c2abf:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
ffffffff812c2ac3:	48 c1 e6 0c          	shl    $0xc,%rsi
 */
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
ffffffff812c2ac7:	48 85 ff             	test   %rdi,%rdi
ffffffff812c2aca:	74 07                	je     ffffffff812c2ad3 <ioremap_page_range+0x28e>
		protval &= __supported_pte_mask;
ffffffff812c2acc:	48 23 15 75 66 79 00 	and    0x796675(%rip),%rdx        # ffffffff81a59148 <__supported_pte_mask>
	*ptep = native_make_pte(0);
}

static inline void native_set_pte(pte_t *ptep, pte_t pte)
{
	*ptep = pte;
ffffffff812c2ad3:	48 09 f2             	or     %rsi,%rdx
		return -ENOMEM;
	do {
		BUG_ON(!pte_none(*pte));
		set_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot));
		pfn++;
	} while (pte++, addr += PAGE_SIZE, addr != end);
ffffffff812c2ad6:	49 81 c7 00 10 00 00 	add    $0x1000,%r15
	if (!pte)
		return -ENOMEM;
	do {
		BUG_ON(!pte_none(*pte));
		set_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot));
		pfn++;
ffffffff812c2add:	49 ff c2             	inc    %r10
ffffffff812c2ae0:	48 89 10             	mov    %rdx,(%rax)
	} while (pte++, addr += PAGE_SIZE, addr != end);
ffffffff812c2ae3:	48 83 c0 08          	add    $0x8,%rax
ffffffff812c2ae7:	4d 39 f8             	cmp    %r15,%r8
ffffffff812c2aea:	0f 84 22 ff ff ff    	je     ffffffff812c2a12 <ioremap_page_range+0x1cd>
	pfn = phys_addr >> PAGE_SHIFT;
	pte = pte_alloc_kernel(pmd, addr);
	if (!pte)
		return -ENOMEM;
	do {
		BUG_ON(!pte_none(*pte));
ffffffff812c2af0:	48 83 38 00          	cmpq   $0x0,(%rax)
ffffffff812c2af4:	74 c6                	je     ffffffff812c2abc <ioremap_page_range+0x277>
ffffffff812c2af6:	0f 0b                	ud2    
	do {
		next = pgd_addr_end(addr, end);
		err = ioremap_pud_range(pgd, addr, next, phys_addr+addr, prot);
		if (err)
			break;
	} while (pgd++, addr = next, addr != end);
ffffffff812c2af8:	49 83 c6 08          	add    $0x8,%r14
ffffffff812c2afc:	4c 3b 65 c0          	cmp    -0x40(%rbp),%r12
ffffffff812c2b00:	74 08                	je     ffffffff812c2b0a <ioremap_page_range+0x2c5>
ffffffff812c2b02:	4d 89 e7             	mov    %r12,%r15
ffffffff812c2b05:	e9 85 fd ff ff       	jmpq   ffffffff812c288f <ioremap_page_range+0x4a>
ffffffff812c2b0a:	31 c0                	xor    %eax,%eax
ffffffff812c2b0c:	eb 05                	jmp    ffffffff812c2b13 <ioremap_page_range+0x2ce>
 */
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
ffffffff812c2b0e:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax

	flush_cache_vmap(start, end);

	return err;
}
ffffffff812c2b13:	48 83 c4 58          	add    $0x58,%rsp
ffffffff812c2b17:	5b                   	pop    %rbx
ffffffff812c2b18:	41 5c                	pop    %r12
ffffffff812c2b1a:	41 5d                	pop    %r13
ffffffff812c2b1c:	41 5e                	pop    %r14
ffffffff812c2b1e:	41 5f                	pop    %r15
ffffffff812c2b20:	5d                   	pop    %rbp
ffffffff812c2b21:	c3                   	retq   

ffffffff812c2b22 <kobj_attr_show>:
{
	struct kobj_attribute *kattr;
	ssize_t ret = -EIO;

	kattr = container_of(attr, struct kobj_attribute, attr);
	if (kattr->show)
ffffffff812c2b22:	48 8b 4e 10          	mov    0x10(%rsi),%rcx
ffffffff812c2b26:	48 85 c9             	test   %rcx,%rcx
ffffffff812c2b29:	74 08                	je     ffffffff812c2b33 <kobj_attr_show+0x11>
}

/* default kobject attribute operations */
static ssize_t kobj_attr_show(struct kobject *kobj, struct attribute *attr,
			      char *buf)
{
ffffffff812c2b2b:	55                   	push   %rbp
ffffffff812c2b2c:	48 89 e5             	mov    %rsp,%rbp
	struct kobj_attribute *kattr;
	ssize_t ret = -EIO;

	kattr = container_of(attr, struct kobj_attribute, attr);
	if (kattr->show)
		ret = kattr->show(kobj, kattr, buf);
ffffffff812c2b2f:	ff d1                	callq  *%rcx
	return ret;
}
ffffffff812c2b31:	5d                   	pop    %rbp
ffffffff812c2b32:	c3                   	retq   
ffffffff812c2b33:	48 c7 c0 fb ff ff ff 	mov    $0xfffffffffffffffb,%rax
ffffffff812c2b3a:	c3                   	retq   

ffffffff812c2b3b <kobj_attr_store>:
{
	struct kobj_attribute *kattr;
	ssize_t ret = -EIO;

	kattr = container_of(attr, struct kobj_attribute, attr);
	if (kattr->store)
ffffffff812c2b3b:	4c 8b 46 18          	mov    0x18(%rsi),%r8
ffffffff812c2b3f:	4d 85 c0             	test   %r8,%r8
ffffffff812c2b42:	74 09                	je     ffffffff812c2b4d <kobj_attr_store+0x12>
	return ret;
}

static ssize_t kobj_attr_store(struct kobject *kobj, struct attribute *attr,
			       const char *buf, size_t count)
{
ffffffff812c2b44:	55                   	push   %rbp
ffffffff812c2b45:	48 89 e5             	mov    %rsp,%rbp
	struct kobj_attribute *kattr;
	ssize_t ret = -EIO;

	kattr = container_of(attr, struct kobj_attribute, attr);
	if (kattr->store)
		ret = kattr->store(kobj, kattr, buf, count);
ffffffff812c2b48:	41 ff d0             	callq  *%r8
	return ret;
}
ffffffff812c2b4b:	5d                   	pop    %rbp
ffffffff812c2b4c:	c3                   	retq   
ffffffff812c2b4d:	48 c7 c0 fb ff ff ff 	mov    $0xfffffffffffffffb,%rax
ffffffff812c2b54:	c3                   	retq   

ffffffff812c2b55 <dynamic_kobj_release>:
		kref_put(&kobj->kref, kobject_release);
	}
}

static void dynamic_kobj_release(struct kobject *kobj)
{
ffffffff812c2b55:	55                   	push   %rbp
	pr_debug("kobject: (%p): %s\n", kobj, __func__);
ffffffff812c2b56:	48 c7 c2 90 f0 63 81 	mov    $0xffffffff8163f090,%rdx
ffffffff812c2b5d:	48 89 fe             	mov    %rdi,%rsi
		kref_put(&kobj->kref, kobject_release);
	}
}

static void dynamic_kobj_release(struct kobject *kobj)
{
ffffffff812c2b60:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2b63:	53                   	push   %rbx
ffffffff812c2b64:	50                   	push   %rax
ffffffff812c2b65:	48 89 fb             	mov    %rdi,%rbx
	pr_debug("kobject: (%p): %s\n", kobj, __func__);
ffffffff812c2b68:	31 c0                	xor    %eax,%eax
ffffffff812c2b6a:	48 c7 c7 7b 65 7b 81 	mov    $0xffffffff817b657b,%rdi
ffffffff812c2b71:	e8 1c 04 17 00       	callq  ffffffff81432f92 <printk>
	kfree(kobj);
ffffffff812c2b76:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2b79:	e8 0b de e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812c2b7e:	5a                   	pop    %rdx
ffffffff812c2b7f:	5b                   	pop    %rbx
ffffffff812c2b80:	5d                   	pop    %rbp
ffffffff812c2b81:	c3                   	retq   

ffffffff812c2b82 <kset_release>:
	spin_unlock(&kset->list_lock);
	return ret;
}

static void kset_release(struct kobject *kobj)
{
ffffffff812c2b82:	55                   	push   %rbp
	struct kset *kset = container_of(kobj, struct kset, kobj);
	pr_debug("kobject: '%s' (%p): %s\n",
ffffffff812c2b83:	48 89 fa             	mov    %rdi,%rdx
ffffffff812c2b86:	48 c7 c1 60 f0 63 81 	mov    $0xffffffff8163f060,%rcx
	spin_unlock(&kset->list_lock);
	return ret;
}

static void kset_release(struct kobject *kobj)
{
ffffffff812c2b8d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2b90:	53                   	push   %rbx
ffffffff812c2b91:	50                   	push   %rax
	struct kset *kset = container_of(kobj, struct kset, kobj);
	pr_debug("kobject: '%s' (%p): %s\n",
ffffffff812c2b92:	48 8b 37             	mov    (%rdi),%rsi
	spin_unlock(&kset->list_lock);
	return ret;
}

static void kset_release(struct kobject *kobj)
{
ffffffff812c2b95:	48 89 fb             	mov    %rdi,%rbx
	struct kset *kset = container_of(kobj, struct kset, kobj);
	pr_debug("kobject: '%s' (%p): %s\n",
ffffffff812c2b98:	31 c0                	xor    %eax,%eax
ffffffff812c2b9a:	48 c7 c7 90 65 7b 81 	mov    $0xffffffff817b6590,%rdi
ffffffff812c2ba1:	e8 ec 03 17 00       	callq  ffffffff81432f92 <printk>
		 kobject_name(kobj), kobj, __func__);
	kfree(kset);
ffffffff812c2ba6:	48 8d 7b d8          	lea    -0x28(%rbx),%rdi
ffffffff812c2baa:	e8 da dd e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812c2baf:	5a                   	pop    %rdx
ffffffff812c2bb0:	5b                   	pop    %rbx
ffffffff812c2bb1:	5d                   	pop    %rbp
ffffffff812c2bb2:	c3                   	retq   

ffffffff812c2bb3 <kobject_init>:
 * After this function is called, the kobject MUST be cleaned up by a call
 * to kobject_put(), not by a call to kfree directly to ensure that all of
 * the memory is cleaned up properly.
 */
void kobject_init(struct kobject *kobj, struct kobj_type *ktype)
{
ffffffff812c2bb3:	55                   	push   %rbp
	char *err_str;

	if (!kobj) {
ffffffff812c2bb4:	48 85 ff             	test   %rdi,%rdi
 * After this function is called, the kobject MUST be cleaned up by a call
 * to kobject_put(), not by a call to kfree directly to ensure that all of
 * the memory is cleaned up properly.
 */
void kobject_init(struct kobject *kobj, struct kobj_type *ktype)
{
ffffffff812c2bb7:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2bba:	41 54                	push   %r12
ffffffff812c2bbc:	53                   	push   %rbx
ffffffff812c2bbd:	48 89 fb             	mov    %rdi,%rbx
	char *err_str;

	if (!kobj) {
ffffffff812c2bc0:	74 49                	je     ffffffff812c2c0b <kobject_init+0x58>
		err_str = "invalid kobject pointer!";
		goto error;
	}
	if (!ktype) {
ffffffff812c2bc2:	48 85 f6             	test   %rsi,%rsi
ffffffff812c2bc5:	49 89 f4             	mov    %rsi,%r12
ffffffff812c2bc8:	74 4a                	je     ffffffff812c2c14 <kobject_init+0x61>
		err_str = "must have a ktype to be initialized properly!\n";
		goto error;
	}
	if (kobj->state_initialized) {
ffffffff812c2bca:	f6 47 3c 01          	testb  $0x1,0x3c(%rdi)
ffffffff812c2bce:	74 16                	je     ffffffff812c2be6 <kobject_init+0x33>
		/* do not error out as sometimes we can recover */
		printk(KERN_ERR "kobject (%p): tried to init an initialized "
ffffffff812c2bd0:	48 89 fe             	mov    %rdi,%rsi
ffffffff812c2bd3:	31 c0                	xor    %eax,%eax
ffffffff812c2bd5:	48 c7 c7 f2 65 7b 81 	mov    $0xffffffff817b65f2,%rdi
ffffffff812c2bdc:	e8 b1 03 17 00       	callq  ffffffff81432f92 <printk>
		       "object, something is seriously wrong.\n", kobj);
		dump_stack();
ffffffff812c2be1:	e8 83 1e 17 00       	callq  ffffffff81434a69 <dump_stack>
static void kobject_init_internal(struct kobject *kobj)
{
	if (!kobj)
		return;
	kref_init(&kobj->kref);
	INIT_LIST_HEAD(&kobj->entry);
ffffffff812c2be6:	48 8d 43 08          	lea    0x8(%rbx),%rax
 *
 * Atomically sets the value of @v to @i.
 */
static inline void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
ffffffff812c2bea:	c7 43 38 01 00 00 00 	movl   $0x1,0x38(%rbx)
		       "object, something is seriously wrong.\n", kobj);
		dump_stack();
	}

	kobject_init_internal(kobj);
	kobj->ktype = ktype;
ffffffff812c2bf1:	4c 89 63 28          	mov    %r12,0x28(%rbx)
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
ffffffff812c2bf5:	48 89 43 08          	mov    %rax,0x8(%rbx)
	list->prev = list;
ffffffff812c2bf9:	48 89 43 10          	mov    %rax,0x10(%rbx)
		return;
	kref_init(&kobj->kref);
	INIT_LIST_HEAD(&kobj->entry);
	kobj->state_in_sysfs = 0;
	kobj->state_add_uevent_sent = 0;
	kobj->state_remove_uevent_sent = 0;
ffffffff812c2bfd:	8a 43 3c             	mov    0x3c(%rbx),%al
ffffffff812c2c00:	83 e0 f1             	and    $0xfffffff1,%eax
	kobj->state_initialized = 1;
ffffffff812c2c03:	83 c8 01             	or     $0x1,%eax
ffffffff812c2c06:	88 43 3c             	mov    %al,0x3c(%rbx)
		dump_stack();
	}

	kobject_init_internal(kobj);
	kobj->ktype = ktype;
	return;
ffffffff812c2c09:	eb 26                	jmp    ffffffff812c2c31 <kobject_init+0x7e>
void kobject_init(struct kobject *kobj, struct kobj_type *ktype)
{
	char *err_str;

	if (!kobj) {
		err_str = "invalid kobject pointer!";
ffffffff812c2c0b:	48 c7 c2 aa 65 7b 81 	mov    $0xffffffff817b65aa,%rdx
ffffffff812c2c12:	eb 07                	jmp    ffffffff812c2c1b <kobject_init+0x68>
		goto error;
	}
	if (!ktype) {
		err_str = "must have a ktype to be initialized properly!\n";
ffffffff812c2c14:	48 c7 c2 c3 65 7b 81 	mov    $0xffffffff817b65c3,%rdx
	kobject_init_internal(kobj);
	kobj->ktype = ktype;
	return;

error:
	printk(KERN_ERR "kobject (%p): %s\n", kobj, err_str);
ffffffff812c2c1b:	48 89 de             	mov    %rbx,%rsi
ffffffff812c2c1e:	48 c7 c7 46 66 7b 81 	mov    $0xffffffff817b6646,%rdi
ffffffff812c2c25:	31 c0                	xor    %eax,%eax
ffffffff812c2c27:	e8 66 03 17 00       	callq  ffffffff81432f92 <printk>
	dump_stack();
ffffffff812c2c2c:	e8 38 1e 17 00       	callq  ffffffff81434a69 <dump_stack>
}
ffffffff812c2c31:	5b                   	pop    %rbx
ffffffff812c2c32:	41 5c                	pop    %r12
ffffffff812c2c34:	5d                   	pop    %rbp
ffffffff812c2c35:	c3                   	retq   

ffffffff812c2c36 <kobject_put>:
 *
 * Decrement the refcount, and if 0, call kobject_cleanup().
 */
void kobject_put(struct kobject *kobj)
{
	if (kobj) {
ffffffff812c2c36:	48 85 ff             	test   %rdi,%rdi
ffffffff812c2c39:	74 44                	je     ffffffff812c2c7f <kobject_put+0x49>
 * @kobj: object.
 *
 * Decrement the refcount, and if 0, call kobject_cleanup().
 */
void kobject_put(struct kobject *kobj)
{
ffffffff812c2c3b:	55                   	push   %rbp
ffffffff812c2c3c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2c3f:	53                   	push   %rbx
ffffffff812c2c40:	52                   	push   %rdx
	if (kobj) {
		if (!kobj->state_initialized)
ffffffff812c2c41:	f6 47 3c 01          	testb  $0x1,0x3c(%rdi)
ffffffff812c2c45:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c2c48:	75 20                	jne    ffffffff812c2c6a <kobject_put+0x34>
			WARN(1, KERN_WARNING "kobject: '%s' (%p): is not "
ffffffff812c2c4a:	48 8b 0f             	mov    (%rdi),%rcx
ffffffff812c2c4d:	49 89 f8             	mov    %rdi,%r8
ffffffff812c2c50:	48 c7 c2 5a 66 7b 81 	mov    $0xffffffff817b665a,%rdx
ffffffff812c2c57:	be a3 02 00 00       	mov    $0x2a3,%esi
ffffffff812c2c5c:	48 c7 c7 a8 66 7b 81 	mov    $0xffffffff817b66a8,%rdi
ffffffff812c2c63:	31 c0                	xor    %eax,%eax
ffffffff812c2c65:	e8 cc 36 da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
			       "initialized, yet kobject_put() is being "
			       "called.\n", kobject_name(kobj), kobj);
		kref_put(&kobj->kref, kobject_release);
ffffffff812c2c6a:	48 8d 7b 38          	lea    0x38(%rbx),%rdi
 * true if the result is zero, or false for all
 * other cases.
 */
static inline int atomic_sub_and_test(int i, atomic_t *v)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "subl", v->counter, "er", i, "%0", "e");
ffffffff812c2c6e:	f0 83 6b 38 01       	lock subl $0x1,0x38(%rbx)
ffffffff812c2c73:	74 02                	je     ffffffff812c2c77 <kobject_put+0x41>
	}
}
ffffffff812c2c75:	eb 05                	jmp    ffffffff812c2c7c <kobject_put+0x46>
	     void (*release)(struct kref *kref))
{
	WARN_ON(release == NULL);

	if (atomic_sub_and_test((int) count, &kref->refcount)) {
		release(kref);
ffffffff812c2c77:	e8 95 00 00 00       	callq  ffffffff812c2d11 <kobject_release>
ffffffff812c2c7c:	58                   	pop    %rax
ffffffff812c2c7d:	5b                   	pop    %rbx
ffffffff812c2c7e:	5d                   	pop    %rbp
ffffffff812c2c7f:	c3                   	retq   

ffffffff812c2c80 <kobj_kset_leave>:
}

/* remove the kobject from its kset's list */
static void kobj_kset_leave(struct kobject *kobj)
{
	if (!kobj->kset)
ffffffff812c2c80:	48 8b 47 20          	mov    0x20(%rdi),%rax
ffffffff812c2c84:	48 85 c0             	test   %rax,%rax
ffffffff812c2c87:	74 45                	je     ffffffff812c2cce <kobj_kset_leave+0x4e>
	spin_unlock(&kobj->kset->list_lock);
}

/* remove the kobject from its kset's list */
static void kobj_kset_leave(struct kobject *kobj)
{
ffffffff812c2c89:	55                   	push   %rbp
ffffffff812c2c8a:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2c8d:	41 54                	push   %r12
ffffffff812c2c8f:	53                   	push   %rbx
ffffffff812c2c90:	48 89 fb             	mov    %rdi,%rbx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c2c93:	48 8d 78 10          	lea    0x10(%rax),%rdi
	if (!kobj->kset)
		return;

	spin_lock(&kobj->kset->list_lock);
	list_del_init(&kobj->entry);
ffffffff812c2c97:	4c 8d 63 08          	lea    0x8(%rbx),%r12
ffffffff812c2c9b:	e8 90 59 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static inline void list_del_init(struct list_head *entry)
{
	__list_del_entry(entry);
ffffffff812c2ca0:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c2ca3:	e8 6b 22 01 00       	callq  ffffffff812d4f13 <__list_del_entry>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c2ca8:	48 8b 43 20          	mov    0x20(%rbx),%rax
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
ffffffff812c2cac:	4c 89 63 08          	mov    %r12,0x8(%rbx)
	list->prev = list;
ffffffff812c2cb0:	4c 89 63 10          	mov    %r12,0x10(%rbx)
ffffffff812c2cb4:	48 8d 78 10          	lea    0x10(%rax),%rdi
ffffffff812c2cb8:	e8 ef 59 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	return k ? to_kset(kobject_get(&k->kobj)) : NULL;
}

static inline void kset_put(struct kset *k)
{
	kobject_put(&k->kobj);
ffffffff812c2cbd:	48 8b 7b 20          	mov    0x20(%rbx),%rdi
ffffffff812c2cc1:	48 83 c7 28          	add    $0x28,%rdi
ffffffff812c2cc5:	e8 6c ff ff ff       	callq  ffffffff812c2c36 <kobject_put>
	spin_unlock(&kobj->kset->list_lock);
	kset_put(kobj->kset);
}
ffffffff812c2cca:	5b                   	pop    %rbx
ffffffff812c2ccb:	41 5c                	pop    %r12
ffffffff812c2ccd:	5d                   	pop    %rbp
ffffffff812c2cce:	c3                   	retq   

ffffffff812c2ccf <kobject_del>:
 */
void kobject_del(struct kobject *kobj)
{
	struct kernfs_node *sd;

	if (!kobj)
ffffffff812c2ccf:	48 85 ff             	test   %rdi,%rdi
ffffffff812c2cd2:	74 3c                	je     ffffffff812c2d10 <kobject_del+0x41>
/**
 * kobject_del - unlink kobject from hierarchy.
 * @kobj: object.
 */
void kobject_del(struct kobject *kobj)
{
ffffffff812c2cd4:	55                   	push   %rbp
ffffffff812c2cd5:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2cd8:	41 54                	push   %r12
ffffffff812c2cda:	53                   	push   %rbx
	struct kernfs_node *sd;

	if (!kobj)
		return;

	sd = kobj->sd;
ffffffff812c2cdb:	4c 8b 67 30          	mov    0x30(%rdi),%r12
ffffffff812c2cdf:	48 89 fb             	mov    %rdi,%rbx
	sysfs_remove_dir(kobj);
ffffffff812c2ce2:	e8 e0 e7 e8 ff       	callq  ffffffff811514c7 <sysfs_remove_dir>
	return kn;
}

static inline void sysfs_put(struct kernfs_node *kn)
{
	kernfs_put(kn);
ffffffff812c2ce7:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c2cea:	e8 42 bf e8 ff       	callq  ffffffff8114ec31 <kernfs_put>
	sysfs_put(sd);

	kobj->state_in_sysfs = 0;
ffffffff812c2cef:	80 63 3c fd          	andb   $0xfd,0x3c(%rbx)
	kobj_kset_leave(kobj);
ffffffff812c2cf3:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2cf6:	e8 85 ff ff ff       	callq  ffffffff812c2c80 <kobj_kset_leave>
	kobject_put(kobj->parent);
ffffffff812c2cfb:	48 8b 7b 18          	mov    0x18(%rbx),%rdi
ffffffff812c2cff:	e8 32 ff ff ff       	callq  ffffffff812c2c36 <kobject_put>
	kobj->parent = NULL;
ffffffff812c2d04:	48 c7 43 18 00 00 00 	movq   $0x0,0x18(%rbx)
ffffffff812c2d0b:	00 
}
ffffffff812c2d0c:	5b                   	pop    %rbx
ffffffff812c2d0d:	41 5c                	pop    %r12
ffffffff812c2d0f:	5d                   	pop    %rbp
ffffffff812c2d10:	c3                   	retq   

ffffffff812c2d11 <kobject_release>:
				     struct kobject, release));
}
#endif

static void kobject_release(struct kref *kref)
{
ffffffff812c2d11:	55                   	push   %rbp
static void kobject_cleanup(struct kobject *kobj)
{
	struct kobj_type *t = get_ktype(kobj);
	const char *name = kobj->name;

	pr_debug("kobject: '%s' (%p): %s, parent %p\n",
ffffffff812c2d12:	31 c0                	xor    %eax,%eax
ffffffff812c2d14:	48 c7 c1 b0 f0 63 81 	mov    $0xffffffff8163f0b0,%rcx
				     struct kobject, release));
}
#endif

static void kobject_release(struct kref *kref)
{
ffffffff812c2d1b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2d1e:	41 56                	push   %r14
ffffffff812c2d20:	41 55                	push   %r13
ffffffff812c2d22:	41 54                	push   %r12
ffffffff812c2d24:	53                   	push   %rbx
	struct kobject *kobj = container_of(kref, struct kobject, kref);
ffffffff812c2d25:	4c 8d 67 c8          	lea    -0x38(%rdi),%r12
 * @kobj: object to cleanup
 */
static void kobject_cleanup(struct kobject *kobj)
{
	struct kobj_type *t = get_ktype(kobj);
	const char *name = kobj->name;
ffffffff812c2d29:	4c 8b 77 c8          	mov    -0x38(%rdi),%r14
ffffffff812c2d2d:	4c 8b 6f f0          	mov    -0x10(%rdi),%r13
				     struct kobject, release));
}
#endif

static void kobject_release(struct kref *kref)
{
ffffffff812c2d31:	48 89 fb             	mov    %rdi,%rbx
static void kobject_cleanup(struct kobject *kobj)
{
	struct kobj_type *t = get_ktype(kobj);
	const char *name = kobj->name;

	pr_debug("kobject: '%s' (%p): %s, parent %p\n",
ffffffff812c2d34:	4c 8b 47 e0          	mov    -0x20(%rdi),%r8
ffffffff812c2d38:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c2d3b:	48 c7 c7 b6 66 7b 81 	mov    $0xffffffff817b66b6,%rdi
ffffffff812c2d42:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c2d45:	e8 48 02 17 00       	callq  ffffffff81432f92 <printk>
		 kobject_name(kobj), kobj, __func__, kobj->parent);

	if (t && !t->release)
ffffffff812c2d4a:	4d 85 ed             	test   %r13,%r13
ffffffff812c2d4d:	74 1c                	je     ffffffff812c2d6b <kobject_release+0x5a>
ffffffff812c2d4f:	49 83 7d 00 00       	cmpq   $0x0,0x0(%r13)
ffffffff812c2d54:	75 15                	jne    ffffffff812c2d6b <kobject_release+0x5a>
		pr_debug("kobject: '%s' (%p): does not have a release() "
ffffffff812c2d56:	48 8b 73 c8          	mov    -0x38(%rbx),%rsi
ffffffff812c2d5a:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c2d5d:	48 c7 c7 db 66 7b 81 	mov    $0xffffffff817b66db,%rdi
ffffffff812c2d64:	31 c0                	xor    %eax,%eax
ffffffff812c2d66:	e8 27 02 17 00       	callq  ffffffff81432f92 <printk>
			 "function, it is broken and must be fixed.\n",
			 kobject_name(kobj), kobj);

	/* send "remove" if the caller did not do it but sent "add" */
	if (kobj->state_add_uevent_sent && !kobj->state_remove_uevent_sent) {
ffffffff812c2d6b:	8a 43 04             	mov    0x4(%rbx),%al
ffffffff812c2d6e:	83 e0 0c             	and    $0xc,%eax
ffffffff812c2d71:	3c 04                	cmp    $0x4,%al
ffffffff812c2d73:	75 22                	jne    ffffffff812c2d97 <kobject_release+0x86>
		pr_debug("kobject: '%s' (%p): auto cleanup 'remove' event\n",
ffffffff812c2d75:	48 8b 73 c8          	mov    -0x38(%rbx),%rsi
ffffffff812c2d79:	48 c7 c7 36 67 7b 81 	mov    $0xffffffff817b6736,%rdi
ffffffff812c2d80:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c2d83:	31 c0                	xor    %eax,%eax
ffffffff812c2d85:	e8 08 02 17 00       	callq  ffffffff81432f92 <printk>
			 kobject_name(kobj), kobj);
		kobject_uevent(kobj, KOBJ_REMOVE);
ffffffff812c2d8a:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c2d8f:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c2d92:	e8 28 14 00 00       	callq  ffffffff812c41bf <kobject_uevent>
	}

	/* remove from sysfs if the caller did not do it */
	if (kobj->state_in_sysfs) {
ffffffff812c2d97:	f6 43 04 02          	testb  $0x2,0x4(%rbx)
ffffffff812c2d9b:	74 1d                	je     ffffffff812c2dba <kobject_release+0xa9>
		pr_debug("kobject: '%s' (%p): auto cleanup kobject_del\n",
ffffffff812c2d9d:	48 8b 73 c8          	mov    -0x38(%rbx),%rsi
ffffffff812c2da1:	48 c7 c7 69 67 7b 81 	mov    $0xffffffff817b6769,%rdi
ffffffff812c2da8:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c2dab:	31 c0                	xor    %eax,%eax
ffffffff812c2dad:	e8 e0 01 17 00       	callq  ffffffff81432f92 <printk>
			 kobject_name(kobj), kobj);
		kobject_del(kobj);
ffffffff812c2db2:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c2db5:	e8 15 ff ff ff       	callq  ffffffff812c2ccf <kobject_del>
	}

	if (t && t->release) {
ffffffff812c2dba:	4d 85 ed             	test   %r13,%r13
ffffffff812c2dbd:	74 23                	je     ffffffff812c2de2 <kobject_release+0xd1>
ffffffff812c2dbf:	49 83 7d 00 00       	cmpq   $0x0,0x0(%r13)
ffffffff812c2dc4:	74 1c                	je     ffffffff812c2de2 <kobject_release+0xd1>
		pr_debug("kobject: '%s' (%p): calling ktype release\n",
ffffffff812c2dc6:	48 8b 73 c8          	mov    -0x38(%rbx),%rsi
ffffffff812c2dca:	48 c7 c7 99 67 7b 81 	mov    $0xffffffff817b6799,%rdi
ffffffff812c2dd1:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c2dd4:	31 c0                	xor    %eax,%eax
ffffffff812c2dd6:	e8 b7 01 17 00       	callq  ffffffff81432f92 <printk>
			 kobject_name(kobj), kobj);
		t->release(kobj);
ffffffff812c2ddb:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c2dde:	41 ff 55 00          	callq  *0x0(%r13)
	}

	/* free name if we allocated it */
	if (name) {
ffffffff812c2de2:	4d 85 f6             	test   %r14,%r14
ffffffff812c2de5:	74 19                	je     ffffffff812c2e00 <kobject_release+0xef>
		pr_debug("kobject: '%s': free name\n", name);
ffffffff812c2de7:	48 c7 c7 c6 67 7b 81 	mov    $0xffffffff817b67c6,%rdi
ffffffff812c2dee:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c2df1:	31 c0                	xor    %eax,%eax
ffffffff812c2df3:	e8 9a 01 17 00       	callq  ffffffff81432f92 <printk>
		kfree(name);
ffffffff812c2df8:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c2dfb:	e8 89 db e3 ff       	callq  ffffffff81100989 <kfree>

	schedule_delayed_work(&kobj->release, delay);
#else
	kobject_cleanup(kobj);
#endif
}
ffffffff812c2e00:	5b                   	pop    %rbx
ffffffff812c2e01:	41 5c                	pop    %r12
ffffffff812c2e03:	41 5d                	pop    %r13
ffffffff812c2e05:	41 5e                	pop    %r14
ffffffff812c2e07:	5d                   	pop    %rbp
ffffffff812c2e08:	c3                   	retq   

ffffffff812c2e09 <kset_unregister>:
 * kset_unregister - remove a kset.
 * @k: kset.
 */
void kset_unregister(struct kset *k)
{
	if (!k)
ffffffff812c2e09:	48 85 ff             	test   %rdi,%rdi
ffffffff812c2e0c:	74 1d                	je     ffffffff812c2e2b <kset_unregister+0x22>
/**
 * kset_unregister - remove a kset.
 * @k: kset.
 */
void kset_unregister(struct kset *k)
{
ffffffff812c2e0e:	55                   	push   %rbp
ffffffff812c2e0f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2e12:	53                   	push   %rbx
	if (!k)
		return;
	kobject_del(&k->kobj);
ffffffff812c2e13:	48 8d 5f 28          	lea    0x28(%rdi),%rbx
/**
 * kset_unregister - remove a kset.
 * @k: kset.
 */
void kset_unregister(struct kset *k)
{
ffffffff812c2e17:	50                   	push   %rax
	if (!k)
		return;
	kobject_del(&k->kobj);
ffffffff812c2e18:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2e1b:	e8 af fe ff ff       	callq  ffffffff812c2ccf <kobject_del>
	kobject_put(&k->kobj);
ffffffff812c2e20:	48 89 df             	mov    %rbx,%rdi
ffffffff812c2e23:	e8 0e fe ff ff       	callq  ffffffff812c2c36 <kobject_put>
}
ffffffff812c2e28:	5a                   	pop    %rdx
ffffffff812c2e29:	5b                   	pop    %rbx
ffffffff812c2e2a:	5d                   	pop    %rbp
ffffffff812c2e2b:	c3                   	retq   

ffffffff812c2e2c <kobject_get>:
/**
 * kobject_get - increment refcount for object.
 * @kobj: object.
 */
struct kobject *kobject_get(struct kobject *kobj)
{
ffffffff812c2e2c:	55                   	push   %rbp
	if (kobj) {
ffffffff812c2e2d:	48 85 ff             	test   %rdi,%rdi
/**
 * kobject_get - increment refcount for object.
 * @kobj: object.
 */
struct kobject *kobject_get(struct kobject *kobj)
{
ffffffff812c2e30:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2e33:	53                   	push   %rbx
ffffffff812c2e34:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c2e37:	51                   	push   %rcx
	if (kobj) {
ffffffff812c2e38:	74 57                	je     ffffffff812c2e91 <kobject_get+0x65>
		if (!kobj->state_initialized)
ffffffff812c2e3a:	f6 47 3c 01          	testb  $0x1,0x3c(%rdi)
ffffffff812c2e3e:	75 20                	jne    ffffffff812c2e60 <kobject_get+0x34>
			WARN(1, KERN_WARNING "kobject: '%s' (%p): is not "
ffffffff812c2e40:	48 8b 0f             	mov    (%rdi),%rcx
ffffffff812c2e43:	49 89 f8             	mov    %rdi,%r8
ffffffff812c2e46:	48 c7 c2 e2 67 7b 81 	mov    $0xffffffff817b67e2,%rdx
ffffffff812c2e4d:	be 47 02 00 00       	mov    $0x247,%esi
ffffffff812c2e52:	48 c7 c7 a8 66 7b 81 	mov    $0xffffffff817b66a8,%rdi
ffffffff812c2e59:	31 c0                	xor    %eax,%eax
ffffffff812c2e5b:	e8 d6 34 da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
 *
 * Atomically adds @i to @v and returns @i + @v
 */
static inline int atomic_add_return(int i, atomic_t *v)
{
	return i + xadd(&v->counter, i);
ffffffff812c2e60:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c2e65:	f0 0f c1 43 38       	lock xadd %eax,0x38(%rbx)
{
	/* If refcount was 0 before incrementing then we have a race
	 * condition when this kref is freeing by some other thread right now.
	 * In this case one should use kref_get_unless_zero()
	 */
	WARN_ON_ONCE(atomic_inc_return(&kref->refcount) < 2);
ffffffff812c2e6a:	ff c0                	inc    %eax
ffffffff812c2e6c:	ff c8                	dec    %eax
ffffffff812c2e6e:	7f 21                	jg     ffffffff812c2e91 <kobject_get+0x65>
ffffffff812c2e70:	80 3d 6f 44 79 00 00 	cmpb   $0x0,0x79446f(%rip)        # ffffffff81a572e6 <__warned.7682>
ffffffff812c2e77:	75 18                	jne    ffffffff812c2e91 <kobject_get+0x65>
ffffffff812c2e79:	be 2f 00 00 00       	mov    $0x2f,%esi
ffffffff812c2e7e:	48 c7 c7 cc 0c 79 81 	mov    $0xffffffff81790ccc,%rdi
ffffffff812c2e85:	e8 2b 35 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c2e8a:	c6 05 55 44 79 00 01 	movb   $0x1,0x794455(%rip)        # ffffffff81a572e6 <__warned.7682>
			       "initialized, yet kobject_get() is being "
			       "called.\n", kobject_name(kobj), kobj);
		kref_get(&kobj->kref);
	}
	return kobj;
}
ffffffff812c2e91:	48 89 d8             	mov    %rbx,%rax
ffffffff812c2e94:	5a                   	pop    %rdx
ffffffff812c2e95:	5b                   	pop    %rbx
ffffffff812c2e96:	5d                   	pop    %rbp
ffffffff812c2e97:	c3                   	retq   

ffffffff812c2e98 <kzalloc>:
 * kzalloc - allocate memory. The memory is set to zero.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline void *kzalloc(size_t size, gfp_t flags)
{
ffffffff812c2e98:	55                   	push   %rbp
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812c2e99:	81 ce 00 80 00 00    	or     $0x8000,%esi
 * kzalloc - allocate memory. The memory is set to zero.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline void *kzalloc(size_t size, gfp_t flags)
{
ffffffff812c2e9f:	48 89 e5             	mov    %rsp,%rbp
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812c2ea2:	e8 e4 e2 e3 ff       	callq  ffffffff8110118b <__kmalloc>
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline void *kzalloc(size_t size, gfp_t flags)
{
	return kmalloc(size, flags | __GFP_ZERO);
}
ffffffff812c2ea7:	5d                   	pop    %rbp
ffffffff812c2ea8:	c3                   	retq   

ffffffff812c2ea9 <kobject_get_path>:
 * @gfp_mask:	the allocation type used to allocate the path
 *
 * The result must be freed by the caller with kfree().
 */
char *kobject_get_path(struct kobject *kobj, gfp_t gfp_mask)
{
ffffffff812c2ea9:	55                   	push   %rbp
}

static int get_kobj_path_length(struct kobject *kobj)
{
	int length = 1;
	struct kobject *parent = kobj;
ffffffff812c2eaa:	48 89 fa             	mov    %rdi,%rdx
	 * Add 1 to strlen for leading '/' of each level.
	 */
	do {
		if (kobject_name(parent) == NULL)
			return 0;
		length += strlen(kobject_name(parent)) + 1;
ffffffff812c2ead:	49 83 c8 ff          	or     $0xffffffffffffffff,%r8
ffffffff812c2eb1:	31 c0                	xor    %eax,%eax
 * @gfp_mask:	the allocation type used to allocate the path
 *
 * The result must be freed by the caller with kfree().
 */
char *kobject_get_path(struct kobject *kobj, gfp_t gfp_mask)
{
ffffffff812c2eb3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2eb6:	41 57                	push   %r15
ffffffff812c2eb8:	41 56                	push   %r14
ffffffff812c2eba:	41 55                	push   %r13
ffffffff812c2ebc:	41 54                	push   %r12
ffffffff812c2ebe:	49 89 fc             	mov    %rdi,%r12
ffffffff812c2ec1:	53                   	push   %rbx
ffffffff812c2ec2:	51                   	push   %rcx
	return 0;
}

static int get_kobj_path_length(struct kobject *kobj)
{
	int length = 1;
ffffffff812c2ec3:	bb 01 00 00 00       	mov    $0x1,%ebx
ffffffff812c2ec8:	48 8b 3a             	mov    (%rdx),%rdi
	/* walk up the ancestors until we hit the one pointing to the
	 * root.
	 * Add 1 to strlen for leading '/' of each level.
	 */
	do {
		if (kobject_name(parent) == NULL)
ffffffff812c2ecb:	48 85 ff             	test   %rdi,%rdi
ffffffff812c2ece:	75 07                	jne    ffffffff812c2ed7 <kobject_get_path+0x2e>
	char *path;
	int len;

	len = get_kobj_path_length(kobj);
	if (len == 0)
		return NULL;
ffffffff812c2ed0:	31 c0                	xor    %eax,%eax
ffffffff812c2ed2:	e9 8a 00 00 00       	jmpq   ffffffff812c2f61 <kobject_get_path+0xb8>
	 * Add 1 to strlen for leading '/' of each level.
	 */
	do {
		if (kobject_name(parent) == NULL)
			return 0;
		length += strlen(kobject_name(parent)) + 1;
ffffffff812c2ed7:	4c 89 c1             	mov    %r8,%rcx
		parent = parent->parent;
ffffffff812c2eda:	48 8b 52 18          	mov    0x18(%rdx),%rdx
	 * Add 1 to strlen for leading '/' of each level.
	 */
	do {
		if (kobject_name(parent) == NULL)
			return 0;
		length += strlen(kobject_name(parent)) + 1;
ffffffff812c2ede:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c2ee0:	48 f7 d1             	not    %rcx
ffffffff812c2ee3:	01 cb                	add    %ecx,%ebx
		parent = parent->parent;
	} while (parent);
ffffffff812c2ee5:	48 85 d2             	test   %rdx,%rdx
ffffffff812c2ee8:	75 de                	jne    ffffffff812c2ec8 <kobject_get_path+0x1f>
{
	char *path;
	int len;

	len = get_kobj_path_length(kobj);
	if (len == 0)
ffffffff812c2eea:	85 db                	test   %ebx,%ebx
ffffffff812c2eec:	74 e2                	je     ffffffff812c2ed0 <kobject_get_path+0x27>
		return NULL;
	path = kzalloc(len, gfp_mask);
ffffffff812c2eee:	48 63 fb             	movslq %ebx,%rdi
ffffffff812c2ef1:	e8 a2 ff ff ff       	callq  ffffffff812c2e98 <kzalloc>
	if (!path)
ffffffff812c2ef6:	48 85 c0             	test   %rax,%rax
	int len;

	len = get_kobj_path_length(kobj);
	if (len == 0)
		return NULL;
	path = kzalloc(len, gfp_mask);
ffffffff812c2ef9:	49 89 c6             	mov    %rax,%r14
	if (!path)
ffffffff812c2efc:	74 d2                	je     ffffffff812c2ed0 <kobject_get_path+0x27>

static void fill_kobj_path(struct kobject *kobj, char *path, int length)
{
	struct kobject *parent;

	--length;
ffffffff812c2efe:	ff cb                	dec    %ebx
	for (parent = kobj; parent; parent = parent->parent) {
ffffffff812c2f00:	4d 89 e7             	mov    %r12,%r15
		int cur = strlen(kobject_name(parent));
ffffffff812c2f03:	49 83 cd ff          	or     $0xffffffffffffffff,%r13
static void fill_kobj_path(struct kobject *kobj, char *path, int length)
{
	struct kobject *parent;

	--length;
	for (parent = kobj; parent; parent = parent->parent) {
ffffffff812c2f07:	4d 85 ff             	test   %r15,%r15
ffffffff812c2f0a:	74 33                	je     ffffffff812c2f3f <kobject_get_path+0x96>
ffffffff812c2f0c:	49 8b 37             	mov    (%r15),%rsi
		int cur = strlen(kobject_name(parent));
ffffffff812c2f0f:	31 c0                	xor    %eax,%eax
ffffffff812c2f11:	4c 89 e9             	mov    %r13,%rcx
ffffffff812c2f14:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c2f17:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c2f19:	48 f7 d1             	not    %rcx
ffffffff812c2f1c:	48 ff c9             	dec    %rcx
		/* back up enough to print this name with '/' */
		length -= cur;
ffffffff812c2f1f:	29 cb                	sub    %ecx,%ebx
		strncpy(path + length, kobject_name(parent), cur);
ffffffff812c2f21:	48 63 d1             	movslq %ecx,%rdx
ffffffff812c2f24:	48 63 fb             	movslq %ebx,%rdi
		*(path + --length) = '/';
ffffffff812c2f27:	ff cb                	dec    %ebx
	--length;
	for (parent = kobj; parent; parent = parent->parent) {
		int cur = strlen(kobject_name(parent));
		/* back up enough to print this name with '/' */
		length -= cur;
		strncpy(path + length, kobject_name(parent), cur);
ffffffff812c2f29:	4c 01 f7             	add    %r14,%rdi
ffffffff812c2f2c:	e8 db 4c 00 00       	callq  ffffffff812c7c0c <strncpy>
		*(path + --length) = '/';
ffffffff812c2f31:	48 63 c3             	movslq %ebx,%rax
ffffffff812c2f34:	41 c6 04 06 2f       	movb   $0x2f,(%r14,%rax,1)
static void fill_kobj_path(struct kobject *kobj, char *path, int length)
{
	struct kobject *parent;

	--length;
	for (parent = kobj; parent; parent = parent->parent) {
ffffffff812c2f39:	4d 8b 7f 18          	mov    0x18(%r15),%r15
ffffffff812c2f3d:	eb c8                	jmp    ffffffff812c2f07 <kobject_get_path+0x5e>
		length -= cur;
		strncpy(path + length, kobject_name(parent), cur);
		*(path + --length) = '/';
	}

	pr_debug("kobject: '%s' (%p): %s: path = '%s'\n", kobject_name(kobj),
ffffffff812c2f3f:	49 8b 34 24          	mov    (%r12),%rsi
ffffffff812c2f43:	4d 89 f0             	mov    %r14,%r8
ffffffff812c2f46:	48 c7 c1 d8 f0 63 81 	mov    $0xffffffff8163f0d8,%rcx
ffffffff812c2f4d:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c2f50:	48 c7 c7 30 68 7b 81 	mov    $0xffffffff817b6830,%rdi
ffffffff812c2f57:	31 c0                	xor    %eax,%eax
ffffffff812c2f59:	e8 34 00 17 00       	callq  ffffffff81432f92 <printk>
ffffffff812c2f5e:	4c 89 f0             	mov    %r14,%rax
	if (!path)
		return NULL;
	fill_kobj_path(kobj, path, len);

	return path;
}
ffffffff812c2f61:	5a                   	pop    %rdx
ffffffff812c2f62:	5b                   	pop    %rbx
ffffffff812c2f63:	41 5c                	pop    %r12
ffffffff812c2f65:	41 5d                	pop    %r13
ffffffff812c2f67:	41 5e                	pop    %r14
ffffffff812c2f69:	41 5f                	pop    %r15
ffffffff812c2f6b:	5d                   	pop    %rbp
ffffffff812c2f6c:	c3                   	retq   

ffffffff812c2f6d <kobject_set_name_vargs>:
 * @fmt: format string used to build the name
 * @vargs: vargs to format the string.
 */
int kobject_set_name_vargs(struct kobject *kobj, const char *fmt,
				  va_list vargs)
{
ffffffff812c2f6d:	55                   	push   %rbp
ffffffff812c2f6e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2f71:	41 54                	push   %r12
ffffffff812c2f73:	53                   	push   %rbx
	const char *old_name = kobj->name;
ffffffff812c2f74:	4c 8b 27             	mov    (%rdi),%r12
	char *s;

	if (kobj->name && !fmt)
ffffffff812c2f77:	4d 85 e4             	test   %r12,%r12
ffffffff812c2f7a:	74 07                	je     ffffffff812c2f83 <kobject_set_name_vargs+0x16>
		return 0;
ffffffff812c2f7c:	31 c0                	xor    %eax,%eax
				  va_list vargs)
{
	const char *old_name = kobj->name;
	char *s;

	if (kobj->name && !fmt)
ffffffff812c2f7e:	48 85 f6             	test   %rsi,%rsi
ffffffff812c2f81:	74 40                	je     ffffffff812c2fc3 <kobject_set_name_vargs+0x56>
ffffffff812c2f83:	48 89 fb             	mov    %rdi,%rbx
		return 0;

	kobj->name = kvasprintf(GFP_KERNEL, fmt, vargs);
ffffffff812c2f86:	bf d0 00 00 00       	mov    $0xd0,%edi
ffffffff812c2f8b:	e8 d9 9d 00 00       	callq  ffffffff812ccd69 <kvasprintf>
	if (!kobj->name) {
ffffffff812c2f90:	48 85 c0             	test   %rax,%rax
	char *s;

	if (kobj->name && !fmt)
		return 0;

	kobj->name = kvasprintf(GFP_KERNEL, fmt, vargs);
ffffffff812c2f93:	48 89 03             	mov    %rax,(%rbx)
	if (!kobj->name) {
ffffffff812c2f96:	75 0a                	jne    ffffffff812c2fa2 <kobject_set_name_vargs+0x35>
		kobj->name = old_name;
ffffffff812c2f98:	4c 89 23             	mov    %r12,(%rbx)
		return -ENOMEM;
ffffffff812c2f9b:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
ffffffff812c2fa0:	eb 21                	jmp    ffffffff812c2fc3 <kobject_set_name_vargs+0x56>
	}

	/* ewww... some of these buggers have '/' in the name ... */
	while ((s = strchr(kobj->name, '/')))
ffffffff812c2fa2:	48 8b 3b             	mov    (%rbx),%rdi
ffffffff812c2fa5:	be 2f 00 00 00       	mov    $0x2f,%esi
ffffffff812c2faa:	e8 ee 4c 00 00       	callq  ffffffff812c7c9d <strchr>
ffffffff812c2faf:	48 85 c0             	test   %rax,%rax
ffffffff812c2fb2:	74 05                	je     ffffffff812c2fb9 <kobject_set_name_vargs+0x4c>
		s[0] = '!';
ffffffff812c2fb4:	c6 00 21             	movb   $0x21,(%rax)
ffffffff812c2fb7:	eb e9                	jmp    ffffffff812c2fa2 <kobject_set_name_vargs+0x35>

	kfree(old_name);
ffffffff812c2fb9:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c2fbc:	e8 c8 d9 e3 ff       	callq  ffffffff81100989 <kfree>
	return 0;
ffffffff812c2fc1:	31 c0                	xor    %eax,%eax
}
ffffffff812c2fc3:	5b                   	pop    %rbx
ffffffff812c2fc4:	41 5c                	pop    %r12
ffffffff812c2fc6:	5d                   	pop    %rbp
ffffffff812c2fc7:	c3                   	retq   

ffffffff812c2fc8 <kobject_set_name>:
 * This sets the name of the kobject.  If you have already added the
 * kobject to the system, you must call kobject_rename() in order to
 * change the name of the kobject.
 */
int kobject_set_name(struct kobject *kobj, const char *fmt, ...)
{
ffffffff812c2fc8:	55                   	push   %rbp
ffffffff812c2fc9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c2fcc:	48 83 ec 50          	sub    $0x50,%rsp
	va_list vargs;
	int retval;

	va_start(vargs, fmt);
ffffffff812c2fd0:	48 8d 45 10          	lea    0x10(%rbp),%rax
 * This sets the name of the kobject.  If you have already added the
 * kobject to the system, you must call kobject_rename() in order to
 * change the name of the kobject.
 */
int kobject_set_name(struct kobject *kobj, const char *fmt, ...)
{
ffffffff812c2fd4:	48 89 55 e0          	mov    %rdx,-0x20(%rbp)
	va_list vargs;
	int retval;

	va_start(vargs, fmt);
	retval = kobject_set_name_vargs(kobj, fmt, vargs);
ffffffff812c2fd8:	48 8d 55 b8          	lea    -0x48(%rbp),%rdx
 * This sets the name of the kobject.  If you have already added the
 * kobject to the system, you must call kobject_rename() in order to
 * change the name of the kobject.
 */
int kobject_set_name(struct kobject *kobj, const char *fmt, ...)
{
ffffffff812c2fdc:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
ffffffff812c2fe0:	4c 89 45 f0          	mov    %r8,-0x10(%rbp)
	va_list vargs;
	int retval;

	va_start(vargs, fmt);
ffffffff812c2fe4:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812c2fe8:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
 * This sets the name of the kobject.  If you have already added the
 * kobject to the system, you must call kobject_rename() in order to
 * change the name of the kobject.
 */
int kobject_set_name(struct kobject *kobj, const char *fmt, ...)
{
ffffffff812c2fec:	4c 89 4d f8          	mov    %r9,-0x8(%rbp)
	va_list vargs;
	int retval;

	va_start(vargs, fmt);
ffffffff812c2ff0:	c7 45 b8 10 00 00 00 	movl   $0x10,-0x48(%rbp)
ffffffff812c2ff7:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	retval = kobject_set_name_vargs(kobj, fmt, vargs);
ffffffff812c2ffb:	e8 6d ff ff ff       	callq  ffffffff812c2f6d <kobject_set_name_vargs>
	va_end(vargs);

	return retval;
}
ffffffff812c3000:	c9                   	leaveq 
ffffffff812c3001:	c3                   	retq   

ffffffff812c3002 <kobject_create>:
 * The kobject structure returned from here must be cleaned up with a
 * call to kobject_put() and not kfree(), as kobject_init() has
 * already been called on this structure.
 */
struct kobject *kobject_create(void)
{
ffffffff812c3002:	55                   	push   %rbp
	struct kobject *kobj;

	kobj = kzalloc(sizeof(*kobj), GFP_KERNEL);
ffffffff812c3003:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812c3008:	bf 40 00 00 00       	mov    $0x40,%edi
 * The kobject structure returned from here must be cleaned up with a
 * call to kobject_put() and not kfree(), as kobject_init() has
 * already been called on this structure.
 */
struct kobject *kobject_create(void)
{
ffffffff812c300d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3010:	48 83 ec 10          	sub    $0x10,%rsp
	struct kobject *kobj;

	kobj = kzalloc(sizeof(*kobj), GFP_KERNEL);
ffffffff812c3014:	e8 7f fe ff ff       	callq  ffffffff812c2e98 <kzalloc>
	if (!kobj)
ffffffff812c3019:	48 85 c0             	test   %rax,%rax
ffffffff812c301c:	74 17                	je     ffffffff812c3035 <kobject_create+0x33>
		return NULL;

	kobject_init(kobj, &dynamic_kobj_ktype);
ffffffff812c301e:	48 89 c7             	mov    %rax,%rdi
ffffffff812c3021:	48 c7 c6 80 5f a3 81 	mov    $0xffffffff81a35f80,%rsi
ffffffff812c3028:	48 89 45 f8          	mov    %rax,-0x8(%rbp)
ffffffff812c302c:	e8 82 fb ff ff       	callq  ffffffff812c2bb3 <kobject_init>
ffffffff812c3031:	48 8b 45 f8          	mov    -0x8(%rbp),%rax
	return kobj;
}
ffffffff812c3035:	c9                   	leaveq 
ffffffff812c3036:	c3                   	retq   

ffffffff812c3037 <kset_init>:
/**
 * kset_init - initialize a kset for use
 * @k: kset
 */
void kset_init(struct kset *k)
{
ffffffff812c3037:	55                   	push   %rbp
	kset_put(kobj->kset);
}

static void kobject_init_internal(struct kobject *kobj)
{
	if (!kobj)
ffffffff812c3038:	48 83 ff d8          	cmp    $0xffffffffffffffd8,%rdi
/**
 * kset_init - initialize a kset for use
 * @k: kset
 */
void kset_init(struct kset *k)
{
ffffffff812c303c:	48 89 e5             	mov    %rsp,%rbp
	kset_put(kobj->kset);
}

static void kobject_init_internal(struct kobject *kobj)
{
	if (!kobj)
ffffffff812c303f:	74 1f                	je     ffffffff812c3060 <kset_init+0x29>
		return;
	kref_init(&kobj->kref);
	INIT_LIST_HEAD(&kobj->entry);
ffffffff812c3041:	48 8d 47 30          	lea    0x30(%rdi),%rax
 *
 * Atomically sets the value of @v to @i.
 */
static inline void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
ffffffff812c3045:	c7 47 60 01 00 00 00 	movl   $0x1,0x60(%rdi)
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
ffffffff812c304c:	48 89 47 30          	mov    %rax,0x30(%rdi)
	list->prev = list;
ffffffff812c3050:	48 89 47 38          	mov    %rax,0x38(%rdi)
	kobj->state_in_sysfs = 0;
	kobj->state_add_uevent_sent = 0;
	kobj->state_remove_uevent_sent = 0;
ffffffff812c3054:	8a 47 64             	mov    0x64(%rdi),%al
ffffffff812c3057:	83 e0 f1             	and    $0xfffffff1,%eax
	kobj->state_initialized = 1;
ffffffff812c305a:	83 c8 01             	or     $0x1,%eax
ffffffff812c305d:	88 47 64             	mov    %al,0x64(%rdi)
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
ffffffff812c3060:	48 89 3f             	mov    %rdi,(%rdi)
	list->prev = list;
ffffffff812c3063:	48 89 7f 08          	mov    %rdi,0x8(%rdi)
 */
void kset_init(struct kset *k)
{
	kobject_init_internal(&k->kobj);
	INIT_LIST_HEAD(&k->list);
	spin_lock_init(&k->list_lock);
ffffffff812c3067:	48 c7 c2 90 17 bb 81 	mov    $0xffffffff81bb1790,%rdx
ffffffff812c306e:	48 83 c7 10          	add    $0x10,%rdi
ffffffff812c3072:	48 c7 c6 57 68 7b 81 	mov    $0xffffffff817b6857,%rsi
ffffffff812c3079:	e8 f5 cc dc ff       	callq  ffffffff8108fd73 <__raw_spin_lock_init>
}
ffffffff812c307e:	5d                   	pop    %rbp
ffffffff812c307f:	c3                   	retq   

ffffffff812c3080 <kset_find_obj>:
 * Lock kset via @kset->subsys, and iterate over @kset->list,
 * looking for a matching kobject. If matching object is found
 * take a reference and return the object.
 */
struct kobject *kset_find_obj(struct kset *kset, const char *name)
{
ffffffff812c3080:	55                   	push   %rbp
ffffffff812c3081:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3084:	41 56                	push   %r14
ffffffff812c3086:	41 55                	push   %r13
ffffffff812c3088:	41 54                	push   %r12
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c308a:	4c 8d 67 10          	lea    0x10(%rdi),%r12
ffffffff812c308e:	53                   	push   %rbx
ffffffff812c308f:	49 89 fd             	mov    %rdi,%r13
ffffffff812c3092:	49 89 f6             	mov    %rsi,%r14
ffffffff812c3095:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3098:	e8 93 55 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	struct kobject *k;
	struct kobject *ret = NULL;

	spin_lock(&kset->list_lock);

	list_for_each_entry(k, &kset->list, entry) {
ffffffff812c309d:	49 8b 45 00          	mov    0x0(%r13),%rax
ffffffff812c30a1:	48 8d 58 f8          	lea    -0x8(%rax),%rbx
ffffffff812c30a5:	48 8d 43 08          	lea    0x8(%rbx),%rax
ffffffff812c30a9:	49 39 c5             	cmp    %rax,%r13
ffffffff812c30ac:	74 1b                	je     ffffffff812c30c9 <kset_find_obj+0x49>
ffffffff812c30ae:	48 8b 3b             	mov    (%rbx),%rdi
		if (kobject_name(k) && !strcmp(kobject_name(k), name)) {
ffffffff812c30b1:	48 85 ff             	test   %rdi,%rdi
ffffffff812c30b4:	74 29                	je     ffffffff812c30df <kset_find_obj+0x5f>
ffffffff812c30b6:	4c 89 f6             	mov    %r14,%rsi
ffffffff812c30b9:	e8 9a 4b 00 00       	callq  ffffffff812c7c58 <strcmp>
ffffffff812c30be:	85 c0                	test   %eax,%eax
ffffffff812c30c0:	75 1d                	jne    ffffffff812c30df <kset_find_obj+0x5f>
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return ACCESS_ONCE((v)->counter);
ffffffff812c30c2:	8b 53 38             	mov    0x38(%rbx),%edx
static inline int __atomic_add_unless(atomic_t *v, int a, int u)
{
	int c, old;
	c = atomic_read(v);
	for (;;) {
		if (unlikely(c == (u)))
ffffffff812c30c5:	85 d2                	test   %edx,%edx
ffffffff812c30c7:	75 04                	jne    ffffffff812c30cd <kset_find_obj+0x4d>
}

static struct kobject * __must_check kobject_get_unless_zero(struct kobject *kobj)
{
	if (!kref_get_unless_zero(&kobj->kref))
		kobj = NULL;
ffffffff812c30c9:	31 db                	xor    %ebx,%ebx
ffffffff812c30cb:	eb 1c                	jmp    ffffffff812c30e9 <kset_find_obj+0x69>
			break;
		old = atomic_cmpxchg((v), c, c + (a));
ffffffff812c30cd:	8d 4a 01             	lea    0x1(%rdx),%ecx
#define atomic_inc_return(v)  (atomic_add_return(1, v))
#define atomic_dec_return(v)  (atomic_sub_return(1, v))

static inline int atomic_cmpxchg(atomic_t *v, int old, int new)
{
	return cmpxchg(&v->counter, old, new);
ffffffff812c30d0:	89 d0                	mov    %edx,%eax
ffffffff812c30d2:	f0 0f b1 4b 38       	lock cmpxchg %ecx,0x38(%rbx)
	c = atomic_read(v);
	for (;;) {
		if (unlikely(c == (u)))
			break;
		old = atomic_cmpxchg((v), c, c + (a));
		if (likely(old == c))
ffffffff812c30d7:	39 c2                	cmp    %eax,%edx
ffffffff812c30d9:	74 0e                	je     ffffffff812c30e9 <kset_find_obj+0x69>
ffffffff812c30db:	89 c2                	mov    %eax,%edx
ffffffff812c30dd:	eb e6                	jmp    ffffffff812c30c5 <kset_find_obj+0x45>
	struct kobject *k;
	struct kobject *ret = NULL;

	spin_lock(&kset->list_lock);

	list_for_each_entry(k, &kset->list, entry) {
ffffffff812c30df:	48 8b 5b 08          	mov    0x8(%rbx),%rbx
ffffffff812c30e3:	48 83 eb 08          	sub    $0x8,%rbx
ffffffff812c30e7:	eb bc                	jmp    ffffffff812c30a5 <kset_find_obj+0x25>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c30e9:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c30ec:	e8 bb 55 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
		}
	}

	spin_unlock(&kset->list_lock);
	return ret;
}
ffffffff812c30f1:	48 89 d8             	mov    %rbx,%rax
ffffffff812c30f4:	5b                   	pop    %rbx
ffffffff812c30f5:	41 5c                	pop    %r12
ffffffff812c30f7:	41 5d                	pop    %r13
ffffffff812c30f9:	41 5e                	pop    %r14
ffffffff812c30fb:	5d                   	pop    %rbp
ffffffff812c30fc:	c3                   	retq   

ffffffff812c30fd <kobj_ns_type_register>:

static DEFINE_SPINLOCK(kobj_ns_type_lock);
static const struct kobj_ns_type_operations *kobj_ns_ops_tbl[KOBJ_NS_TYPES];

int kobj_ns_type_register(const struct kobj_ns_type_operations *ops)
{
ffffffff812c30fd:	55                   	push   %rbp
ffffffff812c30fe:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3101:	41 55                	push   %r13
ffffffff812c3103:	41 54                	push   %r12
ffffffff812c3105:	53                   	push   %rbx
ffffffff812c3106:	51                   	push   %rcx
ffffffff812c3107:	49 89 fc             	mov    %rdi,%r12
	enum kobj_ns_type type = ops->type;
ffffffff812c310a:	44 8b 2f             	mov    (%rdi),%r13d
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c310d:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi

	error = -EINVAL;
	if (type >= KOBJ_NS_TYPES)
		goto out;

	error = -EINVAL;
ffffffff812c3114:	bb ea ff ff ff       	mov    $0xffffffea,%ebx
ffffffff812c3119:	e8 12 55 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	if (type <= KOBJ_NS_TYPE_NONE)
ffffffff812c311e:	41 ff cd             	dec    %r13d
ffffffff812c3121:	75 18                	jne    ffffffff812c313b <kobj_ns_type_register+0x3e>
		goto out;

	error = -EBUSY;
	if (kobj_ns_ops_tbl[type])
ffffffff812c3123:	48 83 3d 6d e6 8e 00 	cmpq   $0x0,0x8ee66d(%rip)        # ffffffff81bb1798 <__key.17030+0x8>
ffffffff812c312a:	00 

	error = -EINVAL;
	if (type <= KOBJ_NS_TYPE_NONE)
		goto out;

	error = -EBUSY;
ffffffff812c312b:	bb f0 ff ff ff       	mov    $0xfffffff0,%ebx
	if (kobj_ns_ops_tbl[type])
ffffffff812c3130:	75 09                	jne    ffffffff812c313b <kobj_ns_type_register+0x3e>
		goto out;

	error = 0;
	kobj_ns_ops_tbl[type] = ops;
ffffffff812c3132:	4c 89 25 5f e6 8e 00 	mov    %r12,0x8ee65f(%rip)        # ffffffff81bb1798 <__key.17030+0x8>

	error = -EBUSY;
	if (kobj_ns_ops_tbl[type])
		goto out;

	error = 0;
ffffffff812c3139:	31 db                	xor    %ebx,%ebx
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c313b:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3142:	e8 65 55 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	kobj_ns_ops_tbl[type] = ops;

out:
	spin_unlock(&kobj_ns_type_lock);
	return error;
}
ffffffff812c3147:	5a                   	pop    %rdx
ffffffff812c3148:	89 d8                	mov    %ebx,%eax
ffffffff812c314a:	5b                   	pop    %rbx
ffffffff812c314b:	41 5c                	pop    %r12
ffffffff812c314d:	41 5d                	pop    %r13
ffffffff812c314f:	5d                   	pop    %rbp
ffffffff812c3150:	c3                   	retq   

ffffffff812c3151 <kobj_ns_type_registered>:

int kobj_ns_type_registered(enum kobj_ns_type type)
{
ffffffff812c3151:	55                   	push   %rbp
ffffffff812c3152:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3155:	41 54                	push   %r12
ffffffff812c3157:	53                   	push   %rbx
ffffffff812c3158:	41 89 fc             	mov    %edi,%r12d
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c315b:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3162:	e8 c9 54 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	int registered = 0;

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES))
ffffffff812c3167:	41 ff cc             	dec    %r12d
ffffffff812c316a:	75 0f                	jne    ffffffff812c317b <kobj_ns_type_registered+0x2a>
		registered = kobj_ns_ops_tbl[type] != NULL;
ffffffff812c316c:	31 db                	xor    %ebx,%ebx
ffffffff812c316e:	48 83 3d 22 e6 8e 00 	cmpq   $0x0,0x8ee622(%rip)        # ffffffff81bb1798 <__key.17030+0x8>
ffffffff812c3175:	00 
ffffffff812c3176:	0f 95 c3             	setne  %bl
ffffffff812c3179:	eb 02                	jmp    ffffffff812c317d <kobj_ns_type_registered+0x2c>
ffffffff812c317b:	31 db                	xor    %ebx,%ebx
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c317d:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3184:	e8 23 55 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&kobj_ns_type_lock);

	return registered;
}
ffffffff812c3189:	89 d8                	mov    %ebx,%eax
ffffffff812c318b:	5b                   	pop    %rbx
ffffffff812c318c:	41 5c                	pop    %r12
ffffffff812c318e:	5d                   	pop    %rbp
ffffffff812c318f:	c3                   	retq   

ffffffff812c3190 <kobj_child_ns_ops>:

const struct kobj_ns_type_operations *kobj_child_ns_ops(struct kobject *parent)
{
	const struct kobj_ns_type_operations *ops = NULL;
ffffffff812c3190:	31 c0                	xor    %eax,%eax

	if (parent && parent->ktype && parent->ktype->child_ns_type)
ffffffff812c3192:	48 85 ff             	test   %rdi,%rdi
ffffffff812c3195:	74 19                	je     ffffffff812c31b0 <kobj_child_ns_ops+0x20>
ffffffff812c3197:	48 8b 57 28          	mov    0x28(%rdi),%rdx
ffffffff812c319b:	48 85 d2             	test   %rdx,%rdx
ffffffff812c319e:	74 10                	je     ffffffff812c31b0 <kobj_child_ns_ops+0x20>
ffffffff812c31a0:	48 8b 52 18          	mov    0x18(%rdx),%rdx
ffffffff812c31a4:	48 85 d2             	test   %rdx,%rdx
ffffffff812c31a7:	74 07                	je     ffffffff812c31b0 <kobj_child_ns_ops+0x20>

	return registered;
}

const struct kobj_ns_type_operations *kobj_child_ns_ops(struct kobject *parent)
{
ffffffff812c31a9:	55                   	push   %rbp
ffffffff812c31aa:	48 89 e5             	mov    %rsp,%rbp
	const struct kobj_ns_type_operations *ops = NULL;

	if (parent && parent->ktype && parent->ktype->child_ns_type)
		ops = parent->ktype->child_ns_type(parent);
ffffffff812c31ad:	ff d2                	callq  *%rdx

	return ops;
}
ffffffff812c31af:	5d                   	pop    %rbp
ffffffff812c31b0:	c3                   	retq   

ffffffff812c31b1 <kobj_ns_ops>:

const struct kobj_ns_type_operations *kobj_ns_ops(struct kobject *kobj)
{
ffffffff812c31b1:	55                   	push   %rbp
	return kobj_child_ns_ops(kobj->parent);
ffffffff812c31b2:	48 8b 7f 18          	mov    0x18(%rdi),%rdi

	return ops;
}

const struct kobj_ns_type_operations *kobj_ns_ops(struct kobject *kobj)
{
ffffffff812c31b6:	48 89 e5             	mov    %rsp,%rbp
	return kobj_child_ns_ops(kobj->parent);
ffffffff812c31b9:	e8 d2 ff ff ff       	callq  ffffffff812c3190 <kobj_child_ns_ops>
}
ffffffff812c31be:	5d                   	pop    %rbp
ffffffff812c31bf:	c3                   	retq   

ffffffff812c31c0 <kobject_namespace>:
 * Returns namespace tag of @kobj if its parent has namespace ops enabled
 * and thus @kobj should have a namespace tag associated with it.  Returns
 * %NULL otherwise.
 */
const void *kobject_namespace(struct kobject *kobj)
{
ffffffff812c31c0:	55                   	push   %rbp
ffffffff812c31c1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c31c4:	53                   	push   %rbx
ffffffff812c31c5:	51                   	push   %rcx
ffffffff812c31c6:	48 89 fb             	mov    %rdi,%rbx
	const struct kobj_ns_type_operations *ns_ops = kobj_ns_ops(kobj);
ffffffff812c31c9:	e8 e3 ff ff ff       	callq  ffffffff812c31b1 <kobj_ns_ops>
ffffffff812c31ce:	48 89 c2             	mov    %rax,%rdx

	if (!ns_ops || ns_ops->type == KOBJ_NS_TYPE_NONE)
		return NULL;
ffffffff812c31d1:	31 c0                	xor    %eax,%eax
 */
const void *kobject_namespace(struct kobject *kobj)
{
	const struct kobj_ns_type_operations *ns_ops = kobj_ns_ops(kobj);

	if (!ns_ops || ns_ops->type == KOBJ_NS_TYPE_NONE)
ffffffff812c31d3:	48 85 d2             	test   %rdx,%rdx
ffffffff812c31d6:	74 0f                	je     ffffffff812c31e7 <kobject_namespace+0x27>
ffffffff812c31d8:	83 3a 00             	cmpl   $0x0,(%rdx)
ffffffff812c31db:	74 0a                	je     ffffffff812c31e7 <kobject_namespace+0x27>
		return NULL;

	return kobj->ktype->namespace(kobj);
ffffffff812c31dd:	48 8b 43 28          	mov    0x28(%rbx),%rax
ffffffff812c31e1:	48 89 df             	mov    %rbx,%rdi
ffffffff812c31e4:	ff 50 20             	callq  *0x20(%rax)
}
ffffffff812c31e7:	5a                   	pop    %rdx
ffffffff812c31e8:	5b                   	pop    %rbx
ffffffff812c31e9:	5d                   	pop    %rbp
ffffffff812c31ea:	c3                   	retq   

ffffffff812c31eb <kobject_add_internal>:
	kobj->state_initialized = 1;
}


static int kobject_add_internal(struct kobject *kobj)
{
ffffffff812c31eb:	55                   	push   %rbp
	int error = 0;
	struct kobject *parent;

	if (!kobj)
ffffffff812c31ec:	48 85 ff             	test   %rdi,%rdi
	kobj->state_initialized = 1;
}


static int kobject_add_internal(struct kobject *kobj)
{
ffffffff812c31ef:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c31f2:	41 57                	push   %r15
ffffffff812c31f4:	41 56                	push   %r14
ffffffff812c31f6:	41 55                	push   %r13
ffffffff812c31f8:	41 54                	push   %r12
ffffffff812c31fa:	53                   	push   %rbx
ffffffff812c31fb:	41 50                	push   %r8
	int error = 0;
	struct kobject *parent;

	if (!kobj)
ffffffff812c31fd:	0f 84 35 01 00 00    	je     ffffffff812c3338 <kobject_add_internal+0x14d>
		return -ENOENT;

	if (!kobj->name || !kobj->name[0]) {
ffffffff812c3203:	48 8b 07             	mov    (%rdi),%rax
ffffffff812c3206:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c3209:	48 85 c0             	test   %rax,%rax
ffffffff812c320c:	74 05                	je     ffffffff812c3213 <kobject_add_internal+0x28>
ffffffff812c320e:	80 38 00             	cmpb   $0x0,(%rax)
ffffffff812c3211:	75 28                	jne    ffffffff812c323b <kobject_add_internal+0x50>
		WARN(1, "kobject: (%p): attempted to be registered with empty "
ffffffff812c3213:	48 89 d9             	mov    %rbx,%rcx
ffffffff812c3216:	48 c7 c2 7d 68 7b 81 	mov    $0xffffffff817b687d,%rdx
ffffffff812c321d:	be d2 00 00 00       	mov    $0xd2,%esi
ffffffff812c3222:	48 c7 c7 a8 66 7b 81 	mov    $0xffffffff817b66a8,%rdi
ffffffff812c3229:	31 c0                	xor    %eax,%eax
			 "name!\n", kobj);
		return -EINVAL;
ffffffff812c322b:	41 bc ea ff ff ff    	mov    $0xffffffea,%r12d

	if (!kobj)
		return -ENOENT;

	if (!kobj->name || !kobj->name[0]) {
		WARN(1, "kobject: (%p): attempted to be registered with empty "
ffffffff812c3231:	e8 00 31 da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
			 "name!\n", kobj);
		return -EINVAL;
ffffffff812c3236:	e9 39 02 00 00       	jmpq   ffffffff812c3474 <kobject_add_internal+0x289>
	}

	parent = kobject_get(kobj->parent);
ffffffff812c323b:	48 8b 7f 18          	mov    0x18(%rdi),%rdi
ffffffff812c323f:	e8 e8 fb ff ff       	callq  ffffffff812c2e2c <kobject_get>

	/* join kset if set, use it as parent if we do not already have one */
	if (kobj->kset) {
ffffffff812c3244:	48 8b 7b 20          	mov    0x20(%rbx),%rdi
		WARN(1, "kobject: (%p): attempted to be registered with empty "
			 "name!\n", kobj);
		return -EINVAL;
	}

	parent = kobject_get(kobj->parent);
ffffffff812c3248:	49 89 c5             	mov    %rax,%r13

	/* join kset if set, use it as parent if we do not already have one */
	if (kobj->kset) {
ffffffff812c324b:	48 85 ff             	test   %rdi,%rdi
ffffffff812c324e:	74 52                	je     ffffffff812c32a2 <kobject_add_internal+0xb7>
		if (!parent)
ffffffff812c3250:	48 85 c0             	test   %rax,%rax
ffffffff812c3253:	75 0c                	jne    ffffffff812c3261 <kobject_add_internal+0x76>
			parent = kobject_get(&kobj->kset->kobj);
ffffffff812c3255:	48 83 c7 28          	add    $0x28,%rdi
ffffffff812c3259:	e8 ce fb ff ff       	callq  ffffffff812c2e2c <kobject_get>
ffffffff812c325e:	49 89 c5             	mov    %rax,%r13
EXPORT_SYMBOL_GPL(kobject_get_path);

/* add the kobject to its kset's list */
static void kobj_kset_join(struct kobject *kobj)
{
	if (!kobj->kset)
ffffffff812c3261:	48 8b 7b 20          	mov    0x20(%rbx),%rdi
ffffffff812c3265:	48 85 ff             	test   %rdi,%rdi
ffffffff812c3268:	74 34                	je     ffffffff812c329e <kobject_add_internal+0xb3>
	return kobj ? container_of(kobj, struct kset, kobj) : NULL;
}

static inline struct kset *kset_get(struct kset *k)
{
	return k ? to_kset(kobject_get(&k->kobj)) : NULL;
ffffffff812c326a:	48 83 c7 28          	add    $0x28,%rdi
ffffffff812c326e:	e8 b9 fb ff ff       	callq  ffffffff812c2e2c <kobject_get>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c3273:	48 8b 43 20          	mov    0x20(%rbx),%rax
ffffffff812c3277:	48 8d 78 10          	lea    0x10(%rax),%rdi
ffffffff812c327b:	e8 b0 53 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
		return;

	kset_get(kobj->kset);
	spin_lock(&kobj->kset->list_lock);
	list_add_tail(&kobj->entry, &kobj->kset->list);
ffffffff812c3280:	48 8b 53 20          	mov    0x20(%rbx),%rdx
ffffffff812c3284:	48 8d 7b 08          	lea    0x8(%rbx),%rdi
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
ffffffff812c3288:	48 8b 72 08          	mov    0x8(%rdx),%rsi
ffffffff812c328c:	e8 d9 1b 01 00       	callq  ffffffff812d4e6a <__list_add>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c3291:	48 8b 43 20          	mov    0x20(%rbx),%rax
ffffffff812c3295:	48 8d 78 10          	lea    0x10(%rax),%rdi
ffffffff812c3299:	e8 0e 54 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	/* join kset if set, use it as parent if we do not already have one */
	if (kobj->kset) {
		if (!parent)
			parent = kobject_get(&kobj->kset->kobj);
		kobj_kset_join(kobj);
		kobj->parent = parent;
ffffffff812c329e:	4c 89 6b 18          	mov    %r13,0x18(%rbx)
	}

	pr_debug("kobject: '%s' (%p): %s: parent: '%s', set: '%s'\n",
ffffffff812c32a2:	48 8b 43 20          	mov    0x20(%rbx),%rax
ffffffff812c32a6:	49 c7 c1 6f 68 7b 81 	mov    $0xffffffff817b686f,%r9
ffffffff812c32ad:	48 85 c0             	test   %rax,%rax
ffffffff812c32b0:	74 04                	je     ffffffff812c32b6 <kobject_add_internal+0xcb>
ffffffff812c32b2:	4c 8b 48 28          	mov    0x28(%rax),%r9
ffffffff812c32b6:	4d 85 ed             	test   %r13,%r13
ffffffff812c32b9:	49 c7 c0 6f 68 7b 81 	mov    $0xffffffff817b686f,%r8
ffffffff812c32c0:	74 04                	je     ffffffff812c32c6 <kobject_add_internal+0xdb>
ffffffff812c32c2:	4d 8b 45 00          	mov    0x0(%r13),%r8
ffffffff812c32c6:	48 8b 33             	mov    (%rbx),%rsi
ffffffff812c32c9:	48 c7 c1 c0 f0 63 81 	mov    $0xffffffff8163f0c0,%rcx
ffffffff812c32d0:	48 89 da             	mov    %rbx,%rdx
ffffffff812c32d3:	48 c7 c7 b9 68 7b 81 	mov    $0xffffffff817b68b9,%rdi
ffffffff812c32da:	31 c0                	xor    %eax,%eax
ffffffff812c32dc:	e8 b1 fc 16 00       	callq  ffffffff81432f92 <printk>
static int create_dir(struct kobject *kobj)
{
	const struct kobj_ns_type_operations *ops;
	int error;

	error = sysfs_create_dir_ns(kobj, kobject_namespace(kobj));
ffffffff812c32e1:	48 89 df             	mov    %rbx,%rdi
ffffffff812c32e4:	e8 d7 fe ff ff       	callq  ffffffff812c31c0 <kobject_namespace>
ffffffff812c32e9:	48 89 df             	mov    %rbx,%rdi
ffffffff812c32ec:	48 89 c6             	mov    %rax,%rsi
ffffffff812c32ef:	e8 5b e1 e8 ff       	callq  ffffffff8115144f <sysfs_create_dir_ns>
	if (error)
ffffffff812c32f4:	85 c0                	test   %eax,%eax
ffffffff812c32f6:	41 89 c4             	mov    %eax,%r12d
ffffffff812c32f9:	0f 85 f6 00 00 00    	jne    ffffffff812c33f5 <kobject_add_internal+0x20a>
ffffffff812c32ff:	4c 8b 73 28          	mov    0x28(%rbx),%r14
	struct kobj_type *t = get_ktype(kobj);
	struct attribute *attr;
	int error = 0;
	int i;

	if (t && t->default_attrs) {
ffffffff812c3303:	4d 85 f6             	test   %r14,%r14
ffffffff812c3306:	74 48                	je     ffffffff812c3350 <kobject_add_internal+0x165>
ffffffff812c3308:	45 31 ff             	xor    %r15d,%r15d
ffffffff812c330b:	49 83 7e 10 00       	cmpq   $0x0,0x10(%r14)
ffffffff812c3310:	74 3e                	je     ffffffff812c3350 <kobject_add_internal+0x165>
		for (i = 0; (attr = t->default_attrs[i]) != NULL; i++) {
ffffffff812c3312:	49 8b 56 10          	mov    0x10(%r14),%rdx
ffffffff812c3316:	49 63 c7             	movslq %r15d,%rax
ffffffff812c3319:	48 8b 34 c2          	mov    (%rdx,%rax,8),%rsi
ffffffff812c331d:	48 85 f6             	test   %rsi,%rsi
ffffffff812c3320:	74 2e                	je     ffffffff812c3350 <kobject_add_internal+0x165>
#endif /* CONFIG_SYSFS */

static inline int __must_check sysfs_create_file(struct kobject *kobj,
						 const struct attribute *attr)
{
	return sysfs_create_file_ns(kobj, attr, NULL);
ffffffff812c3322:	31 d2                	xor    %edx,%edx
ffffffff812c3324:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3327:	e8 ea de e8 ff       	callq  ffffffff81151216 <sysfs_create_file_ns>
			error = sysfs_create_file(kobj, attr);
			if (error)
ffffffff812c332c:	85 c0                	test   %eax,%eax
ffffffff812c332e:	41 89 c4             	mov    %eax,%r12d
ffffffff812c3331:	75 10                	jne    ffffffff812c3343 <kobject_add_internal+0x158>
	struct attribute *attr;
	int error = 0;
	int i;

	if (t && t->default_attrs) {
		for (i = 0; (attr = t->default_attrs[i]) != NULL; i++) {
ffffffff812c3333:	41 ff c7             	inc    %r15d
ffffffff812c3336:	eb da                	jmp    ffffffff812c3312 <kobject_add_internal+0x127>
{
	int error = 0;
	struct kobject *parent;

	if (!kobj)
		return -ENOENT;
ffffffff812c3338:	41 bc fe ff ff ff    	mov    $0xfffffffe,%r12d
ffffffff812c333e:	e9 31 01 00 00       	jmpq   ffffffff812c3474 <kobject_add_internal+0x289>
	if (error)
		return error;

	error = populate_dir(kobj);
	if (error) {
		sysfs_remove_dir(kobj);
ffffffff812c3343:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3346:	e8 7c e1 e8 ff       	callq  ffffffff811514c7 <sysfs_remove_dir>
ffffffff812c334b:	e9 a5 00 00 00       	jmpq   ffffffff812c33f5 <kobject_add_internal+0x20a>
	return kernfs_find_and_get(parent, name);
}

static inline struct kernfs_node *sysfs_get(struct kernfs_node *kn)
{
	kernfs_get(kn);
ffffffff812c3350:	48 8b 7b 30          	mov    0x30(%rbx),%rdi
ffffffff812c3354:	e8 90 b4 e8 ff       	callq  ffffffff8114e7e9 <kernfs_get>

	/*
	 * If @kobj has ns_ops, its children need to be filtered based on
	 * their namespace tags.  Enable namespace support on @kobj->sd.
	 */
	ops = kobj_child_ns_ops(kobj);
ffffffff812c3359:	48 89 df             	mov    %rbx,%rdi
ffffffff812c335c:	e8 2f fe ff ff       	callq  ffffffff812c3190 <kobj_child_ns_ops>
	if (ops) {
ffffffff812c3361:	48 85 c0             	test   %rax,%rax
ffffffff812c3364:	0f 84 82 00 00 00    	je     ffffffff812c33ec <kobject_add_internal+0x201>
		BUG_ON(ops->type <= KOBJ_NS_TYPE_NONE);
ffffffff812c336a:	8b 00                	mov    (%rax),%eax
ffffffff812c336c:	85 c0                	test   %eax,%eax
ffffffff812c336e:	75 02                	jne    ffffffff812c3372 <kobject_add_internal+0x187>
ffffffff812c3370:	0f 0b                	ud2    
		BUG_ON(ops->type >= KOBJ_NS_TYPES);
ffffffff812c3372:	83 f8 01             	cmp    $0x1,%eax
ffffffff812c3375:	76 02                	jbe    ffffffff812c3379 <kobject_add_internal+0x18e>
ffffffff812c3377:	0f 0b                	ud2    
		BUG_ON(!kobj_ns_type_registered(ops->type));
ffffffff812c3379:	bf 01 00 00 00       	mov    $0x1,%edi
ffffffff812c337e:	e8 ce fd ff ff       	callq  ffffffff812c3151 <kobj_ns_type_registered>
ffffffff812c3383:	85 c0                	test   %eax,%eax
ffffffff812c3385:	75 02                	jne    ffffffff812c3389 <kobject_add_internal+0x19e>
ffffffff812c3387:	0f 0b                	ud2    

		sysfs_enable_ns(kobj->sd);
ffffffff812c3389:	4c 8b 63 30          	mov    0x30(%rbx),%r12
 * under it.  All children of @kn must have non-NULL namespace tags and
 * only the ones which match the super_block's tag will be visible.
 */
static inline void kernfs_enable_ns(struct kernfs_node *kn)
{
	WARN_ON_ONCE(kernfs_type(kn) != KERNFS_DIR);
ffffffff812c338d:	66 41 8b 44 24 68    	mov    0x68(%r12),%ax
ffffffff812c3393:	83 e0 0f             	and    $0xf,%eax
ffffffff812c3396:	66 ff c8             	dec    %ax
ffffffff812c3399:	74 21                	je     ffffffff812c33bc <kobject_add_internal+0x1d1>
ffffffff812c339b:	80 3d 46 3f 79 00 00 	cmpb   $0x0,0x793f46(%rip)        # ffffffff81a572e8 <__warned.7029>
ffffffff812c33a2:	75 18                	jne    ffffffff812c33bc <kobject_add_internal+0x1d1>
ffffffff812c33a4:	be fc 00 00 00       	mov    $0xfc,%esi
ffffffff812c33a9:	48 c7 c7 ec 68 7b 81 	mov    $0xffffffff817b68ec,%rdi
ffffffff812c33b0:	e8 00 30 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c33b5:	c6 05 2c 3f 79 00 01 	movb   $0x1,0x793f2c(%rip)        # ffffffff81a572e8 <__warned.7029>
	WARN_ON_ONCE(!RB_EMPTY_ROOT(&kn->dir.children));
ffffffff812c33bc:	49 83 7c 24 48 00    	cmpq   $0x0,0x48(%r12)
ffffffff812c33c2:	74 21                	je     ffffffff812c33e5 <kobject_add_internal+0x1fa>
ffffffff812c33c4:	80 3d 1c 3f 79 00 00 	cmpb   $0x0,0x793f1c(%rip)        # ffffffff81a572e7 <__warned.7034>
ffffffff812c33cb:	75 18                	jne    ffffffff812c33e5 <kobject_add_internal+0x1fa>
ffffffff812c33cd:	be fd 00 00 00       	mov    $0xfd,%esi
ffffffff812c33d2:	48 c7 c7 ec 68 7b 81 	mov    $0xffffffff817b68ec,%rdi
ffffffff812c33d9:	e8 d7 2f da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c33de:	c6 05 02 3f 79 00 01 	movb   $0x1,0x793f02(%rip)        # ffffffff81a572e7 <__warned.7034>
	kn->flags |= KERNFS_NS;
ffffffff812c33e5:	66 41 83 4c 24 68 20 	orw    $0x20,0x68(%r12)
		else
			WARN(1, "%s failed for %s (error: %d parent: %s)\n",
			     __func__, kobject_name(kobj), error,
			     parent ? kobject_name(parent) : "'none'");
	} else
		kobj->state_in_sysfs = 1;
ffffffff812c33ec:	80 4b 3c 02          	orb    $0x2,0x3c(%rbx)
ffffffff812c33f0:	45 31 e4             	xor    %r12d,%r12d
ffffffff812c33f3:	eb 7f                	jmp    ffffffff812c3474 <kobject_add_internal+0x289>
		 parent ? kobject_name(parent) : "<NULL>",
		 kobj->kset ? kobject_name(&kobj->kset->kobj) : "<NULL>");

	error = create_dir(kobj);
	if (error) {
		kobj_kset_leave(kobj);
ffffffff812c33f5:	48 89 df             	mov    %rbx,%rdi
ffffffff812c33f8:	e8 83 f8 ff ff       	callq  ffffffff812c2c80 <kobj_kset_leave>
		kobject_put(parent);
ffffffff812c33fd:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c3400:	e8 31 f8 ff ff       	callq  ffffffff812c2c36 <kobject_put>
		kobj->parent = NULL;

		/* be noisy on error issues */
		if (error == -EEXIST)
ffffffff812c3405:	41 83 fc ef          	cmp    $0xffffffef,%r12d

	error = create_dir(kobj);
	if (error) {
		kobj_kset_leave(kobj);
		kobject_put(parent);
		kobj->parent = NULL;
ffffffff812c3409:	48 c7 43 18 00 00 00 	movq   $0x0,0x18(%rbx)
ffffffff812c3410:	00 

		/* be noisy on error issues */
		if (error == -EEXIST)
ffffffff812c3411:	75 26                	jne    ffffffff812c3439 <kobject_add_internal+0x24e>
			WARN(1, "%s failed for %s with "
ffffffff812c3413:	4c 8b 03             	mov    (%rbx),%r8
ffffffff812c3416:	48 c7 c1 c0 f0 63 81 	mov    $0xffffffff8163f0c0,%rcx
ffffffff812c341d:	48 c7 c2 03 69 7b 81 	mov    $0xffffffff817b6903,%rdx
ffffffff812c3424:	be f0 00 00 00       	mov    $0xf0,%esi
ffffffff812c3429:	48 c7 c7 a8 66 7b 81 	mov    $0xffffffff817b66a8,%rdi
ffffffff812c3430:	31 c0                	xor    %eax,%eax
ffffffff812c3432:	e8 ff 2e da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
ffffffff812c3437:	eb 3b                	jmp    ffffffff812c3474 <kobject_add_internal+0x289>
			     "-EEXIST, don't try to register things with "
			     "the same name in the same directory.\n",
			     __func__, kobject_name(kobj));
		else
			WARN(1, "%s failed for %s (error: %d parent: %s)\n",
ffffffff812c3439:	4d 85 ed             	test   %r13,%r13
ffffffff812c343c:	48 c7 c0 76 68 7b 81 	mov    $0xffffffff817b6876,%rax
ffffffff812c3443:	74 04                	je     ffffffff812c3449 <kobject_add_internal+0x25e>
ffffffff812c3445:	49 8b 45 00          	mov    0x0(%r13),%rax
ffffffff812c3449:	52                   	push   %rdx
ffffffff812c344a:	50                   	push   %rax
ffffffff812c344b:	48 c7 c1 c0 f0 63 81 	mov    $0xffffffff8163f0c0,%rcx
ffffffff812c3452:	4c 8b 03             	mov    (%rbx),%r8
ffffffff812c3455:	be f4 00 00 00       	mov    $0xf4,%esi
ffffffff812c345a:	45 89 e1             	mov    %r12d,%r9d
ffffffff812c345d:	48 c7 c2 6a 69 7b 81 	mov    $0xffffffff817b696a,%rdx
ffffffff812c3464:	48 c7 c7 a8 66 7b 81 	mov    $0xffffffff817b66a8,%rdi
ffffffff812c346b:	31 c0                	xor    %eax,%eax
ffffffff812c346d:	e8 c4 2e da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
ffffffff812c3472:	59                   	pop    %rcx
ffffffff812c3473:	5e                   	pop    %rsi
			     parent ? kobject_name(parent) : "'none'");
	} else
		kobj->state_in_sysfs = 1;

	return error;
}
ffffffff812c3474:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812c3478:	44 89 e0             	mov    %r12d,%eax
ffffffff812c347b:	5b                   	pop    %rbx
ffffffff812c347c:	41 5c                	pop    %r12
ffffffff812c347e:	41 5d                	pop    %r13
ffffffff812c3480:	41 5e                	pop    %r14
ffffffff812c3482:	41 5f                	pop    %r15
ffffffff812c3484:	5d                   	pop    %rbp
ffffffff812c3485:	c3                   	retq   

ffffffff812c3486 <kobject_add>:
 * kobject_uevent() with the UEVENT_ADD parameter to ensure that
 * userspace is properly notified of this kobject's creation.
 */
int kobject_add(struct kobject *kobj, struct kobject *parent,
		const char *fmt, ...)
{
ffffffff812c3486:	55                   	push   %rbp
	va_list args;
	int retval;

	if (!kobj)
		return -EINVAL;
ffffffff812c3487:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
 * kobject_uevent() with the UEVENT_ADD parameter to ensure that
 * userspace is properly notified of this kobject's creation.
 */
int kobject_add(struct kobject *kobj, struct kobject *parent,
		const char *fmt, ...)
{
ffffffff812c348c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c348f:	41 55                	push   %r13
ffffffff812c3491:	41 54                	push   %r12
ffffffff812c3493:	53                   	push   %rbx
ffffffff812c3494:	48 83 ec 58          	sub    $0x58,%rsp
	va_list args;
	int retval;

	if (!kobj)
ffffffff812c3498:	48 85 ff             	test   %rdi,%rdi
 * kobject_uevent() with the UEVENT_ADD parameter to ensure that
 * userspace is properly notified of this kobject's creation.
 */
int kobject_add(struct kobject *kobj, struct kobject *parent,
		const char *fmt, ...)
{
ffffffff812c349b:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
ffffffff812c349f:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
ffffffff812c34a3:	4c 89 4d d8          	mov    %r9,-0x28(%rbp)
	va_list args;
	int retval;

	if (!kobj)
ffffffff812c34a7:	74 75                	je     ffffffff812c351e <kobject_add+0x98>
		return -EINVAL;

	if (!kobj->state_initialized) {
ffffffff812c34a9:	f6 47 3c 01          	testb  $0x1,0x3c(%rdi)
ffffffff812c34ad:	49 89 f5             	mov    %rsi,%r13
ffffffff812c34b0:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c34b3:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c34b6:	75 20                	jne    ffffffff812c34d8 <kobject_add+0x52>
		printk(KERN_ERR "kobject '%s' (%p): tried to add an "
ffffffff812c34b8:	48 8b 37             	mov    (%rdi),%rsi
ffffffff812c34bb:	48 89 fa             	mov    %rdi,%rdx
ffffffff812c34be:	31 c0                	xor    %eax,%eax
ffffffff812c34c0:	48 c7 c7 93 69 7b 81 	mov    $0xffffffff817b6993,%rdi
ffffffff812c34c7:	e8 c6 fa 16 00       	callq  ffffffff81432f92 <printk>
		       "uninitialized object, something is seriously wrong.\n",
		       kobject_name(kobj), kobj);
		dump_stack();
ffffffff812c34cc:	e8 98 15 17 00       	callq  ffffffff81434a69 <dump_stack>
		return -EINVAL;
ffffffff812c34d1:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
ffffffff812c34d6:	eb 46                	jmp    ffffffff812c351e <kobject_add+0x98>
	}
	va_start(args, fmt);
ffffffff812c34d8:	48 8d 45 10          	lea    0x10(%rbp),%rax
static int kobject_add_varg(struct kobject *kobj, struct kobject *parent,
			    const char *fmt, va_list vargs)
{
	int retval;

	retval = kobject_set_name_vargs(kobj, fmt, vargs);
ffffffff812c34dc:	48 8d 55 98          	lea    -0x68(%rbp),%rdx
		       "uninitialized object, something is seriously wrong.\n",
		       kobject_name(kobj), kobj);
		dump_stack();
		return -EINVAL;
	}
	va_start(args, fmt);
ffffffff812c34e0:	c7 45 98 18 00 00 00 	movl   $0x18,-0x68(%rbp)
ffffffff812c34e7:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
ffffffff812c34eb:	48 8d 45 b0          	lea    -0x50(%rbp),%rax
ffffffff812c34ef:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
static int kobject_add_varg(struct kobject *kobj, struct kobject *parent,
			    const char *fmt, va_list vargs)
{
	int retval;

	retval = kobject_set_name_vargs(kobj, fmt, vargs);
ffffffff812c34f3:	e8 75 fa ff ff       	callq  ffffffff812c2f6d <kobject_set_name_vargs>
	if (retval) {
ffffffff812c34f8:	85 c0                	test   %eax,%eax
static int kobject_add_varg(struct kobject *kobj, struct kobject *parent,
			    const char *fmt, va_list vargs)
{
	int retval;

	retval = kobject_set_name_vargs(kobj, fmt, vargs);
ffffffff812c34fa:	41 89 c4             	mov    %eax,%r12d
	if (retval) {
ffffffff812c34fd:	74 13                	je     ffffffff812c3512 <kobject_add+0x8c>
		printk(KERN_ERR "kobject: can not set name properly!\n");
ffffffff812c34ff:	48 c7 c7 ed 69 7b 81 	mov    $0xffffffff817b69ed,%rdi
ffffffff812c3506:	31 c0                	xor    %eax,%eax
ffffffff812c3508:	e8 85 fa 16 00       	callq  ffffffff81432f92 <printk>
ffffffff812c350d:	44 89 e0             	mov    %r12d,%eax
ffffffff812c3510:	eb 0c                	jmp    ffffffff812c351e <kobject_add+0x98>
		return retval;
	}
	kobj->parent = parent;
ffffffff812c3512:	4c 89 6b 18          	mov    %r13,0x18(%rbx)
	return kobject_add_internal(kobj);
ffffffff812c3516:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3519:	e8 cd fc ff ff       	callq  ffffffff812c31eb <kobject_add_internal>
	va_start(args, fmt);
	retval = kobject_add_varg(kobj, parent, fmt, args);
	va_end(args);

	return retval;
}
ffffffff812c351e:	48 83 c4 58          	add    $0x58,%rsp
ffffffff812c3522:	5b                   	pop    %rbx
ffffffff812c3523:	41 5c                	pop    %r12
ffffffff812c3525:	41 5d                	pop    %r13
ffffffff812c3527:	5d                   	pop    %rbp
ffffffff812c3528:	c3                   	retq   

ffffffff812c3529 <kobject_create_and_add>:
 * it is no longer being used.
 *
 * If the kobject was not able to be created, NULL will be returned.
 */
struct kobject *kobject_create_and_add(const char *name, struct kobject *parent)
{
ffffffff812c3529:	55                   	push   %rbp
ffffffff812c352a:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c352d:	41 55                	push   %r13
ffffffff812c352f:	41 54                	push   %r12
ffffffff812c3531:	53                   	push   %rbx
ffffffff812c3532:	49 89 fd             	mov    %rdi,%r13
	struct kobject *kobj;
	int retval;

	kobj = kobject_create();
	if (!kobj)
		return NULL;
ffffffff812c3535:	31 db                	xor    %ebx,%ebx
 * it is no longer being used.
 *
 * If the kobject was not able to be created, NULL will be returned.
 */
struct kobject *kobject_create_and_add(const char *name, struct kobject *parent)
{
ffffffff812c3537:	48 83 ec 18          	sub    $0x18,%rsp
ffffffff812c353b:	48 89 75 d8          	mov    %rsi,-0x28(%rbp)
	struct kobject *kobj;
	int retval;

	kobj = kobject_create();
ffffffff812c353f:	e8 be fa ff ff       	callq  ffffffff812c3002 <kobject_create>
	if (!kobj)
ffffffff812c3544:	48 85 c0             	test   %rax,%rax
ffffffff812c3547:	74 43                	je     ffffffff812c358c <kobject_create_and_add+0x63>
		return NULL;

	retval = kobject_add(kobj, parent, "%s", name);
ffffffff812c3549:	48 8b 75 d8          	mov    -0x28(%rbp),%rsi
ffffffff812c354d:	49 89 c4             	mov    %rax,%r12
ffffffff812c3550:	48 89 c7             	mov    %rax,%rdi
ffffffff812c3553:	4c 89 e9             	mov    %r13,%rcx
ffffffff812c3556:	31 c0                	xor    %eax,%eax
ffffffff812c3558:	48 c7 c2 0f b6 7c 81 	mov    $0xffffffff817cb60f,%rdx
ffffffff812c355f:	4c 89 e3             	mov    %r12,%rbx
ffffffff812c3562:	e8 1f ff ff ff       	callq  ffffffff812c3486 <kobject_add>
	if (retval) {
ffffffff812c3567:	85 c0                	test   %eax,%eax
ffffffff812c3569:	74 21                	je     ffffffff812c358c <kobject_create_and_add+0x63>
		printk(KERN_WARNING "%s: kobject_add error: %d\n",
ffffffff812c356b:	89 c2                	mov    %eax,%edx
ffffffff812c356d:	48 c7 c6 70 f0 63 81 	mov    $0xffffffff8163f070,%rsi
ffffffff812c3574:	48 c7 c7 14 6a 7b 81 	mov    $0xffffffff817b6a14,%rdi
ffffffff812c357b:	31 c0                	xor    %eax,%eax
		       __func__, retval);
		kobject_put(kobj);
		kobj = NULL;
ffffffff812c357d:	31 db                	xor    %ebx,%ebx
	if (!kobj)
		return NULL;

	retval = kobject_add(kobj, parent, "%s", name);
	if (retval) {
		printk(KERN_WARNING "%s: kobject_add error: %d\n",
ffffffff812c357f:	e8 0e fa 16 00       	callq  ffffffff81432f92 <printk>
		       __func__, retval);
		kobject_put(kobj);
ffffffff812c3584:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3587:	e8 aa f6 ff ff       	callq  ffffffff812c2c36 <kobject_put>
		kobj = NULL;
	}
	return kobj;
}
ffffffff812c358c:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c3590:	48 89 d8             	mov    %rbx,%rax
ffffffff812c3593:	5b                   	pop    %rbx
ffffffff812c3594:	41 5c                	pop    %r12
ffffffff812c3596:	41 5d                	pop    %r13
ffffffff812c3598:	5d                   	pop    %rbp
ffffffff812c3599:	c3                   	retq   

ffffffff812c359a <kset_register>:
/**
 * kset_register - initialize and add a kset.
 * @k: kset.
 */
int kset_register(struct kset *k)
{
ffffffff812c359a:	55                   	push   %rbp
	int err;

	if (!k)
ffffffff812c359b:	48 85 ff             	test   %rdi,%rdi
/**
 * kset_register - initialize and add a kset.
 * @k: kset.
 */
int kset_register(struct kset *k)
{
ffffffff812c359e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c35a1:	41 54                	push   %r12
	int err;

	if (!k)
		return -EINVAL;
ffffffff812c35a3:	41 bc ea ff ff ff    	mov    $0xffffffea,%r12d
/**
 * kset_register - initialize and add a kset.
 * @k: kset.
 */
int kset_register(struct kset *k)
{
ffffffff812c35a9:	53                   	push   %rbx
	int err;

	if (!k)
ffffffff812c35aa:	74 25                	je     ffffffff812c35d1 <kset_register+0x37>
ffffffff812c35ac:	48 89 fb             	mov    %rdi,%rbx
		return -EINVAL;

	kset_init(k);
	err = kobject_add_internal(&k->kobj);
ffffffff812c35af:	48 83 c3 28          	add    $0x28,%rbx
	int err;

	if (!k)
		return -EINVAL;

	kset_init(k);
ffffffff812c35b3:	e8 7f fa ff ff       	callq  ffffffff812c3037 <kset_init>
	err = kobject_add_internal(&k->kobj);
ffffffff812c35b8:	48 89 df             	mov    %rbx,%rdi
ffffffff812c35bb:	e8 2b fc ff ff       	callq  ffffffff812c31eb <kobject_add_internal>
	if (err)
ffffffff812c35c0:	85 c0                	test   %eax,%eax

	if (!k)
		return -EINVAL;

	kset_init(k);
	err = kobject_add_internal(&k->kobj);
ffffffff812c35c2:	41 89 c4             	mov    %eax,%r12d
	if (err)
ffffffff812c35c5:	75 0a                	jne    ffffffff812c35d1 <kset_register+0x37>
		return err;
	kobject_uevent(&k->kobj, KOBJ_ADD);
ffffffff812c35c7:	31 f6                	xor    %esi,%esi
ffffffff812c35c9:	48 89 df             	mov    %rbx,%rdi
ffffffff812c35cc:	e8 ee 0b 00 00       	callq  ffffffff812c41bf <kobject_uevent>
	return 0;
}
ffffffff812c35d1:	44 89 e0             	mov    %r12d,%eax
ffffffff812c35d4:	5b                   	pop    %rbx
ffffffff812c35d5:	41 5c                	pop    %r12
ffffffff812c35d7:	5d                   	pop    %rbp
ffffffff812c35d8:	c3                   	retq   

ffffffff812c35d9 <kset_create_and_add>:
 * If the kset was not able to be created, NULL will be returned.
 */
struct kset *kset_create_and_add(const char *name,
				 const struct kset_uevent_ops *uevent_ops,
				 struct kobject *parent_kobj)
{
ffffffff812c35d9:	55                   	push   %rbp
ffffffff812c35da:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c35dd:	41 56                	push   %r14
ffffffff812c35df:	41 55                	push   %r13
ffffffff812c35e1:	41 54                	push   %r12
ffffffff812c35e3:	53                   	push   %rbx
ffffffff812c35e4:	49 89 fe             	mov    %rdi,%r14
ffffffff812c35e7:	49 89 f5             	mov    %rsi,%r13
				struct kobject *parent_kobj)
{
	struct kset *kset;
	int retval;

	kset = kzalloc(sizeof(*kset), GFP_KERNEL);
ffffffff812c35ea:	bf 70 00 00 00       	mov    $0x70,%edi
ffffffff812c35ef:	be d0 00 00 00       	mov    $0xd0,%esi
 * If the kset was not able to be created, NULL will be returned.
 */
struct kset *kset_create_and_add(const char *name,
				 const struct kset_uevent_ops *uevent_ops,
				 struct kobject *parent_kobj)
{
ffffffff812c35f4:	49 89 d4             	mov    %rdx,%r12
				struct kobject *parent_kobj)
{
	struct kset *kset;
	int retval;

	kset = kzalloc(sizeof(*kset), GFP_KERNEL);
ffffffff812c35f7:	e8 9c f8 ff ff       	callq  ffffffff812c2e98 <kzalloc>
	if (!kset)
ffffffff812c35fc:	48 85 c0             	test   %rax,%rax
ffffffff812c35ff:	74 4b                	je     ffffffff812c364c <kset_create_and_add+0x73>
		return NULL;
	retval = kobject_set_name(&kset->kobj, "%s", name);
ffffffff812c3601:	48 8d 78 28          	lea    0x28(%rax),%rdi
ffffffff812c3605:	48 89 c3             	mov    %rax,%rbx
ffffffff812c3608:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c360b:	31 c0                	xor    %eax,%eax
ffffffff812c360d:	48 c7 c6 0f b6 7c 81 	mov    $0xffffffff817cb60f,%rsi
ffffffff812c3614:	e8 af f9 ff ff       	callq  ffffffff812c2fc8 <kobject_set_name>
	if (retval) {
ffffffff812c3619:	85 c0                	test   %eax,%eax
		kfree(kset);
ffffffff812c361b:	48 89 df             	mov    %rbx,%rdi

	kset = kzalloc(sizeof(*kset), GFP_KERNEL);
	if (!kset)
		return NULL;
	retval = kobject_set_name(&kset->kobj, "%s", name);
	if (retval) {
ffffffff812c361e:	75 27                	jne    ffffffff812c3647 <kset_create_and_add+0x6e>
	int error;

	kset = kset_create(name, uevent_ops, parent_kobj);
	if (!kset)
		return NULL;
	error = kset_register(kset);
ffffffff812c3620:	48 89 df             	mov    %rbx,%rdi
	retval = kobject_set_name(&kset->kobj, "%s", name);
	if (retval) {
		kfree(kset);
		return NULL;
	}
	kset->uevent_ops = uevent_ops;
ffffffff812c3623:	4c 89 6b 68          	mov    %r13,0x68(%rbx)
	kset->kobj.parent = parent_kobj;
ffffffff812c3627:	4c 89 63 40          	mov    %r12,0x40(%rbx)
	/*
	 * The kobject of this kset will have a type of kset_ktype and belong to
	 * no kset itself.  That way we can properly free it when it is
	 * finished being used.
	 */
	kset->kobj.ktype = &kset_ktype;
ffffffff812c362b:	48 c7 43 50 40 5f a3 	movq   $0xffffffff81a35f40,0x50(%rbx)
ffffffff812c3632:	81 
	kset->kobj.kset = NULL;
ffffffff812c3633:	48 c7 43 48 00 00 00 	movq   $0x0,0x48(%rbx)
ffffffff812c363a:	00 
	int error;

	kset = kset_create(name, uevent_ops, parent_kobj);
	if (!kset)
		return NULL;
	error = kset_register(kset);
ffffffff812c363b:	e8 5a ff ff ff       	callq  ffffffff812c359a <kset_register>
	if (error) {
ffffffff812c3640:	85 c0                	test   %eax,%eax
ffffffff812c3642:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3645:	74 07                	je     ffffffff812c364e <kset_create_and_add+0x75>
		kfree(kset);
ffffffff812c3647:	e8 3d d3 e3 ff       	callq  ffffffff81100989 <kfree>
	struct kset *kset;
	int error;

	kset = kset_create(name, uevent_ops, parent_kobj);
	if (!kset)
		return NULL;
ffffffff812c364c:	31 ff                	xor    %edi,%edi
	if (error) {
		kfree(kset);
		return NULL;
	}
	return kset;
}
ffffffff812c364e:	5b                   	pop    %rbx
ffffffff812c364f:	48 89 f8             	mov    %rdi,%rax
ffffffff812c3652:	41 5c                	pop    %r12
ffffffff812c3654:	41 5d                	pop    %r13
ffffffff812c3656:	41 5e                	pop    %r14
ffffffff812c3658:	5d                   	pop    %rbp
ffffffff812c3659:	c3                   	retq   

ffffffff812c365a <kobject_init_and_add>:
 * kobject_add().  The same type of error handling after a call to
 * kobject_add() and kobject lifetime rules are the same here.
 */
int kobject_init_and_add(struct kobject *kobj, struct kobj_type *ktype,
			 struct kobject *parent, const char *fmt, ...)
{
ffffffff812c365a:	55                   	push   %rbp
ffffffff812c365b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c365e:	41 55                	push   %r13
ffffffff812c3660:	41 54                	push   %r12
ffffffff812c3662:	53                   	push   %rbx
ffffffff812c3663:	49 89 fc             	mov    %rdi,%r12
ffffffff812c3666:	48 89 cb             	mov    %rcx,%rbx
ffffffff812c3669:	49 89 d5             	mov    %rdx,%r13
ffffffff812c366c:	48 83 ec 58          	sub    $0x58,%rsp
ffffffff812c3670:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
ffffffff812c3674:	4c 89 4d d8          	mov    %r9,-0x28(%rbp)
	va_list args;
	int retval;

	kobject_init(kobj, ktype);
ffffffff812c3678:	e8 36 f5 ff ff       	callq  ffffffff812c2bb3 <kobject_init>

	va_start(args, fmt);
ffffffff812c367d:	48 8d 45 10          	lea    0x10(%rbp),%rax
static int kobject_add_varg(struct kobject *kobj, struct kobject *parent,
			    const char *fmt, va_list vargs)
{
	int retval;

	retval = kobject_set_name_vargs(kobj, fmt, vargs);
ffffffff812c3681:	48 8d 55 98          	lea    -0x68(%rbp),%rdx
ffffffff812c3685:	48 89 de             	mov    %rbx,%rsi
ffffffff812c3688:	4c 89 e7             	mov    %r12,%rdi
	va_list args;
	int retval;

	kobject_init(kobj, ktype);

	va_start(args, fmt);
ffffffff812c368b:	c7 45 98 20 00 00 00 	movl   $0x20,-0x68(%rbp)
ffffffff812c3692:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
ffffffff812c3696:	48 8d 45 b0          	lea    -0x50(%rbp),%rax
ffffffff812c369a:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
static int kobject_add_varg(struct kobject *kobj, struct kobject *parent,
			    const char *fmt, va_list vargs)
{
	int retval;

	retval = kobject_set_name_vargs(kobj, fmt, vargs);
ffffffff812c369e:	e8 ca f8 ff ff       	callq  ffffffff812c2f6d <kobject_set_name_vargs>
	if (retval) {
ffffffff812c36a3:	85 c0                	test   %eax,%eax
ffffffff812c36a5:	74 14                	je     ffffffff812c36bb <kobject_init_and_add+0x61>
ffffffff812c36a7:	89 c3                	mov    %eax,%ebx
		printk(KERN_ERR "kobject: can not set name properly!\n");
ffffffff812c36a9:	48 c7 c7 ed 69 7b 81 	mov    $0xffffffff817b69ed,%rdi
ffffffff812c36b0:	31 c0                	xor    %eax,%eax
ffffffff812c36b2:	e8 db f8 16 00       	callq  ffffffff81432f92 <printk>
ffffffff812c36b7:	89 d8                	mov    %ebx,%eax
ffffffff812c36b9:	eb 0d                	jmp    ffffffff812c36c8 <kobject_init_and_add+0x6e>
		return retval;
	}
	kobj->parent = parent;
ffffffff812c36bb:	4d 89 6c 24 18       	mov    %r13,0x18(%r12)
	return kobject_add_internal(kobj);
ffffffff812c36c0:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c36c3:	e8 23 fb ff ff       	callq  ffffffff812c31eb <kobject_add_internal>
	va_start(args, fmt);
	retval = kobject_add_varg(kobj, parent, fmt, args);
	va_end(args);

	return retval;
}
ffffffff812c36c8:	48 83 c4 58          	add    $0x58,%rsp
ffffffff812c36cc:	5b                   	pop    %rbx
ffffffff812c36cd:	41 5c                	pop    %r12
ffffffff812c36cf:	41 5d                	pop    %r13
ffffffff812c36d1:	5d                   	pop    %rbp
ffffffff812c36d2:	c3                   	retq   

ffffffff812c36d3 <kobject_rename>:
 * exclusion between two different calls of kobject_rename
 * on the same kobject and to ensure that new_name is valid and
 * won't conflict with other kobjects.
 */
int kobject_rename(struct kobject *kobj, const char *new_name)
{
ffffffff812c36d3:	55                   	push   %rbp
ffffffff812c36d4:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c36d7:	41 57                	push   %r15
ffffffff812c36d9:	41 56                	push   %r14
ffffffff812c36db:	41 55                	push   %r13
ffffffff812c36dd:	41 54                	push   %r12
ffffffff812c36df:	49 89 f5             	mov    %rsi,%r13
ffffffff812c36e2:	53                   	push   %rbx
ffffffff812c36e3:	48 83 ec 28          	sub    $0x28,%rsp
	const char *devpath = NULL;
	const char *dup_name = NULL, *name;
	char *devpath_string = NULL;
	char *envp[2];

	kobj = kobject_get(kobj);
ffffffff812c36e7:	e8 40 f7 ff ff       	callq  ffffffff812c2e2c <kobject_get>
ffffffff812c36ec:	48 89 c3             	mov    %rax,%rbx
	if (!kobj)
		return -EINVAL;
ffffffff812c36ef:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
	const char *dup_name = NULL, *name;
	char *devpath_string = NULL;
	char *envp[2];

	kobj = kobject_get(kobj);
	if (!kobj)
ffffffff812c36f4:	48 85 db             	test   %rbx,%rbx
ffffffff812c36f7:	0f 84 f1 00 00 00    	je     ffffffff812c37ee <kobject_rename+0x11b>
		return -EINVAL;
	if (!kobj->parent)
ffffffff812c36fd:	48 83 7b 18 00       	cmpq   $0x0,0x18(%rbx)
ffffffff812c3702:	0f 84 e6 00 00 00    	je     ffffffff812c37ee <kobject_rename+0x11b>
		return -EINVAL;

	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c3708:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812c370d:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3710:	e8 94 f7 ff ff       	callq  ffffffff812c2ea9 <kobject_get_path>
	if (!devpath) {
ffffffff812c3715:	48 85 c0             	test   %rax,%rax
	if (!kobj)
		return -EINVAL;
	if (!kobj->parent)
		return -EINVAL;

	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c3718:	49 89 c6             	mov    %rax,%r14
	if (!devpath) {
ffffffff812c371b:	75 10                	jne    ffffffff812c372d <kobject_rename+0x5a>
int kobject_rename(struct kobject *kobj, const char *new_name)
{
	int error = 0;
	const char *devpath = NULL;
	const char *dup_name = NULL, *name;
	char *devpath_string = NULL;
ffffffff812c371d:	45 31 ff             	xor    %r15d,%r15d
 */
int kobject_rename(struct kobject *kobj, const char *new_name)
{
	int error = 0;
	const char *devpath = NULL;
	const char *dup_name = NULL, *name;
ffffffff812c3720:	31 c9                	xor    %ecx,%ecx
	if (!kobj->parent)
		return -EINVAL;

	devpath = kobject_get_path(kobj, GFP_KERNEL);
	if (!devpath) {
		error = -ENOMEM;
ffffffff812c3722:	41 bc f4 ff ff ff    	mov    $0xfffffff4,%r12d
ffffffff812c3728:	e9 9e 00 00 00       	jmpq   ffffffff812c37cb <kobject_rename+0xf8>
		goto out;
	}
	devpath_string = kmalloc(strlen(devpath) + 15, GFP_KERNEL);
ffffffff812c372d:	31 c0                	xor    %eax,%eax
ffffffff812c372f:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812c3733:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c3736:	f2 ae                	repnz scas %es:(%rdi),%al
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812c3738:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812c373d:	48 f7 d1             	not    %rcx
ffffffff812c3740:	48 8d 79 0e          	lea    0xe(%rcx),%rdi
ffffffff812c3744:	e8 42 da e3 ff       	callq  ffffffff8110118b <__kmalloc>
	if (!devpath_string) {
ffffffff812c3749:	48 85 c0             	test   %rax,%rax
ffffffff812c374c:	49 89 c7             	mov    %rax,%r15
ffffffff812c374f:	74 cc                	je     ffffffff812c371d <kobject_rename+0x4a>
		error = -ENOMEM;
		goto out;
	}
	sprintf(devpath_string, "DEVPATH_OLD=%s", devpath);
ffffffff812c3751:	48 89 c7             	mov    %rax,%rdi
ffffffff812c3754:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c3757:	48 c7 c6 31 6a 7b 81 	mov    $0xffffffff817b6a31,%rsi
ffffffff812c375e:	31 c0                	xor    %eax,%eax
	envp[0] = devpath_string;
	envp[1] = NULL;

	name = dup_name = kstrdup(new_name, GFP_KERNEL);
	if (!name) {
		error = -ENOMEM;
ffffffff812c3760:	41 bc f4 ff ff ff    	mov    $0xfffffff4,%r12d
	devpath_string = kmalloc(strlen(devpath) + 15, GFP_KERNEL);
	if (!devpath_string) {
		error = -ENOMEM;
		goto out;
	}
	sprintf(devpath_string, "DEVPATH_OLD=%s", devpath);
ffffffff812c3766:	e8 aa 70 00 00       	callq  ffffffff812ca815 <sprintf>
	envp[0] = devpath_string;
	envp[1] = NULL;

	name = dup_name = kstrdup(new_name, GFP_KERNEL);
ffffffff812c376b:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812c3770:	4c 89 ef             	mov    %r13,%rdi
	if (!devpath_string) {
		error = -ENOMEM;
		goto out;
	}
	sprintf(devpath_string, "DEVPATH_OLD=%s", devpath);
	envp[0] = devpath_string;
ffffffff812c3773:	4c 89 7d c0          	mov    %r15,-0x40(%rbp)
	envp[1] = NULL;
ffffffff812c3777:	48 c7 45 c8 00 00 00 	movq   $0x0,-0x38(%rbp)
ffffffff812c377e:	00 

	name = dup_name = kstrdup(new_name, GFP_KERNEL);
ffffffff812c377f:	e8 ca 26 e2 ff       	callq  ffffffff810e5e4e <kstrdup>
	if (!name) {
ffffffff812c3784:	48 85 c0             	test   %rax,%rax
	}
	sprintf(devpath_string, "DEVPATH_OLD=%s", devpath);
	envp[0] = devpath_string;
	envp[1] = NULL;

	name = dup_name = kstrdup(new_name, GFP_KERNEL);
ffffffff812c3787:	48 89 c1             	mov    %rax,%rcx
	if (!name) {
ffffffff812c378a:	74 3f                	je     ffffffff812c37cb <kobject_rename+0xf8>
		error = -ENOMEM;
		goto out;
	}

	error = sysfs_rename_dir_ns(kobj, new_name, kobject_namespace(kobj));
ffffffff812c378c:	48 89 df             	mov    %rbx,%rdi
ffffffff812c378f:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812c3793:	e8 28 fa ff ff       	callq  ffffffff812c31c0 <kobject_namespace>
ffffffff812c3798:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c379b:	48 89 c2             	mov    %rax,%rdx
ffffffff812c379e:	48 89 df             	mov    %rbx,%rdi
ffffffff812c37a1:	e8 8f dd e8 ff       	callq  ffffffff81151535 <sysfs_rename_dir_ns>
	if (error)
ffffffff812c37a6:	85 c0                	test   %eax,%eax
	if (!name) {
		error = -ENOMEM;
		goto out;
	}

	error = sysfs_rename_dir_ns(kobj, new_name, kobject_namespace(kobj));
ffffffff812c37a8:	41 89 c4             	mov    %eax,%r12d
	if (error)
ffffffff812c37ab:	48 8b 4d b8          	mov    -0x48(%rbp),%rcx
ffffffff812c37af:	75 1a                	jne    ffffffff812c37cb <kobject_rename+0xf8>
		goto out;

	/* Install the new kobject name */
	dup_name = kobj->name;
ffffffff812c37b1:	4c 8b 2b             	mov    (%rbx),%r13
	kobj->name = name;

	/* This function is mostly/only used for network interface.
	 * Some hotplug package track interfaces by their name and
	 * therefore want to know when the name is changed by the user. */
	kobject_uevent_env(kobj, KOBJ_MOVE, envp);
ffffffff812c37b4:	48 8d 55 c0          	lea    -0x40(%rbp),%rdx
	if (error)
		goto out;

	/* Install the new kobject name */
	dup_name = kobj->name;
	kobj->name = name;
ffffffff812c37b8:	48 89 0b             	mov    %rcx,(%rbx)

	/* This function is mostly/only used for network interface.
	 * Some hotplug package track interfaces by their name and
	 * therefore want to know when the name is changed by the user. */
	kobject_uevent_env(kobj, KOBJ_MOVE, envp);
ffffffff812c37bb:	be 03 00 00 00       	mov    $0x3,%esi
ffffffff812c37c0:	48 89 df             	mov    %rbx,%rdi
ffffffff812c37c3:	e8 98 04 00 00       	callq  ffffffff812c3c60 <kobject_uevent_env>
	error = sysfs_rename_dir_ns(kobj, new_name, kobject_namespace(kobj));
	if (error)
		goto out;

	/* Install the new kobject name */
	dup_name = kobj->name;
ffffffff812c37c8:	4c 89 e9             	mov    %r13,%rcx
	 * Some hotplug package track interfaces by their name and
	 * therefore want to know when the name is changed by the user. */
	kobject_uevent_env(kobj, KOBJ_MOVE, envp);

out:
	kfree(dup_name);
ffffffff812c37cb:	48 89 cf             	mov    %rcx,%rdi
ffffffff812c37ce:	e8 b6 d1 e3 ff       	callq  ffffffff81100989 <kfree>
	kfree(devpath_string);
ffffffff812c37d3:	4c 89 ff             	mov    %r15,%rdi
ffffffff812c37d6:	e8 ae d1 e3 ff       	callq  ffffffff81100989 <kfree>
	kfree(devpath);
ffffffff812c37db:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c37de:	e8 a6 d1 e3 ff       	callq  ffffffff81100989 <kfree>
	kobject_put(kobj);
ffffffff812c37e3:	48 89 df             	mov    %rbx,%rdi
ffffffff812c37e6:	e8 4b f4 ff ff       	callq  ffffffff812c2c36 <kobject_put>

	return error;
ffffffff812c37eb:	44 89 e0             	mov    %r12d,%eax
}
ffffffff812c37ee:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c37f2:	5b                   	pop    %rbx
ffffffff812c37f3:	41 5c                	pop    %r12
ffffffff812c37f5:	41 5d                	pop    %r13
ffffffff812c37f7:	41 5e                	pop    %r14
ffffffff812c37f9:	41 5f                	pop    %r15
ffffffff812c37fb:	5d                   	pop    %rbp
ffffffff812c37fc:	c3                   	retq   

ffffffff812c37fd <kobject_move>:
 * kobject_move - move object to another parent
 * @kobj: object in question.
 * @new_parent: object's new parent (can be NULL)
 */
int kobject_move(struct kobject *kobj, struct kobject *new_parent)
{
ffffffff812c37fd:	55                   	push   %rbp
ffffffff812c37fe:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3801:	41 57                	push   %r15
ffffffff812c3803:	41 56                	push   %r14
ffffffff812c3805:	41 55                	push   %r13
ffffffff812c3807:	41 54                	push   %r12
ffffffff812c3809:	49 89 f4             	mov    %rsi,%r12
ffffffff812c380c:	53                   	push   %rbx
ffffffff812c380d:	48 83 ec 18          	sub    $0x18,%rsp
	struct kobject *old_parent;
	const char *devpath = NULL;
	char *devpath_string = NULL;
	char *envp[2];

	kobj = kobject_get(kobj);
ffffffff812c3811:	e8 16 f6 ff ff       	callq  ffffffff812c2e2c <kobject_get>
ffffffff812c3816:	48 89 c3             	mov    %rax,%rbx
ffffffff812c3819:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
	if (!kobj)
ffffffff812c381e:	48 85 db             	test   %rbx,%rbx
ffffffff812c3821:	0f 84 ec 00 00 00    	je     ffffffff812c3913 <kobject_move+0x116>
		return -EINVAL;
	new_parent = kobject_get(new_parent);
ffffffff812c3827:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c382a:	e8 fd f5 ff ff       	callq  ffffffff812c2e2c <kobject_get>
	if (!new_parent) {
ffffffff812c382f:	48 85 c0             	test   %rax,%rax
	char *envp[2];

	kobj = kobject_get(kobj);
	if (!kobj)
		return -EINVAL;
	new_parent = kobject_get(new_parent);
ffffffff812c3832:	49 89 c5             	mov    %rax,%r13
	if (!new_parent) {
ffffffff812c3835:	75 15                	jne    ffffffff812c384c <kobject_move+0x4f>
		if (kobj->kset)
ffffffff812c3837:	48 8b 7b 20          	mov    0x20(%rbx),%rdi
ffffffff812c383b:	48 85 ff             	test   %rdi,%rdi
ffffffff812c383e:	74 0c                	je     ffffffff812c384c <kobject_move+0x4f>
			new_parent = kobject_get(&kobj->kset->kobj);
ffffffff812c3840:	48 83 c7 28          	add    $0x28,%rdi
ffffffff812c3844:	e8 e3 f5 ff ff       	callq  ffffffff812c2e2c <kobject_get>
ffffffff812c3849:	49 89 c5             	mov    %rax,%r13
	}

	/* old object path */
	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c384c:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812c3851:	48 89 df             	mov    %rbx,%rdi
int kobject_move(struct kobject *kobj, struct kobject *new_parent)
{
	int error;
	struct kobject *old_parent;
	const char *devpath = NULL;
	char *devpath_string = NULL;
ffffffff812c3854:	45 31 ff             	xor    %r15d,%r15d
		if (kobj->kset)
			new_parent = kobject_get(&kobj->kset->kobj);
	}

	/* old object path */
	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c3857:	e8 4d f6 ff ff       	callq  ffffffff812c2ea9 <kobject_get_path>
	if (!devpath) {
ffffffff812c385c:	48 85 c0             	test   %rax,%rax
		if (kobj->kset)
			new_parent = kobject_get(&kobj->kset->kobj);
	}

	/* old object path */
	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c385f:	49 89 c6             	mov    %rax,%r14
	if (!devpath) {
		error = -ENOMEM;
ffffffff812c3862:	41 bc f4 ff ff ff    	mov    $0xfffffff4,%r12d
			new_parent = kobject_get(&kobj->kset->kobj);
	}

	/* old object path */
	devpath = kobject_get_path(kobj, GFP_KERNEL);
	if (!devpath) {
ffffffff812c3868:	0f 84 82 00 00 00    	je     ffffffff812c38f0 <kobject_move+0xf3>
		error = -ENOMEM;
		goto out;
	}
	devpath_string = kmalloc(strlen(devpath) + 15, GFP_KERNEL);
ffffffff812c386e:	31 c0                	xor    %eax,%eax
ffffffff812c3870:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812c3874:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c3877:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c3879:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812c387e:	48 f7 d1             	not    %rcx
ffffffff812c3881:	48 8d 79 0e          	lea    0xe(%rcx),%rdi
ffffffff812c3885:	e8 01 d9 e3 ff       	callq  ffffffff8110118b <__kmalloc>
	if (!devpath_string) {
ffffffff812c388a:	48 85 c0             	test   %rax,%rax
ffffffff812c388d:	49 89 c7             	mov    %rax,%r15
ffffffff812c3890:	74 5e                	je     ffffffff812c38f0 <kobject_move+0xf3>
		error = -ENOMEM;
		goto out;
	}
	sprintf(devpath_string, "DEVPATH_OLD=%s", devpath);
ffffffff812c3892:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c3895:	48 c7 c6 31 6a 7b 81 	mov    $0xffffffff817b6a31,%rsi
ffffffff812c389c:	48 89 c7             	mov    %rax,%rdi
ffffffff812c389f:	31 c0                	xor    %eax,%eax
ffffffff812c38a1:	e8 6f 6f 00 00       	callq  ffffffff812ca815 <sprintf>
	envp[0] = devpath_string;
	envp[1] = NULL;
	error = sysfs_move_dir_ns(kobj, new_parent, kobject_namespace(kobj));
ffffffff812c38a6:	48 89 df             	mov    %rbx,%rdi
	if (!devpath_string) {
		error = -ENOMEM;
		goto out;
	}
	sprintf(devpath_string, "DEVPATH_OLD=%s", devpath);
	envp[0] = devpath_string;
ffffffff812c38a9:	4c 89 7d c0          	mov    %r15,-0x40(%rbp)
	envp[1] = NULL;
ffffffff812c38ad:	48 c7 45 c8 00 00 00 	movq   $0x0,-0x38(%rbp)
ffffffff812c38b4:	00 
	error = sysfs_move_dir_ns(kobj, new_parent, kobject_namespace(kobj));
ffffffff812c38b5:	e8 06 f9 ff ff       	callq  ffffffff812c31c0 <kobject_namespace>
ffffffff812c38ba:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c38bd:	48 89 c2             	mov    %rax,%rdx
ffffffff812c38c0:	48 89 df             	mov    %rbx,%rdi
ffffffff812c38c3:	e8 bd dc e8 ff       	callq  ffffffff81151585 <sysfs_move_dir_ns>
	if (error)
ffffffff812c38c8:	85 c0                	test   %eax,%eax
		goto out;
	}
	sprintf(devpath_string, "DEVPATH_OLD=%s", devpath);
	envp[0] = devpath_string;
	envp[1] = NULL;
	error = sysfs_move_dir_ns(kobj, new_parent, kobject_namespace(kobj));
ffffffff812c38ca:	41 89 c4             	mov    %eax,%r12d
	if (error)
ffffffff812c38cd:	75 21                	jne    ffffffff812c38f0 <kobject_move+0xf3>
		goto out;
	old_parent = kobj->parent;
ffffffff812c38cf:	48 8b 7b 18          	mov    0x18(%rbx),%rdi
	kobj->parent = new_parent;
ffffffff812c38d3:	4c 89 6b 18          	mov    %r13,0x18(%rbx)
	new_parent = NULL;
ffffffff812c38d7:	45 31 ed             	xor    %r13d,%r13d
	kobject_put(old_parent);
ffffffff812c38da:	e8 57 f3 ff ff       	callq  ffffffff812c2c36 <kobject_put>
	kobject_uevent_env(kobj, KOBJ_MOVE, envp);
ffffffff812c38df:	48 8d 55 c0          	lea    -0x40(%rbp),%rdx
ffffffff812c38e3:	be 03 00 00 00       	mov    $0x3,%esi
ffffffff812c38e8:	48 89 df             	mov    %rbx,%rdi
ffffffff812c38eb:	e8 70 03 00 00       	callq  ffffffff812c3c60 <kobject_uevent_env>
out:
	kobject_put(new_parent);
ffffffff812c38f0:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c38f3:	e8 3e f3 ff ff       	callq  ffffffff812c2c36 <kobject_put>
	kobject_put(kobj);
ffffffff812c38f8:	48 89 df             	mov    %rbx,%rdi
ffffffff812c38fb:	e8 36 f3 ff ff       	callq  ffffffff812c2c36 <kobject_put>
	kfree(devpath_string);
ffffffff812c3900:	4c 89 ff             	mov    %r15,%rdi
ffffffff812c3903:	e8 81 d0 e3 ff       	callq  ffffffff81100989 <kfree>
	kfree(devpath);
ffffffff812c3908:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c390b:	e8 79 d0 e3 ff       	callq  ffffffff81100989 <kfree>
	return error;
ffffffff812c3910:	44 89 e0             	mov    %r12d,%eax
}
ffffffff812c3913:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c3917:	5b                   	pop    %rbx
ffffffff812c3918:	41 5c                	pop    %r12
ffffffff812c391a:	41 5d                	pop    %r13
ffffffff812c391c:	41 5e                	pop    %r14
ffffffff812c391e:	41 5f                	pop    %r15
ffffffff812c3920:	5d                   	pop    %rbp
ffffffff812c3921:	c3                   	retq   

ffffffff812c3922 <kobj_ns_current_may_mount>:
{
	return kobj_child_ns_ops(kobj->parent);
}

bool kobj_ns_current_may_mount(enum kobj_ns_type type)
{
ffffffff812c3922:	55                   	push   %rbp
ffffffff812c3923:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3926:	53                   	push   %rbx
ffffffff812c3927:	89 fb                	mov    %edi,%ebx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c3929:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3930:	48 83 ec 18          	sub    $0x18,%rsp
ffffffff812c3934:	e8 f7 4c 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	bool may_mount = true;

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c3939:	ff cb                	dec    %ebx
	return kobj_child_ns_ops(kobj->parent);
}

bool kobj_ns_current_may_mount(enum kobj_ns_type type)
{
	bool may_mount = true;
ffffffff812c393b:	b0 01                	mov    $0x1,%al

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c393d:	75 0f                	jne    ffffffff812c394e <kobj_ns_current_may_mount+0x2c>
	    kobj_ns_ops_tbl[type])
ffffffff812c393f:	48 8b 15 52 de 8e 00 	mov    0x8ede52(%rip),%rdx        # ffffffff81bb1798 <__key.17030+0x8>
bool kobj_ns_current_may_mount(enum kobj_ns_type type)
{
	bool may_mount = true;

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c3946:	48 85 d2             	test   %rdx,%rdx
ffffffff812c3949:	74 03                	je     ffffffff812c394e <kobj_ns_current_may_mount+0x2c>
	    kobj_ns_ops_tbl[type])
		may_mount = kobj_ns_ops_tbl[type]->current_may_mount();
ffffffff812c394b:	ff 52 08             	callq  *0x8(%rdx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c394e:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3955:	88 45 ef             	mov    %al,-0x11(%rbp)
ffffffff812c3958:	e8 4f 4d 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&kobj_ns_type_lock);

	return may_mount;
}
ffffffff812c395d:	8a 45 ef             	mov    -0x11(%rbp),%al
ffffffff812c3960:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c3964:	5b                   	pop    %rbx
ffffffff812c3965:	5d                   	pop    %rbp
ffffffff812c3966:	c3                   	retq   

ffffffff812c3967 <kobj_ns_grab_current>:

void *kobj_ns_grab_current(enum kobj_ns_type type)
{
ffffffff812c3967:	55                   	push   %rbp
ffffffff812c3968:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c396b:	53                   	push   %rbx
ffffffff812c396c:	89 fb                	mov    %edi,%ebx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c396e:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3975:	48 83 ec 18          	sub    $0x18,%rsp
ffffffff812c3979:	e8 b2 4c 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	void *ns = NULL;
ffffffff812c397e:	31 c0                	xor    %eax,%eax

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c3980:	ff cb                	dec    %ebx
ffffffff812c3982:	75 0f                	jne    ffffffff812c3993 <kobj_ns_grab_current+0x2c>
	    kobj_ns_ops_tbl[type])
ffffffff812c3984:	48 8b 15 0d de 8e 00 	mov    0x8ede0d(%rip),%rdx        # ffffffff81bb1798 <__key.17030+0x8>
void *kobj_ns_grab_current(enum kobj_ns_type type)
{
	void *ns = NULL;

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c398b:	48 85 d2             	test   %rdx,%rdx
ffffffff812c398e:	74 03                	je     ffffffff812c3993 <kobj_ns_grab_current+0x2c>
	    kobj_ns_ops_tbl[type])
		ns = kobj_ns_ops_tbl[type]->grab_current_ns();
ffffffff812c3990:	ff 52 10             	callq  *0x10(%rdx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c3993:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c399a:	48 89 45 e8          	mov    %rax,-0x18(%rbp)
ffffffff812c399e:	e8 09 4d 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&kobj_ns_type_lock);

	return ns;
}
ffffffff812c39a3:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
ffffffff812c39a7:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c39ab:	5b                   	pop    %rbx
ffffffff812c39ac:	5d                   	pop    %rbp
ffffffff812c39ad:	c3                   	retq   

ffffffff812c39ae <kobj_ns_netlink>:

const void *kobj_ns_netlink(enum kobj_ns_type type, struct sock *sk)
{
ffffffff812c39ae:	55                   	push   %rbp
ffffffff812c39af:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c39b2:	41 54                	push   %r12
ffffffff812c39b4:	53                   	push   %rbx
ffffffff812c39b5:	41 89 fc             	mov    %edi,%r12d
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c39b8:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c39bf:	48 89 f3             	mov    %rsi,%rbx
ffffffff812c39c2:	48 83 ec 10          	sub    $0x10,%rsp
ffffffff812c39c6:	e8 65 4c 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	const void *ns = NULL;
ffffffff812c39cb:	31 c0                	xor    %eax,%eax

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c39cd:	41 ff cc             	dec    %r12d
ffffffff812c39d0:	75 12                	jne    ffffffff812c39e4 <kobj_ns_netlink+0x36>
	    kobj_ns_ops_tbl[type])
ffffffff812c39d2:	48 8b 15 bf dd 8e 00 	mov    0x8eddbf(%rip),%rdx        # ffffffff81bb1798 <__key.17030+0x8>
const void *kobj_ns_netlink(enum kobj_ns_type type, struct sock *sk)
{
	const void *ns = NULL;

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c39d9:	48 85 d2             	test   %rdx,%rdx
ffffffff812c39dc:	74 06                	je     ffffffff812c39e4 <kobj_ns_netlink+0x36>
	    kobj_ns_ops_tbl[type])
		ns = kobj_ns_ops_tbl[type]->netlink_ns(sk);
ffffffff812c39de:	48 89 df             	mov    %rbx,%rdi
ffffffff812c39e1:	ff 52 18             	callq  *0x18(%rdx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c39e4:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c39eb:	48 89 45 e8          	mov    %rax,-0x18(%rbp)
ffffffff812c39ef:	e8 b8 4c 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&kobj_ns_type_lock);

	return ns;
}
ffffffff812c39f4:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
ffffffff812c39f8:	5a                   	pop    %rdx
ffffffff812c39f9:	59                   	pop    %rcx
ffffffff812c39fa:	5b                   	pop    %rbx
ffffffff812c39fb:	41 5c                	pop    %r12
ffffffff812c39fd:	5d                   	pop    %rbp
ffffffff812c39fe:	c3                   	retq   

ffffffff812c39ff <kobj_ns_initial>:

const void *kobj_ns_initial(enum kobj_ns_type type)
{
ffffffff812c39ff:	55                   	push   %rbp
ffffffff812c3a00:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3a03:	53                   	push   %rbx
ffffffff812c3a04:	89 fb                	mov    %edi,%ebx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c3a06:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3a0d:	48 83 ec 18          	sub    $0x18,%rsp
ffffffff812c3a11:	e8 1a 4c 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	const void *ns = NULL;
ffffffff812c3a16:	31 c0                	xor    %eax,%eax

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c3a18:	ff cb                	dec    %ebx
ffffffff812c3a1a:	75 0f                	jne    ffffffff812c3a2b <kobj_ns_initial+0x2c>
	    kobj_ns_ops_tbl[type])
ffffffff812c3a1c:	48 8b 15 75 dd 8e 00 	mov    0x8edd75(%rip),%rdx        # ffffffff81bb1798 <__key.17030+0x8>
const void *kobj_ns_initial(enum kobj_ns_type type)
{
	const void *ns = NULL;

	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c3a23:	48 85 d2             	test   %rdx,%rdx
ffffffff812c3a26:	74 03                	je     ffffffff812c3a2b <kobj_ns_initial+0x2c>
	    kobj_ns_ops_tbl[type])
		ns = kobj_ns_ops_tbl[type]->initial_ns();
ffffffff812c3a28:	ff 52 20             	callq  *0x20(%rdx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c3a2b:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3a32:	48 89 45 e8          	mov    %rax,-0x18(%rbp)
ffffffff812c3a36:	e8 71 4c 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&kobj_ns_type_lock);

	return ns;
}
ffffffff812c3a3b:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
ffffffff812c3a3f:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c3a43:	5b                   	pop    %rbx
ffffffff812c3a44:	5d                   	pop    %rbp
ffffffff812c3a45:	c3                   	retq   

ffffffff812c3a46 <kobj_ns_drop>:

void kobj_ns_drop(enum kobj_ns_type type, void *ns)
{
ffffffff812c3a46:	55                   	push   %rbp
ffffffff812c3a47:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3a4a:	41 54                	push   %r12
ffffffff812c3a4c:	53                   	push   %rbx
ffffffff812c3a4d:	41 89 fc             	mov    %edi,%r12d
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812c3a50:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3a57:	48 89 f3             	mov    %rsi,%rbx
ffffffff812c3a5a:	e8 d1 4b 17 00       	callq  ffffffff81438630 <_raw_spin_lock>
	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c3a5f:	41 ff cc             	dec    %r12d
ffffffff812c3a62:	75 1a                	jne    ffffffff812c3a7e <kobj_ns_drop+0x38>
	    kobj_ns_ops_tbl[type] && kobj_ns_ops_tbl[type]->drop_ns)
ffffffff812c3a64:	48 8b 05 2d dd 8e 00 	mov    0x8edd2d(%rip),%rax        # ffffffff81bb1798 <__key.17030+0x8>
}

void kobj_ns_drop(enum kobj_ns_type type, void *ns)
{
	spin_lock(&kobj_ns_type_lock);
	if ((type > KOBJ_NS_TYPE_NONE) && (type < KOBJ_NS_TYPES) &&
ffffffff812c3a6b:	48 85 c0             	test   %rax,%rax
ffffffff812c3a6e:	74 0e                	je     ffffffff812c3a7e <kobj_ns_drop+0x38>
	    kobj_ns_ops_tbl[type] && kobj_ns_ops_tbl[type]->drop_ns)
ffffffff812c3a70:	48 8b 40 28          	mov    0x28(%rax),%rax
ffffffff812c3a74:	48 85 c0             	test   %rax,%rax
ffffffff812c3a77:	74 05                	je     ffffffff812c3a7e <kobj_ns_drop+0x38>
		kobj_ns_ops_tbl[type]->drop_ns(ns);
ffffffff812c3a79:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3a7c:	ff d0                	callq  *%rax
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812c3a7e:	48 c7 c7 20 5f a3 81 	mov    $0xffffffff81a35f20,%rdi
ffffffff812c3a85:	e8 22 4c 17 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&kobj_ns_type_lock);
}
ffffffff812c3a8a:	5b                   	pop    %rbx
ffffffff812c3a8b:	41 5c                	pop    %r12
ffffffff812c3a8d:	5d                   	pop    %rbp
ffffffff812c3a8e:	c3                   	retq   

ffffffff812c3a8f <cleanup_uevent_env>:
	env->buflen += len + 1;
	return 0;
}

static void cleanup_uevent_env(struct subprocess_info *info)
{
ffffffff812c3a8f:	55                   	push   %rbp
	kfree(info->data);
ffffffff812c3a90:	48 8b 7f 58          	mov    0x58(%rdi),%rdi
	env->buflen += len + 1;
	return 0;
}

static void cleanup_uevent_env(struct subprocess_info *info)
{
ffffffff812c3a94:	48 89 e5             	mov    %rsp,%rbp
	kfree(info->data);
ffffffff812c3a97:	e8 ed ce e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812c3a9c:	5d                   	pop    %rbp
ffffffff812c3a9d:	c3                   	retq   

ffffffff812c3a9e <kobj_bcast_filter>:
	return ret;
}

#ifdef CONFIG_NET
static int kobj_bcast_filter(struct sock *dsk, struct sk_buff *skb, void *data)
{
ffffffff812c3a9e:	55                   	push   %rbp
ffffffff812c3a9f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3aa2:	41 55                	push   %r13
ffffffff812c3aa4:	41 54                	push   %r12
ffffffff812c3aa6:	53                   	push   %rbx
ffffffff812c3aa7:	51                   	push   %rcx
ffffffff812c3aa8:	49 89 fd             	mov    %rdi,%r13
	struct kobject *kobj = data, *ksobj;
	const struct kobj_ns_type_operations *ops;

	ops = kobj_ns_ops(kobj);
ffffffff812c3aab:	48 89 d7             	mov    %rdx,%rdi
	return ret;
}

#ifdef CONFIG_NET
static int kobj_bcast_filter(struct sock *dsk, struct sk_buff *skb, void *data)
{
ffffffff812c3aae:	49 89 d4             	mov    %rdx,%r12
	struct kobject *kobj = data, *ksobj;
	const struct kobj_ns_type_operations *ops;

	ops = kobj_ns_ops(kobj);
ffffffff812c3ab1:	e8 fb f6 ff ff       	callq  ffffffff812c31b1 <kobj_ns_ops>
	if (!ops && kobj->kset) {
ffffffff812c3ab6:	48 85 c0             	test   %rax,%rax
ffffffff812c3ab9:	48 89 c3             	mov    %rax,%rbx
ffffffff812c3abc:	75 24                	jne    ffffffff812c3ae2 <kobj_bcast_filter+0x44>
ffffffff812c3abe:	49 8b 44 24 20       	mov    0x20(%r12),%rax
ffffffff812c3ac3:	48 85 c0             	test   %rax,%rax
ffffffff812c3ac6:	75 04                	jne    ffffffff812c3acc <kobj_bcast_filter+0x2e>
		ns = kobj->ktype->namespace(kobj);
		sock_ns = ops->netlink_ns(dsk);
		return sock_ns != ns;
	}

	return 0;
ffffffff812c3ac8:	31 c0                	xor    %eax,%eax
ffffffff812c3aca:	eb 42                	jmp    ffffffff812c3b0e <kobj_bcast_filter+0x70>
	const struct kobj_ns_type_operations *ops;

	ops = kobj_ns_ops(kobj);
	if (!ops && kobj->kset) {
		ksobj = &kobj->kset->kobj;
		if (ksobj->parent != NULL)
ffffffff812c3acc:	48 8b 78 40          	mov    0x40(%rax),%rdi
ffffffff812c3ad0:	48 85 ff             	test   %rdi,%rdi
ffffffff812c3ad3:	74 f3                	je     ffffffff812c3ac8 <kobj_bcast_filter+0x2a>
			ops = kobj_ns_ops(ksobj->parent);
ffffffff812c3ad5:	e8 d7 f6 ff ff       	callq  ffffffff812c31b1 <kobj_ns_ops>
	}

	if (ops && ops->netlink_ns && kobj->ktype->namespace) {
ffffffff812c3ada:	48 85 c0             	test   %rax,%rax

	ops = kobj_ns_ops(kobj);
	if (!ops && kobj->kset) {
		ksobj = &kobj->kset->kobj;
		if (ksobj->parent != NULL)
			ops = kobj_ns_ops(ksobj->parent);
ffffffff812c3add:	48 89 c3             	mov    %rax,%rbx
	}

	if (ops && ops->netlink_ns && kobj->ktype->namespace) {
ffffffff812c3ae0:	74 e6                	je     ffffffff812c3ac8 <kobj_bcast_filter+0x2a>
ffffffff812c3ae2:	48 83 7b 18 00       	cmpq   $0x0,0x18(%rbx)
ffffffff812c3ae7:	74 df                	je     ffffffff812c3ac8 <kobj_bcast_filter+0x2a>
ffffffff812c3ae9:	49 8b 44 24 28       	mov    0x28(%r12),%rax
ffffffff812c3aee:	48 8b 40 20          	mov    0x20(%rax),%rax
ffffffff812c3af2:	48 85 c0             	test   %rax,%rax
ffffffff812c3af5:	74 d1                	je     ffffffff812c3ac8 <kobj_bcast_filter+0x2a>
		const void *sock_ns, *ns;
		ns = kobj->ktype->namespace(kobj);
ffffffff812c3af7:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3afa:	ff d0                	callq  *%rax
ffffffff812c3afc:	49 89 c4             	mov    %rax,%r12
		sock_ns = ops->netlink_ns(dsk);
ffffffff812c3aff:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c3b02:	ff 53 18             	callq  *0x18(%rbx)
		return sock_ns != ns;
ffffffff812c3b05:	49 39 c4             	cmp    %rax,%r12
ffffffff812c3b08:	0f 95 c0             	setne  %al
ffffffff812c3b0b:	0f b6 c0             	movzbl %al,%eax
	}

	return 0;
}
ffffffff812c3b0e:	5a                   	pop    %rdx
ffffffff812c3b0f:	5b                   	pop    %rbx
ffffffff812c3b10:	41 5c                	pop    %r12
ffffffff812c3b12:	41 5d                	pop    %r13
ffffffff812c3b14:	5d                   	pop    %rbp
ffffffff812c3b15:	c3                   	retq   

ffffffff812c3b16 <add_uevent_var>:
 *
 * Returns 0 if environment variable was added successfully or -ENOMEM
 * if no space was available.
 */
int add_uevent_var(struct kobj_uevent_env *env, const char *format, ...)
{
ffffffff812c3b16:	55                   	push   %rbp
ffffffff812c3b17:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3b1a:	41 54                	push   %r12
ffffffff812c3b1c:	53                   	push   %rbx
ffffffff812c3b1d:	48 83 ec 50          	sub    $0x50,%rsp
ffffffff812c3b21:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
ffffffff812c3b25:	48 89 4d d8          	mov    %rcx,-0x28(%rbp)
ffffffff812c3b29:	4c 89 45 e0          	mov    %r8,-0x20(%rbp)
ffffffff812c3b2d:	4c 89 4d e8          	mov    %r9,-0x18(%rbp)
	va_list args;
	int len;

	if (env->envp_idx >= ARRAY_SIZE(env->envp)) {
ffffffff812c3b31:	83 bf 18 01 00 00 1f 	cmpl   $0x1f,0x118(%rdi)
ffffffff812c3b38:	76 0e                	jbe    ffffffff812c3b48 <add_uevent_var+0x32>
		WARN(1, KERN_ERR "add_uevent_var: too many keys\n");
ffffffff812c3b3a:	48 c7 c2 40 6a 7b 81 	mov    $0xffffffff817b6a40,%rdx
ffffffff812c3b41:	be 88 01 00 00       	mov    $0x188,%esi
ffffffff812c3b46:	eb 65                	jmp    ffffffff812c3bad <add_uevent_var+0x97>
		return -ENOMEM;
	}

	va_start(args, format);
ffffffff812c3b48:	48 8d 45 10          	lea    0x10(%rbp),%rax
	len = vsnprintf(&env->buf[env->buflen],
ffffffff812c3b4c:	41 bc 00 08 00 00    	mov    $0x800,%r12d
ffffffff812c3b52:	49 89 f2             	mov    %rsi,%r10
ffffffff812c3b55:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c3b58:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c3b5b:	48 8d 4d a8          	lea    -0x58(%rbp),%rcx
	if (env->envp_idx >= ARRAY_SIZE(env->envp)) {
		WARN(1, KERN_ERR "add_uevent_var: too many keys\n");
		return -ENOMEM;
	}

	va_start(args, format);
ffffffff812c3b5f:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
ffffffff812c3b63:	48 8d 45 c0          	lea    -0x40(%rbp),%rax
	len = vsnprintf(&env->buf[env->buflen],
ffffffff812c3b67:	4c 89 d2             	mov    %r10,%rdx
	if (env->envp_idx >= ARRAY_SIZE(env->envp)) {
		WARN(1, KERN_ERR "add_uevent_var: too many keys\n");
		return -ENOMEM;
	}

	va_start(args, format);
ffffffff812c3b6a:	c7 45 a8 10 00 00 00 	movl   $0x10,-0x58(%rbp)
ffffffff812c3b71:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
	len = vsnprintf(&env->buf[env->buflen],
ffffffff812c3b75:	48 63 87 1c 09 00 00 	movslq 0x91c(%rdi),%rax
ffffffff812c3b7c:	48 8d bc 07 1c 01 00 	lea    0x11c(%rdi,%rax,1),%rdi
ffffffff812c3b83:	00 
ffffffff812c3b84:	48 29 c6             	sub    %rax,%rsi
ffffffff812c3b87:	e8 34 68 00 00       	callq  ffffffff812ca3c0 <vsnprintf>
			sizeof(env->buf) - env->buflen,
			format, args);
	va_end(args);

	if (len >= (sizeof(env->buf) - env->buflen)) {
ffffffff812c3b8c:	48 63 b3 1c 09 00 00 	movslq 0x91c(%rbx),%rsi
ffffffff812c3b93:	48 63 d0             	movslq %eax,%rdx
ffffffff812c3b96:	49 29 f4             	sub    %rsi,%r12
ffffffff812c3b99:	48 89 f1             	mov    %rsi,%rcx
ffffffff812c3b9c:	4c 39 e2             	cmp    %r12,%rdx
ffffffff812c3b9f:	72 21                	jb     ffffffff812c3bc2 <add_uevent_var+0xac>
		WARN(1, KERN_ERR "add_uevent_var: buffer size too small\n");
ffffffff812c3ba1:	48 c7 c2 76 6a 7b 81 	mov    $0xffffffff817b6a76,%rdx
ffffffff812c3ba8:	be 93 01 00 00       	mov    $0x193,%esi
ffffffff812c3bad:	31 c0                	xor    %eax,%eax
ffffffff812c3baf:	48 c7 c7 61 6a 7b 81 	mov    $0xffffffff817b6a61,%rdi
ffffffff812c3bb6:	e8 7b 27 da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
		return -ENOMEM;
ffffffff812c3bbb:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
ffffffff812c3bc0:	eb 29                	jmp    ffffffff812c3beb <add_uevent_var+0xd5>
	}

	env->envp[env->envp_idx++] = &env->buf[env->buflen];
ffffffff812c3bc2:	48 63 93 18 01 00 00 	movslq 0x118(%rbx),%rdx
	env->buflen += len + 1;
ffffffff812c3bc9:	8d 44 08 01          	lea    0x1(%rax,%rcx,1),%eax
	if (len >= (sizeof(env->buf) - env->buflen)) {
		WARN(1, KERN_ERR "add_uevent_var: buffer size too small\n");
		return -ENOMEM;
	}

	env->envp[env->envp_idx++] = &env->buf[env->buflen];
ffffffff812c3bcd:	48 8d b4 33 1c 01 00 	lea    0x11c(%rbx,%rsi,1),%rsi
ffffffff812c3bd4:	00 
ffffffff812c3bd5:	8d 7a 01             	lea    0x1(%rdx),%edi
ffffffff812c3bd8:	89 bb 18 01 00 00    	mov    %edi,0x118(%rbx)
ffffffff812c3bde:	48 89 74 d3 18       	mov    %rsi,0x18(%rbx,%rdx,8)
	env->buflen += len + 1;
ffffffff812c3be3:	89 83 1c 09 00 00    	mov    %eax,0x91c(%rbx)
	return 0;
ffffffff812c3be9:	31 c0                	xor    %eax,%eax
}
ffffffff812c3beb:	48 83 c4 50          	add    $0x50,%rsp
ffffffff812c3bef:	5b                   	pop    %rbx
ffffffff812c3bf0:	41 5c                	pop    %r12
ffffffff812c3bf2:	5d                   	pop    %rbp
ffffffff812c3bf3:	c3                   	retq   

ffffffff812c3bf4 <uevent_net_exit>:
	mutex_unlock(&uevent_sock_mutex);
	return 0;
}

static void uevent_net_exit(struct net *net)
{
ffffffff812c3bf4:	55                   	push   %rbp
ffffffff812c3bf5:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3bf8:	41 54                	push   %r12
ffffffff812c3bfa:	53                   	push   %rbx
ffffffff812c3bfb:	49 89 fc             	mov    %rdi,%r12
	struct uevent_sock *ue_sk;

	mutex_lock(&uevent_sock_mutex);
ffffffff812c3bfe:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
ffffffff812c3c05:	e8 88 39 17 00       	callq  ffffffff81437592 <mutex_lock>
	list_for_each_entry(ue_sk, &uevent_sock_list, list) {
ffffffff812c3c0a:	48 8b 1d 2f 24 77 00 	mov    0x77242f(%rip),%rbx        # ffffffff81a36040 <uevent_sock_list>
ffffffff812c3c11:	48 81 fb 40 60 a3 81 	cmp    $0xffffffff81a36040,%rbx
ffffffff812c3c18:	74 0e                	je     ffffffff812c3c28 <uevent_net_exit+0x34>
		if (sock_net(ue_sk->sk) == net)
ffffffff812c3c1a:	49 81 fc 00 bf a4 81 	cmp    $0xffffffff81a4bf00,%r12
ffffffff812c3c21:	74 13                	je     ffffffff812c3c36 <uevent_net_exit+0x42>
static void uevent_net_exit(struct net *net)
{
	struct uevent_sock *ue_sk;

	mutex_lock(&uevent_sock_mutex);
	list_for_each_entry(ue_sk, &uevent_sock_list, list) {
ffffffff812c3c23:	48 8b 1b             	mov    (%rbx),%rbx
ffffffff812c3c26:	eb e9                	jmp    ffffffff812c3c11 <uevent_net_exit+0x1d>
		if (sock_net(ue_sk->sk) == net)
			goto found;
	}
	mutex_unlock(&uevent_sock_mutex);
ffffffff812c3c28:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
ffffffff812c3c2f:	e8 80 3a 17 00       	callq  ffffffff814376b4 <mutex_unlock>
	return;
ffffffff812c3c34:	eb 25                	jmp    ffffffff812c3c5b <uevent_net_exit+0x67>

found:
	list_del(&ue_sk->list);
ffffffff812c3c36:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3c39:	e8 67 13 01 00       	callq  ffffffff812d4fa5 <list_del>
	mutex_unlock(&uevent_sock_mutex);
ffffffff812c3c3e:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
ffffffff812c3c45:	e8 6a 3a 17 00       	callq  ffffffff814376b4 <mutex_unlock>

	netlink_kernel_release(ue_sk->sk);
ffffffff812c3c4a:	48 8b 7b 10          	mov    0x10(%rbx),%rdi
ffffffff812c3c4e:	e8 6e af 11 00       	callq  ffffffff813debc1 <netlink_kernel_release>
	kfree(ue_sk);
ffffffff812c3c53:	48 89 df             	mov    %rbx,%rdi
ffffffff812c3c56:	e8 2e cd e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812c3c5b:	5b                   	pop    %rbx
ffffffff812c3c5c:	41 5c                	pop    %r12
ffffffff812c3c5e:	5d                   	pop    %rbp
ffffffff812c3c5f:	c3                   	retq   

ffffffff812c3c60 <kobject_uevent_env>:
 * Returns 0 if kobject_uevent_env() is completed with success or the
 * corresponding error when it fails.
 */
int kobject_uevent_env(struct kobject *kobj, enum kobject_action action,
		       char *envp_ext[])
{
ffffffff812c3c60:	55                   	push   %rbp
	struct kobj_uevent_env *env;
	const char *action_string = kobject_actions[action];
ffffffff812c3c61:	89 f0                	mov    %esi,%eax
	int retval = 0;
#ifdef CONFIG_NET
	struct uevent_sock *ue_sk;
#endif

	pr_debug("kobject: '%s' (%p): %s\n",
ffffffff812c3c63:	48 c7 c1 00 f1 63 81 	mov    $0xffffffff8163f100,%rcx
 * Returns 0 if kobject_uevent_env() is completed with success or the
 * corresponding error when it fails.
 */
int kobject_uevent_env(struct kobject *kobj, enum kobject_action action,
		       char *envp_ext[])
{
ffffffff812c3c6a:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c3c6d:	41 57                	push   %r15
ffffffff812c3c6f:	41 56                	push   %r14
ffffffff812c3c71:	41 55                	push   %r13
ffffffff812c3c73:	41 54                	push   %r12
ffffffff812c3c75:	53                   	push   %rbx
ffffffff812c3c76:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c3c79:	48 83 ec 38          	sub    $0x38,%rsp
	struct kobj_uevent_env *env;
	const char *action_string = kobject_actions[action];
ffffffff812c3c7d:	48 8b 04 c5 20 f1 63 	mov    -0x7e9c0ee0(,%rax,8),%rax
ffffffff812c3c84:	81 
 * Returns 0 if kobject_uevent_env() is completed with success or the
 * corresponding error when it fails.
 */
int kobject_uevent_env(struct kobject *kobj, enum kobject_action action,
		       char *envp_ext[])
{
ffffffff812c3c85:	89 75 b0             	mov    %esi,-0x50(%rbp)
	int retval = 0;
#ifdef CONFIG_NET
	struct uevent_sock *ue_sk;
#endif

	pr_debug("kobject: '%s' (%p): %s\n",
ffffffff812c3c88:	48 8b 37             	mov    (%rdi),%rsi
 * Returns 0 if kobject_uevent_env() is completed with success or the
 * corresponding error when it fails.
 */
int kobject_uevent_env(struct kobject *kobj, enum kobject_action action,
		       char *envp_ext[])
{
ffffffff812c3c8b:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
	int retval = 0;
#ifdef CONFIG_NET
	struct uevent_sock *ue_sk;
#endif

	pr_debug("kobject: '%s' (%p): %s\n",
ffffffff812c3c8f:	48 89 fa             	mov    %rdi,%rdx
ffffffff812c3c92:	48 c7 c7 90 65 7b 81 	mov    $0xffffffff817b6590,%rdi
 */
int kobject_uevent_env(struct kobject *kobj, enum kobject_action action,
		       char *envp_ext[])
{
	struct kobj_uevent_env *env;
	const char *action_string = kobject_actions[action];
ffffffff812c3c99:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
	int retval = 0;
#ifdef CONFIG_NET
	struct uevent_sock *ue_sk;
#endif

	pr_debug("kobject: '%s' (%p): %s\n",
ffffffff812c3c9d:	31 c0                	xor    %eax,%eax
ffffffff812c3c9f:	e8 ee f2 16 00       	callq  ffffffff81432f92 <printk>
		 kobject_name(kobj), kobj, __func__);

	/* search the kset we belong to */
	top_kobj = kobj;
ffffffff812c3ca4:	48 89 d8             	mov    %rbx,%rax
	while (!top_kobj->kset && top_kobj->parent)
ffffffff812c3ca7:	4c 8b 68 20          	mov    0x20(%rax),%r13
ffffffff812c3cab:	4d 85 ed             	test   %r13,%r13
ffffffff812c3cae:	75 2f                	jne    ffffffff812c3cdf <kobject_uevent_env+0x7f>
ffffffff812c3cb0:	48 8b 40 18          	mov    0x18(%rax),%rax
ffffffff812c3cb4:	48 85 c0             	test   %rax,%rax
ffffffff812c3cb7:	75 ee                	jne    ffffffff812c3ca7 <kobject_uevent_env+0x47>
		top_kobj = top_kobj->parent;

	if (!top_kobj->kset) {
		pr_debug("kobject: '%s' (%p): %s: attempted to send uevent "
ffffffff812c3cb9:	48 8b 33             	mov    (%rbx),%rsi
ffffffff812c3cbc:	48 c7 c1 00 f1 63 81 	mov    $0xffffffff8163f100,%rcx
ffffffff812c3cc3:	48 89 da             	mov    %rbx,%rdx
ffffffff812c3cc6:	48 c7 c7 fd 6b 7b 81 	mov    $0xffffffff817b6bfd,%rdi
ffffffff812c3ccd:	31 c0                	xor    %eax,%eax
ffffffff812c3ccf:	e8 be f2 16 00       	callq  ffffffff81432f92 <printk>
			 "without kset!\n", kobject_name(kobj), kobj,
			 __func__);
		return -EINVAL;
ffffffff812c3cd4:	41 b9 ea ff ff ff    	mov    $0xffffffea,%r9d
ffffffff812c3cda:	e9 ce 04 00 00       	jmpq   ffffffff812c41ad <kobject_uevent_env+0x54d>

	kset = top_kobj->kset;
	uevent_ops = kset->uevent_ops;

	/* skip the event, if uevent_suppress is set*/
	if (kobj->uevent_suppress) {
ffffffff812c3cdf:	f6 43 3c 10          	testb  $0x10,0x3c(%rbx)
			 __func__);
		return -EINVAL;
	}

	kset = top_kobj->kset;
	uevent_ops = kset->uevent_ops;
ffffffff812c3ce3:	4d 8b 75 68          	mov    0x68(%r13),%r14

	/* skip the event, if uevent_suppress is set*/
	if (kobj->uevent_suppress) {
ffffffff812c3ce7:	74 16                	je     ffffffff812c3cff <kobject_uevent_env+0x9f>
		pr_debug("kobject: '%s' (%p): %s: uevent_suppress "
ffffffff812c3ce9:	48 c7 c1 00 f1 63 81 	mov    $0xffffffff8163f100,%rcx
ffffffff812c3cf0:	48 89 da             	mov    %rbx,%rdx
ffffffff812c3cf3:	48 8b 33             	mov    (%rbx),%rsi
ffffffff812c3cf6:	48 c7 c7 9f 6a 7b 81 	mov    $0xffffffff817b6a9f,%rdi
ffffffff812c3cfd:	eb 6b                	jmp    ffffffff812c3d6a <kobject_uevent_env+0x10a>
				 "caused the event to drop!\n",
				 kobject_name(kobj), kobj, __func__);
		return 0;
	}
	/* skip the event, if the filter returns zero. */
	if (uevent_ops && uevent_ops->filter)
ffffffff812c3cff:	4d 85 f6             	test   %r14,%r14
ffffffff812c3d02:	74 43                	je     ffffffff812c3d47 <kobject_uevent_env+0xe7>
ffffffff812c3d04:	49 8b 06             	mov    (%r14),%rax
ffffffff812c3d07:	48 85 c0             	test   %rax,%rax
ffffffff812c3d0a:	0f 84 83 04 00 00    	je     ffffffff812c4193 <kobject_uevent_env+0x533>
		if (!uevent_ops->filter(kset, kobj)) {
ffffffff812c3d10:	48 89 de             	mov    %rbx,%rsi
ffffffff812c3d13:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c3d16:	ff d0                	callq  *%rax
ffffffff812c3d18:	85 c0                	test   %eax,%eax
ffffffff812c3d1a:	0f 85 73 04 00 00    	jne    ffffffff812c4193 <kobject_uevent_env+0x533>
			pr_debug("kobject: '%s' (%p): %s: filter function "
ffffffff812c3d20:	48 8b 33             	mov    (%rbx),%rsi
ffffffff812c3d23:	89 45 c8             	mov    %eax,-0x38(%rbp)
ffffffff812c3d26:	48 c7 c1 00 f1 63 81 	mov    $0xffffffff8163f100,%rcx
ffffffff812c3d2d:	48 89 da             	mov    %rbx,%rdx
ffffffff812c3d30:	48 c7 c7 e4 6a 7b 81 	mov    $0xffffffff817b6ae4,%rdi
ffffffff812c3d37:	31 c0                	xor    %eax,%eax
ffffffff812c3d39:	e8 54 f2 16 00       	callq  ffffffff81432f92 <printk>
				 "caused the event to drop!\n",
				 kobject_name(kobj), kobj, __func__);
			return 0;
ffffffff812c3d3e:	44 8b 4d c8          	mov    -0x38(%rbp),%r9d
ffffffff812c3d42:	e9 66 04 00 00       	jmpq   ffffffff812c41ad <kobject_uevent_env+0x54d>
ffffffff812c3d47:	49 8b 45 28          	mov    0x28(%r13),%rax
ffffffff812c3d4b:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
	/* originating subsystem */
	if (uevent_ops && uevent_ops->name)
		subsystem = uevent_ops->name(kset, kobj);
	else
		subsystem = kobject_name(&kset->kobj);
	if (!subsystem) {
ffffffff812c3d4f:	48 83 7d c0 00       	cmpq   $0x0,-0x40(%rbp)
ffffffff812c3d54:	75 23                	jne    ffffffff812c3d79 <kobject_uevent_env+0x119>
		pr_debug("kobject: '%s' (%p): %s: unset subsystem caused the "
ffffffff812c3d56:	48 8b 33             	mov    (%rbx),%rsi
ffffffff812c3d59:	48 c7 c1 00 f1 63 81 	mov    $0xffffffff8163f100,%rcx
ffffffff812c3d60:	48 89 da             	mov    %rbx,%rdx
ffffffff812c3d63:	48 c7 c7 29 6b 7b 81 	mov    $0xffffffff817b6b29,%rdi
ffffffff812c3d6a:	31 c0                	xor    %eax,%eax
ffffffff812c3d6c:	e8 21 f2 16 00       	callq  ffffffff81432f92 <printk>
			 "event to drop!\n", kobject_name(kobj), kobj,
			 __func__);
		return 0;
ffffffff812c3d71:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c3d74:	e9 34 04 00 00       	jmpq   ffffffff812c41ad <kobject_uevent_env+0x54d>

#else /* CONFIG_TRACING */
static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
		gfp_t flags, size_t size)
{
	void *ret = kmem_cache_alloc(s, flags);
ffffffff812c3d79:	48 8b 3d 20 e9 8d 00 	mov    0x8de920(%rip),%rdi        # ffffffff81ba26a0 <kmalloc_caches+0x60>
ffffffff812c3d80:	be d0 80 00 00       	mov    $0x80d0,%esi
ffffffff812c3d85:	e8 47 d3 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
	}

	/* environment buffer */
	env = kzalloc(sizeof(struct kobj_uevent_env), GFP_KERNEL);
	if (!env)
ffffffff812c3d8a:	48 85 c0             	test   %rax,%rax
ffffffff812c3d8d:	49 89 c4             	mov    %rax,%r12
		return -ENOMEM;
ffffffff812c3d90:	41 b9 f4 ff ff ff    	mov    $0xfffffff4,%r9d
		return 0;
	}

	/* environment buffer */
	env = kzalloc(sizeof(struct kobj_uevent_env), GFP_KERNEL);
	if (!env)
ffffffff812c3d96:	0f 84 11 04 00 00    	je     ffffffff812c41ad <kobject_uevent_env+0x54d>
		return -ENOMEM;

	/* complete object path */
	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c3d9c:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812c3da1:	48 89 df             	mov    %rbx,%rdi
	if (!devpath) {
		retval = -ENOENT;
ffffffff812c3da4:	41 bf fe ff ff ff    	mov    $0xfffffffe,%r15d
	env = kzalloc(sizeof(struct kobj_uevent_env), GFP_KERNEL);
	if (!env)
		return -ENOMEM;

	/* complete object path */
	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c3daa:	e8 fa f0 ff ff       	callq  ffffffff812c2ea9 <kobject_get_path>
	if (!devpath) {
ffffffff812c3daf:	48 85 c0             	test   %rax,%rax
	env = kzalloc(sizeof(struct kobj_uevent_env), GFP_KERNEL);
	if (!env)
		return -ENOMEM;

	/* complete object path */
	devpath = kobject_get_path(kobj, GFP_KERNEL);
ffffffff812c3db2:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	if (!devpath) {
ffffffff812c3db6:	0f 84 c1 03 00 00    	je     ffffffff812c417d <kobject_uevent_env+0x51d>
		retval = -ENOENT;
		goto exit;
	}

	/* default keys */
	retval = add_uevent_var(env, "ACTION=%s", action_string);
ffffffff812c3dbc:	48 8b 55 b8          	mov    -0x48(%rbp),%rdx
ffffffff812c3dc0:	31 c0                	xor    %eax,%eax
ffffffff812c3dc2:	48 c7 c6 6e 6b 7b 81 	mov    $0xffffffff817b6b6e,%rsi
ffffffff812c3dc9:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3dcc:	e8 45 fd ff ff       	callq  ffffffff812c3b16 <add_uevent_var>
	if (retval)
ffffffff812c3dd1:	85 c0                	test   %eax,%eax
		retval = -ENOENT;
		goto exit;
	}

	/* default keys */
	retval = add_uevent_var(env, "ACTION=%s", action_string);
ffffffff812c3dd3:	41 89 c7             	mov    %eax,%r15d
	if (retval)
ffffffff812c3dd6:	0f 85 a1 03 00 00    	jne    ffffffff812c417d <kobject_uevent_env+0x51d>
		goto exit;
	retval = add_uevent_var(env, "DEVPATH=%s", devpath);
ffffffff812c3ddc:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
ffffffff812c3de0:	31 c0                	xor    %eax,%eax
ffffffff812c3de2:	48 c7 c6 78 6b 7b 81 	mov    $0xffffffff817b6b78,%rsi
ffffffff812c3de9:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3dec:	e8 25 fd ff ff       	callq  ffffffff812c3b16 <add_uevent_var>
	if (retval)
ffffffff812c3df1:	85 c0                	test   %eax,%eax

	/* default keys */
	retval = add_uevent_var(env, "ACTION=%s", action_string);
	if (retval)
		goto exit;
	retval = add_uevent_var(env, "DEVPATH=%s", devpath);
ffffffff812c3df3:	41 89 c7             	mov    %eax,%r15d
	if (retval)
ffffffff812c3df6:	0f 85 81 03 00 00    	jne    ffffffff812c417d <kobject_uevent_env+0x51d>
		goto exit;
	retval = add_uevent_var(env, "SUBSYSTEM=%s", subsystem);
ffffffff812c3dfc:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
ffffffff812c3e00:	31 c0                	xor    %eax,%eax
ffffffff812c3e02:	48 c7 c6 83 6b 7b 81 	mov    $0xffffffff817b6b83,%rsi
ffffffff812c3e09:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3e0c:	e8 05 fd ff ff       	callq  ffffffff812c3b16 <add_uevent_var>
	if (retval)
ffffffff812c3e11:	85 c0                	test   %eax,%eax
	if (retval)
		goto exit;
	retval = add_uevent_var(env, "DEVPATH=%s", devpath);
	if (retval)
		goto exit;
	retval = add_uevent_var(env, "SUBSYSTEM=%s", subsystem);
ffffffff812c3e13:	41 89 c7             	mov    %eax,%r15d
	if (retval)
ffffffff812c3e16:	0f 85 61 03 00 00    	jne    ffffffff812c417d <kobject_uevent_env+0x51d>
ffffffff812c3e1c:	31 c9                	xor    %ecx,%ecx
		goto exit;

	/* keys passed in from the caller */
	if (envp_ext) {
ffffffff812c3e1e:	48 83 7d a8 00       	cmpq   $0x0,-0x58(%rbp)
ffffffff812c3e23:	75 2b                	jne    ffffffff812c3e50 <kobject_uevent_env+0x1f0>
				goto exit;
		}
	}

	/* let the kset specific function add its stuff */
	if (uevent_ops && uevent_ops->uevent) {
ffffffff812c3e25:	4d 85 f6             	test   %r14,%r14
ffffffff812c3e28:	75 38                	jne    ffffffff812c3e62 <kobject_uevent_env+0x202>
ffffffff812c3e2a:	eb 74                	jmp    ffffffff812c3ea0 <kobject_uevent_env+0x240>
		goto exit;

	/* keys passed in from the caller */
	if (envp_ext) {
		for (i = 0; envp_ext[i]; i++) {
			retval = add_uevent_var(env, "%s", envp_ext[i]);
ffffffff812c3e2c:	31 c0                	xor    %eax,%eax
ffffffff812c3e2e:	48 c7 c6 0f b6 7c 81 	mov    $0xffffffff817cb60f,%rsi
ffffffff812c3e35:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3e38:	89 4d a4             	mov    %ecx,-0x5c(%rbp)
ffffffff812c3e3b:	e8 d6 fc ff ff       	callq  ffffffff812c3b16 <add_uevent_var>
			if (retval)
ffffffff812c3e40:	85 c0                	test   %eax,%eax
		goto exit;

	/* keys passed in from the caller */
	if (envp_ext) {
		for (i = 0; envp_ext[i]; i++) {
			retval = add_uevent_var(env, "%s", envp_ext[i]);
ffffffff812c3e42:	41 89 c7             	mov    %eax,%r15d
			if (retval)
ffffffff812c3e45:	0f 85 32 03 00 00    	jne    ffffffff812c417d <kobject_uevent_env+0x51d>
	if (retval)
		goto exit;

	/* keys passed in from the caller */
	if (envp_ext) {
		for (i = 0; envp_ext[i]; i++) {
ffffffff812c3e4b:	8b 4d a4             	mov    -0x5c(%rbp),%ecx
ffffffff812c3e4e:	ff c1                	inc    %ecx
ffffffff812c3e50:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
ffffffff812c3e54:	48 63 c1             	movslq %ecx,%rax
ffffffff812c3e57:	48 8b 14 c2          	mov    (%rdx,%rax,8),%rdx
ffffffff812c3e5b:	48 85 d2             	test   %rdx,%rdx
ffffffff812c3e5e:	75 cc                	jne    ffffffff812c3e2c <kobject_uevent_env+0x1cc>
ffffffff812c3e60:	eb c3                	jmp    ffffffff812c3e25 <kobject_uevent_env+0x1c5>
				goto exit;
		}
	}

	/* let the kset specific function add its stuff */
	if (uevent_ops && uevent_ops->uevent) {
ffffffff812c3e62:	49 8b 46 10          	mov    0x10(%r14),%rax
ffffffff812c3e66:	48 85 c0             	test   %rax,%rax
ffffffff812c3e69:	74 35                	je     ffffffff812c3ea0 <kobject_uevent_env+0x240>
		retval = uevent_ops->uevent(kset, kobj, env);
ffffffff812c3e6b:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c3e6e:	48 89 de             	mov    %rbx,%rsi
ffffffff812c3e71:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c3e74:	ff d0                	callq  *%rax
		if (retval) {
ffffffff812c3e76:	85 c0                	test   %eax,%eax
		}
	}

	/* let the kset specific function add its stuff */
	if (uevent_ops && uevent_ops->uevent) {
		retval = uevent_ops->uevent(kset, kobj, env);
ffffffff812c3e78:	41 89 c7             	mov    %eax,%r15d
		if (retval) {
ffffffff812c3e7b:	74 23                	je     ffffffff812c3ea0 <kobject_uevent_env+0x240>
			pr_debug("kobject: '%s' (%p): %s: uevent() returned "
ffffffff812c3e7d:	48 8b 33             	mov    (%rbx),%rsi
ffffffff812c3e80:	41 89 c0             	mov    %eax,%r8d
ffffffff812c3e83:	48 c7 c1 00 f1 63 81 	mov    $0xffffffff8163f100,%rcx
ffffffff812c3e8a:	48 89 da             	mov    %rbx,%rdx
ffffffff812c3e8d:	48 c7 c7 90 6b 7b 81 	mov    $0xffffffff817b6b90,%rdi
ffffffff812c3e94:	31 c0                	xor    %eax,%eax
ffffffff812c3e96:	e8 f7 f0 16 00       	callq  ffffffff81432f92 <printk>
				 "%d\n", kobject_name(kobj), kobj,
				 __func__, retval);
			goto exit;
ffffffff812c3e9b:	e9 dd 02 00 00       	jmpq   ffffffff812c417d <kobject_uevent_env+0x51d>
	 * Mark "add" and "remove" events in the object to ensure proper
	 * events to userspace during automatic cleanup. If the object did
	 * send an "add" event, "remove" will automatically generated by
	 * the core, if not already done by the caller.
	 */
	if (action == KOBJ_ADD)
ffffffff812c3ea0:	83 7d b0 00          	cmpl   $0x0,-0x50(%rbp)
ffffffff812c3ea4:	75 06                	jne    ffffffff812c3eac <kobject_uevent_env+0x24c>
		kobj->state_add_uevent_sent = 1;
ffffffff812c3ea6:	80 4b 3c 04          	orb    $0x4,0x3c(%rbx)
ffffffff812c3eaa:	eb 0a                	jmp    ffffffff812c3eb6 <kobject_uevent_env+0x256>
	else if (action == KOBJ_REMOVE)
ffffffff812c3eac:	83 7d b0 01          	cmpl   $0x1,-0x50(%rbp)
ffffffff812c3eb0:	75 04                	jne    ffffffff812c3eb6 <kobject_uevent_env+0x256>
		kobj->state_remove_uevent_sent = 1;
ffffffff812c3eb2:	80 4b 3c 08          	orb    $0x8,0x3c(%rbx)

	mutex_lock(&uevent_sock_mutex);
ffffffff812c3eb6:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
ffffffff812c3ebd:	e8 d0 36 17 00       	callq  ffffffff81437592 <mutex_lock>
	/* we will send an event, so request a new sequence number */
	retval = add_uevent_var(env, "SEQNUM=%llu", (unsigned long long)++uevent_seqnum);
ffffffff812c3ec2:	48 8b 05 d7 d9 8e 00 	mov    0x8ed9d7(%rip),%rax        # ffffffff81bb18a0 <uevent_seqnum>
ffffffff812c3ec9:	48 c7 c6 c0 6b 7b 81 	mov    $0xffffffff817b6bc0,%rsi
ffffffff812c3ed0:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c3ed3:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812c3ed7:	31 c0                	xor    %eax,%eax
ffffffff812c3ed9:	48 89 15 c0 d9 8e 00 	mov    %rdx,0x8ed9c0(%rip)        # ffffffff81bb18a0 <uevent_seqnum>
ffffffff812c3ee0:	e8 31 fc ff ff       	callq  ffffffff812c3b16 <add_uevent_var>
	if (retval) {
ffffffff812c3ee5:	85 c0                	test   %eax,%eax
	else if (action == KOBJ_REMOVE)
		kobj->state_remove_uevent_sent = 1;

	mutex_lock(&uevent_sock_mutex);
	/* we will send an event, so request a new sequence number */
	retval = add_uevent_var(env, "SEQNUM=%llu", (unsigned long long)++uevent_seqnum);
ffffffff812c3ee7:	41 89 c5             	mov    %eax,%r13d
	if (retval) {
ffffffff812c3eea:	74 14                	je     ffffffff812c3f00 <kobject_uevent_env+0x2a0>
		mutex_unlock(&uevent_sock_mutex);
ffffffff812c3eec:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
		goto exit;
ffffffff812c3ef3:	45 89 ef             	mov    %r13d,%r15d

	mutex_lock(&uevent_sock_mutex);
	/* we will send an event, so request a new sequence number */
	retval = add_uevent_var(env, "SEQNUM=%llu", (unsigned long long)++uevent_seqnum);
	if (retval) {
		mutex_unlock(&uevent_sock_mutex);
ffffffff812c3ef6:	e8 b9 37 17 00       	callq  ffffffff814376b4 <mutex_unlock>
		goto exit;
ffffffff812c3efb:	e9 7d 02 00 00       	jmpq   ffffffff812c417d <kobject_uevent_env+0x51d>
	}

#if defined(CONFIG_NET)
	/* send netlink message */
	list_for_each_entry(ue_sk, &uevent_sock_list, list) {
ffffffff812c3f00:	4c 8b 2d 39 21 77 00 	mov    0x772139(%rip),%r13        # ffffffff81a36040 <uevent_sock_list>
ffffffff812c3f07:	45 31 ff             	xor    %r15d,%r15d
ffffffff812c3f0a:	49 81 fd 40 60 a3 81 	cmp    $0xffffffff81a36040,%r13
ffffffff812c3f11:	0f 84 1e 01 00 00    	je     ffffffff812c4035 <kobject_uevent_env+0x3d5>
		struct sock *uevent_sock = ue_sk->sk;
ffffffff812c3f17:	49 8b 45 10          	mov    0x10(%r13),%rax
		struct sk_buff *skb;
		size_t len;

		if (!netlink_has_listeners(uevent_sock, 1))
ffffffff812c3f1b:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c3f20:	48 89 c7             	mov    %rax,%rdi
	}

#if defined(CONFIG_NET)
	/* send netlink message */
	list_for_each_entry(ue_sk, &uevent_sock_list, list) {
		struct sock *uevent_sock = ue_sk->sk;
ffffffff812c3f23:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
		struct sk_buff *skb;
		size_t len;

		if (!netlink_has_listeners(uevent_sock, 1))
ffffffff812c3f27:	e8 4b af 11 00       	callq  ffffffff813dee77 <netlink_has_listeners>
ffffffff812c3f2c:	85 c0                	test   %eax,%eax
ffffffff812c3f2e:	0f 84 f8 00 00 00    	je     ffffffff812c402c <kobject_uevent_env+0x3cc>
			continue;

		/* allocate message with the maximum possible size */
		len = strlen(action_string) + strlen(devpath) + 2;
ffffffff812c3f34:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812c3f38:	31 c0                	xor    %eax,%eax
ffffffff812c3f3a:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
struct sk_buff *__build_skb(void *data, unsigned int frag_size);
struct sk_buff *build_skb(void *data, unsigned int frag_size);
static inline struct sk_buff *alloc_skb(unsigned int size,
					gfp_t priority)
{
	return __alloc_skb(size, priority, 0, NUMA_NO_NODE);
ffffffff812c3f3e:	31 d2                	xor    %edx,%edx
ffffffff812c3f40:	be d0 00 00 00       	mov    $0xd0,%esi
							    kobj);
			/* ENOBUFS should be handled in userspace */
			if (retval == -ENOBUFS || retval == -ESRCH)
				retval = 0;
		} else
			retval = -ENOMEM;
ffffffff812c3f45:	41 bf f4 ff ff ff    	mov    $0xfffffff4,%r15d

		if (!netlink_has_listeners(uevent_sock, 1))
			continue;

		/* allocate message with the maximum possible size */
		len = strlen(action_string) + strlen(devpath) + 2;
ffffffff812c3f4b:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c3f4d:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
ffffffff812c3f51:	48 f7 d1             	not    %rcx
ffffffff812c3f54:	49 89 ce             	mov    %rcx,%r14
ffffffff812c3f57:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812c3f5b:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c3f5d:	41 8b bc 24 1c 09 00 	mov    0x91c(%r12),%edi
ffffffff812c3f64:	00 
ffffffff812c3f65:	48 89 c8             	mov    %rcx,%rax
ffffffff812c3f68:	83 c9 ff             	or     $0xffffffff,%ecx
ffffffff812c3f6b:	48 f7 d0             	not    %rax
		skb = alloc_skb(len + env->buflen, GFP_KERNEL);
ffffffff812c3f6e:	49 01 c6             	add    %rax,%r14
ffffffff812c3f71:	44 01 f7             	add    %r14d,%edi
ffffffff812c3f74:	e8 4b 92 0f 00       	callq  ffffffff813bd1c4 <__alloc_skb>
		if (skb) {
ffffffff812c3f79:	48 85 c0             	test   %rax,%rax
ffffffff812c3f7c:	0f 84 aa 00 00 00    	je     ffffffff812c402c <kobject_uevent_env+0x3cc>
			char *scratch;

			/* add header */
			scratch = skb_put(skb, len);
ffffffff812c3f82:	44 89 f6             	mov    %r14d,%esi
ffffffff812c3f85:	48 89 c7             	mov    %rax,%rdi
ffffffff812c3f88:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
ffffffff812c3f8c:	e8 bb 73 0f 00       	callq  ffffffff813bb34c <skb_put>
			sprintf(scratch, "%s@%s", action_string, devpath);
ffffffff812c3f91:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
ffffffff812c3f95:	48 8b 55 b8          	mov    -0x48(%rbp),%rdx
ffffffff812c3f99:	48 89 c7             	mov    %rax,%rdi
ffffffff812c3f9c:	48 c7 c6 cc 6b 7b 81 	mov    $0xffffffff817b6bcc,%rsi
ffffffff812c3fa3:	31 c0                	xor    %eax,%eax

			/* copy keys to our continuous event payload buffer */
			for (i = 0; i < env->envp_idx; i++) {
ffffffff812c3fa5:	45 31 f6             	xor    %r14d,%r14d
		if (skb) {
			char *scratch;

			/* add header */
			scratch = skb_put(skb, len);
			sprintf(scratch, "%s@%s", action_string, devpath);
ffffffff812c3fa8:	e8 68 68 00 00       	callq  ffffffff812ca815 <sprintf>
ffffffff812c3fad:	44 89 f0             	mov    %r14d,%eax
ffffffff812c3fb0:	49 ff c6             	inc    %r14

			/* copy keys to our continuous event payload buffer */
			for (i = 0; i < env->envp_idx; i++) {
ffffffff812c3fb3:	41 3b 84 24 18 01 00 	cmp    0x118(%r12),%eax
ffffffff812c3fba:	00 
				len = strlen(env->envp[i]) + 1;
				scratch = skb_put(skb, len);
				strcpy(scratch, env->envp[i]);
ffffffff812c3fbb:	4c 8b 55 a8          	mov    -0x58(%rbp),%r10
			/* add header */
			scratch = skb_put(skb, len);
			sprintf(scratch, "%s@%s", action_string, devpath);

			/* copy keys to our continuous event payload buffer */
			for (i = 0; i < env->envp_idx; i++) {
ffffffff812c3fbf:	7d 2e                	jge    ffffffff812c3fef <kobject_uevent_env+0x38f>
				len = strlen(env->envp[i]) + 1;
ffffffff812c3fc1:	4b 8b 7c f4 10       	mov    0x10(%r12,%r14,8),%rdi
ffffffff812c3fc6:	31 c0                	xor    %eax,%eax
ffffffff812c3fc8:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812c3fcc:	f2 ae                	repnz scas %es:(%rdi),%al
				scratch = skb_put(skb, len);
ffffffff812c3fce:	4c 89 d7             	mov    %r10,%rdi
ffffffff812c3fd1:	4c 89 55 a8          	mov    %r10,-0x58(%rbp)
			scratch = skb_put(skb, len);
			sprintf(scratch, "%s@%s", action_string, devpath);

			/* copy keys to our continuous event payload buffer */
			for (i = 0; i < env->envp_idx; i++) {
				len = strlen(env->envp[i]) + 1;
ffffffff812c3fd5:	48 89 ce             	mov    %rcx,%rsi
ffffffff812c3fd8:	48 f7 d6             	not    %rsi
				scratch = skb_put(skb, len);
ffffffff812c3fdb:	e8 6c 73 0f 00       	callq  ffffffff813bb34c <skb_put>
				strcpy(scratch, env->envp[i]);
ffffffff812c3fe0:	4b 8b 74 f4 10       	mov    0x10(%r12,%r14,8),%rsi
ffffffff812c3fe5:	48 89 c7             	mov    %rax,%rdi
ffffffff812c3fe8:	e8 07 3c 00 00       	callq  ffffffff812c7bf4 <strcpy>
ffffffff812c3fed:	eb be                	jmp    ffffffff812c3fad <kobject_uevent_env+0x34d>
			}

			NETLINK_CB(skb).dst_group = 1;
			retval = netlink_broadcast_filtered(uevent_sock, skb,
ffffffff812c3fef:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
				len = strlen(env->envp[i]) + 1;
				scratch = skb_put(skb, len);
				strcpy(scratch, env->envp[i]);
			}

			NETLINK_CB(skb).dst_group = 1;
ffffffff812c3ff3:	41 c7 42 38 01 00 00 	movl   $0x1,0x38(%r10)
ffffffff812c3ffa:	00 
			retval = netlink_broadcast_filtered(uevent_sock, skb,
ffffffff812c3ffb:	41 b8 d0 00 00 00    	mov    $0xd0,%r8d
ffffffff812c4001:	56                   	push   %rsi
ffffffff812c4002:	53                   	push   %rbx
ffffffff812c4003:	31 d2                	xor    %edx,%edx
ffffffff812c4005:	49 c7 c1 9e 3a 2c 81 	mov    $0xffffffff812c3a9e,%r9
ffffffff812c400c:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812c4011:	4c 89 d6             	mov    %r10,%rsi
ffffffff812c4014:	e8 24 b4 11 00       	callq  ffffffff813df43d <netlink_broadcast_filtered>
							    0, 1, GFP_KERNEL,
							    kobj_bcast_filter,
							    kobj);
			/* ENOBUFS should be handled in userspace */
			if (retval == -ENOBUFS || retval == -ESRCH)
ffffffff812c4019:	83 f8 97             	cmp    $0xffffff97,%eax
				scratch = skb_put(skb, len);
				strcpy(scratch, env->envp[i]);
			}

			NETLINK_CB(skb).dst_group = 1;
			retval = netlink_broadcast_filtered(uevent_sock, skb,
ffffffff812c401c:	41 89 c7             	mov    %eax,%r15d
							    0, 1, GFP_KERNEL,
							    kobj_bcast_filter,
							    kobj);
			/* ENOBUFS should be handled in userspace */
			if (retval == -ENOBUFS || retval == -ESRCH)
ffffffff812c401f:	5f                   	pop    %rdi
ffffffff812c4020:	41 58                	pop    %r8
ffffffff812c4022:	74 05                	je     ffffffff812c4029 <kobject_uevent_env+0x3c9>
ffffffff812c4024:	83 f8 fd             	cmp    $0xfffffffd,%eax
ffffffff812c4027:	75 03                	jne    ffffffff812c402c <kobject_uevent_env+0x3cc>
				retval = 0;
ffffffff812c4029:	45 31 ff             	xor    %r15d,%r15d
		goto exit;
	}

#if defined(CONFIG_NET)
	/* send netlink message */
	list_for_each_entry(ue_sk, &uevent_sock_list, list) {
ffffffff812c402c:	4d 8b 6d 00          	mov    0x0(%r13),%r13
ffffffff812c4030:	e9 d5 fe ff ff       	jmpq   ffffffff812c3f0a <kobject_uevent_env+0x2aa>
				retval = 0;
		} else
			retval = -ENOMEM;
	}
#endif
	mutex_unlock(&uevent_sock_mutex);
ffffffff812c4035:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
ffffffff812c403c:	e8 73 36 17 00       	callq  ffffffff814376b4 <mutex_unlock>

#ifdef CONFIG_UEVENT_HELPER
	/* call uevent_helper, usually only enabled during early boot */
	if (uevent_helper[0] && !kobj_usermode_filter(kobj)) {
ffffffff812c4041:	80 3d 58 d7 8e 00 00 	cmpb   $0x0,0x8ed758(%rip)        # ffffffff81bb17a0 <uevent_helper>
ffffffff812c4048:	0f 84 2f 01 00 00    	je     ffffffff812c417d <kobject_uevent_env+0x51d>
#ifdef CONFIG_UEVENT_HELPER
static int kobj_usermode_filter(struct kobject *kobj)
{
	const struct kobj_ns_type_operations *ops;

	ops = kobj_ns_ops(kobj);
ffffffff812c404e:	48 89 df             	mov    %rbx,%rdi
ffffffff812c4051:	e8 5b f1 ff ff       	callq  ffffffff812c31b1 <kobj_ns_ops>
	if (ops) {
ffffffff812c4056:	48 85 c0             	test   %rax,%rax
#ifdef CONFIG_UEVENT_HELPER
static int kobj_usermode_filter(struct kobject *kobj)
{
	const struct kobj_ns_type_operations *ops;

	ops = kobj_ns_ops(kobj);
ffffffff812c4059:	49 89 c5             	mov    %rax,%r13
	if (ops) {
ffffffff812c405c:	75 1e                	jne    ffffffff812c407c <kobject_uevent_env+0x41c>
#ifdef CONFIG_UEVENT_HELPER
	/* call uevent_helper, usually only enabled during early boot */
	if (uevent_helper[0] && !kobj_usermode_filter(kobj)) {
		struct subprocess_info *info;

		retval = add_uevent_var(env, "HOME=/");
ffffffff812c405e:	31 c0                	xor    %eax,%eax
ffffffff812c4060:	48 c7 c6 71 44 78 81 	mov    $0xffffffff81784471,%rsi
ffffffff812c4067:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c406a:	e8 a7 fa ff ff       	callq  ffffffff812c3b16 <add_uevent_var>
		if (retval)
ffffffff812c406f:	85 c0                	test   %eax,%eax
#ifdef CONFIG_UEVENT_HELPER
	/* call uevent_helper, usually only enabled during early boot */
	if (uevent_helper[0] && !kobj_usermode_filter(kobj)) {
		struct subprocess_info *info;

		retval = add_uevent_var(env, "HOME=/");
ffffffff812c4071:	41 89 c7             	mov    %eax,%r15d
		if (retval)
ffffffff812c4074:	0f 85 03 01 00 00    	jne    ffffffff812c417d <kobject_uevent_env+0x51d>
ffffffff812c407a:	eb 1c                	jmp    ffffffff812c4098 <kobject_uevent_env+0x438>
	const struct kobj_ns_type_operations *ops;

	ops = kobj_ns_ops(kobj);
	if (ops) {
		const void *init_ns, *ns;
		ns = kobj->ktype->namespace(kobj);
ffffffff812c407c:	48 8b 43 28          	mov    0x28(%rbx),%rax
ffffffff812c4080:	48 89 df             	mov    %rbx,%rdi
ffffffff812c4083:	ff 50 20             	callq  *0x20(%rax)
ffffffff812c4086:	48 89 c3             	mov    %rax,%rbx
		init_ns = ops->initial_ns();
ffffffff812c4089:	41 ff 55 20          	callq  *0x20(%r13)
#endif
	mutex_unlock(&uevent_sock_mutex);

#ifdef CONFIG_UEVENT_HELPER
	/* call uevent_helper, usually only enabled during early boot */
	if (uevent_helper[0] && !kobj_usermode_filter(kobj)) {
ffffffff812c408d:	48 39 c3             	cmp    %rax,%rbx
ffffffff812c4090:	0f 85 e7 00 00 00    	jne    ffffffff812c417d <kobject_uevent_env+0x51d>
ffffffff812c4096:	eb c6                	jmp    ffffffff812c405e <kobject_uevent_env+0x3fe>
		struct subprocess_info *info;

		retval = add_uevent_var(env, "HOME=/");
		if (retval)
			goto exit;
		retval = add_uevent_var(env,
ffffffff812c4098:	31 c0                	xor    %eax,%eax
ffffffff812c409a:	48 c7 c6 c7 3c 79 81 	mov    $0xffffffff81793cc7,%rsi
ffffffff812c40a1:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c40a4:	e8 6d fa ff ff       	callq  ffffffff812c3b16 <add_uevent_var>
					"PATH=/sbin:/bin:/usr/sbin:/usr/bin");
		if (retval)
ffffffff812c40a9:	85 c0                	test   %eax,%eax
		struct subprocess_info *info;

		retval = add_uevent_var(env, "HOME=/");
		if (retval)
			goto exit;
		retval = add_uevent_var(env,
ffffffff812c40ab:	41 89 c7             	mov    %eax,%r15d
					"PATH=/sbin:/bin:/usr/sbin:/usr/bin");
		if (retval)
ffffffff812c40ae:	0f 85 c9 00 00 00    	jne    ffffffff812c417d <kobject_uevent_env+0x51d>

static int init_uevent_argv(struct kobj_uevent_env *env, const char *subsystem)
{
	int len;

	len = strlcpy(&env->buf[env->buflen], subsystem,
ffffffff812c40b4:	49 63 84 24 1c 09 00 	movslq 0x91c(%r12),%rax
ffffffff812c40bb:	00 
ffffffff812c40bc:	bb 00 08 00 00       	mov    $0x800,%ebx
ffffffff812c40c1:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
ffffffff812c40c5:	48 89 da             	mov    %rbx,%rdx
ffffffff812c40c8:	49 8d bc 04 1c 01 00 	lea    0x11c(%r12,%rax,1),%rdi
ffffffff812c40cf:	00 
ffffffff812c40d0:	48 29 c2             	sub    %rax,%rdx
ffffffff812c40d3:	e8 21 3f 00 00       	callq  ffffffff812c7ff9 <strlcpy>
		      sizeof(env->buf) - env->buflen);
	if (len >= (sizeof(env->buf) - env->buflen)) {
ffffffff812c40d8:	49 63 8c 24 1c 09 00 	movslq 0x91c(%r12),%rcx
ffffffff812c40df:	00 
ffffffff812c40e0:	48 63 f0             	movslq %eax,%rsi
ffffffff812c40e3:	48 29 cb             	sub    %rcx,%rbx
ffffffff812c40e6:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c40e9:	48 39 de             	cmp    %rbx,%rsi
ffffffff812c40ec:	72 22                	jb     ffffffff812c4110 <kobject_uevent_env+0x4b0>
		WARN(1, KERN_ERR "init_uevent_argv: buffer size too small\n");
ffffffff812c40ee:	48 c7 c2 d2 6b 7b 81 	mov    $0xffffffff817b6bd2,%rdx
ffffffff812c40f5:	be 88 00 00 00       	mov    $0x88,%esi
ffffffff812c40fa:	48 c7 c7 61 6a 7b 81 	mov    $0xffffffff817b6a61,%rdi
ffffffff812c4101:	31 c0                	xor    %eax,%eax
		return -ENOMEM;
ffffffff812c4103:	41 bf f4 ff ff ff    	mov    $0xfffffff4,%r15d
	int len;

	len = strlcpy(&env->buf[env->buflen], subsystem,
		      sizeof(env->buf) - env->buflen);
	if (len >= (sizeof(env->buf) - env->buflen)) {
		WARN(1, KERN_ERR "init_uevent_argv: buffer size too small\n");
ffffffff812c4109:	e8 28 22 da ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
ffffffff812c410e:	eb 6d                	jmp    ffffffff812c417d <kobject_uevent_env+0x51d>

	env->argv[0] = uevent_helper;
	env->argv[1] = &env->buf[env->buflen];
	env->argv[2] = NULL;

	env->buflen += len + 1;
ffffffff812c4110:	8d 44 02 01          	lea    0x1(%rdx,%rax,1),%eax
		WARN(1, KERN_ERR "init_uevent_argv: buffer size too small\n");
		return -ENOMEM;
	}

	env->argv[0] = uevent_helper;
	env->argv[1] = &env->buf[env->buflen];
ffffffff812c4114:	49 8d 8c 0c 1c 01 00 	lea    0x11c(%r12,%rcx,1),%rcx
ffffffff812c411b:	00 
	if (len >= (sizeof(env->buf) - env->buflen)) {
		WARN(1, KERN_ERR "init_uevent_argv: buffer size too small\n");
		return -ENOMEM;
	}

	env->argv[0] = uevent_helper;
ffffffff812c411c:	49 c7 04 24 a0 17 bb 	movq   $0xffffffff81bb17a0,(%r12)
ffffffff812c4123:	81 
	env->argv[1] = &env->buf[env->buflen];
	env->argv[2] = NULL;
ffffffff812c4124:	49 c7 44 24 10 00 00 	movq   $0x0,0x10(%r12)
ffffffff812c412b:	00 00 
		if (retval)
			goto exit;

		retval = -ENOMEM;
		info = call_usermodehelper_setup(env->argv[0], env->argv,
						 env->envp, GFP_KERNEL,
ffffffff812c412d:	49 8d 54 24 18       	lea    0x18(%r12),%rdx
		retval = init_uevent_argv(env, subsystem);
		if (retval)
			goto exit;

		retval = -ENOMEM;
		info = call_usermodehelper_setup(env->argv[0], env->argv,
ffffffff812c4132:	45 31 c0             	xor    %r8d,%r8d
		WARN(1, KERN_ERR "init_uevent_argv: buffer size too small\n");
		return -ENOMEM;
	}

	env->argv[0] = uevent_helper;
	env->argv[1] = &env->buf[env->buflen];
ffffffff812c4135:	49 89 4c 24 08       	mov    %rcx,0x8(%r12)
	env->argv[2] = NULL;

	env->buflen += len + 1;
ffffffff812c413a:	41 89 84 24 1c 09 00 	mov    %eax,0x91c(%r12)
ffffffff812c4141:	00 
		retval = init_uevent_argv(env, subsystem);
		if (retval)
			goto exit;

		retval = -ENOMEM;
		info = call_usermodehelper_setup(env->argv[0], env->argv,
ffffffff812c4142:	b9 d0 00 00 00       	mov    $0xd0,%ecx
ffffffff812c4147:	50                   	push   %rax
ffffffff812c4148:	41 54                	push   %r12
ffffffff812c414a:	49 c7 c1 8f 3a 2c 81 	mov    $0xffffffff812c3a8f,%r9
ffffffff812c4151:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c4154:	48 c7 c7 a0 17 bb 81 	mov    $0xffffffff81bb17a0,%rdi
			goto exit;
		retval = init_uevent_argv(env, subsystem);
		if (retval)
			goto exit;

		retval = -ENOMEM;
ffffffff812c415b:	41 bf f4 ff ff ff    	mov    $0xfffffff4,%r15d
		info = call_usermodehelper_setup(env->argv[0], env->argv,
ffffffff812c4161:	e8 9e fb da ff       	callq  ffffffff81073d04 <call_usermodehelper_setup>
						 env->envp, GFP_KERNEL,
						 NULL, cleanup_uevent_env, env);
		if (info) {
ffffffff812c4166:	48 85 c0             	test   %rax,%rax
ffffffff812c4169:	5a                   	pop    %rdx
ffffffff812c416a:	59                   	pop    %rcx
ffffffff812c416b:	74 10                	je     ffffffff812c417d <kobject_uevent_env+0x51d>
			retval = call_usermodehelper_exec(info, UMH_NO_WAIT);
ffffffff812c416d:	31 f6                	xor    %esi,%esi
ffffffff812c416f:	48 89 c7             	mov    %rax,%rdi
			env = NULL;	/* freed by cleanup_uevent_env */
ffffffff812c4172:	45 31 e4             	xor    %r12d,%r12d
		retval = -ENOMEM;
		info = call_usermodehelper_setup(env->argv[0], env->argv,
						 env->envp, GFP_KERNEL,
						 NULL, cleanup_uevent_env, env);
		if (info) {
			retval = call_usermodehelper_exec(info, UMH_NO_WAIT);
ffffffff812c4175:	e8 1b fc da ff       	callq  ffffffff81073d95 <call_usermodehelper_exec>
ffffffff812c417a:	41 89 c7             	mov    %eax,%r15d
		}
	}
#endif

exit:
	kfree(devpath);
ffffffff812c417d:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
ffffffff812c4181:	e8 03 c8 e3 ff       	callq  ffffffff81100989 <kfree>
	kfree(env);
ffffffff812c4186:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c4189:	e8 fb c7 e3 ff       	callq  ffffffff81100989 <kfree>
	return retval;
ffffffff812c418e:	45 89 f9             	mov    %r15d,%r9d
ffffffff812c4191:	eb 1a                	jmp    ffffffff812c41ad <kobject_uevent_env+0x54d>
				 kobject_name(kobj), kobj, __func__);
			return 0;
		}

	/* originating subsystem */
	if (uevent_ops && uevent_ops->name)
ffffffff812c4193:	49 8b 46 08          	mov    0x8(%r14),%rax
ffffffff812c4197:	48 85 c0             	test   %rax,%rax
ffffffff812c419a:	0f 84 a7 fb ff ff    	je     ffffffff812c3d47 <kobject_uevent_env+0xe7>
		subsystem = uevent_ops->name(kset, kobj);
ffffffff812c41a0:	48 89 de             	mov    %rbx,%rsi
ffffffff812c41a3:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c41a6:	ff d0                	callq  *%rax
ffffffff812c41a8:	e9 9e fb ff ff       	jmpq   ffffffff812c3d4b <kobject_uevent_env+0xeb>

exit:
	kfree(devpath);
	kfree(env);
	return retval;
}
ffffffff812c41ad:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812c41b1:	44 89 c8             	mov    %r9d,%eax
ffffffff812c41b4:	5b                   	pop    %rbx
ffffffff812c41b5:	41 5c                	pop    %r12
ffffffff812c41b7:	41 5d                	pop    %r13
ffffffff812c41b9:	41 5e                	pop    %r14
ffffffff812c41bb:	41 5f                	pop    %r15
ffffffff812c41bd:	5d                   	pop    %rbp
ffffffff812c41be:	c3                   	retq   

ffffffff812c41bf <kobject_uevent>:
 *
 * Returns 0 if kobject_uevent() is completed with success or the
 * corresponding error when it fails.
 */
int kobject_uevent(struct kobject *kobj, enum kobject_action action)
{
ffffffff812c41bf:	55                   	push   %rbp
	return kobject_uevent_env(kobj, action, NULL);
ffffffff812c41c0:	31 d2                	xor    %edx,%edx
 *
 * Returns 0 if kobject_uevent() is completed with success or the
 * corresponding error when it fails.
 */
int kobject_uevent(struct kobject *kobj, enum kobject_action action)
{
ffffffff812c41c2:	48 89 e5             	mov    %rsp,%rbp
	return kobject_uevent_env(kobj, action, NULL);
ffffffff812c41c5:	e8 96 fa ff ff       	callq  ffffffff812c3c60 <kobject_uevent_env>
}
ffffffff812c41ca:	5d                   	pop    %rbp
ffffffff812c41cb:	c3                   	retq   

ffffffff812c41cc <uevent_net_init>:
}
EXPORT_SYMBOL_GPL(add_uevent_var);

#if defined(CONFIG_NET)
static int uevent_net_init(struct net *net)
{
ffffffff812c41cc:	55                   	push   %rbp
	struct uevent_sock *ue_sk;
	struct netlink_kernel_cfg cfg = {
ffffffff812c41cd:	31 c0                	xor    %eax,%eax
ffffffff812c41cf:	b9 0c 00 00 00       	mov    $0xc,%ecx
ffffffff812c41d4:	be d0 80 00 00       	mov    $0x80d0,%esi
}
EXPORT_SYMBOL_GPL(add_uevent_var);

#if defined(CONFIG_NET)
static int uevent_net_init(struct net *net)
{
ffffffff812c41d9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c41dc:	41 54                	push   %r12
ffffffff812c41de:	49 89 fc             	mov    %rdi,%r12
ffffffff812c41e1:	53                   	push   %rbx
	struct uevent_sock *ue_sk;
	struct netlink_kernel_cfg cfg = {
ffffffff812c41e2:	48 8d 7d c0          	lea    -0x40(%rbp),%rdi
}
EXPORT_SYMBOL_GPL(add_uevent_var);

#if defined(CONFIG_NET)
static int uevent_net_init(struct net *net)
{
ffffffff812c41e6:	48 83 ec 30          	sub    $0x30,%rsp
	struct uevent_sock *ue_sk;
	struct netlink_kernel_cfg cfg = {
ffffffff812c41ea:	f3 ab                	rep stos %eax,%es:(%rdi)
ffffffff812c41ec:	48 8b 3d 75 e4 8d 00 	mov    0x8de475(%rip),%rdi        # ffffffff81ba2668 <kmalloc_caches+0x28>
ffffffff812c41f3:	c7 45 c0 01 00 00 00 	movl   $0x1,-0x40(%rbp)
ffffffff812c41fa:	c7 45 c4 01 00 00 00 	movl   $0x1,-0x3c(%rbp)
ffffffff812c4201:	e8 cb ce e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
ffffffff812c4206:	48 89 c3             	mov    %rax,%rbx
		.flags	= NL_CFG_F_NONROOT_RECV,
	};

	ue_sk = kzalloc(sizeof(*ue_sk), GFP_KERNEL);
	if (!ue_sk)
		return -ENOMEM;
ffffffff812c4209:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
		.groups	= 1,
		.flags	= NL_CFG_F_NONROOT_RECV,
	};

	ue_sk = kzalloc(sizeof(*ue_sk), GFP_KERNEL);
	if (!ue_sk)
ffffffff812c420e:	48 85 db             	test   %rbx,%rbx
ffffffff812c4211:	74 67                	je     ffffffff812c427a <uevent_net_init+0xae>
					    struct module *module,
					    struct netlink_kernel_cfg *cfg);
static inline struct sock *
netlink_kernel_create(struct net *net, int unit, struct netlink_kernel_cfg *cfg)
{
	return __netlink_kernel_create(net, unit, THIS_MODULE, cfg);
ffffffff812c4213:	48 8d 4d c0          	lea    -0x40(%rbp),%rcx
ffffffff812c4217:	31 d2                	xor    %edx,%edx
ffffffff812c4219:	be 0f 00 00 00       	mov    $0xf,%esi
ffffffff812c421e:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c4221:	e8 ca bf 11 00       	callq  ffffffff813e01f0 <__netlink_kernel_create>
		return -ENOMEM;

	ue_sk->sk = netlink_kernel_create(net, NETLINK_KOBJECT_UEVENT, &cfg);
	if (!ue_sk->sk) {
ffffffff812c4226:	48 85 c0             	test   %rax,%rax

	ue_sk = kzalloc(sizeof(*ue_sk), GFP_KERNEL);
	if (!ue_sk)
		return -ENOMEM;

	ue_sk->sk = netlink_kernel_create(net, NETLINK_KOBJECT_UEVENT, &cfg);
ffffffff812c4229:	48 89 43 10          	mov    %rax,0x10(%rbx)
	if (!ue_sk->sk) {
ffffffff812c422d:	75 1b                	jne    ffffffff812c424a <uevent_net_init+0x7e>
		printk(KERN_ERR
ffffffff812c422f:	48 c7 c7 3f 6c 7b 81 	mov    $0xffffffff817b6c3f,%rdi
ffffffff812c4236:	e8 57 ed 16 00       	callq  ffffffff81432f92 <printk>
		       "kobject_uevent: unable to create netlink socket!\n");
		kfree(ue_sk);
ffffffff812c423b:	48 89 df             	mov    %rbx,%rdi
ffffffff812c423e:	e8 46 c7 e3 ff       	callq  ffffffff81100989 <kfree>
		return -ENODEV;
ffffffff812c4243:	b8 ed ff ff ff       	mov    $0xffffffed,%eax
ffffffff812c4248:	eb 30                	jmp    ffffffff812c427a <uevent_net_init+0xae>
	}
	mutex_lock(&uevent_sock_mutex);
ffffffff812c424a:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
ffffffff812c4251:	e8 3c 33 17 00       	callq  ffffffff81437592 <mutex_lock>
ffffffff812c4256:	48 8b 35 eb 1d 77 00 	mov    0x771deb(%rip),%rsi        # ffffffff81a36048 <uevent_sock_list+0x8>
ffffffff812c425d:	48 c7 c2 40 60 a3 81 	mov    $0xffffffff81a36040,%rdx
ffffffff812c4264:	48 89 df             	mov    %rbx,%rdi
ffffffff812c4267:	e8 fe 0b 01 00       	callq  ffffffff812d4e6a <__list_add>
	list_add_tail(&ue_sk->list, &uevent_sock_list);
	mutex_unlock(&uevent_sock_mutex);
ffffffff812c426c:	48 c7 c7 00 60 a3 81 	mov    $0xffffffff81a36000,%rdi
ffffffff812c4273:	e8 3c 34 17 00       	callq  ffffffff814376b4 <mutex_unlock>
	return 0;
ffffffff812c4278:	31 c0                	xor    %eax,%eax
}
ffffffff812c427a:	48 83 c4 30          	add    $0x30,%rsp
ffffffff812c427e:	5b                   	pop    %rbx
ffffffff812c427f:	41 5c                	pop    %r12
ffffffff812c4281:	5d                   	pop    %rbp
ffffffff812c4282:	c3                   	retq   

ffffffff812c4283 <kobject_action_type>:
			enum kobject_action *type)
{
	enum kobject_action action;
	int ret = -EINVAL;

	if (count && (buf[count-1] == '\n' || buf[count-1] == '\0'))
ffffffff812c4283:	48 85 f6             	test   %rsi,%rsi
 */
int kobject_action_type(const char *buf, size_t count,
			enum kobject_action *type)
{
	enum kobject_action action;
	int ret = -EINVAL;
ffffffff812c4286:	b8 ea ff ff ff       	mov    $0xffffffea,%eax

	if (count && (buf[count-1] == '\n' || buf[count-1] == '\0'))
ffffffff812c428b:	74 76                	je     ffffffff812c4303 <kobject_action_type+0x80>
 *
 * Returns 0 if the action string was recognized.
 */
int kobject_action_type(const char *buf, size_t count,
			enum kobject_action *type)
{
ffffffff812c428d:	55                   	push   %rbp
ffffffff812c428e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4291:	41 57                	push   %r15
ffffffff812c4293:	41 56                	push   %r14
ffffffff812c4295:	41 55                	push   %r13
ffffffff812c4297:	41 54                	push   %r12
ffffffff812c4299:	49 89 d7             	mov    %rdx,%r15
ffffffff812c429c:	53                   	push   %rbx
ffffffff812c429d:	51                   	push   %rcx
ffffffff812c429e:	49 89 fd             	mov    %rdi,%r13
	enum kobject_action action;
	int ret = -EINVAL;

	if (count && (buf[count-1] == '\n' || buf[count-1] == '\0'))
ffffffff812c42a1:	8a 44 37 ff          	mov    -0x1(%rdi,%rsi,1),%al
ffffffff812c42a5:	48 89 f3             	mov    %rsi,%rbx
ffffffff812c42a8:	48 8d 56 ff          	lea    -0x1(%rsi),%rdx
ffffffff812c42ac:	3c 0a                	cmp    $0xa,%al
ffffffff812c42ae:	74 04                	je     ffffffff812c42b4 <kobject_action_type+0x31>
ffffffff812c42b0:	84 c0                	test   %al,%al
ffffffff812c42b2:	75 0d                	jne    ffffffff812c42c1 <kobject_action_type+0x3e>
		count--;

	if (!count)
ffffffff812c42b4:	48 85 d2             	test   %rdx,%rdx
 */
int kobject_action_type(const char *buf, size_t count,
			enum kobject_action *type)
{
	enum kobject_action action;
	int ret = -EINVAL;
ffffffff812c42b7:	b8 ea ff ff ff       	mov    $0xffffffea,%eax

	if (count && (buf[count-1] == '\n' || buf[count-1] == '\0'))
		count--;

	if (!count)
ffffffff812c42bc:	74 3a                	je     ffffffff812c42f8 <kobject_action_type+0x75>
ffffffff812c42be:	48 89 d3             	mov    %rdx,%rbx
ffffffff812c42c1:	45 31 e4             	xor    %r12d,%r12d
		goto out;

	for (action = 0; action < ARRAY_SIZE(kobject_actions); action++) {
		if (strncmp(kobject_actions[action], buf, count) != 0)
ffffffff812c42c4:	4e 8b 34 e5 20 f1 63 	mov    -0x7e9c0ee0(,%r12,8),%r14
ffffffff812c42cb:	81 
ffffffff812c42cc:	48 89 da             	mov    %rbx,%rdx
ffffffff812c42cf:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c42d2:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c42d5:	e8 9e 39 00 00       	callq  ffffffff812c7c78 <strncmp>
ffffffff812c42da:	85 c0                	test   %eax,%eax
ffffffff812c42dc:	75 0c                	jne    ffffffff812c42ea <kobject_action_type+0x67>
			continue;
		if (kobject_actions[action][count] != '\0')
ffffffff812c42de:	41 80 3c 1e 00       	cmpb   $0x0,(%r14,%rbx,1)
ffffffff812c42e3:	75 05                	jne    ffffffff812c42ea <kobject_action_type+0x67>
			continue;
		*type = action;
ffffffff812c42e5:	45 89 27             	mov    %r12d,(%r15)
		ret = 0;
		break;
ffffffff812c42e8:	eb 0e                	jmp    ffffffff812c42f8 <kobject_action_type+0x75>
ffffffff812c42ea:	49 ff c4             	inc    %r12
		count--;

	if (!count)
		goto out;

	for (action = 0; action < ARRAY_SIZE(kobject_actions); action++) {
ffffffff812c42ed:	49 83 fc 06          	cmp    $0x6,%r12
ffffffff812c42f1:	75 d1                	jne    ffffffff812c42c4 <kobject_action_type+0x41>
 */
int kobject_action_type(const char *buf, size_t count,
			enum kobject_action *type)
{
	enum kobject_action action;
	int ret = -EINVAL;
ffffffff812c42f3:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
		ret = 0;
		break;
	}
out:
	return ret;
}
ffffffff812c42f8:	5a                   	pop    %rdx
ffffffff812c42f9:	5b                   	pop    %rbx
ffffffff812c42fa:	41 5c                	pop    %r12
ffffffff812c42fc:	41 5d                	pop    %r13
ffffffff812c42fe:	41 5e                	pop    %r14
ffffffff812c4300:	41 5f                	pop    %r15
ffffffff812c4302:	5d                   	pop    %rbp
ffffffff812c4303:	c3                   	retq   

ffffffff812c4304 <md5_transform>:

#define MD5STEP(f, w, x, y, z, in, s) \
	(w += f(x, y, z) + in, w = (w<<s | w>>(32-s)) + x)

void md5_transform(__u32 *hash, __u32 const *in)
{
ffffffff812c4304:	55                   	push   %rbp
ffffffff812c4305:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4308:	41 57                	push   %r15
ffffffff812c430a:	41 56                	push   %r14
ffffffff812c430c:	41 55                	push   %r13
ffffffff812c430e:	41 54                	push   %r12
ffffffff812c4310:	53                   	push   %rbx
ffffffff812c4311:	48 83 ec 28          	sub    $0x28,%rsp
	a = hash[0];
	b = hash[1];
	c = hash[2];
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
ffffffff812c4315:	44 8b 2e             	mov    (%rsi),%r13d
ffffffff812c4318:	8b 07                	mov    (%rdi),%eax
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
ffffffff812c431a:	8b 5e 04             	mov    0x4(%rsi),%ebx
ffffffff812c431d:	8b 4f 04             	mov    0x4(%rdi),%ecx
ffffffff812c4320:	33 4f 08             	xor    0x8(%rdi),%ecx
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
ffffffff812c4323:	44 8b 47 04          	mov    0x4(%rdi),%r8d
ffffffff812c4327:	42 8d 94 28 78 a4 6a 	lea    -0x28955b88(%rax,%r13,1),%edx
ffffffff812c432e:	d7 
	a = hash[0];
	b = hash[1];
	c = hash[2];
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
ffffffff812c432f:	8b 47 08             	mov    0x8(%rdi),%eax
ffffffff812c4332:	33 47 0c             	xor    0xc(%rdi),%eax
ffffffff812c4335:	23 47 04             	and    0x4(%rdi),%eax
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
ffffffff812c4338:	89 5d d4             	mov    %ebx,-0x2c(%rbp)
	a = hash[0];
	b = hash[1];
	c = hash[2];
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
ffffffff812c433b:	33 47 0c             	xor    0xc(%rdi),%eax
ffffffff812c433e:	8b 5f 0c             	mov    0xc(%rdi),%ebx
ffffffff812c4341:	01 d0                	add    %edx,%eax
ffffffff812c4343:	8b 55 d4             	mov    -0x2c(%rbp),%edx
ffffffff812c4346:	c1 c0 07             	rol    $0x7,%eax
ffffffff812c4349:	03 47 04             	add    0x4(%rdi),%eax
ffffffff812c434c:	8d 94 13 56 b7 c7 e8 	lea    -0x173848aa(%rbx,%rdx,1),%edx
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
ffffffff812c4353:	8b 5e 08             	mov    0x8(%rsi),%ebx
	b = hash[1];
	c = hash[2];
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
ffffffff812c4356:	21 c1                	and    %eax,%ecx
ffffffff812c4358:	33 4f 08             	xor    0x8(%rdi),%ecx
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
ffffffff812c435b:	41 31 c0             	xor    %eax,%r8d
ffffffff812c435e:	89 5d d0             	mov    %ebx,-0x30(%rbp)
ffffffff812c4361:	8b 5f 08             	mov    0x8(%rdi),%ebx
	b = hash[1];
	c = hash[2];
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
ffffffff812c4364:	01 d1                	add    %edx,%ecx
ffffffff812c4366:	8b 55 d0             	mov    -0x30(%rbp),%edx
ffffffff812c4369:	c1 c1 0c             	rol    $0xc,%ecx
ffffffff812c436c:	01 c1                	add    %eax,%ecx
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
ffffffff812c436e:	41 21 c8             	and    %ecx,%r8d
ffffffff812c4371:	44 33 47 04          	xor    0x4(%rdi),%r8d
ffffffff812c4375:	8d 94 13 db 70 20 24 	lea    0x242070db(%rbx,%rdx,1),%edx
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
ffffffff812c437c:	8b 5e 0c             	mov    0xc(%rsi),%ebx
	c = hash[2];
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
ffffffff812c437f:	41 01 d0             	add    %edx,%r8d
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
ffffffff812c4382:	89 5d cc             	mov    %ebx,-0x34(%rbp)
ffffffff812c4385:	8b 5f 04             	mov    0x4(%rdi),%ebx
ffffffff812c4388:	8b 55 cc             	mov    -0x34(%rbp),%edx
	c = hash[2];
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
ffffffff812c438b:	41 c1 c8 0f          	ror    $0xf,%r8d
ffffffff812c438f:	41 01 c8             	add    %ecx,%r8d
ffffffff812c4392:	44 8d 8c 13 ee ce bd 	lea    -0x3e423112(%rbx,%rdx,1),%r9d
ffffffff812c4399:	c1 
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
ffffffff812c439a:	89 c2                	mov    %eax,%edx
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c439c:	8b 5e 10             	mov    0x10(%rsi),%ebx
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
ffffffff812c439f:	31 ca                	xor    %ecx,%edx
ffffffff812c43a1:	44 21 c2             	and    %r8d,%edx
ffffffff812c43a4:	31 c2                	xor    %eax,%edx
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43a6:	89 5d c8             	mov    %ebx,-0x38(%rbp)
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
ffffffff812c43a9:	44 8b 66 1c          	mov    0x1c(%rsi),%r12d
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
ffffffff812c43ad:	44 01 ca             	add    %r9d,%edx
ffffffff812c43b0:	44 8d 8c 18 af 0f 7c 	lea    -0xa83f051(%rax,%rbx,1),%r9d
ffffffff812c43b7:	f5 
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43b8:	89 c8                	mov    %ecx,%eax
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
ffffffff812c43ba:	c1 ca 0a             	ror    $0xa,%edx
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43bd:	44 31 c0             	xor    %r8d,%eax
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c43c0:	8b 5e 14             	mov    0x14(%rsi),%ebx
	d = hash[3];

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
ffffffff812c43c3:	44 01 c2             	add    %r8d,%edx
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
ffffffff812c43c6:	44 8b 7e 24          	mov    0x24(%rsi),%r15d
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
ffffffff812c43ca:	44 8b 56 28          	mov    0x28(%rsi),%r10d

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43ce:	21 d0                	and    %edx,%eax
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
ffffffff812c43d0:	44 8b 5e 30          	mov    0x30(%rsi),%r11d

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43d4:	31 c8                	xor    %ecx,%eax
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c43d6:	89 5d c4             	mov    %ebx,-0x3c(%rbp)

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43d9:	44 01 c8             	add    %r9d,%eax
ffffffff812c43dc:	44 8d 8c 19 2a c6 87 	lea    0x4787c62a(%rcx,%rbx,1),%r9d
ffffffff812c43e3:	47 
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c43e4:	44 89 c1             	mov    %r8d,%ecx

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43e7:	c1 c0 07             	rol    $0x7,%eax
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c43ea:	31 d1                	xor    %edx,%ecx
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c43ec:	8b 5e 18             	mov    0x18(%rsi),%ebx

	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
ffffffff812c43ef:	01 d0                	add    %edx,%eax
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c43f1:	21 c1                	and    %eax,%ecx
ffffffff812c43f3:	44 31 c1             	xor    %r8d,%ecx
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c43f6:	89 5d c0             	mov    %ebx,-0x40(%rbp)
	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c43f9:	44 01 c9             	add    %r9d,%ecx
ffffffff812c43fc:	45 8d 8c 18 13 46 30 	lea    -0x57cfb9ed(%r8,%rbx,1),%r9d
ffffffff812c4403:	a8 
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c4404:	41 89 d0             	mov    %edx,%r8d
	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c4407:	c1 c1 0c             	rol    $0xc,%ecx
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c440a:	41 31 c0             	xor    %eax,%r8d
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c440d:	8b 5e 20             	mov    0x20(%rsi),%ebx
	MD5STEP(F1, a, b, c, d, in[0] + 0xd76aa478, 7);
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
ffffffff812c4410:	01 c1                	add    %eax,%ecx
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c4412:	41 21 c8             	and    %ecx,%r8d
ffffffff812c4415:	41 31 d0             	xor    %edx,%r8d
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c4418:	89 5d bc             	mov    %ebx,-0x44(%rbp)
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c441b:	45 01 c8             	add    %r9d,%r8d
ffffffff812c441e:	46 8d 8c 22 01 95 46 	lea    -0x2b96aff(%rdx,%r12,1),%r9d
ffffffff812c4425:	fd 
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
ffffffff812c4426:	89 c2                	mov    %eax,%edx
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c4428:	41 c1 c8 0f          	ror    $0xf,%r8d
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
ffffffff812c442c:	31 ca                	xor    %ecx,%edx
	MD5STEP(F1, d, a, b, c, in[1] + 0xe8c7b756, 12);
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
ffffffff812c442e:	41 01 c8             	add    %ecx,%r8d
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
ffffffff812c4431:	44 21 c2             	and    %r8d,%edx
ffffffff812c4434:	31 c2                	xor    %eax,%edx
ffffffff812c4436:	44 01 ca             	add    %r9d,%edx
ffffffff812c4439:	44 8d 8c 18 d8 98 80 	lea    0x698098d8(%rax,%rbx,1),%r9d
ffffffff812c4440:	69 
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c4441:	89 c8                	mov    %ecx,%eax
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
ffffffff812c4443:	c1 ca 0a             	ror    $0xa,%edx
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c4446:	44 31 c0             	xor    %r8d,%eax
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
ffffffff812c4449:	8b 5e 2c             	mov    0x2c(%rsi),%ebx
	MD5STEP(F1, c, d, a, b, in[2] + 0x242070db, 17);
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
ffffffff812c444c:	44 01 c2             	add    %r8d,%edx
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c444f:	21 d0                	and    %edx,%eax
ffffffff812c4451:	31 c8                	xor    %ecx,%eax
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
ffffffff812c4453:	89 5d b8             	mov    %ebx,-0x48(%rbp)
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c4456:	44 01 c8             	add    %r9d,%eax
ffffffff812c4459:	46 8d 8c 39 af f7 44 	lea    -0x74bb0851(%rcx,%r15,1),%r9d
ffffffff812c4460:	8b 
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
ffffffff812c4461:	44 89 c1             	mov    %r8d,%ecx
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c4464:	c1 c0 07             	rol    $0x7,%eax
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
ffffffff812c4467:	31 d1                	xor    %edx,%ecx
	MD5STEP(F1, b, c, d, a, in[3] + 0xc1bdceee, 22);
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
ffffffff812c4469:	01 d0                	add    %edx,%eax
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
ffffffff812c446b:	21 c1                	and    %eax,%ecx
ffffffff812c446d:	44 31 c1             	xor    %r8d,%ecx
ffffffff812c4470:	47 8d 84 10 b1 5b ff 	lea    -0xa44f(%r8,%r10,1),%r8d
ffffffff812c4477:	ff 
ffffffff812c4478:	44 01 c9             	add    %r9d,%ecx
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
ffffffff812c447b:	41 89 d1             	mov    %edx,%r9d
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
ffffffff812c447e:	c1 c1 0c             	rol    $0xc,%ecx
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
ffffffff812c4481:	41 31 c1             	xor    %eax,%r9d
	MD5STEP(F1, a, b, c, d, in[4] + 0xf57c0faf, 7);
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
ffffffff812c4484:	01 c1                	add    %eax,%ecx
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
ffffffff812c4486:	41 21 c9             	and    %ecx,%r9d
ffffffff812c4489:	41 31 d1             	xor    %edx,%r9d
ffffffff812c448c:	45 01 c1             	add    %r8d,%r9d
ffffffff812c448f:	44 8d 84 1a be d7 5c 	lea    -0x76a32842(%rdx,%rbx,1),%r8d
ffffffff812c4496:	89 
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
ffffffff812c4497:	89 c2                	mov    %eax,%edx
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
ffffffff812c4499:	41 c1 c9 0f          	ror    $0xf,%r9d
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
ffffffff812c449d:	31 ca                	xor    %ecx,%edx
	MD5STEP(F1, d, a, b, c, in[5] + 0x4787c62a, 12);
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
ffffffff812c449f:	41 01 c9             	add    %ecx,%r9d
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
ffffffff812c44a2:	44 21 ca             	and    %r9d,%edx
ffffffff812c44a5:	31 c2                	xor    %eax,%edx
ffffffff812c44a7:	44 01 c2             	add    %r8d,%edx
ffffffff812c44aa:	46 8d 84 18 22 11 90 	lea    0x6b901122(%rax,%r11,1),%r8d
ffffffff812c44b1:	6b 
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
ffffffff812c44b2:	89 c8                	mov    %ecx,%eax
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
ffffffff812c44b4:	c1 ca 0a             	ror    $0xa,%edx
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
ffffffff812c44b7:	44 31 c8             	xor    %r9d,%eax
	MD5STEP(F1, c, d, a, b, in[6] + 0xa8304613, 17);
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
ffffffff812c44ba:	44 01 ca             	add    %r9d,%edx
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
ffffffff812c44bd:	21 d0                	and    %edx,%eax
	MD5STEP(F1, d, a, b, c, in[13] + 0xfd987193, 12);
	MD5STEP(F1, c, d, a, b, in[14] + 0xa679438e, 17);
ffffffff812c44bf:	41 89 d6             	mov    %edx,%r14d
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
ffffffff812c44c2:	31 c8                	xor    %ecx,%eax
ffffffff812c44c4:	44 01 c0             	add    %r8d,%eax
	MD5STEP(F1, d, a, b, c, in[13] + 0xfd987193, 12);
ffffffff812c44c7:	44 8b 46 34          	mov    0x34(%rsi),%r8d
	MD5STEP(F1, b, c, d, a, in[7] + 0xfd469501, 22);
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
ffffffff812c44cb:	c1 c0 07             	rol    $0x7,%eax
ffffffff812c44ce:	01 d0                	add    %edx,%eax
ffffffff812c44d0:	42 8d 9c 01 93 71 98 	lea    -0x2678e6d(%rcx,%r8,1),%ebx
ffffffff812c44d7:	fd 
	MD5STEP(F1, d, a, b, c, in[13] + 0xfd987193, 12);
ffffffff812c44d8:	44 89 c9             	mov    %r9d,%ecx
	MD5STEP(F1, c, d, a, b, in[14] + 0xa679438e, 17);
ffffffff812c44db:	41 31 c6             	xor    %eax,%r14d
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
	MD5STEP(F1, d, a, b, c, in[13] + 0xfd987193, 12);
ffffffff812c44de:	31 d1                	xor    %edx,%ecx
ffffffff812c44e0:	21 c1                	and    %eax,%ecx
ffffffff812c44e2:	44 31 c9             	xor    %r9d,%ecx
ffffffff812c44e5:	01 d9                	add    %ebx,%ecx
	MD5STEP(F1, c, d, a, b, in[14] + 0xa679438e, 17);
ffffffff812c44e7:	8b 5e 38             	mov    0x38(%rsi),%ebx
	MD5STEP(F1, a, b, c, d, in[8] + 0x698098d8, 7);
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
	MD5STEP(F1, d, a, b, c, in[13] + 0xfd987193, 12);
ffffffff812c44ea:	c1 c1 0c             	rol    $0xc,%ecx
ffffffff812c44ed:	01 c1                	add    %eax,%ecx
ffffffff812c44ef:	45 8d 8c 19 8e 43 79 	lea    -0x5986bc72(%r9,%rbx,1),%r9d
ffffffff812c44f6:	a6 
	MD5STEP(F1, c, d, a, b, in[14] + 0xa679438e, 17);
ffffffff812c44f7:	41 21 ce             	and    %ecx,%r14d
ffffffff812c44fa:	41 31 d6             	xor    %edx,%r14d
ffffffff812c44fd:	45 01 ce             	add    %r9d,%r14d
	MD5STEP(F1, b, c, d, a, in[15] + 0x49b40821, 22);
ffffffff812c4500:	44 8b 4e 3c          	mov    0x3c(%rsi),%r9d
	MD5STEP(F1, d, a, b, c, in[9] + 0x8b44f7af, 12);
	MD5STEP(F1, c, d, a, b, in[10] + 0xffff5bb1, 17);
	MD5STEP(F1, b, c, d, a, in[11] + 0x895cd7be, 22);
	MD5STEP(F1, a, b, c, d, in[12] + 0x6b901122, 7);
	MD5STEP(F1, d, a, b, c, in[13] + 0xfd987193, 12);
	MD5STEP(F1, c, d, a, b, in[14] + 0xa679438e, 17);
ffffffff812c4504:	41 c1 ce 0f          	ror    $0xf,%r14d
ffffffff812c4508:	41 01 ce             	add    %ecx,%r14d
ffffffff812c450b:	42 8d b4 0a 21 08 b4 	lea    0x49b40821(%rdx,%r9,1),%esi
ffffffff812c4512:	49 
	MD5STEP(F1, b, c, d, a, in[15] + 0x49b40821, 22);
ffffffff812c4513:	89 c2                	mov    %eax,%edx
ffffffff812c4515:	31 ca                	xor    %ecx,%edx
ffffffff812c4517:	44 21 f2             	and    %r14d,%edx
ffffffff812c451a:	31 c2                	xor    %eax,%edx
ffffffff812c451c:	01 f2                	add    %esi,%edx
ffffffff812c451e:	8b 75 d4             	mov    -0x2c(%rbp),%esi
ffffffff812c4521:	c1 ca 0a             	ror    $0xa,%edx
ffffffff812c4524:	44 01 f2             	add    %r14d,%edx
ffffffff812c4527:	8d b4 06 62 25 1e f6 	lea    -0x9e1da9e(%rsi,%rax,1),%esi

	MD5STEP(F2, a, b, c, d, in[1] + 0xf61e2562, 5);
ffffffff812c452e:	44 89 f0             	mov    %r14d,%eax
ffffffff812c4531:	31 d0                	xor    %edx,%eax
ffffffff812c4533:	21 c8                	and    %ecx,%eax
ffffffff812c4535:	44 31 f0             	xor    %r14d,%eax
ffffffff812c4538:	01 f0                	add    %esi,%eax
ffffffff812c453a:	8b 75 c0             	mov    -0x40(%rbp),%esi
ffffffff812c453d:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c4540:	01 d0                	add    %edx,%eax
ffffffff812c4542:	8d b4 0e 40 b3 40 c0 	lea    -0x3fbf4cc0(%rsi,%rcx,1),%esi
	MD5STEP(F2, d, a, b, c, in[6] + 0xc040b340, 9);
ffffffff812c4549:	89 d1                	mov    %edx,%ecx
ffffffff812c454b:	31 c1                	xor    %eax,%ecx
ffffffff812c454d:	44 21 f1             	and    %r14d,%ecx
ffffffff812c4550:	31 d1                	xor    %edx,%ecx
ffffffff812c4552:	01 f1                	add    %esi,%ecx
ffffffff812c4554:	8b 75 b8             	mov    -0x48(%rbp),%esi
ffffffff812c4557:	c1 c1 09             	rol    $0x9,%ecx
ffffffff812c455a:	01 c1                	add    %eax,%ecx
ffffffff812c455c:	46 8d b4 36 51 5a 5e 	lea    0x265e5a51(%rsi,%r14,1),%r14d
ffffffff812c4563:	26 
	MD5STEP(F2, c, d, a, b, in[11] + 0x265e5a51, 14);
ffffffff812c4564:	89 c6                	mov    %eax,%esi
ffffffff812c4566:	31 ce                	xor    %ecx,%esi
ffffffff812c4568:	21 d6                	and    %edx,%esi
ffffffff812c456a:	31 c6                	xor    %eax,%esi
ffffffff812c456c:	44 01 f6             	add    %r14d,%esi
ffffffff812c456f:	45 8d b4 15 aa c7 b6 	lea    -0x16493856(%r13,%rdx,1),%r14d
ffffffff812c4576:	e9 
	MD5STEP(F2, b, c, d, a, in[0] + 0xe9b6c7aa, 20);
ffffffff812c4577:	89 ca                	mov    %ecx,%edx
	MD5STEP(F1, c, d, a, b, in[14] + 0xa679438e, 17);
	MD5STEP(F1, b, c, d, a, in[15] + 0x49b40821, 22);

	MD5STEP(F2, a, b, c, d, in[1] + 0xf61e2562, 5);
	MD5STEP(F2, d, a, b, c, in[6] + 0xc040b340, 9);
	MD5STEP(F2, c, d, a, b, in[11] + 0x265e5a51, 14);
ffffffff812c4579:	c1 c6 0e             	rol    $0xe,%esi
ffffffff812c457c:	01 ce                	add    %ecx,%esi
	MD5STEP(F2, b, c, d, a, in[0] + 0xe9b6c7aa, 20);
ffffffff812c457e:	31 f2                	xor    %esi,%edx
ffffffff812c4580:	21 c2                	and    %eax,%edx
ffffffff812c4582:	31 ca                	xor    %ecx,%edx
ffffffff812c4584:	44 01 f2             	add    %r14d,%edx
ffffffff812c4587:	44 8b 75 c4          	mov    -0x3c(%rbp),%r14d
ffffffff812c458b:	c1 ca 0c             	ror    $0xc,%edx
ffffffff812c458e:	01 f2                	add    %esi,%edx
ffffffff812c4590:	45 8d b4 06 5d 10 2f 	lea    -0x29d0efa3(%r14,%rax,1),%r14d
ffffffff812c4597:	d6 
	MD5STEP(F2, a, b, c, d, in[5] + 0xd62f105d, 5);
ffffffff812c4598:	89 f0                	mov    %esi,%eax
ffffffff812c459a:	31 d0                	xor    %edx,%eax
ffffffff812c459c:	21 c8                	and    %ecx,%eax
ffffffff812c459e:	31 f0                	xor    %esi,%eax
ffffffff812c45a0:	44 01 f0             	add    %r14d,%eax
ffffffff812c45a3:	45 8d b4 0a 53 14 44 	lea    0x2441453(%r10,%rcx,1),%r14d
ffffffff812c45aa:	02 
	MD5STEP(F2, d, a, b, c, in[10] + 0x02441453, 9);
ffffffff812c45ab:	89 d1                	mov    %edx,%ecx

	MD5STEP(F2, a, b, c, d, in[1] + 0xf61e2562, 5);
	MD5STEP(F2, d, a, b, c, in[6] + 0xc040b340, 9);
	MD5STEP(F2, c, d, a, b, in[11] + 0x265e5a51, 14);
	MD5STEP(F2, b, c, d, a, in[0] + 0xe9b6c7aa, 20);
	MD5STEP(F2, a, b, c, d, in[5] + 0xd62f105d, 5);
ffffffff812c45ad:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c45b0:	01 d0                	add    %edx,%eax
	MD5STEP(F2, d, a, b, c, in[10] + 0x02441453, 9);
ffffffff812c45b2:	31 c1                	xor    %eax,%ecx
ffffffff812c45b4:	21 f1                	and    %esi,%ecx
ffffffff812c45b6:	31 d1                	xor    %edx,%ecx
ffffffff812c45b8:	44 01 f1             	add    %r14d,%ecx
ffffffff812c45bb:	45 8d b4 31 81 e6 a1 	lea    -0x275e197f(%r9,%rsi,1),%r14d
ffffffff812c45c2:	d8 
	MD5STEP(F2, c, d, a, b, in[15] + 0xd8a1e681, 14);
ffffffff812c45c3:	89 c6                	mov    %eax,%esi
	MD5STEP(F2, a, b, c, d, in[1] + 0xf61e2562, 5);
	MD5STEP(F2, d, a, b, c, in[6] + 0xc040b340, 9);
	MD5STEP(F2, c, d, a, b, in[11] + 0x265e5a51, 14);
	MD5STEP(F2, b, c, d, a, in[0] + 0xe9b6c7aa, 20);
	MD5STEP(F2, a, b, c, d, in[5] + 0xd62f105d, 5);
	MD5STEP(F2, d, a, b, c, in[10] + 0x02441453, 9);
ffffffff812c45c5:	c1 c1 09             	rol    $0x9,%ecx
ffffffff812c45c8:	01 c1                	add    %eax,%ecx
	MD5STEP(F2, c, d, a, b, in[15] + 0xd8a1e681, 14);
ffffffff812c45ca:	31 ce                	xor    %ecx,%esi
ffffffff812c45cc:	21 d6                	and    %edx,%esi
ffffffff812c45ce:	31 c6                	xor    %eax,%esi
ffffffff812c45d0:	44 01 f6             	add    %r14d,%esi
ffffffff812c45d3:	44 8b 75 c8          	mov    -0x38(%rbp),%r14d
ffffffff812c45d7:	c1 c6 0e             	rol    $0xe,%esi
ffffffff812c45da:	01 ce                	add    %ecx,%esi
ffffffff812c45dc:	45 8d b4 16 c8 fb d3 	lea    -0x182c0438(%r14,%rdx,1),%r14d
ffffffff812c45e3:	e7 
	MD5STEP(F2, b, c, d, a, in[4] + 0xe7d3fbc8, 20);
ffffffff812c45e4:	89 ca                	mov    %ecx,%edx
ffffffff812c45e6:	31 f2                	xor    %esi,%edx
ffffffff812c45e8:	21 c2                	and    %eax,%edx
ffffffff812c45ea:	31 ca                	xor    %ecx,%edx
ffffffff812c45ec:	44 01 f2             	add    %r14d,%edx
ffffffff812c45ef:	45 8d b4 07 e6 cd e1 	lea    0x21e1cde6(%r15,%rax,1),%r14d
ffffffff812c45f6:	21 
	MD5STEP(F2, a, b, c, d, in[9] + 0x21e1cde6, 5);
ffffffff812c45f7:	89 f0                	mov    %esi,%eax
	MD5STEP(F2, c, d, a, b, in[11] + 0x265e5a51, 14);
	MD5STEP(F2, b, c, d, a, in[0] + 0xe9b6c7aa, 20);
	MD5STEP(F2, a, b, c, d, in[5] + 0xd62f105d, 5);
	MD5STEP(F2, d, a, b, c, in[10] + 0x02441453, 9);
	MD5STEP(F2, c, d, a, b, in[15] + 0xd8a1e681, 14);
	MD5STEP(F2, b, c, d, a, in[4] + 0xe7d3fbc8, 20);
ffffffff812c45f9:	c1 ca 0c             	ror    $0xc,%edx
ffffffff812c45fc:	01 f2                	add    %esi,%edx
	MD5STEP(F2, a, b, c, d, in[9] + 0x21e1cde6, 5);
ffffffff812c45fe:	31 d0                	xor    %edx,%eax
ffffffff812c4600:	21 c8                	and    %ecx,%eax
ffffffff812c4602:	31 f0                	xor    %esi,%eax
ffffffff812c4604:	44 01 f0             	add    %r14d,%eax
ffffffff812c4607:	44 8d b4 0b d6 07 37 	lea    -0x3cc8f82a(%rbx,%rcx,1),%r14d
ffffffff812c460e:	c3 
	MD5STEP(F2, d, a, b, c, in[14] + 0xc33707d6, 9);
ffffffff812c460f:	89 d1                	mov    %edx,%ecx
	MD5STEP(F2, b, c, d, a, in[0] + 0xe9b6c7aa, 20);
	MD5STEP(F2, a, b, c, d, in[5] + 0xd62f105d, 5);
	MD5STEP(F2, d, a, b, c, in[10] + 0x02441453, 9);
	MD5STEP(F2, c, d, a, b, in[15] + 0xd8a1e681, 14);
	MD5STEP(F2, b, c, d, a, in[4] + 0xe7d3fbc8, 20);
	MD5STEP(F2, a, b, c, d, in[9] + 0x21e1cde6, 5);
ffffffff812c4611:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c4614:	01 d0                	add    %edx,%eax
	MD5STEP(F2, d, a, b, c, in[14] + 0xc33707d6, 9);
ffffffff812c4616:	31 c1                	xor    %eax,%ecx
ffffffff812c4618:	21 f1                	and    %esi,%ecx
ffffffff812c461a:	31 d1                	xor    %edx,%ecx
ffffffff812c461c:	44 01 f1             	add    %r14d,%ecx
ffffffff812c461f:	44 8b 75 cc          	mov    -0x34(%rbp),%r14d
ffffffff812c4623:	c1 c1 09             	rol    $0x9,%ecx
ffffffff812c4626:	01 c1                	add    %eax,%ecx
ffffffff812c4628:	45 8d b4 36 87 0d d5 	lea    -0xb2af279(%r14,%rsi,1),%r14d
ffffffff812c462f:	f4 
	MD5STEP(F2, c, d, a, b, in[3] + 0xf4d50d87, 14);
ffffffff812c4630:	89 c6                	mov    %eax,%esi
ffffffff812c4632:	31 ce                	xor    %ecx,%esi
ffffffff812c4634:	21 d6                	and    %edx,%esi
ffffffff812c4636:	31 c6                	xor    %eax,%esi
ffffffff812c4638:	44 01 f6             	add    %r14d,%esi
ffffffff812c463b:	44 8b 75 bc          	mov    -0x44(%rbp),%r14d
ffffffff812c463f:	c1 c6 0e             	rol    $0xe,%esi
ffffffff812c4642:	01 ce                	add    %ecx,%esi
ffffffff812c4644:	45 8d b4 16 ed 14 5a 	lea    0x455a14ed(%r14,%rdx,1),%r14d
ffffffff812c464b:	45 
	MD5STEP(F2, b, c, d, a, in[8] + 0x455a14ed, 20);
ffffffff812c464c:	89 ca                	mov    %ecx,%edx
ffffffff812c464e:	31 f2                	xor    %esi,%edx
ffffffff812c4650:	21 c2                	and    %eax,%edx
ffffffff812c4652:	31 ca                	xor    %ecx,%edx
ffffffff812c4654:	44 01 f2             	add    %r14d,%edx
ffffffff812c4657:	45 8d b4 00 05 e9 e3 	lea    -0x561c16fb(%r8,%rax,1),%r14d
ffffffff812c465e:	a9 
	MD5STEP(F2, a, b, c, d, in[13] + 0xa9e3e905, 5);
ffffffff812c465f:	89 f0                	mov    %esi,%eax
	MD5STEP(F2, c, d, a, b, in[15] + 0xd8a1e681, 14);
	MD5STEP(F2, b, c, d, a, in[4] + 0xe7d3fbc8, 20);
	MD5STEP(F2, a, b, c, d, in[9] + 0x21e1cde6, 5);
	MD5STEP(F2, d, a, b, c, in[14] + 0xc33707d6, 9);
	MD5STEP(F2, c, d, a, b, in[3] + 0xf4d50d87, 14);
	MD5STEP(F2, b, c, d, a, in[8] + 0x455a14ed, 20);
ffffffff812c4661:	c1 ca 0c             	ror    $0xc,%edx
ffffffff812c4664:	01 f2                	add    %esi,%edx
	MD5STEP(F2, a, b, c, d, in[13] + 0xa9e3e905, 5);
ffffffff812c4666:	31 d0                	xor    %edx,%eax
ffffffff812c4668:	21 c8                	and    %ecx,%eax
ffffffff812c466a:	31 f0                	xor    %esi,%eax
ffffffff812c466c:	44 01 f0             	add    %r14d,%eax
ffffffff812c466f:	44 8b 75 d0          	mov    -0x30(%rbp),%r14d
ffffffff812c4673:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c4676:	01 d0                	add    %edx,%eax
ffffffff812c4678:	45 8d b4 0e f8 a3 ef 	lea    -0x3105c08(%r14,%rcx,1),%r14d
ffffffff812c467f:	fc 
	MD5STEP(F2, d, a, b, c, in[2] + 0xfcefa3f8, 9);
ffffffff812c4680:	89 d1                	mov    %edx,%ecx
ffffffff812c4682:	31 c1                	xor    %eax,%ecx
ffffffff812c4684:	21 f1                	and    %esi,%ecx
ffffffff812c4686:	31 d1                	xor    %edx,%ecx
ffffffff812c4688:	44 01 f1             	add    %r14d,%ecx
ffffffff812c468b:	45 8d b4 34 d9 02 6f 	lea    0x676f02d9(%r12,%rsi,1),%r14d
ffffffff812c4692:	67 
	MD5STEP(F2, c, d, a, b, in[7] + 0x676f02d9, 14);
ffffffff812c4693:	89 c6                	mov    %eax,%esi
	MD5STEP(F2, a, b, c, d, in[9] + 0x21e1cde6, 5);
	MD5STEP(F2, d, a, b, c, in[14] + 0xc33707d6, 9);
	MD5STEP(F2, c, d, a, b, in[3] + 0xf4d50d87, 14);
	MD5STEP(F2, b, c, d, a, in[8] + 0x455a14ed, 20);
	MD5STEP(F2, a, b, c, d, in[13] + 0xa9e3e905, 5);
	MD5STEP(F2, d, a, b, c, in[2] + 0xfcefa3f8, 9);
ffffffff812c4695:	c1 c1 09             	rol    $0x9,%ecx
ffffffff812c4698:	01 c1                	add    %eax,%ecx
	MD5STEP(F2, c, d, a, b, in[7] + 0x676f02d9, 14);
ffffffff812c469a:	31 ce                	xor    %ecx,%esi
ffffffff812c469c:	21 d6                	and    %edx,%esi
ffffffff812c469e:	31 c6                	xor    %eax,%esi
ffffffff812c46a0:	44 01 f6             	add    %r14d,%esi
	MD5STEP(F2, b, c, d, a, in[12] + 0x8d2a4c8a, 20);
ffffffff812c46a3:	41 89 ce             	mov    %ecx,%r14d
	MD5STEP(F2, d, a, b, c, in[14] + 0xc33707d6, 9);
	MD5STEP(F2, c, d, a, b, in[3] + 0xf4d50d87, 14);
	MD5STEP(F2, b, c, d, a, in[8] + 0x455a14ed, 20);
	MD5STEP(F2, a, b, c, d, in[13] + 0xa9e3e905, 5);
	MD5STEP(F2, d, a, b, c, in[2] + 0xfcefa3f8, 9);
	MD5STEP(F2, c, d, a, b, in[7] + 0x676f02d9, 14);
ffffffff812c46a6:	c1 c6 0e             	rol    $0xe,%esi
ffffffff812c46a9:	01 ce                	add    %ecx,%esi
	MD5STEP(F2, b, c, d, a, in[12] + 0x8d2a4c8a, 20);
ffffffff812c46ab:	41 31 f6             	xor    %esi,%r14d
ffffffff812c46ae:	44 89 75 b4          	mov    %r14d,-0x4c(%rbp)
ffffffff812c46b2:	45 8d b4 13 8a 4c 2a 	lea    -0x72d5b376(%r11,%rdx,1),%r14d
ffffffff812c46b9:	8d 
ffffffff812c46ba:	8b 55 b4             	mov    -0x4c(%rbp),%edx
ffffffff812c46bd:	21 c2                	and    %eax,%edx
ffffffff812c46bf:	31 ca                	xor    %ecx,%edx
ffffffff812c46c1:	44 01 f2             	add    %r14d,%edx
ffffffff812c46c4:	44 8b 75 c4          	mov    -0x3c(%rbp),%r14d
ffffffff812c46c8:	c1 ca 0c             	ror    $0xc,%edx
ffffffff812c46cb:	01 f2                	add    %esi,%edx
ffffffff812c46cd:	41 8d 84 06 42 39 fa 	lea    -0x5c6be(%r14,%rax,1),%eax
ffffffff812c46d4:	ff 

	MD5STEP(F3, a, b, c, d, in[5] + 0xfffa3942, 4);
ffffffff812c46d5:	44 8b 75 b4          	mov    -0x4c(%rbp),%r14d
ffffffff812c46d9:	41 31 d6             	xor    %edx,%r14d
ffffffff812c46dc:	41 01 c6             	add    %eax,%r14d
ffffffff812c46df:	8b 45 bc             	mov    -0x44(%rbp),%eax
ffffffff812c46e2:	41 c1 c6 04          	rol    $0x4,%r14d
ffffffff812c46e6:	41 01 d6             	add    %edx,%r14d
ffffffff812c46e9:	8d 84 08 81 f6 71 87 	lea    -0x788e097f(%rax,%rcx,1),%eax
	MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
ffffffff812c46f0:	89 f1                	mov    %esi,%ecx
ffffffff812c46f2:	31 d1                	xor    %edx,%ecx
ffffffff812c46f4:	44 31 f1             	xor    %r14d,%ecx
ffffffff812c46f7:	01 c1                	add    %eax,%ecx
ffffffff812c46f9:	8b 45 b8             	mov    -0x48(%rbp),%eax
ffffffff812c46fc:	c1 c1 0b             	rol    $0xb,%ecx
ffffffff812c46ff:	44 01 f1             	add    %r14d,%ecx
ffffffff812c4702:	8d 84 30 22 61 9d 6d 	lea    0x6d9d6122(%rax,%rsi,1),%eax
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
ffffffff812c4709:	89 d6                	mov    %edx,%esi
ffffffff812c470b:	8d 94 13 0c 38 e5 fd 	lea    -0x21ac7f4(%rbx,%rdx,1),%edx
ffffffff812c4712:	44 31 f6             	xor    %r14d,%esi
ffffffff812c4715:	31 ce                	xor    %ecx,%esi
ffffffff812c4717:	01 c6                	add    %eax,%esi
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
ffffffff812c4719:	44 89 f0             	mov    %r14d,%eax
	MD5STEP(F2, c, d, a, b, in[7] + 0x676f02d9, 14);
	MD5STEP(F2, b, c, d, a, in[12] + 0x8d2a4c8a, 20);

	MD5STEP(F3, a, b, c, d, in[5] + 0xfffa3942, 4);
	MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
ffffffff812c471c:	c1 c6 10             	rol    $0x10,%esi
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
ffffffff812c471f:	31 c8                	xor    %ecx,%eax
	MD5STEP(F2, c, d, a, b, in[7] + 0x676f02d9, 14);
	MD5STEP(F2, b, c, d, a, in[12] + 0x8d2a4c8a, 20);

	MD5STEP(F3, a, b, c, d, in[5] + 0xfffa3942, 4);
	MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
ffffffff812c4721:	01 ce                	add    %ecx,%esi
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
ffffffff812c4723:	31 f0                	xor    %esi,%eax
ffffffff812c4725:	01 d0                	add    %edx,%eax
ffffffff812c4727:	8b 55 d4             	mov    -0x2c(%rbp),%edx
ffffffff812c472a:	c1 c8 09             	ror    $0x9,%eax
ffffffff812c472d:	01 f0                	add    %esi,%eax
ffffffff812c472f:	42 8d 94 32 44 ea be 	lea    -0x5b4115bc(%rdx,%r14,1),%edx
ffffffff812c4736:	a4 
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
ffffffff812c4737:	41 89 ce             	mov    %ecx,%r14d
ffffffff812c473a:	41 31 f6             	xor    %esi,%r14d
ffffffff812c473d:	41 31 c6             	xor    %eax,%r14d
ffffffff812c4740:	41 01 d6             	add    %edx,%r14d
ffffffff812c4743:	8b 55 c8             	mov    -0x38(%rbp),%edx
ffffffff812c4746:	41 c1 c6 04          	rol    $0x4,%r14d
ffffffff812c474a:	41 01 c6             	add    %eax,%r14d
ffffffff812c474d:	8d 94 0a a9 cf de 4b 	lea    0x4bdecfa9(%rdx,%rcx,1),%edx
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
ffffffff812c4754:	89 f1                	mov    %esi,%ecx
ffffffff812c4756:	41 8d b4 34 60 4b bb 	lea    -0x944b4a0(%r12,%rsi,1),%esi
ffffffff812c475d:	f6 
ffffffff812c475e:	31 c1                	xor    %eax,%ecx
ffffffff812c4760:	44 31 f1             	xor    %r14d,%ecx
ffffffff812c4763:	01 d1                	add    %edx,%ecx
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
ffffffff812c4765:	89 c2                	mov    %eax,%edx
ffffffff812c4767:	41 8d 84 02 70 bc bf 	lea    -0x41404390(%r10,%rax,1),%eax
ffffffff812c476e:	be 
	MD5STEP(F3, a, b, c, d, in[5] + 0xfffa3942, 4);
	MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
ffffffff812c476f:	c1 c1 0b             	rol    $0xb,%ecx
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
ffffffff812c4772:	44 31 f2             	xor    %r14d,%edx
	MD5STEP(F3, a, b, c, d, in[5] + 0xfffa3942, 4);
	MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
ffffffff812c4775:	44 01 f1             	add    %r14d,%ecx
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
ffffffff812c4778:	31 ca                	xor    %ecx,%edx
ffffffff812c477a:	01 f2                	add    %esi,%edx
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
ffffffff812c477c:	44 89 f6             	mov    %r14d,%esi
	MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
ffffffff812c477f:	c1 c2 10             	rol    $0x10,%edx
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
ffffffff812c4782:	31 ce                	xor    %ecx,%esi
	MD5STEP(F3, d, a, b, c, in[8] + 0x8771f681, 11);
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
ffffffff812c4784:	01 ca                	add    %ecx,%edx
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
ffffffff812c4786:	31 d6                	xor    %edx,%esi
ffffffff812c4788:	01 c6                	add    %eax,%esi
ffffffff812c478a:	43 8d 84 30 c6 7e 9b 	lea    0x289b7ec6(%r8,%r14,1),%eax
ffffffff812c4791:	28 
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
ffffffff812c4792:	41 89 ce             	mov    %ecx,%r14d
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
ffffffff812c4795:	c1 ce 09             	ror    $0x9,%esi
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
ffffffff812c4798:	41 31 d6             	xor    %edx,%r14d
ffffffff812c479b:	41 8d 8c 0d fa 27 a1 	lea    -0x155ed806(%r13,%rcx,1),%ecx
ffffffff812c47a2:	ea 
	MD5STEP(F3, c, d, a, b, in[11] + 0x6d9d6122, 16);
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
ffffffff812c47a3:	01 d6                	add    %edx,%esi
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
ffffffff812c47a5:	41 31 f6             	xor    %esi,%r14d
ffffffff812c47a8:	41 01 c6             	add    %eax,%r14d
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
ffffffff812c47ab:	89 d0                	mov    %edx,%eax
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
ffffffff812c47ad:	41 c1 c6 04          	rol    $0x4,%r14d
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
ffffffff812c47b1:	31 f0                	xor    %esi,%eax
	MD5STEP(F3, b, c, d, a, in[14] + 0xfde5380c, 23);
	MD5STEP(F3, a, b, c, d, in[1] + 0xa4beea44, 4);
	MD5STEP(F3, d, a, b, c, in[4] + 0x4bdecfa9, 11);
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
ffffffff812c47b3:	41 01 f6             	add    %esi,%r14d
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
ffffffff812c47b6:	44 31 f0             	xor    %r14d,%eax
ffffffff812c47b9:	01 c8                	add    %ecx,%eax
ffffffff812c47bb:	8b 4d cc             	mov    -0x34(%rbp),%ecx
ffffffff812c47be:	c1 c0 0b             	rol    $0xb,%eax
ffffffff812c47c1:	44 01 f0             	add    %r14d,%eax
ffffffff812c47c4:	8d 94 11 85 30 ef d4 	lea    -0x2b10cf7b(%rcx,%rdx,1),%edx
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
ffffffff812c47cb:	89 f1                	mov    %esi,%ecx
ffffffff812c47cd:	44 31 f1             	xor    %r14d,%ecx
ffffffff812c47d0:	31 c1                	xor    %eax,%ecx
ffffffff812c47d2:	01 d1                	add    %edx,%ecx
ffffffff812c47d4:	8b 55 c0             	mov    -0x40(%rbp),%edx
ffffffff812c47d7:	c1 c1 10             	rol    $0x10,%ecx
ffffffff812c47da:	01 c1                	add    %eax,%ecx
ffffffff812c47dc:	8d 94 32 05 1d 88 04 	lea    0x4881d05(%rdx,%rsi,1),%edx
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
ffffffff812c47e3:	44 89 f6             	mov    %r14d,%esi
ffffffff812c47e6:	47 8d b4 37 39 d0 d4 	lea    -0x262b2fc7(%r15,%r14,1),%r14d
ffffffff812c47ed:	d9 
ffffffff812c47ee:	31 c6                	xor    %eax,%esi
ffffffff812c47f0:	31 ce                	xor    %ecx,%esi
ffffffff812c47f2:	01 d6                	add    %edx,%esi
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
ffffffff812c47f4:	89 c2                	mov    %eax,%edx
ffffffff812c47f6:	41 8d 84 03 e5 99 db 	lea    -0x1924661b(%r11,%rax,1),%eax
ffffffff812c47fd:	e6 
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
ffffffff812c47fe:	c1 ce 09             	ror    $0x9,%esi
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
ffffffff812c4801:	31 ca                	xor    %ecx,%edx
	MD5STEP(F3, c, d, a, b, in[7] + 0xf6bb4b60, 16);
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
ffffffff812c4803:	01 ce                	add    %ecx,%esi
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
ffffffff812c4805:	31 f2                	xor    %esi,%edx
ffffffff812c4807:	44 01 f2             	add    %r14d,%edx
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
ffffffff812c480a:	41 89 ce             	mov    %ecx,%r14d
	MD5STEP(F3, b, c, d, a, in[10] + 0xbebfbc70, 23);
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
ffffffff812c480d:	c1 c2 04             	rol    $0x4,%edx
ffffffff812c4810:	01 f2                	add    %esi,%edx
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
ffffffff812c4812:	41 31 f6             	xor    %esi,%r14d
ffffffff812c4815:	41 31 d6             	xor    %edx,%r14d
ffffffff812c4818:	41 01 c6             	add    %eax,%r14d
ffffffff812c481b:	41 8d 84 09 f8 7c a2 	lea    0x1fa27cf8(%r9,%rcx,1),%eax
ffffffff812c4822:	1f 
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
ffffffff812c4823:	89 f1                	mov    %esi,%ecx
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
ffffffff812c4825:	41 c1 c6 0b          	rol    $0xb,%r14d
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
ffffffff812c4829:	31 d1                	xor    %edx,%ecx
	MD5STEP(F3, a, b, c, d, in[13] + 0x289b7ec6, 4);
	MD5STEP(F3, d, a, b, c, in[0] + 0xeaa127fa, 11);
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
ffffffff812c482b:	41 01 d6             	add    %edx,%r14d
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
ffffffff812c482e:	44 31 f1             	xor    %r14d,%ecx
ffffffff812c4831:	01 c1                	add    %eax,%ecx
ffffffff812c4833:	8b 45 d0             	mov    -0x30(%rbp),%eax
ffffffff812c4836:	c1 c1 10             	rol    $0x10,%ecx
ffffffff812c4839:	44 01 f1             	add    %r14d,%ecx
ffffffff812c483c:	8d 84 30 65 56 ac c4 	lea    -0x3b53a99b(%rax,%rsi,1),%eax
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);
ffffffff812c4843:	89 d6                	mov    %edx,%esi
ffffffff812c4845:	41 8d 94 15 44 22 29 	lea    -0xbd6ddbc(%r13,%rdx,1),%edx
ffffffff812c484c:	f4 
ffffffff812c484d:	44 31 f6             	xor    %r14d,%esi
ffffffff812c4850:	31 ce                	xor    %ecx,%esi
ffffffff812c4852:	01 f0                	add    %esi,%eax

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
ffffffff812c4854:	44 89 f6             	mov    %r14d,%esi
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);
ffffffff812c4857:	c1 c8 09             	ror    $0x9,%eax

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
ffffffff812c485a:	f7 d6                	not    %esi
	MD5STEP(F3, c, d, a, b, in[3] + 0xd4ef3085, 16);
	MD5STEP(F3, b, c, d, a, in[6] + 0x04881d05, 23);
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);
ffffffff812c485c:	01 c8                	add    %ecx,%eax

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
ffffffff812c485e:	09 c6                	or     %eax,%esi
ffffffff812c4860:	31 ce                	xor    %ecx,%esi
ffffffff812c4862:	01 f2                	add    %esi,%edx
ffffffff812c4864:	43 8d b4 34 97 ff 2a 	lea    0x432aff97(%r12,%r14,1),%esi
ffffffff812c486b:	43 
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
ffffffff812c486c:	41 89 cc             	mov    %ecx,%r12d
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
ffffffff812c486f:	c1 c2 06             	rol    $0x6,%edx
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
ffffffff812c4872:	41 f7 d4             	not    %r12d
ffffffff812c4875:	8d 8c 0b a7 23 94 ab 	lea    -0x546bdc59(%rbx,%rcx,1),%ecx
	MD5STEP(F3, a, b, c, d, in[9] + 0xd9d4d039, 4);
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
ffffffff812c487c:	01 c2                	add    %eax,%edx
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
ffffffff812c487e:	89 c3                	mov    %eax,%ebx
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
ffffffff812c4880:	41 09 d4             	or     %edx,%r12d
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
ffffffff812c4883:	f7 d3                	not    %ebx
	MD5STEP(F3, d, a, b, c, in[12] + 0xe6db99e5, 11);
	MD5STEP(F3, c, d, a, b, in[15] + 0x1fa27cf8, 16);
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
ffffffff812c4885:	41 31 c4             	xor    %eax,%r12d
ffffffff812c4888:	44 01 e6             	add    %r12d,%esi
ffffffff812c488b:	c1 c6 0a             	rol    $0xa,%esi
ffffffff812c488e:	01 d6                	add    %edx,%esi
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
ffffffff812c4890:	09 f3                	or     %esi,%ebx
ffffffff812c4892:	31 d3                	xor    %edx,%ebx
ffffffff812c4894:	01 d9                	add    %ebx,%ecx
ffffffff812c4896:	8b 5d c4             	mov    -0x3c(%rbp),%ebx
ffffffff812c4899:	c1 c1 0f             	rol    $0xf,%ecx
ffffffff812c489c:	01 f1                	add    %esi,%ecx
ffffffff812c489e:	8d 84 03 39 a0 93 fc 	lea    -0x36c5fc7(%rbx,%rax,1),%eax
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
ffffffff812c48a5:	89 d3                	mov    %edx,%ebx
ffffffff812c48a7:	41 8d 94 13 c3 59 5b 	lea    0x655b59c3(%r11,%rdx,1),%edx
ffffffff812c48ae:	65 
ffffffff812c48af:	f7 d3                	not    %ebx
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
ffffffff812c48b1:	41 89 f3             	mov    %esi,%r11d
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
ffffffff812c48b4:	09 cb                	or     %ecx,%ebx
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
ffffffff812c48b6:	41 f7 d3             	not    %r11d
	MD5STEP(F3, b, c, d, a, in[2] + 0xc4ac5665, 23);

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
ffffffff812c48b9:	31 f3                	xor    %esi,%ebx
ffffffff812c48bb:	01 d8                	add    %ebx,%eax
ffffffff812c48bd:	8b 5d cc             	mov    -0x34(%rbp),%ebx
ffffffff812c48c0:	c1 c8 0b             	ror    $0xb,%eax
ffffffff812c48c3:	01 c8                	add    %ecx,%eax
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
ffffffff812c48c5:	41 09 c3             	or     %eax,%r11d
ffffffff812c48c8:	8d b4 33 92 cc 0c 8f 	lea    -0x70f3336e(%rbx,%rsi,1),%esi
ffffffff812c48cf:	8b 5d d4             	mov    -0x2c(%rbp),%ebx
ffffffff812c48d2:	41 31 cb             	xor    %ecx,%r11d
ffffffff812c48d5:	44 01 da             	add    %r11d,%edx
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
ffffffff812c48d8:	41 89 cb             	mov    %ecx,%r11d
ffffffff812c48db:	41 8d 8c 0a 7d f4 ef 	lea    -0x100b83(%r10,%rcx,1),%ecx
ffffffff812c48e2:	ff 

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
ffffffff812c48e3:	c1 c2 06             	rol    $0x6,%edx
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
ffffffff812c48e6:	41 f7 d3             	not    %r11d
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
ffffffff812c48e9:	41 89 c2             	mov    %eax,%r10d

	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
ffffffff812c48ec:	01 c2                	add    %eax,%edx
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
ffffffff812c48ee:	41 f7 d2             	not    %r10d
	MD5STEP(F4, a, b, c, d, in[0] + 0xf4292244, 6);
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
ffffffff812c48f1:	41 09 d3             	or     %edx,%r11d
ffffffff812c48f4:	41 31 c3             	xor    %eax,%r11d
ffffffff812c48f7:	8d 84 03 d1 5d 84 85 	lea    -0x7a7ba22f(%rbx,%rax,1),%eax
ffffffff812c48fe:	8b 5d bc             	mov    -0x44(%rbp),%ebx
ffffffff812c4901:	44 01 de             	add    %r11d,%esi
ffffffff812c4904:	c1 c6 0a             	rol    $0xa,%esi
ffffffff812c4907:	01 d6                	add    %edx,%esi
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
ffffffff812c4909:	41 09 f2             	or     %esi,%r10d
ffffffff812c490c:	41 31 d2             	xor    %edx,%r10d
ffffffff812c490f:	44 01 d1             	add    %r10d,%ecx
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
ffffffff812c4912:	41 89 d2             	mov    %edx,%r10d
ffffffff812c4915:	8d 94 13 4f 7e a8 6f 	lea    0x6fa87e4f(%rbx,%rdx,1),%edx
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
ffffffff812c491c:	c1 c1 0f             	rol    $0xf,%ecx
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
ffffffff812c491f:	41 f7 d2             	not    %r10d
ffffffff812c4922:	8b 5d c0             	mov    -0x40(%rbp),%ebx
	MD5STEP(F4, d, a, b, c, in[7] + 0x432aff97, 10);
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
ffffffff812c4925:	01 f1                	add    %esi,%ecx
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
ffffffff812c4927:	41 09 ca             	or     %ecx,%r10d
ffffffff812c492a:	41 31 f2             	xor    %esi,%r10d
ffffffff812c492d:	44 01 d0             	add    %r10d,%eax
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
ffffffff812c4930:	41 89 f2             	mov    %esi,%r10d
ffffffff812c4933:	41 8d b4 31 e0 e6 2c 	lea    -0x1d31920(%r9,%rsi,1),%esi
ffffffff812c493a:	fe 
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
ffffffff812c493b:	c1 c8 0b             	ror    $0xb,%eax
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
ffffffff812c493e:	41 f7 d2             	not    %r10d
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
ffffffff812c4941:	41 89 c9             	mov    %ecx,%r9d
	MD5STEP(F4, c, d, a, b, in[14] + 0xab9423a7, 15);
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
ffffffff812c4944:	01 c8                	add    %ecx,%eax
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
ffffffff812c4946:	41 f7 d1             	not    %r9d
	MD5STEP(F4, b, c, d, a, in[5] + 0xfc93a039, 21);
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
ffffffff812c4949:	41 09 c2             	or     %eax,%r10d
ffffffff812c494c:	41 31 ca             	xor    %ecx,%r10d
ffffffff812c494f:	8d 8c 0b 14 43 01 a3 	lea    -0x5cfebcec(%rbx,%rcx,1),%ecx
ffffffff812c4956:	8b 5d b8             	mov    -0x48(%rbp),%ebx
ffffffff812c4959:	44 01 d2             	add    %r10d,%edx
ffffffff812c495c:	c1 c2 06             	rol    $0x6,%edx
ffffffff812c495f:	01 c2                	add    %eax,%edx
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
ffffffff812c4961:	41 09 d1             	or     %edx,%r9d
ffffffff812c4964:	41 31 c1             	xor    %eax,%r9d
ffffffff812c4967:	44 01 ce             	add    %r9d,%esi
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
ffffffff812c496a:	41 89 c1             	mov    %eax,%r9d
ffffffff812c496d:	41 8d 84 00 a1 11 08 	lea    0x4e0811a1(%r8,%rax,1),%eax
ffffffff812c4974:	4e 
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
ffffffff812c4975:	c1 c6 0a             	rol    $0xa,%esi
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
ffffffff812c4978:	41 f7 d1             	not    %r9d
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
ffffffff812c497b:	41 89 d0             	mov    %edx,%r8d
	MD5STEP(F4, a, b, c, d, in[12] + 0x655b59c3, 6);
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
ffffffff812c497e:	01 d6                	add    %edx,%esi
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
ffffffff812c4980:	41 f7 d0             	not    %r8d
	MD5STEP(F4, d, a, b, c, in[3] + 0x8f0ccc92, 10);
	MD5STEP(F4, c, d, a, b, in[10] + 0xffeff47d, 15);
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
ffffffff812c4983:	41 09 f1             	or     %esi,%r9d
ffffffff812c4986:	41 31 d1             	xor    %edx,%r9d
ffffffff812c4989:	44 01 c9             	add    %r9d,%ecx
ffffffff812c498c:	c1 c1 0f             	rol    $0xf,%ecx
ffffffff812c498f:	01 f1                	add    %esi,%ecx
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
ffffffff812c4991:	41 09 c8             	or     %ecx,%r8d
ffffffff812c4994:	41 31 f0             	xor    %esi,%r8d
ffffffff812c4997:	41 01 c0             	add    %eax,%r8d
ffffffff812c499a:	8b 45 c8             	mov    -0x38(%rbp),%eax
ffffffff812c499d:	41 c1 c8 0b          	ror    $0xb,%r8d
ffffffff812c49a1:	41 01 c8             	add    %ecx,%r8d
ffffffff812c49a4:	8d 94 10 82 7e 53 f7 	lea    -0x8ac817e(%rax,%rdx,1),%edx
	MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
ffffffff812c49ab:	89 f0                	mov    %esi,%eax
ffffffff812c49ad:	f7 d0                	not    %eax
ffffffff812c49af:	44 09 c0             	or     %r8d,%eax
ffffffff812c49b2:	31 c8                	xor    %ecx,%eax
ffffffff812c49b4:	01 d0                	add    %edx,%eax
ffffffff812c49b6:	8d 94 33 35 f2 3a bd 	lea    -0x42c50dcb(%rbx,%rsi,1),%edx
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
ffffffff812c49bd:	89 ce                	mov    %ecx,%esi
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
	MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
ffffffff812c49bf:	c1 c0 06             	rol    $0x6,%eax
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
ffffffff812c49c2:	f7 d6                	not    %esi
ffffffff812c49c4:	8b 5d d0             	mov    -0x30(%rbp),%ebx
	MD5STEP(F4, b, c, d, a, in[1] + 0x85845dd1, 21);
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
	MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
ffffffff812c49c7:	44 01 c0             	add    %r8d,%eax
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
ffffffff812c49ca:	09 c6                	or     %eax,%esi
ffffffff812c49cc:	44 31 c6             	xor    %r8d,%esi
ffffffff812c49cf:	01 d6                	add    %edx,%esi
ffffffff812c49d1:	8d 94 0b bb d2 d7 2a 	lea    0x2ad7d2bb(%rbx,%rcx,1),%edx
	MD5STEP(F4, c, d, a, b, in[2] + 0x2ad7d2bb, 15);
ffffffff812c49d8:	44 89 c1             	mov    %r8d,%ecx
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
	MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
ffffffff812c49db:	c1 c6 0a             	rol    $0xa,%esi
	MD5STEP(F4, c, d, a, b, in[2] + 0x2ad7d2bb, 15);
ffffffff812c49de:	f7 d1                	not    %ecx
	MD5STEP(F4, a, b, c, d, in[8] + 0x6fa87e4f, 6);
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
	MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
ffffffff812c49e0:	01 c6                	add    %eax,%esi
	MD5STEP(F4, c, d, a, b, in[2] + 0x2ad7d2bb, 15);
ffffffff812c49e2:	09 f1                	or     %esi,%ecx
ffffffff812c49e4:	31 c1                	xor    %eax,%ecx
ffffffff812c49e6:	01 d1                	add    %edx,%ecx
ffffffff812c49e8:	43 8d 94 07 91 d3 86 	lea    -0x14792c6f(%r15,%r8,1),%edx
ffffffff812c49ef:	eb 
	MD5STEP(F4, b, c, d, a, in[9] + 0xeb86d391, 21);

	hash[0] += a;
ffffffff812c49f0:	44 8b 07             	mov    (%rdi),%r8d
	MD5STEP(F4, d, a, b, c, in[15] + 0xfe2ce6e0, 10);
	MD5STEP(F4, c, d, a, b, in[6] + 0xa3014314, 15);
	MD5STEP(F4, b, c, d, a, in[13] + 0x4e0811a1, 21);
	MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
	MD5STEP(F4, c, d, a, b, in[2] + 0x2ad7d2bb, 15);
ffffffff812c49f3:	c1 c1 0f             	rol    $0xf,%ecx
ffffffff812c49f6:	01 f1                	add    %esi,%ecx
	MD5STEP(F4, b, c, d, a, in[9] + 0xeb86d391, 21);

	hash[0] += a;
ffffffff812c49f8:	41 01 c0             	add    %eax,%r8d
	hash[1] += b;
ffffffff812c49fb:	f7 d0                	not    %eax
ffffffff812c49fd:	09 c8                	or     %ecx,%eax
	MD5STEP(F4, a, b, c, d, in[4] + 0xf7537e82, 6);
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
	MD5STEP(F4, c, d, a, b, in[2] + 0x2ad7d2bb, 15);
	MD5STEP(F4, b, c, d, a, in[9] + 0xeb86d391, 21);

	hash[0] += a;
ffffffff812c49ff:	44 89 07             	mov    %r8d,(%rdi)
	hash[1] += b;
ffffffff812c4a02:	31 f0                	xor    %esi,%eax
ffffffff812c4a04:	01 d0                	add    %edx,%eax
ffffffff812c4a06:	8b 57 04             	mov    0x4(%rdi),%edx
ffffffff812c4a09:	c1 c8 0b             	ror    $0xb,%eax
ffffffff812c4a0c:	01 ca                	add    %ecx,%edx
	hash[2] += c;
ffffffff812c4a0e:	03 4f 08             	add    0x8(%rdi),%ecx
	hash[3] += d;
ffffffff812c4a11:	03 77 0c             	add    0xc(%rdi),%esi
	MD5STEP(F4, d, a, b, c, in[11] + 0xbd3af235, 10);
	MD5STEP(F4, c, d, a, b, in[2] + 0x2ad7d2bb, 15);
	MD5STEP(F4, b, c, d, a, in[9] + 0xeb86d391, 21);

	hash[0] += a;
	hash[1] += b;
ffffffff812c4a14:	01 d0                	add    %edx,%eax
ffffffff812c4a16:	89 47 04             	mov    %eax,0x4(%rdi)
	hash[2] += c;
ffffffff812c4a19:	89 4f 08             	mov    %ecx,0x8(%rdi)
	hash[3] += d;
ffffffff812c4a1c:	89 77 0c             	mov    %esi,0xc(%rdi)
}
ffffffff812c4a1f:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c4a23:	5b                   	pop    %rbx
ffffffff812c4a24:	41 5c                	pop    %r12
ffffffff812c4a26:	41 5d                	pop    %r13
ffffffff812c4a28:	41 5e                	pop    %r14
ffffffff812c4a2a:	41 5f                	pop    %r15
ffffffff812c4a2c:	5d                   	pop    %rbp
ffffffff812c4a2d:	c3                   	retq   

ffffffff812c4a2e <plist_add>:
 *
 * @node:	&struct plist_node pointer
 * @head:	&struct plist_head pointer
 */
void plist_add(struct plist_node *node, struct plist_head *head)
{
ffffffff812c4a2e:	55                   	push   %rbp
ffffffff812c4a2f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4a32:	41 57                	push   %r15
ffffffff812c4a34:	41 56                	push   %r14
ffffffff812c4a36:	41 55                	push   %r13
ffffffff812c4a38:	41 54                	push   %r12
 * plist_node_empty - return !0 if plist_node is not on a list
 * @node:	&struct plist_node pointer
 */
static inline int plist_node_empty(const struct plist_node *node)
{
	return list_empty(&node->node_list);
ffffffff812c4a3a:	4c 8d 67 18          	lea    0x18(%rdi),%r12
ffffffff812c4a3e:	53                   	push   %rbx
ffffffff812c4a3f:	52                   	push   %rdx
ffffffff812c4a40:	49 89 fd             	mov    %rdi,%r13
	struct plist_node *first, *iter, *prev = NULL;
	struct list_head *node_next = &head->node_list;

	plist_check_head(head);
	WARN_ON(!plist_node_empty(node));
ffffffff812c4a43:	4c 39 67 18          	cmp    %r12,0x18(%rdi)
 *
 * @node:	&struct plist_node pointer
 * @head:	&struct plist_head pointer
 */
void plist_add(struct plist_node *node, struct plist_head *head)
{
ffffffff812c4a47:	49 89 f6             	mov    %rsi,%r14
	struct plist_node *first, *iter, *prev = NULL;
	struct list_head *node_next = &head->node_list;
ffffffff812c4a4a:	48 89 f3             	mov    %rsi,%rbx

	plist_check_head(head);
	WARN_ON(!plist_node_empty(node));
ffffffff812c4a4d:	74 11                	je     ffffffff812c4a60 <plist_add+0x32>
ffffffff812c4a4f:	be 50 00 00 00       	mov    $0x50,%esi
ffffffff812c4a54:	48 c7 c7 81 6c 7b 81 	mov    $0xffffffff817b6c81,%rdi
ffffffff812c4a5b:	e8 55 19 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
	WARN_ON(!list_empty(&node->prio_list));
ffffffff812c4a60:	4d 8d 7d 08          	lea    0x8(%r13),%r15
ffffffff812c4a64:	4d 3b 7d 08          	cmp    0x8(%r13),%r15
ffffffff812c4a68:	74 11                	je     ffffffff812c4a7b <plist_add+0x4d>
ffffffff812c4a6a:	be 51 00 00 00       	mov    $0x51,%esi
ffffffff812c4a6f:	48 c7 c7 81 6c 7b 81 	mov    $0xffffffff817b6c81,%rdi
ffffffff812c4a76:	e8 3a 19 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline int list_empty(const struct list_head *head)
{
	return head->next == head;
ffffffff812c4a7b:	49 8b 0e             	mov    (%r14),%rcx

	if (plist_head_empty(head))
ffffffff812c4a7e:	49 39 ce             	cmp    %rcx,%r14
ffffffff812c4a81:	74 4d                	je     ffffffff812c4ad0 <plist_add+0xa2>
		goto ins_node;

	first = iter = plist_first(head);

	do {
		if (node->prio < iter->prio) {
ffffffff812c4a83:	41 8b 75 00          	mov    0x0(%r13),%esi
 *
 * Assumes the plist is _not_ empty.
 */
static inline struct plist_node *plist_first(const struct plist_head *head)
{
	return list_entry(head->node_list.next,
ffffffff812c4a87:	48 83 e9 18          	sub    $0x18,%rcx
 * @node:	&struct plist_node pointer
 * @head:	&struct plist_head pointer
 */
void plist_add(struct plist_node *node, struct plist_head *head)
{
	struct plist_node *first, *iter, *prev = NULL;
ffffffff812c4a8b:	31 ff                	xor    %edi,%edi
ffffffff812c4a8d:	48 89 c8             	mov    %rcx,%rax
		goto ins_node;

	first = iter = plist_first(head);

	do {
		if (node->prio < iter->prio) {
ffffffff812c4a90:	3b 30                	cmp    (%rax),%esi
ffffffff812c4a92:	7d 0c                	jge    ffffffff812c4aa0 <plist_add+0x72>
			node_next = &iter->node_list;
ffffffff812c4a94:	48 8d 58 18          	lea    0x18(%rax),%rbx
			break;
ffffffff812c4a98:	48 89 c1             	mov    %rax,%rcx
ffffffff812c4a9b:	48 89 f8             	mov    %rdi,%rax
ffffffff812c4a9e:	eb 15                	jmp    ffffffff812c4ab5 <plist_add+0x87>
		}

		prev = iter;
		iter = list_entry(iter->prio_list.next,
ffffffff812c4aa0:	48 8b 50 08          	mov    0x8(%rax),%rdx
ffffffff812c4aa4:	48 89 c7             	mov    %rax,%rdi
ffffffff812c4aa7:	48 83 ea 08          	sub    $0x8,%rdx
				struct plist_node, prio_list);
	} while (iter != first);
ffffffff812c4aab:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812c4aae:	74 05                	je     ffffffff812c4ab5 <plist_add+0x87>
ffffffff812c4ab0:	48 89 d0             	mov    %rdx,%rax
ffffffff812c4ab3:	eb db                	jmp    ffffffff812c4a90 <plist_add+0x62>

	if (!prev || prev->prio != node->prio)
ffffffff812c4ab5:	48 85 c0             	test   %rax,%rax
ffffffff812c4ab8:	74 04                	je     ffffffff812c4abe <plist_add+0x90>
ffffffff812c4aba:	3b 30                	cmp    (%rax),%esi
ffffffff812c4abc:	74 15                	je     ffffffff812c4ad3 <plist_add+0xa5>
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
ffffffff812c4abe:	48 8b 71 10          	mov    0x10(%rcx),%rsi
		list_add_tail(&node->prio_list, &iter->prio_list);
ffffffff812c4ac2:	48 8d 51 08          	lea    0x8(%rcx),%rdx
ffffffff812c4ac6:	4c 89 ff             	mov    %r15,%rdi
ffffffff812c4ac9:	e8 9c 03 01 00       	callq  ffffffff812d4e6a <__list_add>
ffffffff812c4ace:	eb 03                	jmp    ffffffff812c4ad3 <plist_add+0xa5>
ffffffff812c4ad0:	4c 89 f3             	mov    %r14,%rbx
ffffffff812c4ad3:	48 8b 73 08          	mov    0x8(%rbx),%rsi
ffffffff812c4ad7:	48 89 da             	mov    %rbx,%rdx
ffffffff812c4ada:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c4add:	e8 88 03 01 00       	callq  ffffffff812d4e6a <__list_add>
ins_node:
	list_add_tail(&node->node_list, node_next);

	plist_check_head(head);
}
ffffffff812c4ae2:	58                   	pop    %rax
ffffffff812c4ae3:	5b                   	pop    %rbx
ffffffff812c4ae4:	41 5c                	pop    %r12
ffffffff812c4ae6:	41 5d                	pop    %r13
ffffffff812c4ae8:	41 5e                	pop    %r14
ffffffff812c4aea:	41 5f                	pop    %r15
ffffffff812c4aec:	5d                   	pop    %rbp
ffffffff812c4aed:	c3                   	retq   

ffffffff812c4aee <plist_del>:
 *
 * @node:	&struct plist_node pointer - entry to be removed
 * @head:	&struct plist_head pointer - list head
 */
void plist_del(struct plist_node *node, struct plist_head *head)
{
ffffffff812c4aee:	55                   	push   %rbp
ffffffff812c4aef:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4af2:	41 54                	push   %r12
ffffffff812c4af4:	53                   	push   %rbx
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline int list_empty(const struct list_head *head)
{
	return head->next == head;
ffffffff812c4af5:	48 8b 57 08          	mov    0x8(%rdi),%rdx
	plist_check_head(head);

	if (!list_empty(&node->prio_list)) {
ffffffff812c4af9:	4c 8d 67 08          	lea    0x8(%rdi),%r12
 *
 * @node:	&struct plist_node pointer - entry to be removed
 * @head:	&struct plist_head pointer - list head
 */
void plist_del(struct plist_node *node, struct plist_head *head)
{
ffffffff812c4afd:	48 89 fb             	mov    %rdi,%rbx
	plist_check_head(head);

	if (!list_empty(&node->prio_list)) {
ffffffff812c4b00:	49 39 d4             	cmp    %rdx,%r12
ffffffff812c4b03:	74 2b                	je     ffffffff812c4b30 <plist_del+0x42>
		if (node->node_list.next != &head->node_list) {
ffffffff812c4b05:	48 8b 47 18          	mov    0x18(%rdi),%rax
ffffffff812c4b09:	48 39 f0             	cmp    %rsi,%rax
ffffffff812c4b0c:	74 12                	je     ffffffff812c4b20 <plist_del+0x32>

			next = list_entry(node->node_list.next,
					struct plist_node, node_list);

			/* add the next plist_node into prio_list */
			if (list_empty(&next->prio_list))
ffffffff812c4b0e:	48 8d 78 f0          	lea    -0x10(%rax),%rdi
ffffffff812c4b12:	48 3b 78 f0          	cmp    -0x10(%rax),%rdi
ffffffff812c4b16:	75 08                	jne    ffffffff812c4b20 <plist_del+0x32>
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static inline void list_add(struct list_head *new, struct list_head *head)
{
	__list_add(new, head, head->next);
ffffffff812c4b18:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c4b1b:	e8 4a 03 01 00       	callq  ffffffff812d4e6a <__list_add>
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static inline void list_del_init(struct list_head *entry)
{
	__list_del_entry(entry);
ffffffff812c4b20:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c4b23:	e8 eb 03 01 00       	callq  ffffffff812d4f13 <__list_del_entry>
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
ffffffff812c4b28:	4c 89 63 08          	mov    %r12,0x8(%rbx)
	list->prev = list;
ffffffff812c4b2c:	4c 89 63 10          	mov    %r12,0x10(%rbx)
				list_add(&next->prio_list, &node->prio_list);
		}
		list_del_init(&node->prio_list);
	}

	list_del_init(&node->node_list);
ffffffff812c4b30:	4c 8d 63 18          	lea    0x18(%rbx),%r12
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static inline void list_del_init(struct list_head *entry)
{
	__list_del_entry(entry);
ffffffff812c4b34:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c4b37:	e8 d7 03 01 00       	callq  ffffffff812d4f13 <__list_del_entry>
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
ffffffff812c4b3c:	4c 89 63 18          	mov    %r12,0x18(%rbx)
	list->prev = list;
ffffffff812c4b40:	4c 89 63 20          	mov    %r12,0x20(%rbx)

	plist_check_head(head);
}
ffffffff812c4b44:	5b                   	pop    %rbx
ffffffff812c4b45:	41 5c                	pop    %r12
ffffffff812c4b47:	5d                   	pop    %rbp
ffffffff812c4b48:	c3                   	retq   

ffffffff812c4b49 <plist_requeue>:
{
	struct plist_node *iter;
	struct list_head *node_next = &head->node_list;

	plist_check_head(head);
	BUG_ON(plist_head_empty(head));
ffffffff812c4b49:	48 3b 36             	cmp    (%rsi),%rsi
ffffffff812c4b4c:	75 02                	jne    ffffffff812c4b50 <plist_requeue+0x7>
ffffffff812c4b4e:	0f 0b                	ud2    
 *
 * @node:	&struct plist_node pointer - entry to be moved
 * @head:	&struct plist_head pointer - list head
 */
void plist_requeue(struct plist_node *node, struct plist_head *head)
{
ffffffff812c4b50:	55                   	push   %rbp
ffffffff812c4b51:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4b54:	41 56                	push   %r14
ffffffff812c4b56:	41 55                	push   %r13
ffffffff812c4b58:	41 54                	push   %r12
ffffffff812c4b5a:	53                   	push   %rbx
 * plist_node_empty - return !0 if plist_node is not on a list
 * @node:	&struct plist_node pointer
 */
static inline int plist_node_empty(const struct plist_node *node)
{
	return list_empty(&node->node_list);
ffffffff812c4b5b:	4c 8d 77 18          	lea    0x18(%rdi),%r14
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline int list_empty(const struct list_head *head)
{
	return head->next == head;
ffffffff812c4b5f:	4c 8b 6f 18          	mov    0x18(%rdi),%r13
	struct plist_node *iter;
	struct list_head *node_next = &head->node_list;

	plist_check_head(head);
	BUG_ON(plist_head_empty(head));
	BUG_ON(plist_node_empty(node));
ffffffff812c4b63:	4d 39 ee             	cmp    %r13,%r14
ffffffff812c4b66:	75 02                	jne    ffffffff812c4b6a <plist_requeue+0x21>
ffffffff812c4b68:	0f 0b                	ud2    

	if (node == plist_last(head))
ffffffff812c4b6a:	48 8b 46 08          	mov    0x8(%rsi),%rax
ffffffff812c4b6e:	48 83 e8 18          	sub    $0x18,%rax
ffffffff812c4b72:	48 39 c7             	cmp    %rax,%rdi
ffffffff812c4b75:	74 3e                	je     ffffffff812c4bb5 <plist_requeue+0x6c>
		return;

	iter = plist_next(node);

	if (node->prio != iter->prio)
ffffffff812c4b77:	41 8b 45 e8          	mov    -0x18(%r13),%eax
ffffffff812c4b7b:	39 07                	cmp    %eax,(%rdi)
ffffffff812c4b7d:	75 36                	jne    ffffffff812c4bb5 <plist_requeue+0x6c>
ffffffff812c4b7f:	49 89 f4             	mov    %rsi,%r12
ffffffff812c4b82:	48 89 fb             	mov    %rdi,%rbx
		return;

	plist_del(node, head);
ffffffff812c4b85:	e8 64 ff ff ff       	callq  ffffffff812c4aee <plist_del>

	plist_for_each_continue(iter, head) {
ffffffff812c4b8a:	49 8b 45 00          	mov    0x0(%r13),%rax
ffffffff812c4b8e:	48 83 e8 18          	sub    $0x18,%rax
ffffffff812c4b92:	48 8d 50 18          	lea    0x18(%rax),%rdx
ffffffff812c4b96:	49 39 d4             	cmp    %rdx,%r12
ffffffff812c4b99:	74 06                	je     ffffffff812c4ba1 <plist_requeue+0x58>
		if (node->prio != iter->prio) {
ffffffff812c4b9b:	8b 08                	mov    (%rax),%ecx
ffffffff812c4b9d:	39 0b                	cmp    %ecx,(%rbx)
ffffffff812c4b9f:	74 0e                	je     ffffffff812c4baf <plist_requeue+0x66>
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
ffffffff812c4ba1:	48 8b 70 20          	mov    0x20(%rax),%rsi
ffffffff812c4ba5:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c4ba8:	e8 bd 02 01 00       	callq  ffffffff812d4e6a <__list_add>
ffffffff812c4bad:	eb 06                	jmp    ffffffff812c4bb5 <plist_requeue+0x6c>
	if (node->prio != iter->prio)
		return;

	plist_del(node, head);

	plist_for_each_continue(iter, head) {
ffffffff812c4baf:	48 8b 40 18          	mov    0x18(%rax),%rax
ffffffff812c4bb3:	eb d9                	jmp    ffffffff812c4b8e <plist_requeue+0x45>
		}
	}
	list_add_tail(&node->node_list, node_next);

	plist_check_head(head);
}
ffffffff812c4bb5:	5b                   	pop    %rbx
ffffffff812c4bb6:	41 5c                	pop    %r12
ffffffff812c4bb8:	41 5d                	pop    %r13
ffffffff812c4bba:	41 5e                	pop    %r14
ffffffff812c4bbc:	5d                   	pop    %rbp
ffffffff812c4bbd:	c3                   	retq   

ffffffff812c4bbe <radix_tree_tagged>:
 *	radix_tree_tagged - test whether any items in the tree are tagged
 *	@root:		radix tree root
 *	@tag:		tag to test
 */
int radix_tree_tagged(struct radix_tree_root *root, unsigned int tag)
{
ffffffff812c4bbe:	55                   	push   %rbp
	return root_tag_get(root, tag);
ffffffff812c4bbf:	8d 4e 19             	lea    0x19(%rsi),%ecx
ffffffff812c4bc2:	b8 01 00 00 00       	mov    $0x1,%eax
 *	radix_tree_tagged - test whether any items in the tree are tagged
 *	@root:		radix tree root
 *	@tag:		tag to test
 */
int radix_tree_tagged(struct radix_tree_root *root, unsigned int tag)
{
ffffffff812c4bc7:	48 89 e5             	mov    %rsp,%rbp
	return root_tag_get(root, tag);
ffffffff812c4bca:	d3 e0                	shl    %cl,%eax
ffffffff812c4bcc:	23 47 04             	and    0x4(%rdi),%eax
}
ffffffff812c4bcf:	5d                   	pop    %rbp
ffffffff812c4bd0:	c3                   	retq   

ffffffff812c4bd1 <__radix_tree_preload>:
 *
 * To make use of this facility, the radix tree must be initialised without
 * __GFP_WAIT being passed to INIT_RADIX_TREE().
 */
static int __radix_tree_preload(gfp_t gfp_mask)
{
ffffffff812c4bd1:	55                   	push   %rbp
ffffffff812c4bd2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4bd5:	41 55                	push   %r13
ffffffff812c4bd7:	41 54                	push   %r12
ffffffff812c4bd9:	41 89 fc             	mov    %edi,%r12d
ffffffff812c4bdc:	53                   	push   %rbx
ffffffff812c4bdd:	51                   	push   %rcx
	struct radix_tree_preload *rtp;
	struct radix_tree_node *node;
	int ret = -ENOMEM;

	preempt_disable();
	rtp = this_cpu_ptr(&radix_tree_preloads);
ffffffff812c4bde:	49 c7 c5 60 09 01 00 	mov    $0x10960,%r13
ffffffff812c4be5:	4c 89 e8             	mov    %r13,%rax
ffffffff812c4be8:	65 48 03 05 28 55 d4 	add    %gs:0x7ed45528(%rip),%rax        # a118 <this_cpu_off>
ffffffff812c4bef:	7e 
ffffffff812c4bf0:	48 89 c3             	mov    %rax,%rbx
	while (rtp->nr < ARRAY_SIZE(rtp->nodes)) {
ffffffff812c4bf3:	83 3b 14             	cmpl   $0x14,(%rbx)
ffffffff812c4bf6:	77 47                	ja     ffffffff812c4c3f <__radix_tree_preload+0x6e>
		preempt_enable();
		node = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);
ffffffff812c4bf8:	48 8b 3d a9 cc 8e 00 	mov    0x8ecca9(%rip),%rdi        # ffffffff81bb18a8 <radix_tree_node_cachep>
ffffffff812c4bff:	44 89 e6             	mov    %r12d,%esi
ffffffff812c4c02:	e8 ca c4 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
		if (node == NULL)
ffffffff812c4c07:	48 85 c0             	test   %rax,%rax
ffffffff812c4c0a:	74 37                	je     ffffffff812c4c43 <__radix_tree_preload+0x72>
			goto out;
		preempt_disable();
		rtp = this_cpu_ptr(&radix_tree_preloads);
ffffffff812c4c0c:	4c 89 ea             	mov    %r13,%rdx
ffffffff812c4c0f:	65 48 03 15 01 55 d4 	add    %gs:0x7ed45501(%rip),%rdx        # a118 <this_cpu_off>
ffffffff812c4c16:	7e 
		if (rtp->nr < ARRAY_SIZE(rtp->nodes))
ffffffff812c4c17:	48 63 0a             	movslq (%rdx),%rcx
		preempt_enable();
		node = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);
		if (node == NULL)
			goto out;
		preempt_disable();
		rtp = this_cpu_ptr(&radix_tree_preloads);
ffffffff812c4c1a:	48 89 d3             	mov    %rdx,%rbx
		if (rtp->nr < ARRAY_SIZE(rtp->nodes))
ffffffff812c4c1d:	83 f9 14             	cmp    $0x14,%ecx
ffffffff812c4c20:	77 0c                	ja     ffffffff812c4c2e <__radix_tree_preload+0x5d>
			rtp->nodes[rtp->nr++] = node;
ffffffff812c4c22:	8d 71 01             	lea    0x1(%rcx),%esi
ffffffff812c4c25:	89 32                	mov    %esi,(%rdx)
ffffffff812c4c27:	48 89 44 ca 08       	mov    %rax,0x8(%rdx,%rcx,8)
ffffffff812c4c2c:	eb c5                	jmp    ffffffff812c4bf3 <__radix_tree_preload+0x22>
		else
			kmem_cache_free(radix_tree_node_cachep, node);
ffffffff812c4c2e:	48 8b 3d 73 cc 8e 00 	mov    0x8ecc73(%rip),%rdi        # ffffffff81bb18a8 <radix_tree_node_cachep>
ffffffff812c4c35:	48 89 c6             	mov    %rax,%rsi
ffffffff812c4c38:	e8 35 b9 e3 ff       	callq  ffffffff81100572 <kmem_cache_free>
ffffffff812c4c3d:	eb b4                	jmp    ffffffff812c4bf3 <__radix_tree_preload+0x22>
	}
	ret = 0;
ffffffff812c4c3f:	31 c0                	xor    %eax,%eax
ffffffff812c4c41:	eb 05                	jmp    ffffffff812c4c48 <__radix_tree_preload+0x77>
 */
static int __radix_tree_preload(gfp_t gfp_mask)
{
	struct radix_tree_preload *rtp;
	struct radix_tree_node *node;
	int ret = -ENOMEM;
ffffffff812c4c43:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
			kmem_cache_free(radix_tree_node_cachep, node);
	}
	ret = 0;
out:
	return ret;
}
ffffffff812c4c48:	5a                   	pop    %rdx
ffffffff812c4c49:	5b                   	pop    %rbx
ffffffff812c4c4a:	41 5c                	pop    %r12
ffffffff812c4c4c:	41 5d                	pop    %r13
ffffffff812c4c4e:	5d                   	pop    %rbp
ffffffff812c4c4f:	c3                   	retq   

ffffffff812c4c50 <radix_tree_preload>:
 *
 * To make use of this facility, the radix tree must be initialised without
 * __GFP_WAIT being passed to INIT_RADIX_TREE().
 */
int radix_tree_preload(gfp_t gfp_mask)
{
ffffffff812c4c50:	55                   	push   %rbp
ffffffff812c4c51:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4c54:	53                   	push   %rbx
ffffffff812c4c55:	89 fb                	mov    %edi,%ebx
	/* Warn on non-sensical use... */
	WARN_ON_ONCE(!(gfp_mask & __GFP_WAIT));
ffffffff812c4c57:	40 80 e7 10          	and    $0x10,%dil
 *
 * To make use of this facility, the radix tree must be initialised without
 * __GFP_WAIT being passed to INIT_RADIX_TREE().
 */
int radix_tree_preload(gfp_t gfp_mask)
{
ffffffff812c4c5b:	51                   	push   %rcx
	/* Warn on non-sensical use... */
	WARN_ON_ONCE(!(gfp_mask & __GFP_WAIT));
ffffffff812c4c5c:	75 21                	jne    ffffffff812c4c7f <radix_tree_preload+0x2f>
ffffffff812c4c5e:	80 3d 84 26 79 00 00 	cmpb   $0x0,0x792684(%rip)        # ffffffff81a572e9 <__warned.18050>
ffffffff812c4c65:	75 18                	jne    ffffffff812c4c7f <radix_tree_preload+0x2f>
ffffffff812c4c67:	be 21 01 00 00       	mov    $0x121,%esi
ffffffff812c4c6c:	48 c7 c7 8d 6c 7b 81 	mov    $0xffffffff817b6c8d,%rdi
ffffffff812c4c73:	e8 3d 17 da ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c4c78:	c6 05 6a 26 79 00 01 	movb   $0x1,0x79266a(%rip)        # ffffffff81a572e9 <__warned.18050>
	return __radix_tree_preload(gfp_mask);
ffffffff812c4c7f:	89 df                	mov    %ebx,%edi
ffffffff812c4c81:	e8 4b ff ff ff       	callq  ffffffff812c4bd1 <__radix_tree_preload>
}
ffffffff812c4c86:	5a                   	pop    %rdx
ffffffff812c4c87:	5b                   	pop    %rbx
ffffffff812c4c88:	5d                   	pop    %rbp
ffffffff812c4c89:	c3                   	retq   

ffffffff812c4c8a <radix_tree_maybe_preload>:
 * We do it, if we decide it helps. On success, return zero with preemption
 * disabled. On error, return -ENOMEM with preemption not disabled.
 */
int radix_tree_maybe_preload(gfp_t gfp_mask)
{
	if (gfp_mask & __GFP_WAIT)
ffffffff812c4c8a:	40 f6 c7 10          	test   $0x10,%dil
ffffffff812c4c8e:	74 0b                	je     ffffffff812c4c9b <radix_tree_maybe_preload+0x11>
 * The same as above function, except we don't guarantee preloading happens.
 * We do it, if we decide it helps. On success, return zero with preemption
 * disabled. On error, return -ENOMEM with preemption not disabled.
 */
int radix_tree_maybe_preload(gfp_t gfp_mask)
{
ffffffff812c4c90:	55                   	push   %rbp
ffffffff812c4c91:	48 89 e5             	mov    %rsp,%rbp
	if (gfp_mask & __GFP_WAIT)
		return __radix_tree_preload(gfp_mask);
ffffffff812c4c94:	e8 38 ff ff ff       	callq  ffffffff812c4bd1 <__radix_tree_preload>
	/* Preloading doesn't help anything with this gfp mask, skip it */
	preempt_disable();
	return 0;
}
ffffffff812c4c99:	5d                   	pop    %rbp
ffffffff812c4c9a:	c3                   	retq   
{
	if (gfp_mask & __GFP_WAIT)
		return __radix_tree_preload(gfp_mask);
	/* Preloading doesn't help anything with this gfp mask, skip it */
	preempt_disable();
	return 0;
ffffffff812c4c9b:	31 c0                	xor    %eax,%eax
ffffffff812c4c9d:	c3                   	retq   

ffffffff812c4c9e <radix_tree_node_rcu_free>:
	BUG_ON(radix_tree_is_indirect_ptr(ret));
	return ret;
}

static void radix_tree_node_rcu_free(struct rcu_head *head)
{
ffffffff812c4c9e:	55                   	push   %rbp
}

static inline void tag_clear(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__clear_bit(offset, node->tags[tag]);
ffffffff812c4c9f:	48 8d 77 f8          	lea    -0x8(%rdi),%rsi
	BUG_ON(radix_tree_is_indirect_ptr(ret));
	return ret;
}

static void radix_tree_node_rcu_free(struct rcu_head *head)
{
ffffffff812c4ca3:	48 89 e5             	mov    %rsp,%rbp
	clear_bit(nr, addr);
}

static inline void __clear_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("btr %1,%0" : ADDR : "Ir" (nr));
ffffffff812c4ca6:	0f ba b7 20 02 00 00 	btrl   $0x0,0x220(%rdi)
ffffffff812c4cad:	00 
ffffffff812c4cae:	0f ba b7 28 02 00 00 	btrl   $0x0,0x228(%rdi)
ffffffff812c4cb5:	00 
ffffffff812c4cb6:	0f ba b7 30 02 00 00 	btrl   $0x0,0x230(%rdi)
ffffffff812c4cbd:	00 
	 * that here to make sure.
	 */
	for (i = 0; i < RADIX_TREE_MAX_TAGS; i++)
		tag_clear(node, i, 0);

	node->slots[0] = NULL;
ffffffff812c4cbe:	48 c7 47 20 00 00 00 	movq   $0x0,0x20(%rdi)
ffffffff812c4cc5:	00 
	node->count = 0;
ffffffff812c4cc6:	c7 47 fc 00 00 00 00 	movl   $0x0,-0x4(%rdi)

	kmem_cache_free(radix_tree_node_cachep, node);
ffffffff812c4ccd:	48 8b 3d d4 cb 8e 00 	mov    0x8ecbd4(%rip),%rdi        # ffffffff81bb18a8 <radix_tree_node_cachep>
ffffffff812c4cd4:	e8 99 b8 e3 ff       	callq  ffffffff81100572 <kmem_cache_free>
}
ffffffff812c4cd9:	5d                   	pop    %rbp
ffffffff812c4cda:	c3                   	retq   

ffffffff812c4cdb <radix_tree_callback>:
{
       int cpu = (long)hcpu;
       struct radix_tree_preload *rtp;

       /* Free per-cpu pool of perloaded nodes */
       if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
ffffffff812c4cdb:	48 83 e6 ef          	and    $0xffffffffffffffef,%rsi
ffffffff812c4cdf:	48 83 fe 07          	cmp    $0x7,%rsi
ffffffff812c4ce3:	75 50                	jne    ffffffff812c4d35 <radix_tree_callback+0x5a>
}

static int radix_tree_callback(struct notifier_block *nfb,
                            unsigned long action,
                            void *hcpu)
{
ffffffff812c4ce5:	55                   	push   %rbp
       int cpu = (long)hcpu;
       struct radix_tree_preload *rtp;

       /* Free per-cpu pool of perloaded nodes */
       if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
               rtp = &per_cpu(radix_tree_preloads, cpu);
ffffffff812c4ce6:	48 63 d2             	movslq %edx,%rdx
}

static int radix_tree_callback(struct notifier_block *nfb,
                            unsigned long action,
                            void *hcpu)
{
ffffffff812c4ce9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4cec:	53                   	push   %rbx
       int cpu = (long)hcpu;
       struct radix_tree_preload *rtp;

       /* Free per-cpu pool of perloaded nodes */
       if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
               rtp = &per_cpu(radix_tree_preloads, cpu);
ffffffff812c4ced:	48 c7 c3 60 09 01 00 	mov    $0x10960,%rbx
}

static int radix_tree_callback(struct notifier_block *nfb,
                            unsigned long action,
                            void *hcpu)
{
ffffffff812c4cf4:	51                   	push   %rcx
       int cpu = (long)hcpu;
       struct radix_tree_preload *rtp;

       /* Free per-cpu pool of perloaded nodes */
       if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
               rtp = &per_cpu(radix_tree_preloads, cpu);
ffffffff812c4cf5:	48 03 1c d5 00 8e a5 	add    -0x7e5a7200(,%rdx,8),%rbx
ffffffff812c4cfc:	81 
               while (rtp->nr) {
ffffffff812c4cfd:	8b 03                	mov    (%rbx),%eax
ffffffff812c4cff:	85 c0                	test   %eax,%eax
ffffffff812c4d01:	74 29                	je     ffffffff812c4d2c <radix_tree_callback+0x51>
                       kmem_cache_free(radix_tree_node_cachep,
                                       rtp->nodes[rtp->nr-1]);
ffffffff812c4d03:	ff c8                	dec    %eax

       /* Free per-cpu pool of perloaded nodes */
       if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
               rtp = &per_cpu(radix_tree_preloads, cpu);
               while (rtp->nr) {
                       kmem_cache_free(radix_tree_node_cachep,
ffffffff812c4d05:	48 8b 3d 9c cb 8e 00 	mov    0x8ecb9c(%rip),%rdi        # ffffffff81bb18a8 <radix_tree_node_cachep>
                                       rtp->nodes[rtp->nr-1]);
ffffffff812c4d0c:	48 98                	cltq   

       /* Free per-cpu pool of perloaded nodes */
       if (action == CPU_DEAD || action == CPU_DEAD_FROZEN) {
               rtp = &per_cpu(radix_tree_preloads, cpu);
               while (rtp->nr) {
                       kmem_cache_free(radix_tree_node_cachep,
ffffffff812c4d0e:	48 8b 74 c3 08       	mov    0x8(%rbx,%rax,8),%rsi
ffffffff812c4d13:	e8 5a b8 e3 ff       	callq  ffffffff81100572 <kmem_cache_free>
                                       rtp->nodes[rtp->nr-1]);
                       rtp->nodes[rtp->nr-1] = NULL;
ffffffff812c4d18:	8b 03                	mov    (%rbx),%eax
ffffffff812c4d1a:	ff c8                	dec    %eax
ffffffff812c4d1c:	48 63 d0             	movslq %eax,%rdx
ffffffff812c4d1f:	48 c7 44 d3 08 00 00 	movq   $0x0,0x8(%rbx,%rdx,8)
ffffffff812c4d26:	00 00 
                       rtp->nr--;
ffffffff812c4d28:	89 03                	mov    %eax,(%rbx)
ffffffff812c4d2a:	eb d1                	jmp    ffffffff812c4cfd <radix_tree_callback+0x22>
               }
       }
       return NOTIFY_OK;
}
ffffffff812c4d2c:	5a                   	pop    %rdx
ffffffff812c4d2d:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c4d32:	5b                   	pop    %rbx
ffffffff812c4d33:	5d                   	pop    %rbp
ffffffff812c4d34:	c3                   	retq   
ffffffff812c4d35:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c4d3a:	c3                   	retq   

ffffffff812c4d3b <tag_get>:
	__clear_bit(offset, node->tags[tag]);
}

static inline int tag_get(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
ffffffff812c4d3b:	55                   	push   %rbp
	return test_bit(offset, node->tags[tag]);
ffffffff812c4d3c:	89 f6                	mov    %esi,%esi

static inline int variable_test_bit(long nr, volatile const unsigned long *addr)
{
	int oldbit;

	asm volatile("bt %2,%1\n\t"
ffffffff812c4d3e:	48 63 c2             	movslq %edx,%rax
	__clear_bit(offset, node->tags[tag]);
}

static inline int tag_get(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
ffffffff812c4d41:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4d44:	48 0f a3 84 f7 28 02 	bt     %rax,0x228(%rdi,%rsi,8)
ffffffff812c4d4b:	00 00 
ffffffff812c4d4d:	19 c0                	sbb    %eax,%eax
	return test_bit(offset, node->tags[tag]);
}
ffffffff812c4d4f:	5d                   	pop    %rbp
ffffffff812c4d50:	c3                   	retq   

ffffffff812c4d51 <radix_tree_next_chunk>:
 * @flags:	RADIX_TREE_ITER_* flags and tag index
 * Returns:	pointer to chunk first slot, or NULL if iteration is over
 */
void **radix_tree_next_chunk(struct radix_tree_root *root,
			     struct radix_tree_iter *iter, unsigned flags)
{
ffffffff812c4d51:	55                   	push   %rbp
ffffffff812c4d52:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4d55:	41 57                	push   %r15
ffffffff812c4d57:	41 56                	push   %r14
ffffffff812c4d59:	41 55                	push   %r13
ffffffff812c4d5b:	41 54                	push   %r12
	unsigned shift, tag = flags & RADIX_TREE_ITER_TAG_MASK;
	struct radix_tree_node *rnode, *node;
	unsigned long index, offset, height;

	if ((flags & RADIX_TREE_ITER_TAGGED) && !root_tag_get(root, tag))
ffffffff812c4d5d:	41 89 d4             	mov    %edx,%r12d
ffffffff812c4d60:	41 81 e4 00 01 00 00 	and    $0x100,%r12d
 * @flags:	RADIX_TREE_ITER_* flags and tag index
 * Returns:	pointer to chunk first slot, or NULL if iteration is over
 */
void **radix_tree_next_chunk(struct radix_tree_root *root,
			     struct radix_tree_iter *iter, unsigned flags)
{
ffffffff812c4d67:	53                   	push   %rbx
	unsigned shift, tag = flags & RADIX_TREE_ITER_TAG_MASK;
ffffffff812c4d68:	0f b6 da             	movzbl %dl,%ebx
	struct radix_tree_node *rnode, *node;
	unsigned long index, offset, height;

	if ((flags & RADIX_TREE_ITER_TAGGED) && !root_tag_get(root, tag))
ffffffff812c4d6b:	74 17                	je     ffffffff812c4d84 <radix_tree_next_chunk+0x33>
ffffffff812c4d6d:	8d 4b 19             	lea    0x19(%rbx),%ecx
ffffffff812c4d70:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c4d75:	d3 e0                	shl    %cl,%eax
ffffffff812c4d77:	89 c1                	mov    %eax,%ecx
		return NULL;
ffffffff812c4d79:	31 c0                	xor    %eax,%eax
{
	unsigned shift, tag = flags & RADIX_TREE_ITER_TAG_MASK;
	struct radix_tree_node *rnode, *node;
	unsigned long index, offset, height;

	if ((flags & RADIX_TREE_ITER_TAGGED) && !root_tag_get(root, tag))
ffffffff812c4d7b:	85 4f 04             	test   %ecx,0x4(%rdi)
ffffffff812c4d7e:	0f 84 8a 01 00 00    	je     ffffffff812c4f0e <radix_tree_next_chunk+0x1bd>
	 * because RADIX_TREE_MAP_SHIFT < BITS_PER_LONG.
	 *
	 * This condition also used by radix_tree_next_slot() to stop
	 * contiguous iterating, and forbid swithing to the next chunk.
	 */
	index = iter->next_index;
ffffffff812c4d84:	4c 8b 46 08          	mov    0x8(%rsi),%r8
	if (!index && iter->index)
ffffffff812c4d88:	4d 85 c0             	test   %r8,%r8
ffffffff812c4d8b:	75 0c                	jne    ffffffff812c4d99 <radix_tree_next_chunk+0x48>
	unsigned shift, tag = flags & RADIX_TREE_ITER_TAG_MASK;
	struct radix_tree_node *rnode, *node;
	unsigned long index, offset, height;

	if ((flags & RADIX_TREE_ITER_TAGGED) && !root_tag_get(root, tag))
		return NULL;
ffffffff812c4d8d:	31 c0                	xor    %eax,%eax
	 *
	 * This condition also used by radix_tree_next_slot() to stop
	 * contiguous iterating, and forbid swithing to the next chunk.
	 */
	index = iter->next_index;
	if (!index && iter->index)
ffffffff812c4d8f:	48 83 3e 00          	cmpq   $0x0,(%rsi)
ffffffff812c4d93:	0f 85 75 01 00 00    	jne    ffffffff812c4f0e <radix_tree_next_chunk+0x1bd>
		return NULL;

	rnode = rcu_dereference_raw(root->rnode);
ffffffff812c4d99:	4c 8b 5f 08          	mov    0x8(%rdi),%r11
	if (radix_tree_is_indirect_ptr(rnode)) {
ffffffff812c4d9d:	41 f6 c3 01          	test   $0x1,%r11b
ffffffff812c4da1:	74 31                	je     ffffffff812c4dd4 <radix_tree_next_chunk+0x83>
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c4da3:	49 83 e3 fe          	and    $0xfffffffffffffffe,%r11
		return NULL;

	node = rnode;
	while (1) {
		if ((flags & RADIX_TREE_ITER_TAGGED) ?
				!test_bit(offset, node->tags[tag]) :
ffffffff812c4da7:	89 d8                	mov    %ebx,%eax
				!node->slots[offset]) {
			/* Hole detected */
			if (flags & RADIX_TREE_ITER_CONTIG)
ffffffff812c4da9:	81 e2 00 02 00 00    	and    $0x200,%edx
	} else
		return NULL;

restart:
	height = rnode->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c4daf:	45 8b 0b             	mov    (%r11),%r9d
		return NULL;

	node = rnode;
	while (1) {
		if ((flags & RADIX_TREE_ITER_TAGGED) ?
				!test_bit(offset, node->tags[tag]) :
ffffffff812c4db2:	4c 8d 2c c5 20 02 00 	lea    0x220(,%rax,8),%r13
ffffffff812c4db9:	00 
			else
				while (++offset	< RADIX_TREE_MAP_SIZE) {
					if (node->slots[offset])
						break;
				}
			index &= ~((RADIX_TREE_MAP_SIZE << shift) - 1);
ffffffff812c4dba:	41 be 40 00 00 00    	mov    $0x40,%r14d
	} else
		return NULL;

restart:
	height = rnode->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c4dc0:	41 81 e1 ff 0f 00 00 	and    $0xfff,%r9d
ffffffff812c4dc7:	45 6b c9 06          	imul   $0x6,%r9d,%r9d
ffffffff812c4dcb:	41 83 e9 06          	sub    $0x6,%r9d
ffffffff812c4dcf:	e9 b9 00 00 00       	jmpq   ffffffff812c4e8d <radix_tree_next_chunk+0x13c>
		return NULL;

	rnode = rcu_dereference_raw(root->rnode);
	if (radix_tree_is_indirect_ptr(rnode)) {
		rnode = indirect_to_ptr(rnode);
	} else if (rnode && !index) {
ffffffff812c4dd4:	4d 85 db             	test   %r11,%r11
ffffffff812c4dd7:	0f 84 2f 01 00 00    	je     ffffffff812c4f0c <radix_tree_next_chunk+0x1bb>
ffffffff812c4ddd:	4d 85 c0             	test   %r8,%r8
ffffffff812c4de0:	0f 85 26 01 00 00    	jne    ffffffff812c4f0c <radix_tree_next_chunk+0x1bb>
		/* Single-slot tree */
		iter->index = 0;
ffffffff812c4de6:	48 c7 06 00 00 00 00 	movq   $0x0,(%rsi)
		iter->next_index = 1;
ffffffff812c4ded:	48 c7 46 08 01 00 00 	movq   $0x1,0x8(%rsi)
ffffffff812c4df4:	00 
		iter->tags = 1;
		return (void **)&root->rnode;
ffffffff812c4df5:	48 8d 47 08          	lea    0x8(%rdi),%rax
		rnode = indirect_to_ptr(rnode);
	} else if (rnode && !index) {
		/* Single-slot tree */
		iter->index = 0;
		iter->next_index = 1;
		iter->tags = 1;
ffffffff812c4df9:	48 c7 46 10 01 00 00 	movq   $0x1,0x10(%rsi)
ffffffff812c4e00:	00 
		return (void **)&root->rnode;
ffffffff812c4e01:	e9 08 01 00 00       	jmpq   ffffffff812c4f0e <radix_tree_next_chunk+0x1bd>
			if (offset == RADIX_TREE_MAP_SIZE)
				goto restart;
		}

		/* This is leaf-node */
		if (!shift)
ffffffff812c4e06:	45 85 d2             	test   %r10d,%r10d
ffffffff812c4e09:	0f 84 c7 00 00 00    	je     ffffffff812c4ed6 <radix_tree_next_chunk+0x185>
			break;

		node = rcu_dereference_raw(node->slots[offset]);
ffffffff812c4e0f:	48 83 c0 04          	add    $0x4,%rax
ffffffff812c4e13:	48 8b 7c c7 08       	mov    0x8(%rdi,%rax,8),%rdi
		if (node == NULL)
ffffffff812c4e18:	48 85 ff             	test   %rdi,%rdi
ffffffff812c4e1b:	74 70                	je     ffffffff812c4e8d <radix_tree_next_chunk+0x13c>
			goto restart;
		shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c4e1d:	41 83 ea 06          	sub    $0x6,%r10d
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c4e21:	4c 89 c0             	mov    %r8,%rax
ffffffff812c4e24:	44 88 d1             	mov    %r10b,%cl
ffffffff812c4e27:	48 d3 e8             	shr    %cl,%rax
ffffffff812c4e2a:	83 e0 3f             	and    $0x3f,%eax
	if (offset >= RADIX_TREE_MAP_SIZE)
		return NULL;

	node = rnode;
	while (1) {
		if ((flags & RADIX_TREE_ITER_TAGGED) ?
ffffffff812c4e2d:	45 85 e4             	test   %r12d,%r12d
ffffffff812c4e30:	74 72                	je     ffffffff812c4ea4 <radix_tree_next_chunk+0x153>
ffffffff812c4e32:	4a 0f a3 44 2f 08    	bt     %rax,0x8(%rdi,%r13,1)
ffffffff812c4e38:	19 c9                	sbb    %ecx,%ecx
ffffffff812c4e3a:	85 c9                	test   %ecx,%ecx
ffffffff812c4e3c:	0f 94 c1             	sete   %cl
ffffffff812c4e3f:	84 c9                	test   %cl,%cl
ffffffff812c4e41:	74 c3                	je     ffffffff812c4e06 <radix_tree_next_chunk+0xb5>
				!test_bit(offset, node->tags[tag]) :
				!node->slots[offset]) {
			/* Hole detected */
			if (flags & RADIX_TREE_ITER_CONTIG)
ffffffff812c4e43:	85 d2                	test   %edx,%edx
ffffffff812c4e45:	0f 85 c1 00 00 00    	jne    ffffffff812c4f0c <radix_tree_next_chunk+0x1bb>
				return NULL;

			if (flags & RADIX_TREE_ITER_TAGGED)
ffffffff812c4e4b:	45 85 e4             	test   %r12d,%r12d
ffffffff812c4e4e:	74 7b                	je     ffffffff812c4ecb <radix_tree_next_chunk+0x17a>
				offset = radix_tree_find_next_bit(
ffffffff812c4e50:	48 8d 48 01          	lea    0x1(%rax),%rcx
			if (tmp)
				return __ffs(tmp) + offset;
			offset += BITS_PER_LONG;
		}
	}
	return size;
ffffffff812c4e54:	b8 40 00 00 00       	mov    $0x40,%eax
			 unsigned long size, unsigned long offset)
{
	if (!__builtin_constant_p(size))
		return find_next_bit(addr, size, offset);

	if (offset < size) {
ffffffff812c4e59:	48 83 f9 40          	cmp    $0x40,%rcx
ffffffff812c4e5d:	75 4d                	jne    ffffffff812c4eac <radix_tree_next_chunk+0x15b>
			else
				while (++offset	< RADIX_TREE_MAP_SIZE) {
					if (node->slots[offset])
						break;
				}
			index &= ~((RADIX_TREE_MAP_SIZE << shift) - 1);
ffffffff812c4e5f:	44 88 d1             	mov    %r10b,%cl
ffffffff812c4e62:	4d 89 f7             	mov    %r14,%r15
ffffffff812c4e65:	49 d3 e7             	shl    %cl,%r15
ffffffff812c4e68:	4c 89 f9             	mov    %r15,%rcx
			index += offset << shift;
ffffffff812c4e6b:	49 89 c7             	mov    %rax,%r15
			else
				while (++offset	< RADIX_TREE_MAP_SIZE) {
					if (node->slots[offset])
						break;
				}
			index &= ~((RADIX_TREE_MAP_SIZE << shift) - 1);
ffffffff812c4e6e:	48 f7 d9             	neg    %rcx
ffffffff812c4e71:	49 21 c8             	and    %rcx,%r8
			index += offset << shift;
ffffffff812c4e74:	44 88 d1             	mov    %r10b,%cl
ffffffff812c4e77:	49 d3 e7             	shl    %cl,%r15
			/* Overflow after ~0UL */
			if (!index)
ffffffff812c4e7a:	4d 01 f8             	add    %r15,%r8
ffffffff812c4e7d:	0f 84 89 00 00 00    	je     ffffffff812c4f0c <radix_tree_next_chunk+0x1bb>
				return NULL;
			if (offset == RADIX_TREE_MAP_SIZE)
ffffffff812c4e83:	48 83 f8 40          	cmp    $0x40,%rax
ffffffff812c4e87:	0f 85 79 ff ff ff    	jne    ffffffff812c4e06 <radix_tree_next_chunk+0xb5>
		return NULL;

restart:
	height = rnode->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
	offset = index >> shift;
ffffffff812c4e8d:	4c 89 c0             	mov    %r8,%rax
ffffffff812c4e90:	44 88 c9             	mov    %r9b,%cl
ffffffff812c4e93:	48 d3 e8             	shr    %cl,%rax

	/* Index outside of the tree */
	if (offset >= RADIX_TREE_MAP_SIZE)
ffffffff812c4e96:	48 83 f8 3f          	cmp    $0x3f,%rax
ffffffff812c4e9a:	77 70                	ja     ffffffff812c4f0c <radix_tree_next_chunk+0x1bb>
ffffffff812c4e9c:	4c 89 df             	mov    %r11,%rdi
ffffffff812c4e9f:	45 89 ca             	mov    %r9d,%r10d
ffffffff812c4ea2:	eb 89                	jmp    ffffffff812c4e2d <radix_tree_next_chunk+0xdc>
		return NULL;

	node = rnode;
	while (1) {
		if ((flags & RADIX_TREE_ITER_TAGGED) ?
ffffffff812c4ea4:	48 83 7c c7 28 00    	cmpq   $0x0,0x28(%rdi,%rax,8)
ffffffff812c4eaa:	eb 90                	jmp    ffffffff812c4e3c <radix_tree_next_chunk+0xeb>

	if (offset < size) {
		unsigned long tmp;

		addr += offset / BITS_PER_LONG;
		tmp = *addr >> (offset % BITS_PER_LONG);
ffffffff812c4eac:	4e 8b 7c 2f 08       	mov    0x8(%rdi,%r13,1),%r15
ffffffff812c4eb1:	49 d3 ef             	shr    %cl,%r15
		if (tmp)
ffffffff812c4eb4:	4d 85 ff             	test   %r15,%r15
ffffffff812c4eb7:	74 a6                	je     ffffffff812c4e5f <radix_tree_next_chunk+0x10e>
 *
 * Undefined if no bit exists, so code should check against 0 first.
 */
static inline unsigned long __ffs(unsigned long word)
{
	asm("rep; bsf %1,%0"
ffffffff812c4eb9:	f3 49 0f bc c7       	tzcnt  %r15,%rax
			return __ffs(tmp) + offset;
ffffffff812c4ebe:	48 01 c8             	add    %rcx,%rax
ffffffff812c4ec1:	eb 9c                	jmp    ffffffff812c4e5f <radix_tree_next_chunk+0x10e>
						node->tags[tag],
						RADIX_TREE_MAP_SIZE,
						offset + 1);
			else
				while (++offset	< RADIX_TREE_MAP_SIZE) {
					if (node->slots[offset])
ffffffff812c4ec3:	48 83 7c c7 28 00    	cmpq   $0x0,0x28(%rdi,%rax,8)
ffffffff812c4ec9:	75 94                	jne    ffffffff812c4e5f <radix_tree_next_chunk+0x10e>
				offset = radix_tree_find_next_bit(
						node->tags[tag],
						RADIX_TREE_MAP_SIZE,
						offset + 1);
			else
				while (++offset	< RADIX_TREE_MAP_SIZE) {
ffffffff812c4ecb:	48 ff c0             	inc    %rax
ffffffff812c4ece:	48 83 f8 40          	cmp    $0x40,%rax
ffffffff812c4ed2:	75 ef                	jne    ffffffff812c4ec3 <radix_tree_next_chunk+0x172>
ffffffff812c4ed4:	eb 89                	jmp    ffffffff812c4e5f <radix_tree_next_chunk+0x10e>
		shift -= RADIX_TREE_MAP_SHIFT;
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
	}

	/* Update the iterator state */
	iter->index = index;
ffffffff812c4ed6:	4c 89 06             	mov    %r8,(%rsi)
	iter->next_index = (index | RADIX_TREE_MAP_MASK) + 1;
ffffffff812c4ed9:	49 83 c8 3f          	or     $0x3f,%r8
ffffffff812c4edd:	49 ff c0             	inc    %r8

	/* Construct iter->tags bit-mask from node->tags[tag] array */
	if (flags & RADIX_TREE_ITER_TAGGED) {
ffffffff812c4ee0:	45 85 e4             	test   %r12d,%r12d
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
	}

	/* Update the iterator state */
	iter->index = index;
	iter->next_index = (index | RADIX_TREE_MAP_MASK) + 1;
ffffffff812c4ee3:	4c 89 46 08          	mov    %r8,0x8(%rsi)

	/* Construct iter->tags bit-mask from node->tags[tag] array */
	if (flags & RADIX_TREE_ITER_TAGGED) {
ffffffff812c4ee7:	74 1c                	je     ffffffff812c4f05 <radix_tree_next_chunk+0x1b4>
		unsigned tag_long, tag_bit;

		tag_long = offset / BITS_PER_LONG;
		tag_bit  = offset % BITS_PER_LONG;
		iter->tags = node->tags[tag][tag_long] >> tag_bit;
ffffffff812c4ee9:	48 89 c2             	mov    %rax,%rdx
ffffffff812c4eec:	89 c1                	mov    %eax,%ecx
ffffffff812c4eee:	48 c1 ea 06          	shr    $0x6,%rdx
ffffffff812c4ef2:	89 d2                	mov    %edx,%edx
ffffffff812c4ef4:	48 8d 54 1a 44       	lea    0x44(%rdx,%rbx,1),%rdx
ffffffff812c4ef9:	48 8b 54 d7 08       	mov    0x8(%rdi,%rdx,8),%rdx
ffffffff812c4efe:	48 d3 ea             	shr    %cl,%rdx
ffffffff812c4f01:	48 89 56 10          	mov    %rdx,0x10(%rsi)
			/* Clip chunk size, here only BITS_PER_LONG tags */
			iter->next_index = index + BITS_PER_LONG;
		}
	}

	return node->slots + offset;
ffffffff812c4f05:	48 8d 44 c7 28       	lea    0x28(%rdi,%rax,8),%rax
ffffffff812c4f0a:	eb 02                	jmp    ffffffff812c4f0e <radix_tree_next_chunk+0x1bd>
	unsigned shift, tag = flags & RADIX_TREE_ITER_TAG_MASK;
	struct radix_tree_node *rnode, *node;
	unsigned long index, offset, height;

	if ((flags & RADIX_TREE_ITER_TAGGED) && !root_tag_get(root, tag))
		return NULL;
ffffffff812c4f0c:	31 c0                	xor    %eax,%eax
			iter->next_index = index + BITS_PER_LONG;
		}
	}

	return node->slots + offset;
}
ffffffff812c4f0e:	5b                   	pop    %rbx
ffffffff812c4f0f:	41 5c                	pop    %r12
ffffffff812c4f11:	41 5d                	pop    %r13
ffffffff812c4f13:	41 5e                	pop    %r14
ffffffff812c4f15:	41 5f                	pop    %r15
ffffffff812c4f17:	5d                   	pop    %rbp
ffffffff812c4f18:	c3                   	retq   

ffffffff812c4f19 <radix_tree_gang_lookup>:
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;
ffffffff812c4f19:	31 c0                	xor    %eax,%eax
{
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
ffffffff812c4f1b:	85 c9                	test   %ecx,%ecx
ffffffff812c4f1d:	0f 84 90 00 00 00    	je     ffffffff812c4fb3 <radix_tree_gang_lookup+0x9a>
 *	have been issued in individual locks, and results stored in 'results'.
 */
unsigned int
radix_tree_gang_lookup(struct radix_tree_root *root, void **results,
			unsigned long first_index, unsigned int max_items)
{
ffffffff812c4f23:	55                   	push   %rbp
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
ffffffff812c4f24:	31 c0                	xor    %eax,%eax
 *	have been issued in individual locks, and results stored in 'results'.
 */
unsigned int
radix_tree_gang_lookup(struct radix_tree_root *root, void **results,
			unsigned long first_index, unsigned int max_items)
{
ffffffff812c4f26:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4f29:	41 56                	push   %r14
ffffffff812c4f2b:	41 55                	push   %r13
ffffffff812c4f2d:	41 54                	push   %r12
ffffffff812c4f2f:	53                   	push   %rbx
ffffffff812c4f30:	49 89 f6             	mov    %rsi,%r14
ffffffff812c4f33:	89 cb                	mov    %ecx,%ebx
ffffffff812c4f35:	49 89 fd             	mov    %rdi,%r13
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;
ffffffff812c4f38:	45 31 e4             	xor    %r12d,%r12d
 *	have been issued in individual locks, and results stored in 'results'.
 */
unsigned int
radix_tree_gang_lookup(struct radix_tree_root *root, void **results,
			unsigned long first_index, unsigned int max_items)
{
ffffffff812c4f3b:	48 83 ec 20          	sub    $0x20,%rsp
	 * unsuccessful or non-tagged then nobody cares about ->tags.
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
	iter->index = 0;
ffffffff812c4f3f:	48 c7 45 c8 00 00 00 	movq   $0x0,-0x38(%rbp)
ffffffff812c4f46:	00 
	iter->next_index = start;
ffffffff812c4f47:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
ffffffff812c4f4b:	48 85 c0             	test   %rax,%rax
ffffffff812c4f4e:	74 3d                	je     ffffffff812c4f8d <radix_tree_gang_lookup+0x74>
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
ffffffff812c4f50:	48 8b 10             	mov    (%rax),%rdx
ffffffff812c4f53:	44 89 e1             	mov    %r12d,%ecx
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c4f56:	48 83 e2 fe          	and    $0xfffffffffffffffe,%rdx
	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
		if (!results[ret])
ffffffff812c4f5a:	48 85 d2             	test   %rdx,%rdx

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
ffffffff812c4f5d:	49 89 14 ce          	mov    %rdx,(%r14,%rcx,8)
		if (!results[ret])
ffffffff812c4f61:	74 08                	je     ffffffff812c4f6b <radix_tree_gang_lookup+0x52>
			continue;
		if (++ret == max_items)
ffffffff812c4f63:	41 ff c4             	inc    %r12d
ffffffff812c4f66:	44 39 e3             	cmp    %r12d,%ebx
ffffffff812c4f69:	74 3a                	je     ffffffff812c4fa5 <radix_tree_gang_lookup+0x8c>
ffffffff812c4f6b:	8b 75 d0             	mov    -0x30(%rbp),%esi
ffffffff812c4f6e:	8d 56 ff             	lea    -0x1(%rsi),%edx
ffffffff812c4f71:	2b 55 c8             	sub    -0x38(%rbp),%edx
ffffffff812c4f74:	48 8d 14 d0          	lea    (%rax,%rdx,8),%rdx
			return slot + offset + 1;
		}
	} else {
		unsigned size = radix_tree_chunk_size(iter) - 1;

		while (size--) {
ffffffff812c4f78:	48 39 d0             	cmp    %rdx,%rax
ffffffff812c4f7b:	74 10                	je     ffffffff812c4f8d <radix_tree_gang_lookup+0x74>
			slot++;
ffffffff812c4f7d:	48 83 c0 08          	add    $0x8,%rax
			iter->index++;
ffffffff812c4f81:	48 ff 45 c8          	incq   -0x38(%rbp)
			if (likely(*slot))
ffffffff812c4f85:	48 83 38 00          	cmpq   $0x0,(%rax)
ffffffff812c4f89:	75 c0                	jne    ffffffff812c4f4b <radix_tree_gang_lookup+0x32>
ffffffff812c4f8b:	eb eb                	jmp    ffffffff812c4f78 <radix_tree_gang_lookup+0x5f>
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
ffffffff812c4f8d:	48 8d 75 c8          	lea    -0x38(%rbp),%rsi
ffffffff812c4f91:	31 d2                	xor    %edx,%edx
ffffffff812c4f93:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c4f96:	e8 b6 fd ff ff       	callq  ffffffff812c4d51 <radix_tree_next_chunk>
ffffffff812c4f9b:	48 85 c0             	test   %rax,%rax
ffffffff812c4f9e:	75 b0                	jne    ffffffff812c4f50 <radix_tree_gang_lookup+0x37>
ffffffff812c4fa0:	44 89 e0             	mov    %r12d,%eax
ffffffff812c4fa3:	eb 02                	jmp    ffffffff812c4fa7 <radix_tree_gang_lookup+0x8e>
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
		if (!results[ret])
			continue;
		if (++ret == max_items)
ffffffff812c4fa5:	89 d8                	mov    %ebx,%eax
			break;
	}

	return ret;
}
ffffffff812c4fa7:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812c4fab:	5b                   	pop    %rbx
ffffffff812c4fac:	41 5c                	pop    %r12
ffffffff812c4fae:	41 5d                	pop    %r13
ffffffff812c4fb0:	41 5e                	pop    %r14
ffffffff812c4fb2:	5d                   	pop    %rbp
ffffffff812c4fb3:	c3                   	retq   

ffffffff812c4fb4 <radix_tree_gang_lookup_slot>:
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;
ffffffff812c4fb4:	31 c0                	xor    %eax,%eax
{
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
ffffffff812c4fb6:	45 85 c0             	test   %r8d,%r8d
ffffffff812c4fb9:	0f 84 a0 00 00 00    	je     ffffffff812c505f <radix_tree_gang_lookup_slot+0xab>
 */
unsigned int
radix_tree_gang_lookup_slot(struct radix_tree_root *root,
			void ***results, unsigned long *indices,
			unsigned long first_index, unsigned int max_items)
{
ffffffff812c4fbf:	55                   	push   %rbp
ffffffff812c4fc0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c4fc3:	41 57                	push   %r15
ffffffff812c4fc5:	41 56                	push   %r14
ffffffff812c4fc7:	41 55                	push   %r13
ffffffff812c4fc9:	41 54                	push   %r12
ffffffff812c4fcb:	45 8d 68 ff          	lea    -0x1(%r8),%r13d
ffffffff812c4fcf:	53                   	push   %rbx
ffffffff812c4fd0:	49 89 d7             	mov    %rdx,%r15
ffffffff812c4fd3:	49 89 f4             	mov    %rsi,%r12
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
ffffffff812c4fd6:	31 db                	xor    %ebx,%ebx
 */
unsigned int
radix_tree_gang_lookup_slot(struct radix_tree_root *root,
			void ***results, unsigned long *indices,
			unsigned long first_index, unsigned int max_items)
{
ffffffff812c4fd8:	48 83 ec 28          	sub    $0x28,%rsp
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
	iter->index = 0;
	iter->next_index = start;
ffffffff812c4fdc:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
ffffffff812c4fe0:	48 89 7d b0          	mov    %rdi,-0x50(%rbp)
	 * unsuccessful or non-tagged then nobody cares about ->tags.
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
	iter->index = 0;
ffffffff812c4fe4:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
ffffffff812c4feb:	00 
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
ffffffff812c4fec:	31 c9                	xor    %ecx,%ecx
ffffffff812c4fee:	48 85 c9             	test   %rcx,%rcx
ffffffff812c4ff1:	41 89 de             	mov    %ebx,%r14d
ffffffff812c4ff4:	74 42                	je     ffffffff812c5038 <radix_tree_gang_lookup_slot+0x84>
		results[ret] = slot;
		if (indices)
ffffffff812c4ff6:	4d 85 ff             	test   %r15,%r15

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
		results[ret] = slot;
ffffffff812c4ff9:	49 89 0c dc          	mov    %rcx,(%r12,%rbx,8)
		if (indices)
ffffffff812c4ffd:	74 08                	je     ffffffff812c5007 <radix_tree_gang_lookup_slot+0x53>
			indices[ret] = iter.index;
ffffffff812c4fff:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
ffffffff812c5003:	49 89 04 df          	mov    %rax,(%r15,%rbx,8)
		if (++ret == max_items)
ffffffff812c5007:	4c 39 eb             	cmp    %r13,%rbx
ffffffff812c500a:	41 8d 46 01          	lea    0x1(%r14),%eax
ffffffff812c500e:	74 41                	je     ffffffff812c5051 <radix_tree_gang_lookup_slot+0x9d>
ffffffff812c5010:	8b 45 c8             	mov    -0x38(%rbp),%eax
ffffffff812c5013:	ff c8                	dec    %eax
ffffffff812c5015:	2b 45 c0             	sub    -0x40(%rbp),%eax
ffffffff812c5018:	48 8d 04 c1          	lea    (%rcx,%rax,8),%rax
			return slot + offset + 1;
		}
	} else {
		unsigned size = radix_tree_chunk_size(iter) - 1;

		while (size--) {
ffffffff812c501c:	48 39 c1             	cmp    %rax,%rcx
ffffffff812c501f:	74 10                	je     ffffffff812c5031 <radix_tree_gang_lookup_slot+0x7d>
			slot++;
ffffffff812c5021:	48 83 c1 08          	add    $0x8,%rcx
			iter->index++;
ffffffff812c5025:	48 ff 45 c0          	incq   -0x40(%rbp)
			if (likely(*slot))
ffffffff812c5029:	48 83 39 00          	cmpq   $0x0,(%rcx)
ffffffff812c502d:	75 04                	jne    ffffffff812c5033 <radix_tree_gang_lookup_slot+0x7f>
ffffffff812c502f:	eb eb                	jmp    ffffffff812c501c <radix_tree_gang_lookup_slot+0x68>
				iter->next_index = 0;
				break;
			}
		}
	}
	return NULL;
ffffffff812c5031:	31 c9                	xor    %ecx,%ecx
ffffffff812c5033:	48 ff c3             	inc    %rbx
ffffffff812c5036:	eb b6                	jmp    ffffffff812c4fee <radix_tree_gang_lookup_slot+0x3a>
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_slot(slot, root, &iter, first_index) {
ffffffff812c5038:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
ffffffff812c503c:	48 8d 75 c0          	lea    -0x40(%rbp),%rsi
ffffffff812c5040:	31 d2                	xor    %edx,%edx
ffffffff812c5042:	e8 0a fd ff ff       	callq  ffffffff812c4d51 <radix_tree_next_chunk>
ffffffff812c5047:	48 85 c0             	test   %rax,%rax
ffffffff812c504a:	48 89 c1             	mov    %rax,%rcx
ffffffff812c504d:	75 a7                	jne    ffffffff812c4ff6 <radix_tree_gang_lookup_slot+0x42>
ffffffff812c504f:	89 d8                	mov    %ebx,%eax
		if (++ret == max_items)
			break;
	}

	return ret;
}
ffffffff812c5051:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c5055:	5b                   	pop    %rbx
ffffffff812c5056:	41 5c                	pop    %r12
ffffffff812c5058:	41 5d                	pop    %r13
ffffffff812c505a:	41 5e                	pop    %r14
ffffffff812c505c:	41 5f                	pop    %r15
ffffffff812c505e:	5d                   	pop    %rbp
ffffffff812c505f:	c3                   	retq   

ffffffff812c5060 <radix_tree_gang_lookup_tag>:
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;
ffffffff812c5060:	31 c0                	xor    %eax,%eax
{
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
ffffffff812c5062:	85 c9                	test   %ecx,%ecx
ffffffff812c5064:	0f 84 b6 00 00 00    	je     ffffffff812c5120 <radix_tree_gang_lookup_tag+0xc0>
 */
unsigned int
radix_tree_gang_lookup_tag(struct radix_tree_root *root, void **results,
		unsigned long first_index, unsigned int max_items,
		unsigned int tag)
{
ffffffff812c506a:	55                   	push   %rbp
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
ffffffff812c506b:	31 c0                	xor    %eax,%eax
 */
unsigned int
radix_tree_gang_lookup_tag(struct radix_tree_root *root, void **results,
		unsigned long first_index, unsigned int max_items,
		unsigned int tag)
{
ffffffff812c506d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5070:	41 57                	push   %r15
ffffffff812c5072:	41 56                	push   %r14
ffffffff812c5074:	41 55                	push   %r13
ffffffff812c5076:	41 54                	push   %r12
ffffffff812c5078:	41 89 cd             	mov    %ecx,%r13d
ffffffff812c507b:	53                   	push   %rbx
ffffffff812c507c:	45 89 c4             	mov    %r8d,%r12d
ffffffff812c507f:	49 89 f7             	mov    %rsi,%r15
ffffffff812c5082:	48 89 fb             	mov    %rdi,%rbx
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;
ffffffff812c5085:	45 31 f6             	xor    %r14d,%r14d
 */
unsigned int
radix_tree_gang_lookup_tag(struct radix_tree_root *root, void **results,
		unsigned long first_index, unsigned int max_items,
		unsigned int tag)
{
ffffffff812c5088:	48 83 ec 20          	sub    $0x20,%rsp
	 * unsuccessful or non-tagged then nobody cares about ->tags.
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
	iter->index = 0;
ffffffff812c508c:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
ffffffff812c5093:	00 
	iter->next_index = start;
ffffffff812c5094:	48 89 55 c8          	mov    %rdx,-0x38(%rbp)
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
ffffffff812c5098:	48 85 c0             	test   %rax,%rax
ffffffff812c509b:	74 56                	je     ffffffff812c50f3 <radix_tree_gang_lookup_tag+0x93>
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
ffffffff812c509d:	48 8b 10             	mov    (%rax),%rdx
ffffffff812c50a0:	44 89 f1             	mov    %r14d,%ecx
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c50a3:	48 83 e2 fe          	and    $0xfffffffffffffffe,%rdx
	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
		if (!results[ret])
ffffffff812c50a7:	48 85 d2             	test   %rdx,%rdx

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
ffffffff812c50aa:	49 89 14 cf          	mov    %rdx,(%r15,%rcx,8)
		if (!results[ret])
ffffffff812c50ae:	74 08                	je     ffffffff812c50b8 <radix_tree_gang_lookup_tag+0x58>
			continue;
		if (++ret == max_items)
ffffffff812c50b0:	41 ff c6             	inc    %r14d
ffffffff812c50b3:	45 39 f5             	cmp    %r14d,%r13d
ffffffff812c50b6:	74 57                	je     ffffffff812c510f <radix_tree_gang_lookup_tag+0xaf>
 */
static __always_inline void **
radix_tree_next_slot(void **slot, struct radix_tree_iter *iter, unsigned flags)
{
	if (flags & RADIX_TREE_ITER_TAGGED) {
		iter->tags >>= 1;
ffffffff812c50b8:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
ffffffff812c50bc:	48 d1 ea             	shr    %rdx
		if (likely(iter->tags & 1ul)) {
ffffffff812c50bf:	f6 c2 01             	test   $0x1,%dl
 */
static __always_inline void **
radix_tree_next_slot(void **slot, struct radix_tree_iter *iter, unsigned flags)
{
	if (flags & RADIX_TREE_ITER_TAGGED) {
		iter->tags >>= 1;
ffffffff812c50c2:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
		if (likely(iter->tags & 1ul)) {
ffffffff812c50c6:	74 0a                	je     ffffffff812c50d2 <radix_tree_gang_lookup_tag+0x72>
			iter->index++;
ffffffff812c50c8:	48 ff 45 c0          	incq   -0x40(%rbp)
			return slot + 1;
ffffffff812c50cc:	48 83 c0 08          	add    $0x8,%rax
ffffffff812c50d0:	eb c6                	jmp    ffffffff812c5098 <radix_tree_gang_lookup_tag+0x38>
		}
		if (!(flags & RADIX_TREE_ITER_CONTIG) && likely(iter->tags)) {
ffffffff812c50d2:	48 85 d2             	test   %rdx,%rdx
ffffffff812c50d5:	74 1c                	je     ffffffff812c50f3 <radix_tree_gang_lookup_tag+0x93>
ffffffff812c50d7:	f3 48 0f bc ca       	tzcnt  %rdx,%rcx
			unsigned offset = __ffs(iter->tags);

			iter->tags >>= offset;
ffffffff812c50dc:	48 d3 ea             	shr    %cl,%rdx
ffffffff812c50df:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
			iter->index += offset + 1;
ffffffff812c50e3:	8d 51 01             	lea    0x1(%rcx),%edx
			return slot + offset + 1;
ffffffff812c50e6:	89 c9                	mov    %ecx,%ecx
		}
		if (!(flags & RADIX_TREE_ITER_CONTIG) && likely(iter->tags)) {
			unsigned offset = __ffs(iter->tags);

			iter->tags >>= offset;
			iter->index += offset + 1;
ffffffff812c50e8:	48 01 55 c0          	add    %rdx,-0x40(%rbp)
			return slot + offset + 1;
ffffffff812c50ec:	48 8d 44 c8 08       	lea    0x8(%rax,%rcx,8),%rax
ffffffff812c50f1:	eb a5                	jmp    ffffffff812c5098 <radix_tree_gang_lookup_tag+0x38>
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
ffffffff812c50f3:	44 89 e2             	mov    %r12d,%edx
ffffffff812c50f6:	48 8d 75 c0          	lea    -0x40(%rbp),%rsi
ffffffff812c50fa:	48 89 df             	mov    %rbx,%rdi
ffffffff812c50fd:	80 ce 01             	or     $0x1,%dh
ffffffff812c5100:	e8 4c fc ff ff       	callq  ffffffff812c4d51 <radix_tree_next_chunk>
ffffffff812c5105:	48 85 c0             	test   %rax,%rax
ffffffff812c5108:	75 93                	jne    ffffffff812c509d <radix_tree_gang_lookup_tag+0x3d>
ffffffff812c510a:	44 89 f0             	mov    %r14d,%eax
ffffffff812c510d:	eb 03                	jmp    ffffffff812c5112 <radix_tree_gang_lookup_tag+0xb2>
		results[ret] = indirect_to_ptr(rcu_dereference_raw(*slot));
		if (!results[ret])
			continue;
		if (++ret == max_items)
ffffffff812c510f:	44 89 e8             	mov    %r13d,%eax
			break;
	}

	return ret;
}
ffffffff812c5112:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812c5116:	5b                   	pop    %rbx
ffffffff812c5117:	41 5c                	pop    %r12
ffffffff812c5119:	41 5d                	pop    %r13
ffffffff812c511b:	41 5e                	pop    %r14
ffffffff812c511d:	41 5f                	pop    %r15
ffffffff812c511f:	5d                   	pop    %rbp
ffffffff812c5120:	c3                   	retq   

ffffffff812c5121 <radix_tree_gang_lookup_tag_slot>:
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;
ffffffff812c5121:	31 c0                	xor    %eax,%eax
{
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;

	if (unlikely(!max_items))
ffffffff812c5123:	85 c9                	test   %ecx,%ecx
ffffffff812c5125:	0f 84 af 00 00 00    	je     ffffffff812c51da <radix_tree_gang_lookup_tag_slot+0xb9>
 */
unsigned int
radix_tree_gang_lookup_tag_slot(struct radix_tree_root *root, void ***results,
		unsigned long first_index, unsigned int max_items,
		unsigned int tag)
{
ffffffff812c512b:	55                   	push   %rbp
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
ffffffff812c512c:	31 c0                	xor    %eax,%eax
 */
unsigned int
radix_tree_gang_lookup_tag_slot(struct radix_tree_root *root, void ***results,
		unsigned long first_index, unsigned int max_items,
		unsigned int tag)
{
ffffffff812c512e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5131:	41 57                	push   %r15
ffffffff812c5133:	41 56                	push   %r14
ffffffff812c5135:	41 55                	push   %r13
ffffffff812c5137:	41 54                	push   %r12
ffffffff812c5139:	41 89 ce             	mov    %ecx,%r14d
ffffffff812c513c:	53                   	push   %rbx
ffffffff812c513d:	45 89 c4             	mov    %r8d,%r12d
ffffffff812c5140:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c5143:	49 89 f5             	mov    %rsi,%r13
	struct radix_tree_iter iter;
	void **slot;
	unsigned int ret = 0;
ffffffff812c5146:	45 31 ff             	xor    %r15d,%r15d
 */
unsigned int
radix_tree_gang_lookup_tag_slot(struct radix_tree_root *root, void ***results,
		unsigned long first_index, unsigned int max_items,
		unsigned int tag)
{
ffffffff812c5149:	48 83 ec 20          	sub    $0x20,%rsp
	 * unsuccessful or non-tagged then nobody cares about ->tags.
	 *
	 * Set index to zero to bypass next_index overflow protection.
	 * See the comment in radix_tree_next_chunk() for details.
	 */
	iter->index = 0;
ffffffff812c514d:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
ffffffff812c5154:	00 
	iter->next_index = start;
ffffffff812c5155:	48 89 55 c8          	mov    %rdx,-0x38(%rbp)
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
ffffffff812c5159:	48 85 c0             	test   %rax,%rax
ffffffff812c515c:	74 4f                	je     ffffffff812c51ad <radix_tree_gang_lookup_tag_slot+0x8c>
		results[ret] = slot;
		if (++ret == max_items)
ffffffff812c515e:	41 ff c7             	inc    %r15d

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
		results[ret] = slot;
ffffffff812c5161:	49 89 45 00          	mov    %rax,0x0(%r13)
		if (++ret == max_items)
ffffffff812c5165:	45 39 fe             	cmp    %r15d,%r14d
ffffffff812c5168:	74 5f                	je     ffffffff812c51c9 <radix_tree_gang_lookup_tag_slot+0xa8>
 */
static __always_inline void **
radix_tree_next_slot(void **slot, struct radix_tree_iter *iter, unsigned flags)
{
	if (flags & RADIX_TREE_ITER_TAGGED) {
		iter->tags >>= 1;
ffffffff812c516a:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
ffffffff812c516e:	48 d1 ea             	shr    %rdx
		if (likely(iter->tags & 1ul)) {
ffffffff812c5171:	f6 c2 01             	test   $0x1,%dl
 */
static __always_inline void **
radix_tree_next_slot(void **slot, struct radix_tree_iter *iter, unsigned flags)
{
	if (flags & RADIX_TREE_ITER_TAGGED) {
		iter->tags >>= 1;
ffffffff812c5174:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
		if (likely(iter->tags & 1ul)) {
ffffffff812c5178:	74 0a                	je     ffffffff812c5184 <radix_tree_gang_lookup_tag_slot+0x63>
			iter->index++;
ffffffff812c517a:	48 ff 45 c0          	incq   -0x40(%rbp)
			return slot + 1;
ffffffff812c517e:	48 83 c0 08          	add    $0x8,%rax
ffffffff812c5182:	eb 23                	jmp    ffffffff812c51a7 <radix_tree_gang_lookup_tag_slot+0x86>
		}
		if (!(flags & RADIX_TREE_ITER_CONTIG) && likely(iter->tags)) {
ffffffff812c5184:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5187:	74 1c                	je     ffffffff812c51a5 <radix_tree_gang_lookup_tag_slot+0x84>
ffffffff812c5189:	f3 48 0f bc ca       	tzcnt  %rdx,%rcx
			unsigned offset = __ffs(iter->tags);

			iter->tags >>= offset;
ffffffff812c518e:	48 d3 ea             	shr    %cl,%rdx
ffffffff812c5191:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
			iter->index += offset + 1;
ffffffff812c5195:	8d 51 01             	lea    0x1(%rcx),%edx
			return slot + offset + 1;
ffffffff812c5198:	89 c9                	mov    %ecx,%ecx
		}
		if (!(flags & RADIX_TREE_ITER_CONTIG) && likely(iter->tags)) {
			unsigned offset = __ffs(iter->tags);

			iter->tags >>= offset;
			iter->index += offset + 1;
ffffffff812c519a:	48 01 55 c0          	add    %rdx,-0x40(%rbp)
			return slot + offset + 1;
ffffffff812c519e:	48 8d 44 c8 08       	lea    0x8(%rax,%rcx,8),%rax
ffffffff812c51a3:	eb 02                	jmp    ffffffff812c51a7 <radix_tree_gang_lookup_tag_slot+0x86>
				iter->next_index = 0;
				break;
			}
		}
	}
	return NULL;
ffffffff812c51a5:	31 c0                	xor    %eax,%eax
ffffffff812c51a7:	49 83 c5 08          	add    $0x8,%r13
ffffffff812c51ab:	eb ac                	jmp    ffffffff812c5159 <radix_tree_gang_lookup_tag_slot+0x38>
	unsigned int ret = 0;

	if (unlikely(!max_items))
		return 0;

	radix_tree_for_each_tagged(slot, root, &iter, first_index, tag) {
ffffffff812c51ad:	44 89 e2             	mov    %r12d,%edx
ffffffff812c51b0:	48 8d 75 c0          	lea    -0x40(%rbp),%rsi
ffffffff812c51b4:	48 89 df             	mov    %rbx,%rdi
ffffffff812c51b7:	80 ce 01             	or     $0x1,%dh
ffffffff812c51ba:	e8 92 fb ff ff       	callq  ffffffff812c4d51 <radix_tree_next_chunk>
ffffffff812c51bf:	48 85 c0             	test   %rax,%rax
ffffffff812c51c2:	75 9a                	jne    ffffffff812c515e <radix_tree_gang_lookup_tag_slot+0x3d>
ffffffff812c51c4:	44 89 f8             	mov    %r15d,%eax
ffffffff812c51c7:	eb 03                	jmp    ffffffff812c51cc <radix_tree_gang_lookup_tag_slot+0xab>
ffffffff812c51c9:	44 89 f0             	mov    %r14d,%eax
		if (++ret == max_items)
			break;
	}

	return ret;
}
ffffffff812c51cc:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812c51d0:	5b                   	pop    %rbx
ffffffff812c51d1:	41 5c                	pop    %r12
ffffffff812c51d3:	41 5d                	pop    %r13
ffffffff812c51d5:	41 5e                	pop    %r14
ffffffff812c51d7:	41 5f                	pop    %r15
ffffffff812c51d9:	5d                   	pop    %rbp
ffffffff812c51da:	c3                   	retq   

ffffffff812c51db <radix_tree_node_ctor>:
}
EXPORT_SYMBOL(radix_tree_tagged);

static void
radix_tree_node_ctor(void *arg)
{
ffffffff812c51db:	48 89 fa             	mov    %rdi,%rdx
	struct radix_tree_node *node = arg;

	memset(node, 0, sizeof(*node));
ffffffff812c51de:	31 c0                	xor    %eax,%eax
ffffffff812c51e0:	b9 90 00 00 00       	mov    $0x90,%ecx
ffffffff812c51e5:	f3 ab                	rep stos %eax,%es:(%rdi)
}
EXPORT_SYMBOL(radix_tree_tagged);

static void
radix_tree_node_ctor(void *arg)
{
ffffffff812c51e7:	55                   	push   %rbp
	struct radix_tree_node *node = arg;

	memset(node, 0, sizeof(*node));
	INIT_LIST_HEAD(&node->private_list);
ffffffff812c51e8:	48 8d 42 18          	lea    0x18(%rdx),%rax
}
EXPORT_SYMBOL(radix_tree_tagged);

static void
radix_tree_node_ctor(void *arg)
{
ffffffff812c51ec:	48 89 e5             	mov    %rsp,%rbp
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
ffffffff812c51ef:	48 89 42 18          	mov    %rax,0x18(%rdx)
	list->prev = list;
ffffffff812c51f3:	48 89 42 20          	mov    %rax,0x20(%rdx)
	struct radix_tree_node *node = arg;

	memset(node, 0, sizeof(*node));
	INIT_LIST_HEAD(&node->private_list);
}
ffffffff812c51f7:	5d                   	pop    %rbp
ffffffff812c51f8:	c3                   	retq   

ffffffff812c51f9 <radix_tree_tag_clear>:
 *	Returns the address of the tagged item on success, else NULL.  ie:
 *	has the same return value and semantics as radix_tree_lookup().
 */
void *radix_tree_tag_clear(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c51f9:	55                   	push   %rbp
ffffffff812c51fa:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c51fd:	41 57                	push   %r15
ffffffff812c51ff:	41 56                	push   %r14
ffffffff812c5201:	41 55                	push   %r13
ffffffff812c5203:	41 54                	push   %r12
	struct radix_tree_node *node = NULL;
	struct radix_tree_node *slot = NULL;
ffffffff812c5205:	45 31 ff             	xor    %r15d,%r15d
 *	Returns the address of the tagged item on success, else NULL.  ie:
 *	has the same return value and semantics as radix_tree_lookup().
 */
void *radix_tree_tag_clear(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c5208:	53                   	push   %rbx
ffffffff812c5209:	48 83 ec 18          	sub    $0x18,%rsp
	struct radix_tree_node *node = NULL;
	struct radix_tree_node *slot = NULL;
	unsigned int height, shift;
	int uninitialized_var(offset);

	height = root->height;
ffffffff812c520d:	8b 07                	mov    (%rdi),%eax
 *	Returns the address of the tagged item on success, else NULL.  ie:
 *	has the same return value and semantics as radix_tree_lookup().
 */
void *radix_tree_tag_clear(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c520f:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)
	struct radix_tree_node *slot = NULL;
	unsigned int height, shift;
	int uninitialized_var(offset);

	height = root->height;
	if (index > radix_tree_maxindex(height))
ffffffff812c5213:	48 3b 34 c5 a0 97 a5 	cmp    -0x7e5a6860(,%rax,8),%rsi
ffffffff812c521a:	81 
ffffffff812c521b:	0f 87 c1 00 00 00    	ja     ffffffff812c52e2 <radix_tree_tag_clear+0xe9>
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5221:	4c 8b 47 08          	mov    0x8(%rdi),%r8
ffffffff812c5225:	48 89 f3             	mov    %rsi,%rbx
ffffffff812c5228:	41 89 d4             	mov    %edx,%r12d

	height = root->height;
	if (index > radix_tree_maxindex(height))
		goto out;

	shift = height * RADIX_TREE_MAP_SHIFT;
ffffffff812c522b:	6b c8 06             	imul   $0x6,%eax,%ecx
 *	has the same return value and semantics as radix_tree_lookup().
 */
void *radix_tree_tag_clear(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
	struct radix_tree_node *node = NULL;
ffffffff812c522e:	45 31 f6             	xor    %r14d,%r14d
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5231:	49 83 e0 fe          	and    $0xfffffffffffffffe,%r8
ffffffff812c5235:	4d 89 c7             	mov    %r8,%r15
		goto out;

	shift = height * RADIX_TREE_MAP_SHIFT;
	slot = indirect_to_ptr(root->rnode);

	while (shift) {
ffffffff812c5238:	85 c9                	test   %ecx,%ecx
ffffffff812c523a:	74 27                	je     ffffffff812c5263 <radix_tree_tag_clear+0x6a>
		if (slot == NULL)
ffffffff812c523c:	4d 85 ff             	test   %r15,%r15
ffffffff812c523f:	0f 84 9d 00 00 00    	je     ffffffff812c52e2 <radix_tree_tag_clear+0xe9>
			goto out;

		shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c5245:	83 e9 06             	sub    $0x6,%ecx
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5248:	49 89 d9             	mov    %rbx,%r9
		node = slot;
		slot = slot->slots[offset];
ffffffff812c524b:	4d 89 fe             	mov    %r15,%r14
	while (shift) {
		if (slot == NULL)
			goto out;

		shift -= RADIX_TREE_MAP_SHIFT;
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c524e:	49 d3 e9             	shr    %cl,%r9
ffffffff812c5251:	44 89 c8             	mov    %r9d,%eax
ffffffff812c5254:	83 e0 3f             	and    $0x3f,%eax
ffffffff812c5257:	89 45 d4             	mov    %eax,-0x2c(%rbp)
		node = slot;
		slot = slot->slots[offset];
ffffffff812c525a:	48 98                	cltq   
ffffffff812c525c:	4d 8b 7c c7 28       	mov    0x28(%r15,%rax,8),%r15
ffffffff812c5261:	eb d5                	jmp    ffffffff812c5238 <radix_tree_tag_clear+0x3f>
	}

	if (slot == NULL)
ffffffff812c5263:	4d 85 ff             	test   %r15,%r15
ffffffff812c5266:	74 7a                	je     ffffffff812c52e2 <radix_tree_tag_clear+0xe9>
}

static inline void tag_clear(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__clear_bit(offset, node->tags[tag]);
ffffffff812c5268:	44 89 e1             	mov    %r12d,%ecx
ffffffff812c526b:	48 8d 41 44          	lea    0x44(%rcx),%rax
ffffffff812c526f:	4c 8d 2c c5 00 00 00 	lea    0x0(,%rax,8),%r13
ffffffff812c5276:	00 
ffffffff812c5277:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
	}

	if (slot == NULL)
		goto out;

	while (node) {
ffffffff812c527b:	4d 85 f6             	test   %r14,%r14
ffffffff812c527e:	74 40                	je     ffffffff812c52c0 <radix_tree_tag_clear+0xc7>
		if (!tag_get(node, tag, offset))
ffffffff812c5280:	8b 55 d4             	mov    -0x2c(%rbp),%edx
ffffffff812c5283:	44 89 e6             	mov    %r12d,%esi
ffffffff812c5286:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c5289:	e8 ad fa ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c528e:	85 c0                	test   %eax,%eax
ffffffff812c5290:	74 50                	je     ffffffff812c52e2 <radix_tree_tag_clear+0xe9>
}

static inline void tag_clear(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__clear_bit(offset, node->tags[tag]);
ffffffff812c5292:	4b 8d 04 2e          	lea    (%r14,%r13,1),%rax
ffffffff812c5296:	4c 63 4d d4          	movslq -0x2c(%rbp),%r9
	clear_bit(nr, addr);
}

static inline void __clear_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("btr %1,%0" : ADDR : "Ir" (nr));
ffffffff812c529a:	4c 0f b3 48 08       	btr    %r9,0x8(%rax)
 */
static inline int any_tag_set(struct radix_tree_node *node, unsigned int tag)
{
	int idx;
	for (idx = 0; idx < RADIX_TREE_TAG_LONGS; idx++) {
		if (node->tags[tag][idx])
ffffffff812c529f:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
ffffffff812c52a3:	49 83 7c c6 08 00    	cmpq   $0x0,0x8(%r14,%rax,8)
ffffffff812c52a9:	75 37                	jne    ffffffff812c52e2 <radix_tree_tag_clear+0xe9>
			goto out;
		tag_clear(node, tag, offset);
		if (any_tag_set(node, tag))
			goto out;

		index >>= RADIX_TREE_MAP_SHIFT;
ffffffff812c52ab:	48 c1 eb 06          	shr    $0x6,%rbx
		offset = index & RADIX_TREE_MAP_MASK;
		node = node->parent;
ffffffff812c52af:	4d 8b 76 08          	mov    0x8(%r14),%r14
		tag_clear(node, tag, offset);
		if (any_tag_set(node, tag))
			goto out;

		index >>= RADIX_TREE_MAP_SHIFT;
		offset = index & RADIX_TREE_MAP_MASK;
ffffffff812c52b3:	41 89 d9             	mov    %ebx,%r9d
ffffffff812c52b6:	41 83 e1 3f          	and    $0x3f,%r9d
ffffffff812c52ba:	44 89 4d d4          	mov    %r9d,-0x2c(%rbp)
ffffffff812c52be:	eb bb                	jmp    ffffffff812c527b <radix_tree_tag_clear+0x82>
ffffffff812c52c0:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
	root->gfp_mask &= __GFP_BITS_MASK;
}

static inline int root_tag_get(struct radix_tree_root *root, unsigned int tag)
{
	return (__force unsigned)root->gfp_mask & (1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c52c4:	41 8d 4c 24 19       	lea    0x19(%r12),%ecx
ffffffff812c52c9:	8b 50 04             	mov    0x4(%rax),%edx
ffffffff812c52cc:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c52d1:	d3 e0                	shl    %cl,%eax
		offset = index & RADIX_TREE_MAP_MASK;
		node = node->parent;
	}

	/* clear the root's tag bit */
	if (root_tag_get(root, tag))
ffffffff812c52d3:	85 d0                	test   %edx,%eax
ffffffff812c52d5:	74 0b                	je     ffffffff812c52e2 <radix_tree_tag_clear+0xe9>
	root->gfp_mask |= (__force gfp_t)(1 << (tag + __GFP_BITS_SHIFT));
}

static inline void root_tag_clear(struct radix_tree_root *root, unsigned int tag)
{
	root->gfp_mask &= (__force gfp_t)~(1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c52d7:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
ffffffff812c52db:	f7 d0                	not    %eax
ffffffff812c52dd:	21 d0                	and    %edx,%eax
ffffffff812c52df:	89 46 04             	mov    %eax,0x4(%rsi)
	if (root_tag_get(root, tag))
		root_tag_clear(root, tag);

out:
	return slot;
}
ffffffff812c52e2:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c52e6:	4c 89 f8             	mov    %r15,%rax
ffffffff812c52e9:	5b                   	pop    %rbx
ffffffff812c52ea:	41 5c                	pop    %r12
ffffffff812c52ec:	41 5d                	pop    %r13
ffffffff812c52ee:	41 5e                	pop    %r14
ffffffff812c52f0:	41 5f                	pop    %r15
ffffffff812c52f2:	5d                   	pop    %rbp
ffffffff812c52f3:	c3                   	retq   

ffffffff812c52f4 <radix_tree_node_alloc>:
 * This assumes that the caller has performed appropriate preallocation, and
 * that the caller has pinned this thread of control to the current CPU.
 */
static struct radix_tree_node *
radix_tree_node_alloc(struct radix_tree_root *root)
{
ffffffff812c52f4:	55                   	push   %rbp
ffffffff812c52f5:	8b 77 04             	mov    0x4(%rdi),%esi
ffffffff812c52f8:	48 89 e5             	mov    %rsp,%rbp
	/*
	 * Preload code isn't irq safe and it doesn't make sence to use
	 * preloading in the interrupt anyway as all the allocations have to
	 * be atomic. So just do normal allocation when in interrupt.
	 */
	if (!(gfp_mask & __GFP_WAIT) && !in_interrupt()) {
ffffffff812c52fb:	40 f6 c6 10          	test   $0x10,%sil
ffffffff812c52ff:	74 14                	je     ffffffff812c5315 <radix_tree_node_alloc+0x21>
		 * for debugging.
		 */
		kmemleak_update_trace(ret);
	}
	if (ret == NULL)
		ret = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);
ffffffff812c5301:	48 8b 3d a0 c5 8e 00 	mov    0x8ec5a0(%rip),%rdi        # ffffffff81bb18a8 <radix_tree_node_cachep>
ffffffff812c5308:	81 e6 ff ff ff 01    	and    $0x1ffffff,%esi
ffffffff812c530e:	e8 be bd e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
ffffffff812c5313:	eb 3d                	jmp    ffffffff812c5352 <radix_tree_node_alloc+0x5e>
ffffffff812c5315:	65 8b 05 7c 56 d4 7e 	mov    %gs:0x7ed4567c(%rip),%eax        # a998 <__preempt_count>
	/*
	 * Preload code isn't irq safe and it doesn't make sence to use
	 * preloading in the interrupt anyway as all the allocations have to
	 * be atomic. So just do normal allocation when in interrupt.
	 */
	if (!(gfp_mask & __GFP_WAIT) && !in_interrupt()) {
ffffffff812c531c:	a9 00 ff 1f 00       	test   $0x1fff00,%eax
ffffffff812c5321:	75 de                	jne    ffffffff812c5301 <radix_tree_node_alloc+0xd>
		/*
		 * Provided the caller has preloaded here, we will always
		 * succeed in getting a node here (and never reach
		 * kmem_cache_alloc)
		 */
		rtp = this_cpu_ptr(&radix_tree_preloads);
ffffffff812c5323:	48 c7 c2 60 09 01 00 	mov    $0x10960,%rdx
ffffffff812c532a:	65 48 03 15 e6 4d d4 	add    %gs:0x7ed44de6(%rip),%rdx        # a118 <this_cpu_off>
ffffffff812c5331:	7e 
		if (rtp->nr) {
ffffffff812c5332:	8b 0a                	mov    (%rdx),%ecx
ffffffff812c5334:	85 c9                	test   %ecx,%ecx
ffffffff812c5336:	74 c9                	je     ffffffff812c5301 <radix_tree_node_alloc+0xd>
ffffffff812c5338:	48 63 c1             	movslq %ecx,%rax
			ret = rtp->nodes[rtp->nr - 1];
			rtp->nodes[rtp->nr - 1] = NULL;
			rtp->nr--;
ffffffff812c533b:	ff c9                	dec    %ecx
ffffffff812c533d:	48 8d 3c c2          	lea    (%rdx,%rax,8),%rdi
		 * succeed in getting a node here (and never reach
		 * kmem_cache_alloc)
		 */
		rtp = this_cpu_ptr(&radix_tree_preloads);
		if (rtp->nr) {
			ret = rtp->nodes[rtp->nr - 1];
ffffffff812c5341:	48 8b 07             	mov    (%rdi),%rax
			rtp->nodes[rtp->nr - 1] = NULL;
ffffffff812c5344:	48 c7 07 00 00 00 00 	movq   $0x0,(%rdi)
			rtp->nr--;
ffffffff812c534b:	89 0a                	mov    %ecx,(%rdx)
		 * Update the allocation stack trace as this is more useful
		 * for debugging.
		 */
		kmemleak_update_trace(ret);
	}
	if (ret == NULL)
ffffffff812c534d:	48 85 c0             	test   %rax,%rax
ffffffff812c5350:	74 af                	je     ffffffff812c5301 <radix_tree_node_alloc+0xd>
		ret = kmem_cache_alloc(radix_tree_node_cachep, gfp_mask);

	BUG_ON(radix_tree_is_indirect_ptr(ret));
ffffffff812c5352:	a8 01                	test   $0x1,%al
ffffffff812c5354:	74 02                	je     ffffffff812c5358 <radix_tree_node_alloc+0x64>
ffffffff812c5356:	0f 0b                	ud2    
	return ret;
}
ffffffff812c5358:	5d                   	pop    %rbp
ffffffff812c5359:	c3                   	retq   

ffffffff812c535a <radix_tree_tag_get>:
{
	unsigned int height, shift;
	struct radix_tree_node *node;

	/* check the root's tag bit */
	if (!root_tag_get(root, tag))
ffffffff812c535a:	8d 4a 19             	lea    0x19(%rdx),%ecx
ffffffff812c535d:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c5362:	d3 e0                	shl    %cl,%eax
ffffffff812c5364:	85 47 04             	test   %eax,0x4(%rdi)
ffffffff812c5367:	0f 84 af 00 00 00    	je     ffffffff812c541c <radix_tree_tag_get+0xc2>
		return 0;

	node = rcu_dereference_raw(root->rnode);
ffffffff812c536d:	4c 8b 47 08          	mov    0x8(%rdi),%r8
	if (node == NULL)
ffffffff812c5371:	4d 85 c0             	test   %r8,%r8
ffffffff812c5374:	0f 84 a2 00 00 00    	je     ffffffff812c541c <radix_tree_tag_get+0xc2>
		return 0;

	if (!radix_tree_is_indirect_ptr(node))
ffffffff812c537a:	41 f6 c0 01          	test   $0x1,%r8b
ffffffff812c537e:	75 09                	jne    ffffffff812c5389 <radix_tree_tag_get+0x2f>
		return (index == 0);
ffffffff812c5380:	31 c0                	xor    %eax,%eax
ffffffff812c5382:	48 85 f6             	test   %rsi,%rsi
ffffffff812c5385:	0f 94 c0             	sete   %al
ffffffff812c5388:	c3                   	retq   
 * the RCU lock is held, unless tag modification and node deletion are excluded
 * from concurrency.
 */
int radix_tree_tag_get(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c5389:	55                   	push   %rbp
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c538a:	49 83 e0 fe          	and    $0xfffffffffffffffe,%r8
 * the RCU lock is held, unless tag modification and node deletion are excluded
 * from concurrency.
 */
int radix_tree_tag_get(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c538e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5391:	41 57                	push   %r15
ffffffff812c5393:	41 56                	push   %r14
ffffffff812c5395:	41 55                	push   %r13
ffffffff812c5397:	41 54                	push   %r12
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5399:	4d 89 c6             	mov    %r8,%r14
 * the RCU lock is held, unless tag modification and node deletion are excluded
 * from concurrency.
 */
int radix_tree_tag_get(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c539c:	53                   	push   %rbx
ffffffff812c539d:	51                   	push   %rcx

	if (!radix_tree_is_indirect_ptr(node))
		return (index == 0);
	node = indirect_to_ptr(node);

	height = node->path & RADIX_TREE_HEIGHT_MASK;
ffffffff812c539e:	45 8b 10             	mov    (%r8),%r10d
ffffffff812c53a1:	41 81 e2 ff 0f 00 00 	and    $0xfff,%r10d
ffffffff812c53a8:	44 89 d0             	mov    %r10d,%eax
	if (index > radix_tree_maxindex(height))
ffffffff812c53ab:	48 3b 34 c5 a0 97 a5 	cmp    -0x7e5a6860(,%rax,8),%rsi
ffffffff812c53b2:	81 

	if (!radix_tree_is_indirect_ptr(node))
		return (index == 0);
	node = indirect_to_ptr(node);

	height = node->path & RADIX_TREE_HEIGHT_MASK;
ffffffff812c53b3:	49 89 c7             	mov    %rax,%r15
	if (index > radix_tree_maxindex(height))
ffffffff812c53b6:	77 4f                	ja     ffffffff812c5407 <radix_tree_tag_get+0xad>
		return 0;

	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c53b8:	6b c8 06             	imul   $0x6,%eax,%ecx
ffffffff812c53bb:	89 d3                	mov    %edx,%ebx
ffffffff812c53bd:	48 89 75 d0          	mov    %rsi,-0x30(%rbp)
ffffffff812c53c1:	44 8d 61 fa          	lea    -0x6(%rcx),%r12d

	for ( ; ; ) {
		int offset;

		if (node == NULL)
ffffffff812c53c5:	4d 85 f6             	test   %r14,%r14
ffffffff812c53c8:	74 3d                	je     ffffffff812c5407 <radix_tree_tag_get+0xad>
			return 0;

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c53ca:	4c 8b 4d d0          	mov    -0x30(%rbp),%r9
ffffffff812c53ce:	44 88 e1             	mov    %r12b,%cl
		if (!tag_get(node, tag, offset))
ffffffff812c53d1:	89 de                	mov    %ebx,%esi
ffffffff812c53d3:	4c 89 f7             	mov    %r14,%rdi
		int offset;

		if (node == NULL)
			return 0;

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c53d6:	49 d3 e9             	shr    %cl,%r9
ffffffff812c53d9:	45 89 cd             	mov    %r9d,%r13d
ffffffff812c53dc:	41 83 e5 3f          	and    $0x3f,%r13d
		if (!tag_get(node, tag, offset))
ffffffff812c53e0:	44 89 ea             	mov    %r13d,%edx
ffffffff812c53e3:	e8 53 f9 ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c53e8:	85 c0                	test   %eax,%eax
ffffffff812c53ea:	74 1b                	je     ffffffff812c5407 <radix_tree_tag_get+0xad>
			return 0;
		if (height == 1)
ffffffff812c53ec:	41 83 ff 01          	cmp    $0x1,%r15d
ffffffff812c53f0:	74 19                	je     ffffffff812c540b <radix_tree_tag_get+0xb1>
			return 1;
		node = rcu_dereference_raw(node->slots[offset]);
ffffffff812c53f2:	4d 63 cd             	movslq %r13d,%r9
		shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c53f5:	41 83 ec 06          	sub    $0x6,%r12d
		height--;
ffffffff812c53f9:	41 ff cf             	dec    %r15d
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
		if (!tag_get(node, tag, offset))
			return 0;
		if (height == 1)
			return 1;
		node = rcu_dereference_raw(node->slots[offset]);
ffffffff812c53fc:	49 83 c1 04          	add    $0x4,%r9
ffffffff812c5400:	4f 8b 74 ce 08       	mov    0x8(%r14,%r9,8),%r14
ffffffff812c5405:	eb be                	jmp    ffffffff812c53c5 <radix_tree_tag_get+0x6b>
	unsigned int height, shift;
	struct radix_tree_node *node;

	/* check the root's tag bit */
	if (!root_tag_get(root, tag))
		return 0;
ffffffff812c5407:	31 c0                	xor    %eax,%eax
ffffffff812c5409:	eb 05                	jmp    ffffffff812c5410 <radix_tree_tag_get+0xb6>

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
		if (!tag_get(node, tag, offset))
			return 0;
		if (height == 1)
			return 1;
ffffffff812c540b:	b8 01 00 00 00       	mov    $0x1,%eax
		node = rcu_dereference_raw(node->slots[offset]);
		shift -= RADIX_TREE_MAP_SHIFT;
		height--;
	}
}
ffffffff812c5410:	5a                   	pop    %rdx
ffffffff812c5411:	5b                   	pop    %rbx
ffffffff812c5412:	41 5c                	pop    %r12
ffffffff812c5414:	41 5d                	pop    %r13
ffffffff812c5416:	41 5e                	pop    %r14
ffffffff812c5418:	41 5f                	pop    %r15
ffffffff812c541a:	5d                   	pop    %rbp
ffffffff812c541b:	c3                   	retq   
	unsigned int height, shift;
	struct radix_tree_node *node;

	/* check the root's tag bit */
	if (!root_tag_get(root, tag))
		return 0;
ffffffff812c541c:	31 c0                	xor    %eax,%eax
			return 1;
		node = rcu_dereference_raw(node->slots[offset]);
		shift -= RADIX_TREE_MAP_SHIFT;
		height--;
	}
}
ffffffff812c541e:	c3                   	retq   

ffffffff812c541f <radix_tree_tag_set>:
			unsigned long index, unsigned int tag)
{
	unsigned int height, shift;
	struct radix_tree_node *slot;

	height = root->height;
ffffffff812c541f:	8b 07                	mov    (%rdi),%eax
	BUG_ON(index > radix_tree_maxindex(height));
ffffffff812c5421:	48 3b 34 c5 a0 97 a5 	cmp    -0x7e5a6860(,%rax,8),%rsi
ffffffff812c5428:	81 
ffffffff812c5429:	76 02                	jbe    ffffffff812c542d <radix_tree_tag_set+0xe>
ffffffff812c542b:	0f 0b                	ud2    
 *	Returns the address of the tagged item.   Setting a tag on a not-present
 *	item is a bug.
 */
void *radix_tree_tag_set(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c542d:	55                   	push   %rbp

	height = root->height;
	BUG_ON(index > radix_tree_maxindex(height));

	slot = indirect_to_ptr(root->rnode);
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c542e:	6b c8 06             	imul   $0x6,%eax,%ecx
 *	Returns the address of the tagged item.   Setting a tag on a not-present
 *	item is a bug.
 */
void *radix_tree_tag_set(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c5431:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5434:	41 57                	push   %r15
ffffffff812c5436:	41 56                	push   %r14
ffffffff812c5438:	41 55                	push   %r13
ffffffff812c543a:	41 54                	push   %r12
ffffffff812c543c:	49 89 ff             	mov    %rdi,%r15
ffffffff812c543f:	53                   	push   %rbx
ffffffff812c5440:	48 89 c3             	mov    %rax,%rbx

	height = root->height;
	BUG_ON(index > radix_tree_maxindex(height));

	slot = indirect_to_ptr(root->rnode);
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c5443:	8d 41 fa             	lea    -0x6(%rcx),%eax
ffffffff812c5446:	49 89 f4             	mov    %rsi,%r12
 *	Returns the address of the tagged item.   Setting a tag on a not-present
 *	item is a bug.
 */
void *radix_tree_tag_set(struct radix_tree_root *root,
			unsigned long index, unsigned int tag)
{
ffffffff812c5449:	48 83 ec 10          	sub    $0x10,%rsp
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c544d:	4c 8b 47 08          	mov    0x8(%rdi),%r8

	height = root->height;
	BUG_ON(index > radix_tree_maxindex(height));

	slot = indirect_to_ptr(root->rnode);
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c5451:	89 45 d0             	mov    %eax,-0x30(%rbp)
}

static inline void tag_set(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__set_bit(offset, node->tags[tag]);
ffffffff812c5454:	89 d0                	mov    %edx,%eax
ffffffff812c5456:	89 55 d4             	mov    %edx,-0x2c(%rbp)
ffffffff812c5459:	4c 8d 2c c5 20 02 00 	lea    0x220(,%rax,8),%r13
ffffffff812c5460:	00 
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5461:	49 83 e0 fe          	and    $0xfffffffffffffffe,%r8
ffffffff812c5465:	4d 89 c6             	mov    %r8,%r14
	BUG_ON(index > radix_tree_maxindex(height));

	slot = indirect_to_ptr(root->rnode);
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;

	while (height > 0) {
ffffffff812c5468:	85 db                	test   %ebx,%ebx
ffffffff812c546a:	74 44                	je     ffffffff812c54b0 <radix_tree_tag_set+0x91>
		int offset;

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c546c:	8a 4d d0             	mov    -0x30(%rbp),%cl
ffffffff812c546f:	4d 89 e2             	mov    %r12,%r10
		if (!tag_get(slot, tag, offset))
ffffffff812c5472:	8b 75 d4             	mov    -0x2c(%rbp),%esi
ffffffff812c5475:	4c 89 f7             	mov    %r14,%rdi
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;

	while (height > 0) {
		int offset;

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5478:	49 d3 ea             	shr    %cl,%r10
ffffffff812c547b:	44 89 d0             	mov    %r10d,%eax
ffffffff812c547e:	83 e0 3f             	and    $0x3f,%eax
		if (!tag_get(slot, tag, offset))
ffffffff812c5481:	89 c2                	mov    %eax,%edx
	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;

	while (height > 0) {
		int offset;

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5483:	89 45 cc             	mov    %eax,-0x34(%rbp)
		if (!tag_get(slot, tag, offset))
ffffffff812c5486:	e8 b0 f8 ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c548b:	85 c0                	test   %eax,%eax
ffffffff812c548d:	4c 63 55 cc          	movslq -0x34(%rbp),%r10
ffffffff812c5491:	75 09                	jne    ffffffff812c549c <radix_tree_tag_set+0x7d>
}

static inline void tag_set(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__set_bit(offset, node->tags[tag]);
ffffffff812c5493:	4b 8d 04 2e          	lea    (%r14,%r13,1),%rax
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
ffffffff812c5497:	4c 0f ab 50 08       	bts    %r10,0x8(%rax)
		int offset;

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
		if (!tag_get(slot, tag, offset))
			tag_set(slot, tag, offset);
		slot = slot->slots[offset];
ffffffff812c549c:	4f 8b 74 d6 28       	mov    0x28(%r14,%r10,8),%r14
		BUG_ON(slot == NULL);
ffffffff812c54a1:	4d 85 f6             	test   %r14,%r14
ffffffff812c54a4:	75 02                	jne    ffffffff812c54a8 <radix_tree_tag_set+0x89>
ffffffff812c54a6:	0f 0b                	ud2    
		shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c54a8:	83 6d d0 06          	subl   $0x6,-0x30(%rbp)
		height--;
ffffffff812c54ac:	ff cb                	dec    %ebx
ffffffff812c54ae:	eb b8                	jmp    ffffffff812c5468 <radix_tree_tag_set+0x49>
	}

	/* set the root's tag bit */
	if (slot && !root_tag_get(root, tag))
ffffffff812c54b0:	4d 85 f6             	test   %r14,%r14
ffffffff812c54b3:	74 1b                	je     ffffffff812c54d0 <radix_tree_tag_set+0xb1>
	root->gfp_mask &= __GFP_BITS_MASK;
}

static inline int root_tag_get(struct radix_tree_root *root, unsigned int tag)
{
	return (__force unsigned)root->gfp_mask & (1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c54b5:	8b 4d d4             	mov    -0x2c(%rbp),%ecx
ffffffff812c54b8:	41 8b 57 04          	mov    0x4(%r15),%edx
ffffffff812c54bc:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c54c1:	83 c1 19             	add    $0x19,%ecx
ffffffff812c54c4:	d3 e0                	shl    %cl,%eax
		shift -= RADIX_TREE_MAP_SHIFT;
		height--;
	}

	/* set the root's tag bit */
	if (slot && !root_tag_get(root, tag))
ffffffff812c54c6:	85 c2                	test   %eax,%edx
ffffffff812c54c8:	75 06                	jne    ffffffff812c54d0 <radix_tree_tag_set+0xb1>
	return test_bit(offset, node->tags[tag]);
}

static inline void root_tag_set(struct radix_tree_root *root, unsigned int tag)
{
	root->gfp_mask |= (__force gfp_t)(1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c54ca:	09 d0                	or     %edx,%eax
ffffffff812c54cc:	41 89 47 04          	mov    %eax,0x4(%r15)
	/* set the root's tag bit */
	if (slot && !root_tag_get(root, tag))
		root_tag_set(root, tag);

	return slot;
}
ffffffff812c54d0:	5a                   	pop    %rdx
ffffffff812c54d1:	4c 89 f0             	mov    %r14,%rax
ffffffff812c54d4:	59                   	pop    %rcx
ffffffff812c54d5:	5b                   	pop    %rbx
ffffffff812c54d6:	41 5c                	pop    %r12
ffffffff812c54d8:	41 5d                	pop    %r13
ffffffff812c54da:	41 5e                	pop    %r14
ffffffff812c54dc:	41 5f                	pop    %r15
ffffffff812c54de:	5d                   	pop    %rbp
ffffffff812c54df:	c3                   	retq   

ffffffff812c54e0 <radix_tree_range_tag_if_tagged>:
 */
unsigned long radix_tree_range_tag_if_tagged(struct radix_tree_root *root,
		unsigned long *first_indexp, unsigned long last_index,
		unsigned long nr_to_tag,
		unsigned int iftag, unsigned int settag)
{
ffffffff812c54e0:	55                   	push   %rbp
ffffffff812c54e1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c54e4:	41 57                	push   %r15
ffffffff812c54e6:	41 56                	push   %r14
ffffffff812c54e8:	41 55                	push   %r13
ffffffff812c54ea:	41 54                	push   %r12
ffffffff812c54ec:	49 89 cc             	mov    %rcx,%r12
ffffffff812c54ef:	53                   	push   %rbx
ffffffff812c54f0:	48 83 ec 48          	sub    $0x48,%rsp
	unsigned int height = root->height;
ffffffff812c54f4:	8b 0f                	mov    (%rdi),%ecx
	struct radix_tree_node *node = NULL;
	struct radix_tree_node *slot;
	unsigned int shift;
	unsigned long tagged = 0;
	unsigned long index = *first_indexp;
ffffffff812c54f6:	48 8b 1e             	mov    (%rsi),%rbx
ffffffff812c54f9:	48 89 5d d0          	mov    %rbx,-0x30(%rbp)

	last_index = min(last_index, radix_tree_maxindex(height));
ffffffff812c54fd:	48 39 14 cd a0 97 a5 	cmp    %rdx,-0x7e5a6860(,%rcx,8)
ffffffff812c5504:	81 
ffffffff812c5505:	48 0f 46 14 cd a0 97 	cmovbe -0x7e5a6860(,%rcx,8),%rdx
ffffffff812c550c:	a5 81 
	if (index > last_index)
		return 0;
	if (!nr_to_tag)
ffffffff812c550e:	4d 85 e4             	test   %r12,%r12
	struct radix_tree_node *slot;
	unsigned int shift;
	unsigned long tagged = 0;
	unsigned long index = *first_indexp;

	last_index = min(last_index, radix_tree_maxindex(height));
ffffffff812c5511:	48 89 55 b0          	mov    %rdx,-0x50(%rbp)
	if (index > last_index)
		return 0;
	if (!nr_to_tag)
ffffffff812c5515:	0f 84 9b 01 00 00    	je     ffffffff812c56b6 <radix_tree_range_tag_if_tagged+0x1d6>
ffffffff812c551b:	48 39 d3             	cmp    %rdx,%rbx
ffffffff812c551e:	48 89 c8             	mov    %rcx,%rax
ffffffff812c5521:	0f 87 8f 01 00 00    	ja     ffffffff812c56b6 <radix_tree_range_tag_if_tagged+0x1d6>
		return 0;
	if (!root_tag_get(root, iftag)) {
ffffffff812c5527:	41 8d 48 19          	lea    0x19(%r8),%ecx
ffffffff812c552b:	bb 01 00 00 00       	mov    $0x1,%ebx
ffffffff812c5530:	89 da                	mov    %ebx,%edx
ffffffff812c5532:	d3 e2                	shl    %cl,%edx
ffffffff812c5534:	85 57 04             	test   %edx,0x4(%rdi)
ffffffff812c5537:	75 0f                	jne    ffffffff812c5548 <radix_tree_range_tag_if_tagged+0x68>
		*first_indexp = last_index + 1;
ffffffff812c5539:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812c553d:	49 ff c2             	inc    %r10
ffffffff812c5540:	4c 89 16             	mov    %r10,(%rsi)
ffffffff812c5543:	e9 6e 01 00 00       	jmpq   ffffffff812c56b6 <radix_tree_range_tag_if_tagged+0x1d6>
		return 0;
	}
	if (height == 0) {
ffffffff812c5548:	85 c0                	test   %eax,%eax
ffffffff812c554a:	75 1d                	jne    ffffffff812c5569 <radix_tree_range_tag_if_tagged+0x89>
		*first_indexp = last_index + 1;
ffffffff812c554c:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
	return test_bit(offset, node->tags[tag]);
}

static inline void root_tag_set(struct radix_tree_root *root, unsigned int tag)
{
	root->gfp_mask |= (__force gfp_t)(1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c5550:	41 8d 49 19          	lea    0x19(%r9),%ecx
		return 0;
	}
	if (height == 0) {
		*first_indexp = last_index + 1;
		root_tag_set(root, settag);
		return 1;
ffffffff812c5554:	b8 01 00 00 00       	mov    $0x1,%eax
	return test_bit(offset, node->tags[tag]);
}

static inline void root_tag_set(struct radix_tree_root *root, unsigned int tag)
{
	root->gfp_mask |= (__force gfp_t)(1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c5559:	d3 e3                	shl    %cl,%ebx
	if (!root_tag_get(root, iftag)) {
		*first_indexp = last_index + 1;
		return 0;
	}
	if (height == 0) {
		*first_indexp = last_index + 1;
ffffffff812c555b:	49 ff c2             	inc    %r10
ffffffff812c555e:	4c 89 16             	mov    %r10,(%rsi)
	return test_bit(offset, node->tags[tag]);
}

static inline void root_tag_set(struct radix_tree_root *root, unsigned int tag)
{
	root->gfp_mask |= (__force gfp_t)(1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c5561:	09 5f 04             	or     %ebx,0x4(%rdi)
ffffffff812c5564:	e9 4f 01 00 00       	jmpq   ffffffff812c56b8 <radix_tree_range_tag_if_tagged+0x1d8>
		*first_indexp = last_index + 1;
		root_tag_set(root, settag);
		return 1;
	}

	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c5569:	6b c8 06             	imul   $0x6,%eax,%ecx
ffffffff812c556c:	44 89 4d c0          	mov    %r9d,-0x40(%rbp)
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5570:	4c 8b 4f 08          	mov    0x8(%rdi),%r9
ffffffff812c5574:	4c 89 65 98          	mov    %r12,-0x68(%rbp)
ffffffff812c5578:	44 89 45 94          	mov    %r8d,-0x6c(%rbp)
ffffffff812c557c:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c557f:	48 89 75 a0          	mov    %rsi,-0x60(%rbp)
{
	unsigned int height = root->height;
	struct radix_tree_node *node = NULL;
	struct radix_tree_node *slot;
	unsigned int shift;
	unsigned long tagged = 0;
ffffffff812c5583:	48 c7 45 b8 00 00 00 	movq   $0x0,-0x48(%rbp)
ffffffff812c558a:	00 
		unsigned long *first_indexp, unsigned long last_index,
		unsigned long nr_to_tag,
		unsigned int iftag, unsigned int settag)
{
	unsigned int height = root->height;
	struct radix_tree_node *node = NULL;
ffffffff812c558b:	45 31 e4             	xor    %r12d,%r12d
		*first_indexp = last_index + 1;
		root_tag_set(root, settag);
		return 1;
	}

	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c558e:	8d 41 fa             	lea    -0x6(%rcx),%eax
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5591:	49 83 e1 fe          	and    $0xfffffffffffffffe,%r9
ffffffff812c5595:	4c 89 4d c8          	mov    %r9,-0x38(%rbp)
		*first_indexp = last_index + 1;
		root_tag_set(root, settag);
		return 1;
	}

	shift = (height - 1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c5599:	89 45 c4             	mov    %eax,-0x3c(%rbp)
}

static inline void tag_set(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__set_bit(offset, node->tags[tag]);
ffffffff812c559c:	8b 45 c0             	mov    -0x40(%rbp),%eax
ffffffff812c559f:	4c 8d 2c c5 20 02 00 	lea    0x220(,%rax,8),%r13
ffffffff812c55a6:	00 

	for (;;) {
		unsigned long upindex;
		int offset;

		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c55a7:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
ffffffff812c55ab:	8a 4d c4             	mov    -0x3c(%rbp),%cl
ffffffff812c55ae:	48 d3 e8             	shr    %cl,%rax
ffffffff812c55b1:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
ffffffff812c55b5:	8b 55 a8             	mov    -0x58(%rbp),%edx
ffffffff812c55b8:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812c55bc:	83 e2 3f             	and    $0x3f,%edx
ffffffff812c55bf:	4c 63 f2             	movslq %edx,%r14
ffffffff812c55c2:	4e 8d 3c f0          	lea    (%rax,%r14,8),%r15
		if (!slot->slots[offset])
ffffffff812c55c6:	49 83 7f 28 00       	cmpq   $0x0,0x28(%r15)
ffffffff812c55cb:	74 71                	je     ffffffff812c563e <radix_tree_range_tag_if_tagged+0x15e>
			goto next;
		if (!tag_get(slot, iftag, offset))
ffffffff812c55cd:	8b 75 94             	mov    -0x6c(%rbp),%esi
ffffffff812c55d0:	48 89 c7             	mov    %rax,%rdi
ffffffff812c55d3:	e8 63 f7 ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c55d8:	85 c0                	test   %eax,%eax
ffffffff812c55da:	74 62                	je     ffffffff812c563e <radix_tree_range_tag_if_tagged+0x15e>
			goto next;
		if (shift) {
ffffffff812c55dc:	83 7d c4 00          	cmpl   $0x0,-0x3c(%rbp)
ffffffff812c55e0:	74 12                	je     ffffffff812c55f4 <radix_tree_range_tag_if_tagged+0x114>
			/* Go down one level */
			shift -= RADIX_TREE_MAP_SHIFT;
			node = slot;
			slot = slot->slots[offset];
ffffffff812c55e2:	49 8b 47 28          	mov    0x28(%r15),%rax
		if (!tag_get(slot, iftag, offset))
			goto next;
		if (shift) {
			/* Go down one level */
			shift -= RADIX_TREE_MAP_SHIFT;
			node = slot;
ffffffff812c55e6:	4c 8b 65 c8          	mov    -0x38(%rbp),%r12
			goto next;
		if (!tag_get(slot, iftag, offset))
			goto next;
		if (shift) {
			/* Go down one level */
			shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c55ea:	83 6d c4 06          	subl   $0x6,-0x3c(%rbp)
			node = slot;
			slot = slot->slots[offset];
ffffffff812c55ee:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
			continue;
ffffffff812c55f2:	eb b3                	jmp    ffffffff812c55a7 <radix_tree_range_tag_if_tagged+0xc7>
}

static inline void tag_set(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__set_bit(offset, node->tags[tag]);
ffffffff812c55f4:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
			slot = slot->slots[offset];
			continue;
		}

		/* tag the leaf */
		tagged++;
ffffffff812c55f8:	48 ff 45 b8          	incq   -0x48(%rbp)
}

static inline void tag_set(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__set_bit(offset, node->tags[tag]);
ffffffff812c55fc:	4c 01 e8             	add    %r13,%rax
ffffffff812c55ff:	4c 0f ab 70 08       	bts    %r14,0x8(%rax)
		tagged++;
		tag_set(slot, settag, offset);

		/* walk back up the path tagging interior nodes */
		upindex = index;
		while (node) {
ffffffff812c5604:	4d 85 e4             	test   %r12,%r12
ffffffff812c5607:	74 35                	je     ffffffff812c563e <radix_tree_range_tag_if_tagged+0x15e>
			upindex >>= RADIX_TREE_MAP_SHIFT;
ffffffff812c5609:	48 c1 6d d0 06       	shrq   $0x6,-0x30(%rbp)
			offset = upindex & RADIX_TREE_MAP_MASK;

			/* stop if we find a node with the tag already set */
			if (tag_get(node, settag, offset))
ffffffff812c560e:	8b 75 c0             	mov    -0x40(%rbp),%esi
ffffffff812c5611:	4c 89 e7             	mov    %r12,%rdi

		/* walk back up the path tagging interior nodes */
		upindex = index;
		while (node) {
			upindex >>= RADIX_TREE_MAP_SHIFT;
			offset = upindex & RADIX_TREE_MAP_MASK;
ffffffff812c5614:	44 8b 75 d0          	mov    -0x30(%rbp),%r14d
ffffffff812c5618:	41 83 e6 3f          	and    $0x3f,%r14d

			/* stop if we find a node with the tag already set */
			if (tag_get(node, settag, offset))
ffffffff812c561c:	44 89 f2             	mov    %r14d,%edx
ffffffff812c561f:	e8 17 f7 ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c5624:	85 c0                	test   %eax,%eax
ffffffff812c5626:	75 13                	jne    ffffffff812c563b <radix_tree_range_tag_if_tagged+0x15b>
}

static inline void tag_set(struct radix_tree_node *node, unsigned int tag,
		int offset)
{
	__set_bit(offset, node->tags[tag]);
ffffffff812c5628:	4b 8d 04 2c          	lea    (%r12,%r13,1),%rax
ffffffff812c562c:	4d 63 f6             	movslq %r14d,%r14
ffffffff812c562f:	4c 0f ab 70 08       	bts    %r14,0x8(%rax)

			/* stop if we find a node with the tag already set */
			if (tag_get(node, settag, offset))
				break;
			tag_set(node, settag, offset);
			node = node->parent;
ffffffff812c5634:	4d 8b 64 24 08       	mov    0x8(%r12),%r12
ffffffff812c5639:	eb c9                	jmp    ffffffff812c5604 <radix_tree_range_tag_if_tagged+0x124>
		 * Since all of this slot's ancestors now have the tag set
		 * from setting it above, we have no further need to walk
		 * back up the tree setting tags, until we update slot to
		 * point to another radix_tree_node.
		 */
		node = NULL;
ffffffff812c563b:	45 31 e4             	xor    %r12d,%r12d

next:
		/* Go to next item at level determined by 'shift' */
		index = ((index >> shift) + 1) << shift;
ffffffff812c563e:	4c 8b 5d a8          	mov    -0x58(%rbp),%r11
ffffffff812c5642:	8a 4d c4             	mov    -0x3c(%rbp),%cl
ffffffff812c5645:	49 ff c3             	inc    %r11
ffffffff812c5648:	49 d3 e3             	shl    %cl,%r11
		/* Overflow can happen when last_index is ~0UL... */
		if (index > last_index || !index)
			break;
		if (tagged >= nr_to_tag)
ffffffff812c564b:	4c 39 5d b0          	cmp    %r11,-0x50(%rbp)
		 */
		node = NULL;

next:
		/* Go to next item at level determined by 'shift' */
		index = ((index >> shift) + 1) << shift;
ffffffff812c564f:	4c 89 5d d0          	mov    %r11,-0x30(%rbp)
		/* Overflow can happen when last_index is ~0UL... */
		if (index > last_index || !index)
			break;
		if (tagged >= nr_to_tag)
ffffffff812c5653:	0f 92 c2             	setb   %dl
ffffffff812c5656:	4d 85 db             	test   %r11,%r11
ffffffff812c5659:	0f 94 c0             	sete   %al
ffffffff812c565c:	08 c2                	or     %al,%dl
ffffffff812c565e:	75 2e                	jne    ffffffff812c568e <radix_tree_range_tag_if_tagged+0x1ae>
ffffffff812c5660:	48 8b 7d 98          	mov    -0x68(%rbp),%rdi
ffffffff812c5664:	48 39 7d b8          	cmp    %rdi,-0x48(%rbp)
ffffffff812c5668:	73 24                	jae    ffffffff812c568e <radix_tree_range_tag_if_tagged+0x1ae>
			break;
		while (((index >> shift) & RADIX_TREE_MAP_MASK) == 0) {
ffffffff812c566a:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
ffffffff812c566e:	8a 4d c4             	mov    -0x3c(%rbp),%cl
ffffffff812c5671:	48 d3 e8             	shr    %cl,%rax
ffffffff812c5674:	a8 3f                	test   $0x3f,%al
ffffffff812c5676:	0f 85 2b ff ff ff    	jne    ffffffff812c55a7 <radix_tree_range_tag_if_tagged+0xc7>
			/*
			 * We've fully scanned this node. Go up. Because
			 * last_index is guaranteed to be in the tree, what
			 * we do below cannot wander astray.
			 */
			slot = slot->parent;
ffffffff812c567c:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
			shift += RADIX_TREE_MAP_SHIFT;
ffffffff812c5680:	83 45 c4 06          	addl   $0x6,-0x3c(%rbp)
			/*
			 * We've fully scanned this node. Go up. Because
			 * last_index is guaranteed to be in the tree, what
			 * we do below cannot wander astray.
			 */
			slot = slot->parent;
ffffffff812c5684:	48 8b 40 08          	mov    0x8(%rax),%rax
ffffffff812c5688:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
ffffffff812c568c:	eb dc                	jmp    ffffffff812c566a <radix_tree_range_tag_if_tagged+0x18a>
	}
	/*
	 * We need not to tag the root tag if there is no tag which is set with
	 * settag within the range from *first_indexp to last_index.
	 */
	if (tagged > 0)
ffffffff812c568e:	48 83 7d b8 00       	cmpq   $0x0,-0x48(%rbp)
ffffffff812c5693:	74 10                	je     ffffffff812c56a5 <radix_tree_range_tag_if_tagged+0x1c5>
	return test_bit(offset, node->tags[tag]);
}

static inline void root_tag_set(struct radix_tree_root *root, unsigned int tag)
{
	root->gfp_mask |= (__force gfp_t)(1 << (tag + __GFP_BITS_SHIFT));
ffffffff812c5695:	8b 4d c0             	mov    -0x40(%rbp),%ecx
ffffffff812c5698:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c569d:	83 c1 19             	add    $0x19,%ecx
ffffffff812c56a0:	d3 e0                	shl    %cl,%eax
ffffffff812c56a2:	09 43 04             	or     %eax,0x4(%rbx)
	 * We need not to tag the root tag if there is no tag which is set with
	 * settag within the range from *first_indexp to last_index.
	 */
	if (tagged > 0)
		root_tag_set(root, settag);
	*first_indexp = index;
ffffffff812c56a5:	48 8b 45 a0          	mov    -0x60(%rbp),%rax
ffffffff812c56a9:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
ffffffff812c56ad:	48 89 30             	mov    %rsi,(%rax)

	return tagged;
ffffffff812c56b0:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812c56b4:	eb 02                	jmp    ffffffff812c56b8 <radix_tree_range_tag_if_tagged+0x1d8>

	last_index = min(last_index, radix_tree_maxindex(height));
	if (index > last_index)
		return 0;
	if (!nr_to_tag)
		return 0;
ffffffff812c56b6:	31 c0                	xor    %eax,%eax
	if (tagged > 0)
		root_tag_set(root, settag);
	*first_indexp = index;

	return tagged;
}
ffffffff812c56b8:	48 83 c4 48          	add    $0x48,%rsp
ffffffff812c56bc:	5b                   	pop    %rbx
ffffffff812c56bd:	41 5c                	pop    %r12
ffffffff812c56bf:	41 5d                	pop    %r13
ffffffff812c56c1:	41 5e                	pop    %r14
ffffffff812c56c3:	41 5f                	pop    %r15
ffffffff812c56c5:	5d                   	pop    %rbp
ffffffff812c56c6:	c3                   	retq   

ffffffff812c56c7 <__radix_tree_create>:
 *
 *	Returns -ENOMEM, or 0 for success.
 */
int __radix_tree_create(struct radix_tree_root *root, unsigned long index,
			struct radix_tree_node **nodep, void ***slotp)
{
ffffffff812c56c7:	55                   	push   %rbp
ffffffff812c56c8:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c56cb:	41 57                	push   %r15
ffffffff812c56cd:	41 56                	push   %r14
ffffffff812c56cf:	41 55                	push   %r13
ffffffff812c56d1:	41 54                	push   %r12
ffffffff812c56d3:	49 89 d6             	mov    %rdx,%r14
ffffffff812c56d6:	53                   	push   %rbx
ffffffff812c56d7:	49 89 f4             	mov    %rsi,%r12
ffffffff812c56da:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c56dd:	49 89 cd             	mov    %rcx,%r13
ffffffff812c56e0:	48 83 ec 18          	sub    $0x18,%rsp
	struct radix_tree_node *node = NULL, *slot;
	unsigned int height, shift, offset;
	int error;

	/* Make sure the tree is high enough.  */
	if (index > radix_tree_maxindex(root->height)) {
ffffffff812c56e4:	8b 07                	mov    (%rdi),%eax
ffffffff812c56e6:	48 3b 34 c5 a0 97 a5 	cmp    -0x7e5a6860(,%rax,8),%rsi
ffffffff812c56ed:	81 
ffffffff812c56ee:	0f 86 ab 00 00 00    	jbe    ffffffff812c579f <__radix_tree_create+0xd8>
ffffffff812c56f4:	49 89 c7             	mov    %rax,%r15
	int tag;

	/* Figure out what the height should be.  */
	height = root->height + 1;
	while (index > radix_tree_maxindex(height))
		height++;
ffffffff812c56f7:	41 ff c7             	inc    %r15d
 *	Return the maximum key which can be store into a
 *	radix tree with height HEIGHT.
 */
static inline unsigned long radix_tree_maxindex(unsigned int height)
{
	return height_to_maxindex[height];
ffffffff812c56fa:	44 89 f8             	mov    %r15d,%eax
	unsigned int height;
	int tag;

	/* Figure out what the height should be.  */
	height = root->height + 1;
	while (index > radix_tree_maxindex(height))
ffffffff812c56fd:	4c 3b 24 c5 a0 97 a5 	cmp    -0x7e5a6860(,%rax,8),%r12
ffffffff812c5704:	81 
ffffffff812c5705:	77 f0                	ja     ffffffff812c56f7 <__radix_tree_create+0x30>
		height++;

	if (root->rnode == NULL) {
ffffffff812c5707:	48 83 7b 08 00       	cmpq   $0x0,0x8(%rbx)
ffffffff812c570c:	75 3d                	jne    ffffffff812c574b <__radix_tree_create+0x84>
		root->height = height;
ffffffff812c570e:	44 89 3b             	mov    %r15d,(%rbx)
ffffffff812c5711:	e9 89 00 00 00       	jmpq   ffffffff812c579f <__radix_tree_create+0xd8>
		BUG_ON(newheight & ~RADIX_TREE_HEIGHT_MASK);
		node->path = newheight;
		node->count = 1;
		node->parent = NULL;
		slot = root->rnode;
		if (newheight > 1) {
ffffffff812c5716:	83 fa 01             	cmp    $0x1,%edx
		}

		/* Increase the height.  */
		newheight = root->height+1;
		BUG_ON(newheight & ~RADIX_TREE_HEIGHT_MASK);
		node->path = newheight;
ffffffff812c5719:	89 10                	mov    %edx,(%rax)
		node->count = 1;
ffffffff812c571b:	c7 40 04 01 00 00 00 	movl   $0x1,0x4(%rax)
		node->parent = NULL;
ffffffff812c5722:	48 c7 40 08 00 00 00 	movq   $0x0,0x8(%rax)
ffffffff812c5729:	00 
		slot = root->rnode;
ffffffff812c572a:	48 8b 4b 08          	mov    0x8(%rbx),%rcx
		if (newheight > 1) {
ffffffff812c572e:	76 08                	jbe    ffffffff812c5738 <__radix_tree_create+0x71>
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5730:	48 83 e1 fe          	and    $0xfffffffffffffffe,%rcx
		node->count = 1;
		node->parent = NULL;
		slot = root->rnode;
		if (newheight > 1) {
			slot = indirect_to_ptr(slot);
			slot->parent = node;
ffffffff812c5734:	48 89 41 08          	mov    %rax,0x8(%rcx)
		}
		node->slots[0] = slot;
ffffffff812c5738:	48 89 48 28          	mov    %rcx,0x28(%rax)
};
static DEFINE_PER_CPU(struct radix_tree_preload, radix_tree_preloads) = { 0, };

static inline void *ptr_to_indirect(void *ptr)
{
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
ffffffff812c573c:	48 83 c8 01          	or     $0x1,%rax
		}
		node->slots[0] = slot;
		node = ptr_to_indirect(node);
		rcu_assign_pointer(root->rnode, node);
		root->height = newheight;
	} while (height > root->height);
ffffffff812c5740:	41 39 d7             	cmp    %edx,%r15d
			slot = indirect_to_ptr(slot);
			slot->parent = node;
		}
		node->slots[0] = slot;
		node = ptr_to_indirect(node);
		rcu_assign_pointer(root->rnode, node);
ffffffff812c5743:	48 89 43 08          	mov    %rax,0x8(%rbx)
		root->height = newheight;
ffffffff812c5747:	89 13                	mov    %edx,(%rbx)
	} while (height > root->height);
ffffffff812c5749:	76 54                	jbe    ffffffff812c579f <__radix_tree_create+0xd8>
		goto out;
	}

	do {
		unsigned int newheight;
		if (!(node = radix_tree_node_alloc(root)))
ffffffff812c574b:	48 89 df             	mov    %rbx,%rdi
ffffffff812c574e:	e8 a1 fb ff ff       	callq  ffffffff812c52f4 <radix_tree_node_alloc>
ffffffff812c5753:	48 85 c0             	test   %rax,%rax
ffffffff812c5756:	75 0a                	jne    ffffffff812c5762 <__radix_tree_create+0x9b>
			return -ENOMEM;
ffffffff812c5758:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
ffffffff812c575d:	e9 e6 00 00 00       	jmpq   ffffffff812c5848 <__radix_tree_create+0x181>

		/* Propagate the aggregated tag info into the new root */
		for (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++) {
			if (root_tag_get(root, tag))
ffffffff812c5762:	f6 43 07 02          	testb  $0x2,0x7(%rbx)
ffffffff812c5766:	74 08                	je     ffffffff812c5770 <__radix_tree_create+0xa9>
ffffffff812c5768:	0f ba a8 28 02 00 00 	btsl   $0x0,0x228(%rax)
ffffffff812c576f:	00 
ffffffff812c5770:	f6 43 07 04          	testb  $0x4,0x7(%rbx)
ffffffff812c5774:	74 08                	je     ffffffff812c577e <__radix_tree_create+0xb7>
ffffffff812c5776:	0f ba a8 30 02 00 00 	btsl   $0x0,0x230(%rax)
ffffffff812c577d:	00 
ffffffff812c577e:	f6 43 07 08          	testb  $0x8,0x7(%rbx)
ffffffff812c5782:	74 08                	je     ffffffff812c578c <__radix_tree_create+0xc5>
ffffffff812c5784:	0f ba a8 38 02 00 00 	btsl   $0x0,0x238(%rax)
ffffffff812c578b:	00 
				tag_set(node, tag, 0);
		}

		/* Increase the height.  */
		newheight = root->height+1;
ffffffff812c578c:	8b 33                	mov    (%rbx),%esi
ffffffff812c578e:	8d 56 01             	lea    0x1(%rsi),%edx
		BUG_ON(newheight & ~RADIX_TREE_HEIGHT_MASK);
ffffffff812c5791:	f7 c2 00 f0 ff ff    	test   $0xfffff000,%edx
ffffffff812c5797:	0f 84 79 ff ff ff    	je     ffffffff812c5716 <__radix_tree_create+0x4f>
ffffffff812c579d:	0f 0b                	ud2    
			return error;
	}

	slot = indirect_to_ptr(root->rnode);

	height = root->height;
ffffffff812c579f:	8b 33                	mov    (%rbx),%esi
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c57a1:	48 8b 43 08          	mov    0x8(%rbx),%rax
	slot = indirect_to_ptr(root->rnode);

	height = root->height;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	offset = 0;			/* uninitialised var warning */
ffffffff812c57a5:	31 d2                	xor    %edx,%edx
	}

	slot = indirect_to_ptr(root->rnode);

	height = root->height;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c57a7:	44 6b fe 06          	imul   $0x6,%esi,%r15d
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c57ab:	48 83 e0 fe          	and    $0xfffffffffffffffe,%rax
	}

	slot = indirect_to_ptr(root->rnode);

	height = root->height;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c57af:	41 8d 4f fa          	lea    -0x6(%r15),%ecx
 *	Returns -ENOMEM, or 0 for success.
 */
int __radix_tree_create(struct radix_tree_root *root, unsigned long index,
			struct radix_tree_node **nodep, void ***slotp)
{
	struct radix_tree_node *node = NULL, *slot;
ffffffff812c57b3:	45 31 ff             	xor    %r15d,%r15d

	height = root->height;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	offset = 0;			/* uninitialised var warning */
	while (height > 0) {
ffffffff812c57b6:	85 f6                	test   %esi,%esi
ffffffff812c57b8:	74 6b                	je     ffffffff812c5825 <__radix_tree_create+0x15e>
		if (slot == NULL) {
ffffffff812c57ba:	48 85 c0             	test   %rax,%rax
ffffffff812c57bd:	75 4c                	jne    ffffffff812c580b <__radix_tree_create+0x144>
			/* Have to add a child node.  */
			if (!(slot = radix_tree_node_alloc(root)))
ffffffff812c57bf:	48 89 df             	mov    %rbx,%rdi
ffffffff812c57c2:	89 55 c4             	mov    %edx,-0x3c(%rbp)
ffffffff812c57c5:	89 4d c8             	mov    %ecx,-0x38(%rbp)
ffffffff812c57c8:	89 75 cc             	mov    %esi,-0x34(%rbp)
ffffffff812c57cb:	e8 24 fb ff ff       	callq  ffffffff812c52f4 <radix_tree_node_alloc>
ffffffff812c57d0:	48 85 c0             	test   %rax,%rax
ffffffff812c57d3:	74 83                	je     ffffffff812c5758 <__radix_tree_create+0x91>
				return -ENOMEM;
			slot->path = height;
ffffffff812c57d5:	8b 75 cc             	mov    -0x34(%rbp),%esi
			slot->parent = node;
			if (node) {
ffffffff812c57d8:	4d 85 ff             	test   %r15,%r15
		if (slot == NULL) {
			/* Have to add a child node.  */
			if (!(slot = radix_tree_node_alloc(root)))
				return -ENOMEM;
			slot->path = height;
			slot->parent = node;
ffffffff812c57db:	4c 89 78 08          	mov    %r15,0x8(%rax)
			if (node) {
ffffffff812c57df:	8b 4d c8             	mov    -0x38(%rbp),%ecx
	while (height > 0) {
		if (slot == NULL) {
			/* Have to add a child node.  */
			if (!(slot = radix_tree_node_alloc(root)))
				return -ENOMEM;
			slot->path = height;
ffffffff812c57e2:	89 30                	mov    %esi,(%rax)
			slot->parent = node;
			if (node) {
ffffffff812c57e4:	74 1a                	je     ffffffff812c5800 <__radix_tree_create+0x139>
				rcu_assign_pointer(node->slots[offset], slot);
ffffffff812c57e6:	8b 7d c4             	mov    -0x3c(%rbp),%edi
ffffffff812c57e9:	48 89 fa             	mov    %rdi,%rdx
ffffffff812c57ec:	48 83 c7 04          	add    $0x4,%rdi
ffffffff812c57f0:	49 89 44 ff 08       	mov    %rax,0x8(%r15,%rdi,8)
				node->count++;
ffffffff812c57f5:	41 ff 47 04          	incl   0x4(%r15)
				slot->path |= offset << RADIX_TREE_HEIGHT_SHIFT;
ffffffff812c57f9:	c1 e2 0c             	shl    $0xc,%edx
ffffffff812c57fc:	09 10                	or     %edx,(%rax)
ffffffff812c57fe:	eb 0b                	jmp    ffffffff812c580b <__radix_tree_create+0x144>
};
static DEFINE_PER_CPU(struct radix_tree_preload, radix_tree_preloads) = { 0, };

static inline void *ptr_to_indirect(void *ptr)
{
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
ffffffff812c5800:	48 89 c2             	mov    %rax,%rdx
ffffffff812c5803:	48 83 ca 01          	or     $0x1,%rdx
			if (node) {
				rcu_assign_pointer(node->slots[offset], slot);
				node->count++;
				slot->path |= offset << RADIX_TREE_HEIGHT_SHIFT;
			} else
				rcu_assign_pointer(root->rnode, ptr_to_indirect(slot));
ffffffff812c5807:	48 89 53 08          	mov    %rdx,0x8(%rbx)
		}

		/* Go a level down */
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c580b:	4c 89 e2             	mov    %r12,%rdx
		node = slot;
ffffffff812c580e:	49 89 c7             	mov    %rax,%r15
		slot = node->slots[offset];
		shift -= RADIX_TREE_MAP_SHIFT;
		height--;
ffffffff812c5811:	ff ce                	dec    %esi
			} else
				rcu_assign_pointer(root->rnode, ptr_to_indirect(slot));
		}

		/* Go a level down */
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5813:	48 d3 ea             	shr    %cl,%rdx
		node = slot;
		slot = node->slots[offset];
		shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c5816:	83 e9 06             	sub    $0x6,%ecx
			} else
				rcu_assign_pointer(root->rnode, ptr_to_indirect(slot));
		}

		/* Go a level down */
		offset = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5819:	83 e2 3f             	and    $0x3f,%edx
		node = slot;
		slot = node->slots[offset];
ffffffff812c581c:	89 d7                	mov    %edx,%edi
ffffffff812c581e:	48 8b 44 f8 28       	mov    0x28(%rax,%rdi,8),%rax
ffffffff812c5823:	eb 91                	jmp    ffffffff812c57b6 <__radix_tree_create+0xef>
		shift -= RADIX_TREE_MAP_SHIFT;
		height--;
	}

	if (nodep)
ffffffff812c5825:	4d 85 f6             	test   %r14,%r14
ffffffff812c5828:	74 03                	je     ffffffff812c582d <__radix_tree_create+0x166>
		*nodep = node;
ffffffff812c582a:	4d 89 3e             	mov    %r15,(%r14)
	if (slotp)
		*slotp = node ? node->slots + offset : (void **)&root->rnode;
	return 0;
ffffffff812c582d:	31 c0                	xor    %eax,%eax
		height--;
	}

	if (nodep)
		*nodep = node;
	if (slotp)
ffffffff812c582f:	4d 85 ed             	test   %r13,%r13
ffffffff812c5832:	74 14                	je     ffffffff812c5848 <__radix_tree_create+0x181>
		*slotp = node ? node->slots + offset : (void **)&root->rnode;
ffffffff812c5834:	48 83 c3 08          	add    $0x8,%rbx
ffffffff812c5838:	4d 85 ff             	test   %r15,%r15
ffffffff812c583b:	74 05                	je     ffffffff812c5842 <__radix_tree_create+0x17b>
ffffffff812c583d:	49 8d 5c d7 28       	lea    0x28(%r15,%rdx,8),%rbx
ffffffff812c5842:	49 89 5d 00          	mov    %rbx,0x0(%r13)
	return 0;
ffffffff812c5846:	31 c0                	xor    %eax,%eax
}
ffffffff812c5848:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c584c:	5b                   	pop    %rbx
ffffffff812c584d:	41 5c                	pop    %r12
ffffffff812c584f:	41 5d                	pop    %r13
ffffffff812c5851:	41 5e                	pop    %r14
ffffffff812c5853:	41 5f                	pop    %r15
ffffffff812c5855:	5d                   	pop    %rbp
ffffffff812c5856:	c3                   	retq   

ffffffff812c5857 <radix_tree_insert>:
 *
 *	Insert an item into the radix tree at position @index.
 */
int radix_tree_insert(struct radix_tree_root *root,
			unsigned long index, void *item)
{
ffffffff812c5857:	55                   	push   %rbp
ffffffff812c5858:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c585b:	41 56                	push   %r14
ffffffff812c585d:	41 55                	push   %r13
ffffffff812c585f:	41 54                	push   %r12
ffffffff812c5861:	53                   	push   %rbx
	struct radix_tree_node *node;
	void **slot;
	int error;

	BUG_ON(radix_tree_is_indirect_ptr(item));
ffffffff812c5862:	41 89 d4             	mov    %edx,%r12d
 *
 *	Insert an item into the radix tree at position @index.
 */
int radix_tree_insert(struct radix_tree_root *root,
			unsigned long index, void *item)
{
ffffffff812c5865:	48 83 ec 10          	sub    $0x10,%rsp
	struct radix_tree_node *node;
	void **slot;
	int error;

	BUG_ON(radix_tree_is_indirect_ptr(item));
ffffffff812c5869:	41 83 e4 01          	and    $0x1,%r12d
ffffffff812c586d:	74 02                	je     ffffffff812c5871 <radix_tree_insert+0x1a>
ffffffff812c586f:	0f 0b                	ud2    
ffffffff812c5871:	49 89 d5             	mov    %rdx,%r13

	error = __radix_tree_create(root, index, &node, &slot);
ffffffff812c5874:	48 8d 4d d8          	lea    -0x28(%rbp),%rcx
ffffffff812c5878:	48 8d 55 d0          	lea    -0x30(%rbp),%rdx
ffffffff812c587c:	48 89 f3             	mov    %rsi,%rbx
ffffffff812c587f:	49 89 fe             	mov    %rdi,%r14
ffffffff812c5882:	e8 40 fe ff ff       	callq  ffffffff812c56c7 <__radix_tree_create>
	if (error)
ffffffff812c5887:	85 c0                	test   %eax,%eax
ffffffff812c5889:	75 5f                	jne    ffffffff812c58ea <radix_tree_insert+0x93>
		return error;
	if (*slot != NULL)
ffffffff812c588b:	48 8b 45 d8          	mov    -0x28(%rbp),%rax
ffffffff812c588f:	48 83 38 00          	cmpq   $0x0,(%rax)
ffffffff812c5893:	75 5a                	jne    ffffffff812c58ef <radix_tree_insert+0x98>
		return -EEXIST;
	rcu_assign_pointer(*slot, item);
ffffffff812c5895:	48 8b 45 d8          	mov    -0x28(%rbp),%rax
ffffffff812c5899:	4c 89 28             	mov    %r13,(%rax)

	if (node) {
ffffffff812c589c:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
ffffffff812c58a0:	48 85 c0             	test   %rax,%rax
ffffffff812c58a3:	74 2f                	je     ffffffff812c58d4 <radix_tree_insert+0x7d>
		node->count++;
ffffffff812c58a5:	ff 40 04             	incl   0x4(%rax)
		BUG_ON(tag_get(node, 0, index & RADIX_TREE_MAP_MASK));
ffffffff812c58a8:	83 e3 3f             	and    $0x3f,%ebx
ffffffff812c58ab:	31 f6                	xor    %esi,%esi
ffffffff812c58ad:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
ffffffff812c58b1:	89 da                	mov    %ebx,%edx
ffffffff812c58b3:	e8 83 f4 ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c58b8:	85 c0                	test   %eax,%eax
ffffffff812c58ba:	74 02                	je     ffffffff812c58be <radix_tree_insert+0x67>
ffffffff812c58bc:	0f 0b                	ud2    
		BUG_ON(tag_get(node, 1, index & RADIX_TREE_MAP_MASK));
ffffffff812c58be:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
ffffffff812c58c2:	89 da                	mov    %ebx,%edx
ffffffff812c58c4:	be 01 00 00 00       	mov    $0x1,%esi
ffffffff812c58c9:	e8 6d f4 ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c58ce:	85 c0                	test   %eax,%eax
ffffffff812c58d0:	74 23                	je     ffffffff812c58f5 <radix_tree_insert+0x9e>
ffffffff812c58d2:	0f 0b                	ud2    
ffffffff812c58d4:	41 8b 46 04          	mov    0x4(%r14),%eax
	} else {
		BUG_ON(root_tag_get(root, 0));
ffffffff812c58d8:	a9 00 00 00 02       	test   $0x2000000,%eax
ffffffff812c58dd:	74 02                	je     ffffffff812c58e1 <radix_tree_insert+0x8a>
ffffffff812c58df:	0f 0b                	ud2    
		BUG_ON(root_tag_get(root, 1));
ffffffff812c58e1:	a9 00 00 00 04       	test   $0x4000000,%eax
ffffffff812c58e6:	74 0d                	je     ffffffff812c58f5 <radix_tree_insert+0x9e>
ffffffff812c58e8:	0f 0b                	ud2    
ffffffff812c58ea:	41 89 c4             	mov    %eax,%r12d
ffffffff812c58ed:	eb 06                	jmp    ffffffff812c58f5 <radix_tree_insert+0x9e>

	error = __radix_tree_create(root, index, &node, &slot);
	if (error)
		return error;
	if (*slot != NULL)
		return -EEXIST;
ffffffff812c58ef:	41 bc ef ff ff ff    	mov    $0xffffffef,%r12d
		BUG_ON(root_tag_get(root, 0));
		BUG_ON(root_tag_get(root, 1));
	}

	return 0;
}
ffffffff812c58f5:	5a                   	pop    %rdx
ffffffff812c58f6:	44 89 e0             	mov    %r12d,%eax
ffffffff812c58f9:	59                   	pop    %rcx
ffffffff812c58fa:	5b                   	pop    %rbx
ffffffff812c58fb:	41 5c                	pop    %r12
ffffffff812c58fd:	41 5d                	pop    %r13
ffffffff812c58ff:	41 5e                	pop    %r14
ffffffff812c5901:	5d                   	pop    %rbp
ffffffff812c5902:	c3                   	retq   

ffffffff812c5903 <__radix_tree_lookup>:
 *	allocated and @root->rnode is used as a direct slot instead of
 *	pointing to a node, in which case *@nodep will be NULL.
 */
void *__radix_tree_lookup(struct radix_tree_root *root, unsigned long index,
			  struct radix_tree_node **nodep, void ***slotp)
{
ffffffff812c5903:	55                   	push   %rbp
ffffffff812c5904:	49 89 ca             	mov    %rcx,%r10
	struct radix_tree_node *node, *parent;
	unsigned int height, shift;
	void **slot;

	node = rcu_dereference_raw(root->rnode);
ffffffff812c5907:	48 8b 4f 08          	mov    0x8(%rdi),%rcx
	if (node == NULL)
		return NULL;
ffffffff812c590b:	31 c0                	xor    %eax,%eax
 *	allocated and @root->rnode is used as a direct slot instead of
 *	pointing to a node, in which case *@nodep will be NULL.
 */
void *__radix_tree_lookup(struct radix_tree_root *root, unsigned long index,
			  struct radix_tree_node **nodep, void ***slotp)
{
ffffffff812c590d:	48 89 e5             	mov    %rsp,%rbp
	struct radix_tree_node *node, *parent;
	unsigned int height, shift;
	void **slot;

	node = rcu_dereference_raw(root->rnode);
	if (node == NULL)
ffffffff812c5910:	48 85 c9             	test   %rcx,%rcx
ffffffff812c5913:	0f 84 88 00 00 00    	je     ffffffff812c59a1 <__radix_tree_lookup+0x9e>
		return NULL;

	if (!radix_tree_is_indirect_ptr(node)) {
ffffffff812c5919:	f6 c1 01             	test   $0x1,%cl
ffffffff812c591c:	75 22                	jne    ffffffff812c5940 <__radix_tree_lookup+0x3d>
		if (index > 0)
ffffffff812c591e:	48 85 f6             	test   %rsi,%rsi
ffffffff812c5921:	75 7e                	jne    ffffffff812c59a1 <__radix_tree_lookup+0x9e>
			return NULL;

		if (nodep)
ffffffff812c5923:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5926:	74 07                	je     ffffffff812c592f <__radix_tree_lookup+0x2c>
			*nodep = NULL;
ffffffff812c5928:	48 c7 02 00 00 00 00 	movq   $0x0,(%rdx)
		if (slotp)
ffffffff812c592f:	4d 85 d2             	test   %r10,%r10
ffffffff812c5932:	48 89 c8             	mov    %rcx,%rax
ffffffff812c5935:	74 6a                	je     ffffffff812c59a1 <__radix_tree_lookup+0x9e>
			*slotp = (void **)&root->rnode;
ffffffff812c5937:	48 83 c7 08          	add    $0x8,%rdi
ffffffff812c593b:	49 89 3a             	mov    %rdi,(%r10)
ffffffff812c593e:	eb 61                	jmp    ffffffff812c59a1 <__radix_tree_lookup+0x9e>
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5940:	48 89 cf             	mov    %rcx,%rdi
	unsigned int height, shift;
	void **slot;

	node = rcu_dereference_raw(root->rnode);
	if (node == NULL)
		return NULL;
ffffffff812c5943:	31 c0                	xor    %eax,%eax
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5945:	48 83 e7 fe          	and    $0xfffffffffffffffe,%rdi
			*slotp = (void **)&root->rnode;
		return node;
	}
	node = indirect_to_ptr(node);

	height = node->path & RADIX_TREE_HEIGHT_MASK;
ffffffff812c5949:	44 8b 07             	mov    (%rdi),%r8d
ffffffff812c594c:	41 81 e0 ff 0f 00 00 	and    $0xfff,%r8d
 *	Return the maximum key which can be store into a
 *	radix tree with height HEIGHT.
 */
static inline unsigned long radix_tree_maxindex(unsigned int height)
{
	return height_to_maxindex[height];
ffffffff812c5953:	44 89 c1             	mov    %r8d,%ecx
		return node;
	}
	node = indirect_to_ptr(node);

	height = node->path & RADIX_TREE_HEIGHT_MASK;
	if (index > radix_tree_maxindex(height))
ffffffff812c5956:	48 3b 34 cd a0 97 a5 	cmp    -0x7e5a6860(,%rcx,8),%rsi
ffffffff812c595d:	81 
ffffffff812c595e:	77 41                	ja     ffffffff812c59a1 <__radix_tree_lookup+0x9e>
		return NULL;

	shift = (height-1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c5960:	41 6b c8 06          	imul   $0x6,%r8d,%ecx
ffffffff812c5964:	83 e9 06             	sub    $0x6,%ecx

	do {
		parent = node;
		slot = node->slots + ((index >> shift) & RADIX_TREE_MAP_MASK);
ffffffff812c5967:	48 89 f0             	mov    %rsi,%rax
ffffffff812c596a:	48 d3 e8             	shr    %cl,%rax
ffffffff812c596d:	83 e0 3f             	and    $0x3f,%eax
ffffffff812c5970:	4c 8d 5c c7 28       	lea    0x28(%rdi,%rax,8),%r11
		node = rcu_dereference_raw(*slot);
ffffffff812c5975:	4d 8b 0b             	mov    (%r11),%r9
		if (node == NULL)
ffffffff812c5978:	4d 85 c9             	test   %r9,%r9
ffffffff812c597b:	74 22                	je     ffffffff812c599f <__radix_tree_lookup+0x9c>
			return NULL;

		shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c597d:	83 e9 06             	sub    $0x6,%ecx
		height--;
	} while (height > 0);
ffffffff812c5980:	41 ff c8             	dec    %r8d
ffffffff812c5983:	74 05                	je     ffffffff812c598a <__radix_tree_lookup+0x87>
ffffffff812c5985:	4c 89 cf             	mov    %r9,%rdi
ffffffff812c5988:	eb dd                	jmp    ffffffff812c5967 <__radix_tree_lookup+0x64>

	if (nodep)
ffffffff812c598a:	48 85 d2             	test   %rdx,%rdx
ffffffff812c598d:	74 03                	je     ffffffff812c5992 <__radix_tree_lookup+0x8f>
		*nodep = parent;
ffffffff812c598f:	48 89 3a             	mov    %rdi,(%rdx)
	if (slotp)
ffffffff812c5992:	4d 85 d2             	test   %r10,%r10
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	do {
		parent = node;
		slot = node->slots + ((index >> shift) & RADIX_TREE_MAP_MASK);
		node = rcu_dereference_raw(*slot);
ffffffff812c5995:	4c 89 c8             	mov    %r9,%rax
		height--;
	} while (height > 0);

	if (nodep)
		*nodep = parent;
	if (slotp)
ffffffff812c5998:	74 07                	je     ffffffff812c59a1 <__radix_tree_lookup+0x9e>
		*slotp = slot;
ffffffff812c599a:	4d 89 1a             	mov    %r11,(%r10)
ffffffff812c599d:	eb 02                	jmp    ffffffff812c59a1 <__radix_tree_lookup+0x9e>
	unsigned int height, shift;
	void **slot;

	node = rcu_dereference_raw(root->rnode);
	if (node == NULL)
		return NULL;
ffffffff812c599f:	31 c0                	xor    %eax,%eax
	if (nodep)
		*nodep = parent;
	if (slotp)
		*slotp = slot;
	return node;
}
ffffffff812c59a1:	5d                   	pop    %rbp
ffffffff812c59a2:	c3                   	retq   

ffffffff812c59a3 <radix_tree_lookup_slot>:
 *	modified by radix_tree_replace_slot, otherwise it must be called
 *	exclusive from other writers. Any dereference of the slot must be done
 *	using radix_tree_deref_slot.
 */
void **radix_tree_lookup_slot(struct radix_tree_root *root, unsigned long index)
{
ffffffff812c59a3:	55                   	push   %rbp
	void **slot;

	if (!__radix_tree_lookup(root, index, NULL, &slot))
ffffffff812c59a4:	31 d2                	xor    %edx,%edx
 *	modified by radix_tree_replace_slot, otherwise it must be called
 *	exclusive from other writers. Any dereference of the slot must be done
 *	using radix_tree_deref_slot.
 */
void **radix_tree_lookup_slot(struct radix_tree_root *root, unsigned long index)
{
ffffffff812c59a6:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c59a9:	48 83 ec 10          	sub    $0x10,%rsp
	void **slot;

	if (!__radix_tree_lookup(root, index, NULL, &slot))
ffffffff812c59ad:	48 8d 4d f8          	lea    -0x8(%rbp),%rcx
ffffffff812c59b1:	e8 4d ff ff ff       	callq  ffffffff812c5903 <__radix_tree_lookup>
ffffffff812c59b6:	31 d2                	xor    %edx,%edx
ffffffff812c59b8:	48 85 c0             	test   %rax,%rax
ffffffff812c59bb:	74 04                	je     ffffffff812c59c1 <radix_tree_lookup_slot+0x1e>
		return NULL;
	return slot;
ffffffff812c59bd:	48 8b 55 f8          	mov    -0x8(%rbp),%rdx
}
ffffffff812c59c1:	48 89 d0             	mov    %rdx,%rax
ffffffff812c59c4:	c9                   	leaveq 
ffffffff812c59c5:	c3                   	retq   

ffffffff812c59c6 <radix_tree_lookup>:
 *	must manage lifetimes of leaf nodes (eg. RCU may also be used to free
 *	them safely). No RCU barriers are required to access or modify the
 *	returned item, however.
 */
void *radix_tree_lookup(struct radix_tree_root *root, unsigned long index)
{
ffffffff812c59c6:	55                   	push   %rbp
	return __radix_tree_lookup(root, index, NULL, NULL);
ffffffff812c59c7:	31 c9                	xor    %ecx,%ecx
ffffffff812c59c9:	31 d2                	xor    %edx,%edx
 *	must manage lifetimes of leaf nodes (eg. RCU may also be used to free
 *	them safely). No RCU barriers are required to access or modify the
 *	returned item, however.
 */
void *radix_tree_lookup(struct radix_tree_root *root, unsigned long index)
{
ffffffff812c59cb:	48 89 e5             	mov    %rsp,%rbp
	return __radix_tree_lookup(root, index, NULL, NULL);
ffffffff812c59ce:	e8 30 ff ff ff       	callq  ffffffff812c5903 <__radix_tree_lookup>
}
ffffffff812c59d3:	5d                   	pop    %rbp
ffffffff812c59d4:	c3                   	retq   

ffffffff812c59d5 <radix_tree_locate_item>:
 *	Returns index where item was found, or -1 if not found.
 *	Caller must hold no lock (since this time-consuming function needs
 *	to be preemptible), and must check afterwards if item is still there.
 */
unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item)
{
ffffffff812c59d5:	55                   	push   %rbp
ffffffff812c59d6:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c59d9:	41 57                	push   %r15
ffffffff812c59db:	41 56                	push   %r14
ffffffff812c59dd:	41 55                	push   %r13
ffffffff812c59df:	41 54                	push   %r12
ffffffff812c59e1:	49 89 ff             	mov    %rdi,%r15
ffffffff812c59e4:	53                   	push   %rbx
	struct radix_tree_node *node;
	unsigned long max_index;
	unsigned long cur_index = 0;
	unsigned long found_index = -1;
ffffffff812c59e5:	49 83 cd ff          	or     $0xffffffffffffffff,%r13
 */
unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item)
{
	struct radix_tree_node *node;
	unsigned long max_index;
	unsigned long cur_index = 0;
ffffffff812c59e9:	31 db                	xor    %ebx,%ebx
	for ( ; height > 1; height--) {
		i = (index >> shift) & RADIX_TREE_MAP_MASK;
		for (;;) {
			if (slot->slots[i] != NULL)
				break;
			index &= ~((1UL << shift) - 1);
ffffffff812c59eb:	41 bc 01 00 00 00    	mov    $0x1,%r12d
 *	Returns index where item was found, or -1 if not found.
 *	Caller must hold no lock (since this time-consuming function needs
 *	to be preemptible), and must check afterwards if item is still there.
 */
unsigned long radix_tree_locate_item(struct radix_tree_root *root, void *item)
{
ffffffff812c59f1:	48 83 ec 18          	sub    $0x18,%rsp
	unsigned long cur_index = 0;
	unsigned long found_index = -1;

	do {
		rcu_read_lock();
		node = rcu_dereference_raw(root->rnode);
ffffffff812c59f5:	49 8b 47 08          	mov    0x8(%r15),%rax
		if (!radix_tree_is_indirect_ptr(node)) {
ffffffff812c59f9:	a8 01                	test   $0x1,%al
ffffffff812c59fb:	75 11                	jne    ffffffff812c5a0e <radix_tree_locate_item+0x39>
			rcu_read_unlock();
			if (node == item)
				found_index = 0;
ffffffff812c59fd:	48 39 f0             	cmp    %rsi,%rax
ffffffff812c5a00:	b8 00 00 00 00       	mov    $0x0,%eax
ffffffff812c5a05:	4c 0f 44 e8          	cmove  %rax,%r13
ffffffff812c5a09:	e9 af 00 00 00       	jmpq   ffffffff812c5abd <radix_tree_locate_item+0xe8>
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5a0e:	48 83 e0 fe          	and    $0xfffffffffffffffe,%rax
				found_index = 0;
			break;
		}

		node = indirect_to_ptr(node);
		max_index = radix_tree_maxindex(node->path &
ffffffff812c5a12:	44 8b 00             	mov    (%rax),%r8d
ffffffff812c5a15:	41 81 e0 ff 0f 00 00 	and    $0xfff,%r8d
 *	Return the maximum key which can be store into a
 *	radix tree with height HEIGHT.
 */
static inline unsigned long radix_tree_maxindex(unsigned int height)
{
	return height_to_maxindex[height];
ffffffff812c5a1c:	44 89 c2             	mov    %r8d,%edx
ffffffff812c5a1f:	4c 8b 34 d5 a0 97 a5 	mov    -0x7e5a6860(,%rdx,8),%r14
ffffffff812c5a26:	81 
		}

		node = indirect_to_ptr(node);
		max_index = radix_tree_maxindex(node->path &
						RADIX_TREE_HEIGHT_MASK);
		if (cur_index > max_index) {
ffffffff812c5a27:	4c 39 f3             	cmp    %r14,%rbx
ffffffff812c5a2a:	76 05                	jbe    ffffffff812c5a31 <radix_tree_locate_item+0x5c>
	preempt_disable();
}

static inline void __rcu_read_unlock(void)
{
	preempt_enable();
ffffffff812c5a2c:	e9 8c 00 00 00       	jmpq   ffffffff812c5abd <radix_tree_locate_item+0xe8>
{
	unsigned int shift, height;
	unsigned long i;

	height = slot->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;
ffffffff812c5a31:	41 6b c8 06          	imul   $0x6,%r8d,%ecx
ffffffff812c5a35:	83 e9 06             	sub    $0x6,%ecx

	for ( ; height > 1; height--) {
ffffffff812c5a38:	41 83 f8 01          	cmp    $0x1,%r8d
ffffffff812c5a3c:	76 46                	jbe    ffffffff812c5a84 <radix_tree_locate_item+0xaf>
		i = (index >> shift) & RADIX_TREE_MAP_MASK;
		for (;;) {
			if (slot->slots[i] != NULL)
				break;
			index &= ~((1UL << shift) - 1);
ffffffff812c5a3e:	4c 89 e7             	mov    %r12,%rdi

	height = slot->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	for ( ; height > 1; height--) {
		i = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5a41:	48 89 da             	mov    %rbx,%rdx
		for (;;) {
			if (slot->slots[i] != NULL)
				break;
			index &= ~((1UL << shift) - 1);
ffffffff812c5a44:	48 d3 e7             	shl    %cl,%rdi

	height = slot->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	for ( ; height > 1; height--) {
		i = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5a47:	48 d3 ea             	shr    %cl,%rdx
		for (;;) {
			if (slot->slots[i] != NULL)
				break;
			index &= ~((1UL << shift) - 1);
ffffffff812c5a4a:	49 89 f9             	mov    %rdi,%r9

	height = slot->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	for ( ; height > 1; height--) {
		i = (index >> shift) & RADIX_TREE_MAP_MASK;
ffffffff812c5a4d:	83 e2 3f             	and    $0x3f,%edx
		for (;;) {
			if (slot->slots[i] != NULL)
				break;
			index &= ~((1UL << shift) - 1);
ffffffff812c5a50:	49 f7 d9             	neg    %r9
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	for ( ; height > 1; height--) {
		i = (index >> shift) & RADIX_TREE_MAP_MASK;
		for (;;) {
			if (slot->slots[i] != NULL)
ffffffff812c5a53:	48 83 7c d0 28 00    	cmpq   $0x0,0x28(%rax,%rdx,8)
ffffffff812c5a59:	75 13                	jne    ffffffff812c5a6e <radix_tree_locate_item+0x99>
				break;
			index &= ~((1UL << shift) - 1);
ffffffff812c5a5b:	4c 21 cb             	and    %r9,%rbx
			index += 1UL << shift;
			if (index == 0)
ffffffff812c5a5e:	48 01 fb             	add    %rdi,%rbx
ffffffff812c5a61:	74 3f                	je     ffffffff812c5aa2 <radix_tree_locate_item+0xcd>
				goto out;	/* 32-bit wraparound */
			i++;
ffffffff812c5a63:	48 ff c2             	inc    %rdx
			if (i == RADIX_TREE_MAP_SIZE)
ffffffff812c5a66:	48 83 fa 40          	cmp    $0x40,%rdx
ffffffff812c5a6a:	75 e7                	jne    ffffffff812c5a53 <radix_tree_locate_item+0x7e>
ffffffff812c5a6c:	eb 34                	jmp    ffffffff812c5aa2 <radix_tree_locate_item+0xcd>
				goto out;
		}

		shift -= RADIX_TREE_MAP_SHIFT;
		slot = rcu_dereference_raw(slot->slots[i]);
ffffffff812c5a6e:	48 83 c2 04          	add    $0x4,%rdx
			i++;
			if (i == RADIX_TREE_MAP_SIZE)
				goto out;
		}

		shift -= RADIX_TREE_MAP_SHIFT;
ffffffff812c5a72:	83 e9 06             	sub    $0x6,%ecx
		slot = rcu_dereference_raw(slot->slots[i]);
ffffffff812c5a75:	48 8b 44 d0 08       	mov    0x8(%rax,%rdx,8),%rax
		if (slot == NULL)
ffffffff812c5a7a:	48 85 c0             	test   %rax,%rax
ffffffff812c5a7d:	74 23                	je     ffffffff812c5aa2 <radix_tree_locate_item+0xcd>
	unsigned long i;

	height = slot->path & RADIX_TREE_HEIGHT_MASK;
	shift = (height-1) * RADIX_TREE_MAP_SHIFT;

	for ( ; height > 1; height--) {
ffffffff812c5a7f:	41 ff c8             	dec    %r8d
ffffffff812c5a82:	eb b4                	jmp    ffffffff812c5a38 <radix_tree_locate_item+0x63>
ffffffff812c5a84:	31 d2                	xor    %edx,%edx
			goto out;
	}

	/* Bottom level: check items */
	for (i = 0; i < RADIX_TREE_MAP_SIZE; i++) {
		if (slot->slots[i] == item) {
ffffffff812c5a86:	48 3b 74 d0 28       	cmp    0x28(%rax,%rdx,8),%rsi
ffffffff812c5a8b:	75 08                	jne    ffffffff812c5a95 <radix_tree_locate_item+0xc0>
			*found_index = index + i;
ffffffff812c5a8d:	4c 8d 2c 13          	lea    (%rbx,%rdx,1),%r13
			index = 0;
ffffffff812c5a91:	31 db                	xor    %ebx,%ebx
ffffffff812c5a93:	eb 0d                	jmp    ffffffff812c5aa2 <radix_tree_locate_item+0xcd>
		if (slot == NULL)
			goto out;
	}

	/* Bottom level: check items */
	for (i = 0; i < RADIX_TREE_MAP_SIZE; i++) {
ffffffff812c5a95:	48 ff c2             	inc    %rdx
ffffffff812c5a98:	48 83 fa 40          	cmp    $0x40,%rdx
ffffffff812c5a9c:	75 e8                	jne    ffffffff812c5a86 <radix_tree_locate_item+0xb1>
			*found_index = index + i;
			index = 0;
			goto out;
		}
	}
	index += RADIX_TREE_MAP_SIZE;
ffffffff812c5a9e:	48 83 c3 40          	add    $0x40,%rbx
ffffffff812c5aa2:	48 89 75 c8          	mov    %rsi,-0x38(%rbp)
			break;
		}

		cur_index = __locate(node, item, cur_index, &found_index);
		rcu_read_unlock();
		cond_resched();
ffffffff812c5aa6:	e8 c5 ff 16 00       	callq  ffffffff81435a70 <_cond_resched>
	} while (cur_index != 0 && cur_index <= max_index);
ffffffff812c5aab:	48 85 db             	test   %rbx,%rbx
ffffffff812c5aae:	74 0d                	je     ffffffff812c5abd <radix_tree_locate_item+0xe8>
ffffffff812c5ab0:	49 39 de             	cmp    %rbx,%r14
ffffffff812c5ab3:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
ffffffff812c5ab7:	0f 83 38 ff ff ff    	jae    ffffffff812c59f5 <radix_tree_locate_item+0x20>

	return found_index;
}
ffffffff812c5abd:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c5ac1:	4c 89 e8             	mov    %r13,%rax
ffffffff812c5ac4:	5b                   	pop    %rbx
ffffffff812c5ac5:	41 5c                	pop    %r12
ffffffff812c5ac7:	41 5d                	pop    %r13
ffffffff812c5ac9:	41 5e                	pop    %r14
ffffffff812c5acb:	41 5f                	pop    %r15
ffffffff812c5acd:	5d                   	pop    %rbp
ffffffff812c5ace:	c3                   	retq   

ffffffff812c5acf <__radix_tree_delete_node>:
 *
 *	Returns %true if @node was freed, %false otherwise.
 */
bool __radix_tree_delete_node(struct radix_tree_root *root,
			      struct radix_tree_node *node)
{
ffffffff812c5acf:	55                   	push   %rbp
ffffffff812c5ad0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5ad3:	41 55                	push   %r13
ffffffff812c5ad5:	41 54                	push   %r12
ffffffff812c5ad7:	53                   	push   %rbx
ffffffff812c5ad8:	51                   	push   %rcx
ffffffff812c5ad9:	48 89 fb             	mov    %rdi,%rbx
	bool deleted = false;
ffffffff812c5adc:	45 31 ed             	xor    %r13d,%r13d

	do {
		struct radix_tree_node *parent;

		if (node->count) {
ffffffff812c5adf:	83 7e 04 00          	cmpl   $0x0,0x4(%rsi)
ffffffff812c5ae3:	74 7a                	je     ffffffff812c5b5f <__radix_tree_delete_node+0x90>
			if (node == indirect_to_ptr(root->rnode)) {
ffffffff812c5ae5:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812c5ae9:	48 83 e0 fe          	and    $0xfffffffffffffffe,%rax
ffffffff812c5aed:	48 39 c6             	cmp    %rax,%rsi
ffffffff812c5af0:	0f 85 bc 00 00 00    	jne    ffffffff812c5bb2 <__radix_tree_delete_node+0xe3>
 *	@root		radix tree root
 */
static inline void radix_tree_shrink(struct radix_tree_root *root)
{
	/* try to shrink tree height */
	while (root->height > 0) {
ffffffff812c5af6:	8b 03                	mov    (%rbx),%eax
ffffffff812c5af8:	85 c0                	test   %eax,%eax
ffffffff812c5afa:	74 5e                	je     ffffffff812c5b5a <__radix_tree_delete_node+0x8b>
		struct radix_tree_node *to_free = root->rnode;
ffffffff812c5afc:	48 8b 7b 08          	mov    0x8(%rbx),%rdi
		struct radix_tree_node *slot;

		BUG_ON(!radix_tree_is_indirect_ptr(to_free));
ffffffff812c5b00:	40 f6 c7 01          	test   $0x1,%dil
ffffffff812c5b04:	75 02                	jne    ffffffff812c5b08 <__radix_tree_delete_node+0x39>
ffffffff812c5b06:	0f 0b                	ud2    
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
}

static inline void *indirect_to_ptr(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INDIRECT_PTR);
ffffffff812c5b08:	48 83 e7 fe          	and    $0xfffffffffffffffe,%rdi

		/*
		 * The candidate node has more than one child, or its child
		 * is not at the leftmost slot, we cannot shrink.
		 */
		if (to_free->count != 1)
ffffffff812c5b0c:	83 7f 04 01          	cmpl   $0x1,0x4(%rdi)
ffffffff812c5b10:	0f 85 9c 00 00 00    	jne    ffffffff812c5bb2 <__radix_tree_delete_node+0xe3>
			break;
		if (!to_free->slots[0])
ffffffff812c5b16:	48 8b 57 28          	mov    0x28(%rdi),%rdx
ffffffff812c5b1a:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5b1d:	0f 84 8f 00 00 00    	je     ffffffff812c5bb2 <__radix_tree_delete_node+0xe3>
		 * was safe to dereference the old pointer to it
		 * (to_free->slots[0]), it will be safe to dereference the new
		 * one (root->rnode) as far as dependent read barriers go.
		 */
		slot = to_free->slots[0];
		if (root->height > 1) {
ffffffff812c5b23:	83 f8 01             	cmp    $0x1,%eax
ffffffff812c5b26:	76 0c                	jbe    ffffffff812c5b34 <__radix_tree_delete_node+0x65>
			slot->parent = NULL;
ffffffff812c5b28:	48 c7 42 08 00 00 00 	movq   $0x0,0x8(%rdx)
ffffffff812c5b2f:	00 
};
static DEFINE_PER_CPU(struct radix_tree_preload, radix_tree_preloads) = { 0, };

static inline void *ptr_to_indirect(void *ptr)
{
	return (void *)((unsigned long)ptr | RADIX_TREE_INDIRECT_PTR);
ffffffff812c5b30:	48 83 ca 01          	or     $0x1,%rdx
		if (root->height > 1) {
			slot->parent = NULL;
			slot = ptr_to_indirect(slot);
		}
		root->rnode = slot;
		root->height--;
ffffffff812c5b34:	8b 03                	mov    (%rbx),%eax
		slot = to_free->slots[0];
		if (root->height > 1) {
			slot->parent = NULL;
			slot = ptr_to_indirect(slot);
		}
		root->rnode = slot;
ffffffff812c5b36:	48 89 53 08          	mov    %rdx,0x8(%rbx)
		root->height--;
ffffffff812c5b3a:	8d 50 ff             	lea    -0x1(%rax),%edx
		 * to retry the entire slot lookup -- the indirect pointer
		 * problem (replacing direct root node with an indirect pointer
		 * also results in a stale slot). So tag the slot as indirect
		 * to force callers to retry.
		 */
		if (root->height == 0)
ffffffff812c5b3d:	85 d2                	test   %edx,%edx
		if (root->height > 1) {
			slot->parent = NULL;
			slot = ptr_to_indirect(slot);
		}
		root->rnode = slot;
		root->height--;
ffffffff812c5b3f:	89 13                	mov    %edx,(%rbx)
		 * to retry the entire slot lookup -- the indirect pointer
		 * problem (replacing direct root node with an indirect pointer
		 * also results in a stale slot). So tag the slot as indirect
		 * to force callers to retry.
		 */
		if (root->height == 0)
ffffffff812c5b41:	75 05                	jne    ffffffff812c5b48 <__radix_tree_delete_node+0x79>
			*((unsigned long *)&to_free->slots[0]) |=
ffffffff812c5b43:	48 83 4f 28 01       	orq    $0x1,0x28(%rdi)
}

static inline void
radix_tree_node_free(struct radix_tree_node *node)
{
	call_rcu(&node->rcu_head, radix_tree_node_rcu_free);
ffffffff812c5b48:	48 83 c7 08          	add    $0x8,%rdi
ffffffff812c5b4c:	48 c7 c6 9e 4c 2c 81 	mov    $0xffffffff812c4c9e,%rsi
ffffffff812c5b53:	e8 5b 6b dd ff       	callq  ffffffff8109c6b3 <call_rcu_sched>
ffffffff812c5b58:	eb 9c                	jmp    ffffffff812c5af6 <__radix_tree_delete_node+0x27>

		if (node->count) {
			if (node == indirect_to_ptr(root->rnode)) {
				radix_tree_shrink(root);
				if (root->height == 0)
					deleted = true;
ffffffff812c5b5a:	41 b5 01             	mov    $0x1,%r13b
ffffffff812c5b5d:	eb 53                	jmp    ffffffff812c5bb2 <__radix_tree_delete_node+0xe3>
			}
			return deleted;
		}

		parent = node->parent;
ffffffff812c5b5f:	4c 8b 66 08          	mov    0x8(%rsi),%r12
		if (parent) {
ffffffff812c5b63:	4d 85 e4             	test   %r12,%r12
ffffffff812c5b66:	74 15                	je     ffffffff812c5b7d <__radix_tree_delete_node+0xae>
			unsigned int offset;

			offset = node->path >> RADIX_TREE_HEIGHT_SHIFT;
			parent->slots[offset] = NULL;
ffffffff812c5b68:	8b 06                	mov    (%rsi),%eax
ffffffff812c5b6a:	c1 e8 0c             	shr    $0xc,%eax
ffffffff812c5b6d:	49 c7 44 c4 28 00 00 	movq   $0x0,0x28(%r12,%rax,8)
ffffffff812c5b74:	00 00 
			parent->count--;
ffffffff812c5b76:	41 ff 4c 24 04       	decl   0x4(%r12)
ffffffff812c5b7b:	eb 15                	jmp    ffffffff812c5b92 <__radix_tree_delete_node+0xc3>
	root->gfp_mask &= (__force gfp_t)~(1 << (tag + __GFP_BITS_SHIFT));
}

static inline void root_tag_clear_all(struct radix_tree_root *root)
{
	root->gfp_mask &= __GFP_BITS_MASK;
ffffffff812c5b7d:	81 63 04 ff ff ff 01 	andl   $0x1ffffff,0x4(%rbx)
			offset = node->path >> RADIX_TREE_HEIGHT_SHIFT;
			parent->slots[offset] = NULL;
			parent->count--;
		} else {
			root_tag_clear_all(root);
			root->height = 0;
ffffffff812c5b84:	c7 03 00 00 00 00    	movl   $0x0,(%rbx)
			root->rnode = NULL;
ffffffff812c5b8a:	48 c7 43 08 00 00 00 	movq   $0x0,0x8(%rbx)
ffffffff812c5b91:	00 
}

static inline void
radix_tree_node_free(struct radix_tree_node *node)
{
	call_rcu(&node->rcu_head, radix_tree_node_rcu_free);
ffffffff812c5b92:	48 8d 7e 08          	lea    0x8(%rsi),%rdi
ffffffff812c5b96:	48 c7 c6 9e 4c 2c 81 	mov    $0xffffffff812c4c9e,%rsi
			root->height = 0;
			root->rnode = NULL;
		}

		radix_tree_node_free(node);
		deleted = true;
ffffffff812c5b9d:	41 b5 01             	mov    $0x1,%r13b
}

static inline void
radix_tree_node_free(struct radix_tree_node *node)
{
	call_rcu(&node->rcu_head, radix_tree_node_rcu_free);
ffffffff812c5ba0:	e8 0e 6b dd ff       	callq  ffffffff8109c6b3 <call_rcu_sched>

		radix_tree_node_free(node);
		deleted = true;

		node = parent;
	} while (node);
ffffffff812c5ba5:	4d 85 e4             	test   %r12,%r12
ffffffff812c5ba8:	74 b0                	je     ffffffff812c5b5a <__radix_tree_delete_node+0x8b>
ffffffff812c5baa:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c5bad:	e9 2d ff ff ff       	jmpq   ffffffff812c5adf <__radix_tree_delete_node+0x10>

	return deleted;
}
ffffffff812c5bb2:	5a                   	pop    %rdx
ffffffff812c5bb3:	44 88 e8             	mov    %r13b,%al
ffffffff812c5bb6:	5b                   	pop    %rbx
ffffffff812c5bb7:	41 5c                	pop    %r12
ffffffff812c5bb9:	41 5d                	pop    %r13
ffffffff812c5bbb:	5d                   	pop    %rbp
ffffffff812c5bbc:	c3                   	retq   

ffffffff812c5bbd <radix_tree_delete_item>:
 *	Returns the address of the deleted item, or NULL if it was not present
 *	or the entry at the given @index was not @item.
 */
void *radix_tree_delete_item(struct radix_tree_root *root,
			     unsigned long index, void *item)
{
ffffffff812c5bbd:	55                   	push   %rbp
ffffffff812c5bbe:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5bc1:	41 56                	push   %r14
ffffffff812c5bc3:	41 55                	push   %r13
ffffffff812c5bc5:	41 54                	push   %r12
ffffffff812c5bc7:	53                   	push   %rbx
ffffffff812c5bc8:	49 89 d6             	mov    %rdx,%r14
	unsigned int offset;
	void **slot;
	void *entry;
	int tag;

	entry = __radix_tree_lookup(root, index, &node, &slot);
ffffffff812c5bcb:	48 8d 4d d8          	lea    -0x28(%rbp),%rcx
ffffffff812c5bcf:	48 8d 55 d0          	lea    -0x30(%rbp),%rdx
 *	Returns the address of the deleted item, or NULL if it was not present
 *	or the entry at the given @index was not @item.
 */
void *radix_tree_delete_item(struct radix_tree_root *root,
			     unsigned long index, void *item)
{
ffffffff812c5bd3:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c5bd6:	48 83 ec 10          	sub    $0x10,%rsp
ffffffff812c5bda:	49 89 f5             	mov    %rsi,%r13
	unsigned int offset;
	void **slot;
	void *entry;
	int tag;

	entry = __radix_tree_lookup(root, index, &node, &slot);
ffffffff812c5bdd:	e8 21 fd ff ff       	callq  ffffffff812c5903 <__radix_tree_lookup>
ffffffff812c5be2:	49 89 c4             	mov    %rax,%r12
	if (!entry)
		return NULL;
ffffffff812c5be5:	31 c0                	xor    %eax,%eax
	void **slot;
	void *entry;
	int tag;

	entry = __radix_tree_lookup(root, index, &node, &slot);
	if (!entry)
ffffffff812c5be7:	4d 85 e4             	test   %r12,%r12
ffffffff812c5bea:	74 75                	je     ffffffff812c5c61 <radix_tree_delete_item+0xa4>
		return NULL;

	if (item && entry != item)
ffffffff812c5bec:	4d 85 f6             	test   %r14,%r14
ffffffff812c5bef:	74 05                	je     ffffffff812c5bf6 <radix_tree_delete_item+0x39>
ffffffff812c5bf1:	4d 39 f4             	cmp    %r14,%r12
ffffffff812c5bf4:	75 6b                	jne    ffffffff812c5c61 <radix_tree_delete_item+0xa4>
		root_tag_clear_all(root);
		root->rnode = NULL;
		return entry;
	}

	offset = index & RADIX_TREE_MAP_MASK;
ffffffff812c5bf6:	45 89 eb             	mov    %r13d,%r11d

	/*
	 * Clear all tags associated with the item to be deleted.
	 * This way of doing it would be inefficient, but seldom is any set.
	 */
	for (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++) {
ffffffff812c5bf9:	45 31 d2             	xor    %r10d,%r10d
		root_tag_clear_all(root);
		root->rnode = NULL;
		return entry;
	}

	offset = index & RADIX_TREE_MAP_MASK;
ffffffff812c5bfc:	41 83 e3 3f          	and    $0x3f,%r11d
		return NULL;

	if (item && entry != item)
		return NULL;

	if (!node) {
ffffffff812c5c00:	48 83 7d d0 00       	cmpq   $0x0,-0x30(%rbp)
ffffffff812c5c05:	75 11                	jne    ffffffff812c5c18 <radix_tree_delete_item+0x5b>
	root->gfp_mask &= (__force gfp_t)~(1 << (tag + __GFP_BITS_SHIFT));
}

static inline void root_tag_clear_all(struct radix_tree_root *root)
{
	root->gfp_mask &= __GFP_BITS_MASK;
ffffffff812c5c07:	81 63 04 ff ff ff 01 	andl   $0x1ffffff,0x4(%rbx)
	if (item && entry != item)
		return NULL;

	if (!node) {
		root_tag_clear_all(root);
		root->rnode = NULL;
ffffffff812c5c0e:	48 c7 43 08 00 00 00 	movq   $0x0,0x8(%rbx)
ffffffff812c5c15:	00 
ffffffff812c5c16:	eb 46                	jmp    ffffffff812c5c5e <radix_tree_delete_item+0xa1>
	/*
	 * Clear all tags associated with the item to be deleted.
	 * This way of doing it would be inefficient, but seldom is any set.
	 */
	for (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++) {
		if (tag_get(node, tag, offset))
ffffffff812c5c18:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
ffffffff812c5c1c:	44 89 da             	mov    %r11d,%edx
ffffffff812c5c1f:	44 89 d6             	mov    %r10d,%esi
ffffffff812c5c22:	e8 14 f1 ff ff       	callq  ffffffff812c4d3b <tag_get>
ffffffff812c5c27:	85 c0                	test   %eax,%eax
ffffffff812c5c29:	74 0e                	je     ffffffff812c5c39 <radix_tree_delete_item+0x7c>
			radix_tree_tag_clear(root, index, tag);
ffffffff812c5c2b:	44 89 d2             	mov    %r10d,%edx
ffffffff812c5c2e:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c5c31:	48 89 df             	mov    %rbx,%rdi
ffffffff812c5c34:	e8 c0 f5 ff ff       	callq  ffffffff812c51f9 <radix_tree_tag_clear>

	/*
	 * Clear all tags associated with the item to be deleted.
	 * This way of doing it would be inefficient, but seldom is any set.
	 */
	for (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++) {
ffffffff812c5c39:	41 ff c2             	inc    %r10d
ffffffff812c5c3c:	41 83 fa 03          	cmp    $0x3,%r10d
ffffffff812c5c40:	75 d6                	jne    ffffffff812c5c18 <radix_tree_delete_item+0x5b>
		if (tag_get(node, tag, offset))
			radix_tree_tag_clear(root, index, tag);
	}

	node->slots[offset] = NULL;
ffffffff812c5c42:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
	node->count--;

	__radix_tree_delete_node(root, node);
ffffffff812c5c46:	48 89 df             	mov    %rbx,%rdi
	for (tag = 0; tag < RADIX_TREE_MAX_TAGS; tag++) {
		if (tag_get(node, tag, offset))
			radix_tree_tag_clear(root, index, tag);
	}

	node->slots[offset] = NULL;
ffffffff812c5c49:	4a c7 44 d8 28 00 00 	movq   $0x0,0x28(%rax,%r11,8)
ffffffff812c5c50:	00 00 
	node->count--;
ffffffff812c5c52:	ff 48 04             	decl   0x4(%rax)

	__radix_tree_delete_node(root, node);
ffffffff812c5c55:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
ffffffff812c5c59:	e8 71 fe ff ff       	callq  ffffffff812c5acf <__radix_tree_delete_node>

	return entry;
ffffffff812c5c5e:	4c 89 e0             	mov    %r12,%rax
}
ffffffff812c5c61:	5a                   	pop    %rdx
ffffffff812c5c62:	59                   	pop    %rcx
ffffffff812c5c63:	5b                   	pop    %rbx
ffffffff812c5c64:	41 5c                	pop    %r12
ffffffff812c5c66:	41 5d                	pop    %r13
ffffffff812c5c68:	41 5e                	pop    %r14
ffffffff812c5c6a:	5d                   	pop    %rbp
ffffffff812c5c6b:	c3                   	retq   

ffffffff812c5c6c <radix_tree_delete>:
 *	Remove the item at @index from the radix tree rooted at @root.
 *
 *	Returns the address of the deleted item, or NULL if it was not present.
 */
void *radix_tree_delete(struct radix_tree_root *root, unsigned long index)
{
ffffffff812c5c6c:	55                   	push   %rbp
	return radix_tree_delete_item(root, index, NULL);
ffffffff812c5c6d:	31 d2                	xor    %edx,%edx
 *	Remove the item at @index from the radix tree rooted at @root.
 *
 *	Returns the address of the deleted item, or NULL if it was not present.
 */
void *radix_tree_delete(struct radix_tree_root *root, unsigned long index)
{
ffffffff812c5c6f:	48 89 e5             	mov    %rsp,%rbp
	return radix_tree_delete_item(root, index, NULL);
ffffffff812c5c72:	e8 46 ff ff ff       	callq  ffffffff812c5bbd <radix_tree_delete_item>
}
ffffffff812c5c77:	5d                   	pop    %rbp
ffffffff812c5c78:	c3                   	retq   

ffffffff812c5c79 <___ratelimit>:
int ___ratelimit(struct ratelimit_state *rs, const char *func)
{
	unsigned long flags;
	int ret;

	if (!rs->interval)
ffffffff812c5c79:	83 7f 18 00          	cmpl   $0x0,0x18(%rdi)
		return 1;
ffffffff812c5c7d:	b8 01 00 00 00       	mov    $0x1,%eax
int ___ratelimit(struct ratelimit_state *rs, const char *func)
{
	unsigned long flags;
	int ret;

	if (!rs->interval)
ffffffff812c5c82:	0f 84 af 00 00 00    	je     ffffffff812c5d37 <___ratelimit+0xbe>
 * RETURNS:
 * 0 means callbacks will be suppressed.
 * 1 means go ahead and do it.
 */
int ___ratelimit(struct ratelimit_state *rs, const char *func)
{
ffffffff812c5c88:	55                   	push   %rbp
ffffffff812c5c89:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5c8c:	41 55                	push   %r13
ffffffff812c5c8e:	41 54                	push   %r12
ffffffff812c5c90:	49 89 f4             	mov    %rsi,%r12
ffffffff812c5c93:	53                   	push   %rbx
ffffffff812c5c94:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c5c97:	51                   	push   %rcx
	/*
	 * "=rm" is safe here, because "pop" adjusts the stack before
	 * it evaluates its effective address -- this is part of the
	 * documented behavior of the "pop" instruction.
	 */
	asm volatile("# __raw_save_flags\n\t"
ffffffff812c5c98:	9c                   	pushfq 
ffffffff812c5c99:	41 5d                	pop    %r13
		     :"memory", "cc");
}

static inline void native_irq_disable(void)
{
	asm volatile("cli": : :"memory");
ffffffff812c5c9b:	fa                   	cli    
	 * If we contend on this state's lock then almost
	 * by definition we are too busy to print a message,
	 * in addition to the one that will be printed by
	 * the entity that is holding the lock already:
	 */
	if (!raw_spin_trylock_irqsave(&rs->lock, flags))
ffffffff812c5c9c:	e8 bc 29 17 00       	callq  ffffffff8143865d <_raw_spin_trylock>
ffffffff812c5ca1:	85 c0                	test   %eax,%eax
ffffffff812c5ca3:	75 08                	jne    ffffffff812c5cad <___ratelimit+0x34>
	return flags;
}

static inline void native_restore_fl(unsigned long flags)
{
	asm volatile("push %0 ; popf"
ffffffff812c5ca5:	41 55                	push   %r13
ffffffff812c5ca7:	9d                   	popfq  
ffffffff812c5ca8:	e9 83 00 00 00       	jmpq   ffffffff812c5d30 <___ratelimit+0xb7>
		return 0;

	if (!rs->begin)
ffffffff812c5cad:	48 83 7b 28 00       	cmpq   $0x0,0x28(%rbx)
ffffffff812c5cb2:	75 0b                	jne    ffffffff812c5cbf <___ratelimit+0x46>
		rs->begin = jiffies;
ffffffff812c5cb4:	48 8b 05 45 13 74 00 	mov    0x741345(%rip),%rax        # ffffffff81a07000 <jiffies>
ffffffff812c5cbb:	48 89 43 28          	mov    %rax,0x28(%rbx)

	if (time_is_before_jiffies(rs->begin + rs->interval)) {
ffffffff812c5cbf:	48 63 43 18          	movslq 0x18(%rbx),%rax
ffffffff812c5cc3:	48 8b 15 36 13 74 00 	mov    0x741336(%rip),%rdx        # ffffffff81a07000 <jiffies>
ffffffff812c5cca:	48 29 d0             	sub    %rdx,%rax
ffffffff812c5ccd:	48 03 43 28          	add    0x28(%rbx),%rax
ffffffff812c5cd1:	79 2e                	jns    ffffffff812c5d01 <___ratelimit+0x88>
		if (rs->missed)
ffffffff812c5cd3:	8b 53 24             	mov    0x24(%rbx),%edx
ffffffff812c5cd6:	85 d2                	test   %edx,%edx
ffffffff812c5cd8:	74 11                	je     ffffffff812c5ceb <___ratelimit+0x72>
			printk(KERN_WARNING "%s: %d callbacks suppressed\n",
ffffffff812c5cda:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c5cdd:	48 c7 c7 ae 6c 7b 81 	mov    $0xffffffff817b6cae,%rdi
ffffffff812c5ce4:	31 c0                	xor    %eax,%eax
ffffffff812c5ce6:	e8 a7 d2 16 00       	callq  ffffffff81432f92 <printk>
				func, rs->missed);
		rs->begin   = 0;
ffffffff812c5ceb:	48 c7 43 28 00 00 00 	movq   $0x0,0x28(%rbx)
ffffffff812c5cf2:	00 
		rs->printed = 0;
ffffffff812c5cf3:	c7 43 20 00 00 00 00 	movl   $0x0,0x20(%rbx)
		rs->missed  = 0;
ffffffff812c5cfa:	c7 43 24 00 00 00 00 	movl   $0x0,0x24(%rbx)
	}
	if (rs->burst && rs->burst > rs->printed) {
ffffffff812c5d01:	8b 53 1c             	mov    0x1c(%rbx),%edx
ffffffff812c5d04:	85 d2                	test   %edx,%edx
ffffffff812c5d06:	74 14                	je     ffffffff812c5d1c <___ratelimit+0xa3>
ffffffff812c5d08:	8b 43 20             	mov    0x20(%rbx),%eax
ffffffff812c5d0b:	39 c2                	cmp    %eax,%edx
ffffffff812c5d0d:	7e 0d                	jle    ffffffff812c5d1c <___ratelimit+0xa3>
		rs->printed++;
ffffffff812c5d0f:	ff c0                	inc    %eax
		ret = 1;
ffffffff812c5d11:	41 bc 01 00 00 00    	mov    $0x1,%r12d
		rs->begin   = 0;
		rs->printed = 0;
		rs->missed  = 0;
	}
	if (rs->burst && rs->burst > rs->printed) {
		rs->printed++;
ffffffff812c5d17:	89 43 20             	mov    %eax,0x20(%rbx)
ffffffff812c5d1a:	eb 06                	jmp    ffffffff812c5d22 <___ratelimit+0xa9>
		ret = 1;
	} else {
		rs->missed++;
ffffffff812c5d1c:	ff 43 24             	incl   0x24(%rbx)
		ret = 0;
ffffffff812c5d1f:	45 31 e4             	xor    %r12d,%r12d
	}
	raw_spin_unlock_irqrestore(&rs->lock, flags);
ffffffff812c5d22:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c5d25:	48 89 df             	mov    %rbx,%rdi
ffffffff812c5d28:	e8 af 29 17 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>

	return ret;
ffffffff812c5d2d:	44 89 e0             	mov    %r12d,%eax
}
ffffffff812c5d30:	5a                   	pop    %rdx
ffffffff812c5d31:	5b                   	pop    %rbx
ffffffff812c5d32:	41 5c                	pop    %r12
ffffffff812c5d34:	41 5d                	pop    %r13
ffffffff812c5d36:	5d                   	pop    %rbp
ffffffff812c5d37:	c3                   	retq   

ffffffff812c5d38 <rb_first>:
 */
struct rb_node *rb_first(const struct rb_root *root)
{
	struct rb_node	*n;

	n = root->rb_node;
ffffffff812c5d38:	48 8b 17             	mov    (%rdi),%rdx

/*
 * This function returns the first node (in sort order) of the tree.
 */
struct rb_node *rb_first(const struct rb_root *root)
{
ffffffff812c5d3b:	55                   	push   %rbp
ffffffff812c5d3c:	31 c0                	xor    %eax,%eax
ffffffff812c5d3e:	48 89 e5             	mov    %rsp,%rbp
	struct rb_node	*n;

	n = root->rb_node;
	if (!n)
ffffffff812c5d41:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5d44:	74 11                	je     ffffffff812c5d57 <rb_first+0x1f>
		return NULL;
	while (n->rb_left)
ffffffff812c5d46:	48 8b 42 10          	mov    0x10(%rdx),%rax
ffffffff812c5d4a:	48 85 c0             	test   %rax,%rax
ffffffff812c5d4d:	74 05                	je     ffffffff812c5d54 <rb_first+0x1c>
ffffffff812c5d4f:	48 89 c2             	mov    %rax,%rdx
ffffffff812c5d52:	eb f2                	jmp    ffffffff812c5d46 <rb_first+0xe>
ffffffff812c5d54:	48 89 d0             	mov    %rdx,%rax
		n = n->rb_left;
	return n;
}
ffffffff812c5d57:	5d                   	pop    %rbp
ffffffff812c5d58:	c3                   	retq   

ffffffff812c5d59 <rb_last>:

struct rb_node *rb_last(const struct rb_root *root)
{
	struct rb_node	*n;

	n = root->rb_node;
ffffffff812c5d59:	48 8b 17             	mov    (%rdi),%rdx
	return n;
}
EXPORT_SYMBOL(rb_first);

struct rb_node *rb_last(const struct rb_root *root)
{
ffffffff812c5d5c:	55                   	push   %rbp
ffffffff812c5d5d:	31 c0                	xor    %eax,%eax
ffffffff812c5d5f:	48 89 e5             	mov    %rsp,%rbp
	struct rb_node	*n;

	n = root->rb_node;
	if (!n)
ffffffff812c5d62:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5d65:	74 11                	je     ffffffff812c5d78 <rb_last+0x1f>
		return NULL;
	while (n->rb_right)
ffffffff812c5d67:	48 8b 42 08          	mov    0x8(%rdx),%rax
ffffffff812c5d6b:	48 85 c0             	test   %rax,%rax
ffffffff812c5d6e:	74 05                	je     ffffffff812c5d75 <rb_last+0x1c>
ffffffff812c5d70:	48 89 c2             	mov    %rax,%rdx
ffffffff812c5d73:	eb f2                	jmp    ffffffff812c5d67 <rb_last+0xe>
ffffffff812c5d75:	48 89 d0             	mov    %rdx,%rax
		n = n->rb_right;
	return n;
}
ffffffff812c5d78:	5d                   	pop    %rbp
ffffffff812c5d79:	c3                   	retq   

ffffffff812c5d7a <rb_next_postorder>:
			return (struct rb_node *)node;
	}
}

struct rb_node *rb_next_postorder(const struct rb_node *node)
{
ffffffff812c5d7a:	55                   	push   %rbp
	const struct rb_node *parent;
	if (!node)
		return NULL;
ffffffff812c5d7b:	31 c0                	xor    %eax,%eax
}

struct rb_node *rb_next_postorder(const struct rb_node *node)
{
	const struct rb_node *parent;
	if (!node)
ffffffff812c5d7d:	48 85 ff             	test   %rdi,%rdi
			return (struct rb_node *)node;
	}
}

struct rb_node *rb_next_postorder(const struct rb_node *node)
{
ffffffff812c5d80:	48 89 e5             	mov    %rsp,%rbp
	const struct rb_node *parent;
	if (!node)
ffffffff812c5d83:	74 29                	je     ffffffff812c5dae <rb_next_postorder+0x34>
		return NULL;
	parent = rb_parent(node);

	/* If we're sitting on node, we've already seen our children */
	if (parent && node == parent->rb_left && parent->rb_right) {
ffffffff812c5d85:	48 8b 17             	mov    (%rdi),%rdx
ffffffff812c5d88:	48 83 e2 fc          	and    $0xfffffffffffffffc,%rdx
ffffffff812c5d8c:	74 20                	je     ffffffff812c5dae <rb_next_postorder+0x34>
ffffffff812c5d8e:	48 3b 7a 10          	cmp    0x10(%rdx),%rdi
ffffffff812c5d92:	48 89 d0             	mov    %rdx,%rax
ffffffff812c5d95:	75 17                	jne    ffffffff812c5dae <rb_next_postorder+0x34>
ffffffff812c5d97:	48 8b 50 08          	mov    0x8(%rax),%rdx
ffffffff812c5d9b:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5d9e:	74 0e                	je     ffffffff812c5dae <rb_next_postorder+0x34>
EXPORT_SYMBOL(rb_replace_node);

static struct rb_node *rb_left_deepest_node(const struct rb_node *node)
{
	for (;;) {
		if (node->rb_left)
ffffffff812c5da0:	48 89 d0             	mov    %rdx,%rax
ffffffff812c5da3:	48 8b 52 10          	mov    0x10(%rdx),%rdx
ffffffff812c5da7:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5daa:	75 f4                	jne    ffffffff812c5da0 <rb_next_postorder+0x26>
ffffffff812c5dac:	eb e9                	jmp    ffffffff812c5d97 <rb_next_postorder+0x1d>
		return rb_left_deepest_node(parent->rb_right);
	} else
		/* Otherwise we are the parent's right node, and the parent
		 * should be next */
		return (struct rb_node *)parent;
}
ffffffff812c5dae:	5d                   	pop    %rbp
ffffffff812c5daf:	c3                   	retq   

ffffffff812c5db0 <rb_next>:
	return n;
}
EXPORT_SYMBOL(rb_last);

struct rb_node *rb_next(const struct rb_node *node)
{
ffffffff812c5db0:	55                   	push   %rbp
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
		return NULL;
ffffffff812c5db1:	31 c0                	xor    %eax,%eax

struct rb_node *rb_next(const struct rb_node *node)
{
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
ffffffff812c5db3:	48 3b 3f             	cmp    (%rdi),%rdi
	return n;
}
EXPORT_SYMBOL(rb_last);

struct rb_node *rb_next(const struct rb_node *node)
{
ffffffff812c5db6:	48 89 e5             	mov    %rsp,%rbp
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
ffffffff812c5db9:	74 2b                	je     ffffffff812c5de6 <rb_next+0x36>

	/*
	 * If we have a right-hand child, go down and then left as far
	 * as we can.
	 */
	if (node->rb_right) {
ffffffff812c5dbb:	48 8b 47 08          	mov    0x8(%rdi),%rax
ffffffff812c5dbf:	48 85 c0             	test   %rax,%rax
ffffffff812c5dc2:	74 17                	je     ffffffff812c5ddb <rb_next+0x2b>
		node = node->rb_right; 
		while (node->rb_left)
ffffffff812c5dc4:	48 8b 50 10          	mov    0x10(%rax),%rdx
ffffffff812c5dc8:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5dcb:	74 19                	je     ffffffff812c5de6 <rb_next+0x36>
ffffffff812c5dcd:	48 89 d0             	mov    %rdx,%rax
ffffffff812c5dd0:	eb f2                	jmp    ffffffff812c5dc4 <rb_next+0x14>
	 * so any 'next' node must be in the general direction of our parent.
	 * Go up the tree; any time the ancestor is a right-hand child of its
	 * parent, keep going up. First time it's a left-hand child of its
	 * parent, said parent is our 'next' node.
	 */
	while ((parent = rb_parent(node)) && node == parent->rb_right)
ffffffff812c5dd2:	48 3b 78 08          	cmp    0x8(%rax),%rdi
ffffffff812c5dd6:	75 0e                	jne    ffffffff812c5de6 <rb_next+0x36>
ffffffff812c5dd8:	48 89 c7             	mov    %rax,%rdi
ffffffff812c5ddb:	48 8b 07             	mov    (%rdi),%rax
ffffffff812c5dde:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax
ffffffff812c5de2:	75 ee                	jne    ffffffff812c5dd2 <rb_next+0x22>
struct rb_node *rb_next(const struct rb_node *node)
{
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
		return NULL;
ffffffff812c5de4:	31 c0                	xor    %eax,%eax
	 */
	while ((parent = rb_parent(node)) && node == parent->rb_right)
		node = parent;

	return parent;
}
ffffffff812c5de6:	5d                   	pop    %rbp
ffffffff812c5de7:	c3                   	retq   

ffffffff812c5de8 <rb_prev>:
EXPORT_SYMBOL(rb_next);

struct rb_node *rb_prev(const struct rb_node *node)
{
ffffffff812c5de8:	55                   	push   %rbp
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
		return NULL;
ffffffff812c5de9:	31 c0                	xor    %eax,%eax

struct rb_node *rb_prev(const struct rb_node *node)
{
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
ffffffff812c5deb:	48 3b 3f             	cmp    (%rdi),%rdi
	return parent;
}
EXPORT_SYMBOL(rb_next);

struct rb_node *rb_prev(const struct rb_node *node)
{
ffffffff812c5dee:	48 89 e5             	mov    %rsp,%rbp
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
ffffffff812c5df1:	74 2b                	je     ffffffff812c5e1e <rb_prev+0x36>

	/*
	 * If we have a left-hand child, go down and then right as far
	 * as we can.
	 */
	if (node->rb_left) {
ffffffff812c5df3:	48 8b 47 10          	mov    0x10(%rdi),%rax
ffffffff812c5df7:	48 85 c0             	test   %rax,%rax
ffffffff812c5dfa:	74 17                	je     ffffffff812c5e13 <rb_prev+0x2b>
		node = node->rb_left; 
		while (node->rb_right)
ffffffff812c5dfc:	48 8b 50 08          	mov    0x8(%rax),%rdx
ffffffff812c5e00:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5e03:	74 19                	je     ffffffff812c5e1e <rb_prev+0x36>
ffffffff812c5e05:	48 89 d0             	mov    %rdx,%rax
ffffffff812c5e08:	eb f2                	jmp    ffffffff812c5dfc <rb_prev+0x14>

	/*
	 * No left-hand children. Go up till we find an ancestor which
	 * is a right-hand child of its parent.
	 */
	while ((parent = rb_parent(node)) && node == parent->rb_left)
ffffffff812c5e0a:	48 3b 78 10          	cmp    0x10(%rax),%rdi
ffffffff812c5e0e:	75 0e                	jne    ffffffff812c5e1e <rb_prev+0x36>
ffffffff812c5e10:	48 89 c7             	mov    %rax,%rdi
ffffffff812c5e13:	48 8b 07             	mov    (%rdi),%rax
ffffffff812c5e16:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax
ffffffff812c5e1a:	75 ee                	jne    ffffffff812c5e0a <rb_prev+0x22>
struct rb_node *rb_prev(const struct rb_node *node)
{
	struct rb_node *parent;

	if (RB_EMPTY_NODE(node))
		return NULL;
ffffffff812c5e1c:	31 c0                	xor    %eax,%eax
	 */
	while ((parent = rb_parent(node)) && node == parent->rb_left)
		node = parent;

	return parent;
}
ffffffff812c5e1e:	5d                   	pop    %rbp
ffffffff812c5e1f:	c3                   	retq   

ffffffff812c5e20 <__rb_rotate_set_parents>:
 */
static inline void
__rb_rotate_set_parents(struct rb_node *old, struct rb_node *new,
			struct rb_root *root, int color)
{
	struct rb_node *parent = rb_parent(old);
ffffffff812c5e20:	48 8b 07             	mov    (%rdi),%rax
 * - old gets assigned new as a parent and 'color' as a color.
 */
static inline void
__rb_rotate_set_parents(struct rb_node *old, struct rb_node *new,
			struct rb_root *root, int color)
{
ffffffff812c5e23:	55                   	push   %rbp
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c5e24:	48 63 c9             	movslq %ecx,%rcx
ffffffff812c5e27:	48 09 f1             	or     %rsi,%rcx
ffffffff812c5e2a:	48 89 e5             	mov    %rsp,%rbp
	struct rb_node *parent = rb_parent(old);
	new->__rb_parent_color = old->__rb_parent_color;
ffffffff812c5e2d:	48 89 06             	mov    %rax,(%rsi)

static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
		  struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
ffffffff812c5e30:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c5e34:	48 89 0f             	mov    %rcx,(%rdi)

static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
		  struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
ffffffff812c5e37:	74 12                	je     ffffffff812c5e4b <__rb_rotate_set_parents+0x2b>
		if (parent->rb_left == old)
ffffffff812c5e39:	48 3b 78 10          	cmp    0x10(%rax),%rdi
ffffffff812c5e3d:	75 06                	jne    ffffffff812c5e45 <__rb_rotate_set_parents+0x25>
			parent->rb_left = new;
ffffffff812c5e3f:	48 89 70 10          	mov    %rsi,0x10(%rax)
ffffffff812c5e43:	eb 09                	jmp    ffffffff812c5e4e <__rb_rotate_set_parents+0x2e>
		else
			parent->rb_right = new;
ffffffff812c5e45:	48 89 70 08          	mov    %rsi,0x8(%rax)
ffffffff812c5e49:	eb 03                	jmp    ffffffff812c5e4e <__rb_rotate_set_parents+0x2e>
	} else
		root->rb_node = new;
ffffffff812c5e4b:	48 89 32             	mov    %rsi,(%rdx)
	rb_set_parent_color(old, new, color);
	__rb_change_child(old, new, parent, root);
}
ffffffff812c5e4e:	5d                   	pop    %rbp
ffffffff812c5e4f:	c3                   	retq   

ffffffff812c5e50 <__rb_erase_color>:
}

/* Non-inline version for rb_erase_augmented() use */
void __rb_erase_color(struct rb_node *parent, struct rb_root *root,
	void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
ffffffff812c5e50:	55                   	push   %rbp
ffffffff812c5e51:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c5e54:	41 57                	push   %r15
ffffffff812c5e56:	41 56                	push   %r14
ffffffff812c5e58:	41 55                	push   %r13
ffffffff812c5e5a:	41 54                	push   %r12
ffffffff812c5e5c:	49 89 d5             	mov    %rdx,%r13
ffffffff812c5e5f:	53                   	push   %rbx
ffffffff812c5e60:	51                   	push   %rcx
ffffffff812c5e61:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c5e64:	49 89 f6             	mov    %rsi,%r14
 */
static __always_inline void
____rb_erase_color(struct rb_node *parent, struct rb_root *root,
	void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
	struct rb_node *node = NULL, *sibling, *tmp1, *tmp2;
ffffffff812c5e67:	31 d2                	xor    %edx,%edx
		 * - node is black (or NULL on first iteration)
		 * - node is not the root (parent is not NULL)
		 * - All leaf paths going through parent and node have a
		 *   black node count that is 1 lower than other leaf paths.
		 */
		sibling = parent->rb_right;
ffffffff812c5e69:	4c 8b 63 08          	mov    0x8(%rbx),%r12
		if (node != sibling) {	/* node == parent->rb_left */
ffffffff812c5e6d:	49 39 d4             	cmp    %rdx,%r12
ffffffff812c5e70:	0f 84 a4 00 00 00    	je     ffffffff812c5f1a <__rb_erase_color+0xca>
			if (rb_is_red(sibling)) {
ffffffff812c5e76:	41 f6 04 24 01       	testb  $0x1,(%r12)
ffffffff812c5e7b:	75 34                	jne    ffffffff812c5eb1 <__rb_erase_color+0x61>
				 *    / \             / \
				 *   N   s    -->    p   Sr
				 *      / \         / \
				 *     Sl  Sr      N   Sl
				 */
				parent->rb_right = tmp1 = sibling->rb_left;
ffffffff812c5e7d:	4d 8b 7c 24 10       	mov    0x10(%r12),%r15
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c5e82:	48 89 d8             	mov    %rbx,%rax
				sibling->rb_left = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c5e85:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c5e88:	48 83 c8 01          	or     $0x1,%rax
ffffffff812c5e8c:	31 c9                	xor    %ecx,%ecx
ffffffff812c5e8e:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c5e91:	48 89 df             	mov    %rbx,%rdi
				 *    / \             / \
				 *   N   s    -->    p   Sr
				 *      / \         / \
				 *     Sl  Sr      N   Sl
				 */
				parent->rb_right = tmp1 = sibling->rb_left;
ffffffff812c5e94:	4c 89 7b 08          	mov    %r15,0x8(%rbx)
				sibling->rb_left = parent;
ffffffff812c5e98:	49 89 5c 24 10       	mov    %rbx,0x10(%r12)
ffffffff812c5e9d:	49 89 07             	mov    %rax,(%r15)
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c5ea0:	e8 7b ff ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
							RB_RED);
				augment_rotate(parent, sibling);
ffffffff812c5ea5:	4c 89 e6             	mov    %r12,%rsi
				sibling = tmp1;
ffffffff812c5ea8:	4d 89 fc             	mov    %r15,%r12
				parent->rb_right = tmp1 = sibling->rb_left;
				sibling->rb_left = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
							RB_RED);
				augment_rotate(parent, sibling);
ffffffff812c5eab:	48 89 df             	mov    %rbx,%rdi
ffffffff812c5eae:	41 ff d5             	callq  *%r13
				sibling = tmp1;
			}
			tmp1 = sibling->rb_right;
ffffffff812c5eb1:	49 8b 44 24 08       	mov    0x8(%r12),%rax
			if (!tmp1 || rb_is_black(tmp1)) {
ffffffff812c5eb6:	48 85 c0             	test   %rax,%rax
ffffffff812c5eb9:	74 05                	je     ffffffff812c5ec0 <__rb_erase_color+0x70>
ffffffff812c5ebb:	f6 00 01             	testb  $0x1,(%rax)
ffffffff812c5ebe:	74 47                	je     ffffffff812c5f07 <__rb_erase_color+0xb7>
				tmp2 = sibling->rb_left;
ffffffff812c5ec0:	4d 8b 7c 24 10       	mov    0x10(%r12),%r15
				if (!tmp2 || rb_is_black(tmp2)) {
ffffffff812c5ec5:	4d 85 ff             	test   %r15,%r15
ffffffff812c5ec8:	0f 84 aa 00 00 00    	je     ffffffff812c5f78 <__rb_erase_color+0x128>
ffffffff812c5ece:	41 f6 07 01          	testb  $0x1,(%r15)
ffffffff812c5ed2:	0f 85 a0 00 00 00    	jne    ffffffff812c5f78 <__rb_erase_color+0x128>
				 *     / \             \
				 *    sl  Sr            s
				 *                       \
				 *                        Sr
				 */
				sibling->rb_left = tmp1 = tmp2->rb_right;
ffffffff812c5ed8:	49 8b 47 08          	mov    0x8(%r15),%rax
				tmp2->rb_right = sibling;
				parent->rb_right = tmp2;
				if (tmp1)
ffffffff812c5edc:	48 85 c0             	test   %rax,%rax
				 *     / \             \
				 *    sl  Sr            s
				 *                       \
				 *                        Sr
				 */
				sibling->rb_left = tmp1 = tmp2->rb_right;
ffffffff812c5edf:	49 89 44 24 10       	mov    %rax,0x10(%r12)
				tmp2->rb_right = sibling;
ffffffff812c5ee4:	4d 89 67 08          	mov    %r12,0x8(%r15)
				parent->rb_right = tmp2;
ffffffff812c5ee8:	4c 89 7b 08          	mov    %r15,0x8(%rbx)
				if (tmp1)
ffffffff812c5eec:	74 0a                	je     ffffffff812c5ef8 <__rb_erase_color+0xa8>
ffffffff812c5eee:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c5ef1:	48 83 ca 01          	or     $0x1,%rdx
ffffffff812c5ef5:	48 89 10             	mov    %rdx,(%rax)
					rb_set_parent_color(tmp1, sibling,
							    RB_BLACK);
				augment_rotate(sibling, tmp2);
ffffffff812c5ef8:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c5efb:	4c 89 fe             	mov    %r15,%rsi
ffffffff812c5efe:	41 ff d5             	callq  *%r13
ffffffff812c5f01:	4c 89 e0             	mov    %r12,%rax
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_right;
			if (!tmp1 || rb_is_black(tmp1)) {
				tmp2 = sibling->rb_left;
ffffffff812c5f04:	4d 89 fc             	mov    %r15,%r12
			 *      / \             / \
			 *     N   S     -->   P   Sr
			 *        / \         / \
			 *      (sl) sr      N  (sl)
			 */
			parent->rb_right = tmp2 = sibling->rb_left;
ffffffff812c5f07:	49 8b 54 24 10       	mov    0x10(%r12),%rdx
ffffffff812c5f0c:	48 89 53 08          	mov    %rdx,0x8(%rbx)
			sibling->rb_left = parent;
ffffffff812c5f10:	49 89 5c 24 10       	mov    %rbx,0x10(%r12)
ffffffff812c5f15:	e9 c3 00 00 00       	jmpq   ffffffff812c5fdd <__rb_erase_color+0x18d>
			__rb_rotate_set_parents(parent, sibling, root,
						RB_BLACK);
			augment_rotate(parent, sibling);
			break;
		} else {
			sibling = parent->rb_left;
ffffffff812c5f1a:	4c 8b 63 10          	mov    0x10(%rbx),%r12
			if (rb_is_red(sibling)) {
ffffffff812c5f1e:	41 f6 04 24 01       	testb  $0x1,(%r12)
ffffffff812c5f23:	75 34                	jne    ffffffff812c5f59 <__rb_erase_color+0x109>
				/* Case 1 - right rotate at parent */
				parent->rb_left = tmp1 = sibling->rb_right;
ffffffff812c5f25:	4d 8b 7c 24 08       	mov    0x8(%r12),%r15
ffffffff812c5f2a:	48 89 d8             	mov    %rbx,%rax
				sibling->rb_right = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c5f2d:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c5f30:	48 83 c8 01          	or     $0x1,%rax
ffffffff812c5f34:	31 c9                	xor    %ecx,%ecx
ffffffff812c5f36:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c5f39:	48 89 df             	mov    %rbx,%rdi
			break;
		} else {
			sibling = parent->rb_left;
			if (rb_is_red(sibling)) {
				/* Case 1 - right rotate at parent */
				parent->rb_left = tmp1 = sibling->rb_right;
ffffffff812c5f3c:	4c 89 7b 10          	mov    %r15,0x10(%rbx)
				sibling->rb_right = parent;
ffffffff812c5f40:	49 89 5c 24 08       	mov    %rbx,0x8(%r12)
ffffffff812c5f45:	49 89 07             	mov    %rax,(%r15)
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c5f48:	e8 d3 fe ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
							RB_RED);
				augment_rotate(parent, sibling);
ffffffff812c5f4d:	4c 89 e6             	mov    %r12,%rsi
				sibling = tmp1;
ffffffff812c5f50:	4d 89 fc             	mov    %r15,%r12
				parent->rb_left = tmp1 = sibling->rb_right;
				sibling->rb_right = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
							RB_RED);
				augment_rotate(parent, sibling);
ffffffff812c5f53:	48 89 df             	mov    %rbx,%rdi
ffffffff812c5f56:	41 ff d5             	callq  *%r13
				sibling = tmp1;
			}
			tmp1 = sibling->rb_left;
ffffffff812c5f59:	49 8b 44 24 10       	mov    0x10(%r12),%rax
			if (!tmp1 || rb_is_black(tmp1)) {
ffffffff812c5f5e:	48 85 c0             	test   %rax,%rax
ffffffff812c5f61:	74 05                	je     ffffffff812c5f68 <__rb_erase_color+0x118>
ffffffff812c5f63:	f6 00 01             	testb  $0x1,(%rax)
ffffffff812c5f66:	74 67                	je     ffffffff812c5fcf <__rb_erase_color+0x17f>
				tmp2 = sibling->rb_right;
ffffffff812c5f68:	4d 8b 7c 24 08       	mov    0x8(%r12),%r15
				if (!tmp2 || rb_is_black(tmp2)) {
ffffffff812c5f6d:	4d 85 ff             	test   %r15,%r15
ffffffff812c5f70:	74 06                	je     ffffffff812c5f78 <__rb_erase_color+0x128>
ffffffff812c5f72:	41 f6 07 01          	testb  $0x1,(%r15)
ffffffff812c5f76:	74 28                	je     ffffffff812c5fa0 <__rb_erase_color+0x150>
ffffffff812c5f78:	49 89 1c 24          	mov    %rbx,(%r12)
					/* Case 2 - sibling color flip */
					rb_set_parent_color(sibling, parent,
							    RB_RED);
					if (rb_is_red(parent))
ffffffff812c5f7c:	48 8b 03             	mov    (%rbx),%rax
ffffffff812c5f7f:	a8 01                	test   $0x1,%al
ffffffff812c5f81:	75 0c                	jne    ffffffff812c5f8f <__rb_erase_color+0x13f>
 *  parentheses and have some accompanying text comment.
 */

static inline void rb_set_black(struct rb_node *rb)
{
	rb->__rb_parent_color |= RB_BLACK;
ffffffff812c5f83:	48 83 c8 01          	or     $0x1,%rax
ffffffff812c5f87:	48 89 03             	mov    %rax,(%rbx)
ffffffff812c5f8a:	e9 85 00 00 00       	jmpq   ffffffff812c6014 <__rb_erase_color+0x1c4>
					if (rb_is_red(parent))
						rb_set_black(parent);
					else {
						node = parent;
						parent = rb_parent(node);
						if (parent)
ffffffff812c5f8f:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax
ffffffff812c5f93:	74 7f                	je     ffffffff812c6014 <__rb_erase_color+0x1c4>
}

/* Non-inline version for rb_erase_augmented() use */
void __rb_erase_color(struct rb_node *parent, struct rb_root *root,
	void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
ffffffff812c5f95:	48 89 da             	mov    %rbx,%rdx
ffffffff812c5f98:	48 89 c3             	mov    %rax,%rbx
ffffffff812c5f9b:	e9 c9 fe ff ff       	jmpq   ffffffff812c5e69 <__rb_erase_color+0x19>
							continue;
					}
					break;
				}
				/* Case 3 - right rotate at sibling */
				sibling->rb_right = tmp1 = tmp2->rb_left;
ffffffff812c5fa0:	49 8b 47 10          	mov    0x10(%r15),%rax
				tmp2->rb_left = sibling;
				parent->rb_left = tmp2;
				if (tmp1)
ffffffff812c5fa4:	48 85 c0             	test   %rax,%rax
							continue;
					}
					break;
				}
				/* Case 3 - right rotate at sibling */
				sibling->rb_right = tmp1 = tmp2->rb_left;
ffffffff812c5fa7:	49 89 44 24 08       	mov    %rax,0x8(%r12)
				tmp2->rb_left = sibling;
ffffffff812c5fac:	4d 89 67 10          	mov    %r12,0x10(%r15)
				parent->rb_left = tmp2;
ffffffff812c5fb0:	4c 89 7b 10          	mov    %r15,0x10(%rbx)
				if (tmp1)
ffffffff812c5fb4:	74 0a                	je     ffffffff812c5fc0 <__rb_erase_color+0x170>
ffffffff812c5fb6:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c5fb9:	48 83 ca 01          	or     $0x1,%rdx
ffffffff812c5fbd:	48 89 10             	mov    %rdx,(%rax)
					rb_set_parent_color(tmp1, sibling,
							    RB_BLACK);
				augment_rotate(sibling, tmp2);
ffffffff812c5fc0:	4c 89 e7             	mov    %r12,%rdi
ffffffff812c5fc3:	4c 89 fe             	mov    %r15,%rsi
ffffffff812c5fc6:	41 ff d5             	callq  *%r13
ffffffff812c5fc9:	4c 89 e0             	mov    %r12,%rax
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_left;
			if (!tmp1 || rb_is_black(tmp1)) {
				tmp2 = sibling->rb_right;
ffffffff812c5fcc:	4d 89 fc             	mov    %r15,%r12
				augment_rotate(sibling, tmp2);
				tmp1 = sibling;
				sibling = tmp2;
			}
			/* Case 4 - left rotate at parent + color flips */
			parent->rb_left = tmp2 = sibling->rb_right;
ffffffff812c5fcf:	49 8b 54 24 08       	mov    0x8(%r12),%rdx
ffffffff812c5fd4:	48 89 53 10          	mov    %rdx,0x10(%rbx)
			sibling->rb_right = parent;
ffffffff812c5fd8:	49 89 5c 24 08       	mov    %rbx,0x8(%r12)
ffffffff812c5fdd:	4c 89 e1             	mov    %r12,%rcx
ffffffff812c5fe0:	48 83 c9 01          	or     $0x1,%rcx
			rb_set_parent_color(tmp1, sibling, RB_BLACK);
			if (tmp2)
ffffffff812c5fe4:	48 85 d2             	test   %rdx,%rdx
ffffffff812c5fe7:	48 89 08             	mov    %rcx,(%rax)
ffffffff812c5fea:	74 0c                	je     ffffffff812c5ff8 <__rb_erase_color+0x1a8>
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
ffffffff812c5fec:	48 8b 02             	mov    (%rdx),%rax
ffffffff812c5fef:	83 e0 01             	and    $0x1,%eax
ffffffff812c5ff2:	48 09 d8             	or     %rbx,%rax
ffffffff812c5ff5:	48 89 02             	mov    %rax,(%rdx)
				rb_set_parent(tmp2, parent);
			__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c5ff8:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c5ffb:	48 89 df             	mov    %rbx,%rdi
ffffffff812c5ffe:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812c6003:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c6006:	e8 15 fe ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
						RB_BLACK);
			augment_rotate(parent, sibling);
ffffffff812c600b:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c600e:	48 89 df             	mov    %rbx,%rdi
ffffffff812c6011:	41 ff d5             	callq  *%r13
/* Non-inline version for rb_erase_augmented() use */
void __rb_erase_color(struct rb_node *parent, struct rb_root *root,
	void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
	____rb_erase_color(parent, root, augment_rotate);
}
ffffffff812c6014:	58                   	pop    %rax
ffffffff812c6015:	5b                   	pop    %rbx
ffffffff812c6016:	41 5c                	pop    %r12
ffffffff812c6018:	41 5d                	pop    %r13
ffffffff812c601a:	41 5e                	pop    %r14
ffffffff812c601c:	41 5f                	pop    %r15
ffffffff812c601e:	5d                   	pop    %rbp
ffffffff812c601f:	c3                   	retq   

ffffffff812c6020 <rb_insert_color>:
static const struct rb_augment_callbacks dummy_callbacks = {
	dummy_propagate, dummy_copy, dummy_rotate
};

void rb_insert_color(struct rb_node *node, struct rb_root *root)
{
ffffffff812c6020:	55                   	push   %rbp
ffffffff812c6021:	48 89 f2             	mov    %rsi,%rdx
ffffffff812c6024:	48 8b 37             	mov    (%rdi),%rsi
ffffffff812c6027:	48 89 f8             	mov    %rdi,%rax
ffffffff812c602a:	48 89 e5             	mov    %rsp,%rbp
		 *
		 * If there is a black parent, we are done.
		 * Otherwise, take some corrective action as we don't
		 * want a red root or two consecutive red nodes.
		 */
		if (!parent) {
ffffffff812c602d:	48 85 f6             	test   %rsi,%rsi
ffffffff812c6030:	75 0c                	jne    ffffffff812c603e <rb_insert_color+0x1e>
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c6032:	48 c7 00 01 00 00 00 	movq   $0x1,(%rax)
ffffffff812c6039:	e9 d1 00 00 00       	jmpq   ffffffff812c610f <rb_insert_color+0xef>
			rb_set_parent_color(node, NULL, RB_BLACK);
			break;
		} else if (rb_is_black(parent))
ffffffff812c603e:	48 8b 3e             	mov    (%rsi),%rdi
ffffffff812c6041:	40 f6 c7 01          	test   $0x1,%dil
ffffffff812c6045:	0f 85 c4 00 00 00    	jne    ffffffff812c610f <rb_insert_color+0xef>
			break;

		gparent = rb_red_parent(parent);

		tmp = gparent->rb_right;
ffffffff812c604b:	48 8b 4f 08          	mov    0x8(%rdi),%rcx
		if (parent != tmp) {	/* parent == gparent->rb_left */
ffffffff812c604f:	48 39 ce             	cmp    %rcx,%rsi
ffffffff812c6052:	74 42                	je     ffffffff812c6096 <rb_insert_color+0x76>
			if (tmp && rb_is_red(tmp)) {
ffffffff812c6054:	48 85 c9             	test   %rcx,%rcx
ffffffff812c6057:	74 05                	je     ffffffff812c605e <rb_insert_color+0x3e>
ffffffff812c6059:	f6 01 01             	testb  $0x1,(%rcx)
ffffffff812c605c:	74 46                	je     ffffffff812c60a4 <rb_insert_color+0x84>
				parent = rb_parent(node);
				rb_set_parent_color(node, parent, RB_RED);
				continue;
			}

			tmp = parent->rb_right;
ffffffff812c605e:	48 8b 4e 08          	mov    0x8(%rsi),%rcx
			if (node == tmp) {
ffffffff812c6062:	48 39 c8             	cmp    %rcx,%rax
ffffffff812c6065:	75 25                	jne    ffffffff812c608c <rb_insert_color+0x6c>
				 *      n         p
				 *
				 * This still leaves us in violation of 4), the
				 * continuation into Case 3 will fix that.
				 */
				parent->rb_right = tmp = node->rb_left;
ffffffff812c6067:	4c 8b 40 10          	mov    0x10(%rax),%r8
				node->rb_left = parent;
				if (tmp)
ffffffff812c606b:	4d 85 c0             	test   %r8,%r8
				 *      n         p
				 *
				 * This still leaves us in violation of 4), the
				 * continuation into Case 3 will fix that.
				 */
				parent->rb_right = tmp = node->rb_left;
ffffffff812c606e:	4c 89 46 08          	mov    %r8,0x8(%rsi)
				node->rb_left = parent;
ffffffff812c6072:	48 89 70 10          	mov    %rsi,0x10(%rax)
				if (tmp)
ffffffff812c6076:	74 0a                	je     ffffffff812c6082 <rb_insert_color+0x62>
ffffffff812c6078:	49 89 f1             	mov    %rsi,%r9
ffffffff812c607b:	49 83 c9 01          	or     $0x1,%r9
ffffffff812c607f:	4d 89 08             	mov    %r9,(%r8)
ffffffff812c6082:	48 89 06             	mov    %rax,(%rsi)
					rb_set_parent_color(tmp, parent,
							    RB_BLACK);
				rb_set_parent_color(parent, node, RB_RED);
				augment_rotate(parent, node);
				parent = node;
				tmp = node->rb_right;
ffffffff812c6085:	48 89 ce             	mov    %rcx,%rsi
ffffffff812c6088:	48 8b 48 08          	mov    0x8(%rax),%rcx
			 *       / \         / \
			 *      p   U  -->  n   g
			 *     /                 \
			 *    n                   U
			 */
			gparent->rb_left = tmp;  /* == parent->rb_right */
ffffffff812c608c:	48 89 4f 10          	mov    %rcx,0x10(%rdi)
			parent->rb_right = gparent;
ffffffff812c6090:	48 89 7e 08          	mov    %rdi,0x8(%rsi)
ffffffff812c6094:	eb 63                	jmp    ffffffff812c60f9 <rb_insert_color+0xd9>
				rb_set_parent_color(tmp, gparent, RB_BLACK);
			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
			augment_rotate(gparent, parent);
			break;
		} else {
			tmp = gparent->rb_left;
ffffffff812c6096:	48 8b 4f 10          	mov    0x10(%rdi),%rcx
			if (tmp && rb_is_red(tmp)) {
ffffffff812c609a:	48 85 c9             	test   %rcx,%rcx
ffffffff812c609d:	74 24                	je     ffffffff812c60c3 <rb_insert_color+0xa3>
ffffffff812c609f:	f6 01 01             	testb  $0x1,(%rcx)
ffffffff812c60a2:	75 1f                	jne    ffffffff812c60c3 <rb_insert_color+0xa3>
ffffffff812c60a4:	48 89 f8             	mov    %rdi,%rax
ffffffff812c60a7:	48 83 c8 01          	or     $0x1,%rax
ffffffff812c60ab:	48 89 01             	mov    %rax,(%rcx)
ffffffff812c60ae:	48 89 06             	mov    %rax,(%rsi)
	rb->__rb_parent_color |= RB_BLACK;
}

static inline struct rb_node *rb_red_parent(struct rb_node *red)
{
	return (struct rb_node *)red->__rb_parent_color;
ffffffff812c60b1:	48 89 f8             	mov    %rdi,%rax
			if (tmp && rb_is_red(tmp)) {
				/* Case 1 - color flips */
				rb_set_parent_color(tmp, gparent, RB_BLACK);
				rb_set_parent_color(parent, gparent, RB_BLACK);
				node = gparent;
				parent = rb_parent(node);
ffffffff812c60b4:	48 8b 37             	mov    (%rdi),%rsi
ffffffff812c60b7:	48 83 e6 fc          	and    $0xfffffffffffffffc,%rsi
ffffffff812c60bb:	48 89 37             	mov    %rsi,(%rdi)
ffffffff812c60be:	e9 6a ff ff ff       	jmpq   ffffffff812c602d <rb_insert_color+0xd>
				rb_set_parent_color(node, parent, RB_RED);
				continue;
			}

			tmp = parent->rb_left;
ffffffff812c60c3:	48 8b 4e 10          	mov    0x10(%rsi),%rcx
			if (node == tmp) {
ffffffff812c60c7:	48 39 c8             	cmp    %rcx,%rax
ffffffff812c60ca:	75 25                	jne    ffffffff812c60f1 <rb_insert_color+0xd1>
				/* Case 2 - right rotate at parent */
				parent->rb_left = tmp = node->rb_right;
ffffffff812c60cc:	4c 8b 40 08          	mov    0x8(%rax),%r8
				node->rb_right = parent;
				if (tmp)
ffffffff812c60d0:	4d 85 c0             	test   %r8,%r8
			}

			tmp = parent->rb_left;
			if (node == tmp) {
				/* Case 2 - right rotate at parent */
				parent->rb_left = tmp = node->rb_right;
ffffffff812c60d3:	4c 89 46 10          	mov    %r8,0x10(%rsi)
				node->rb_right = parent;
ffffffff812c60d7:	48 89 70 08          	mov    %rsi,0x8(%rax)
				if (tmp)
ffffffff812c60db:	74 0a                	je     ffffffff812c60e7 <rb_insert_color+0xc7>
ffffffff812c60dd:	49 89 f1             	mov    %rsi,%r9
ffffffff812c60e0:	49 83 c9 01          	or     $0x1,%r9
ffffffff812c60e4:	4d 89 08             	mov    %r9,(%r8)
ffffffff812c60e7:	48 89 06             	mov    %rax,(%rsi)
					rb_set_parent_color(tmp, parent,
							    RB_BLACK);
				rb_set_parent_color(parent, node, RB_RED);
				augment_rotate(parent, node);
				parent = node;
				tmp = node->rb_left;
ffffffff812c60ea:	48 89 ce             	mov    %rcx,%rsi
ffffffff812c60ed:	48 8b 48 10          	mov    0x10(%rax),%rcx
			}

			/* Case 3 - left rotate at gparent */
			gparent->rb_right = tmp;  /* == parent->rb_left */
ffffffff812c60f1:	48 89 4f 08          	mov    %rcx,0x8(%rdi)
			parent->rb_left = gparent;
ffffffff812c60f5:	48 89 7e 10          	mov    %rdi,0x10(%rsi)
			if (tmp)
ffffffff812c60f9:	48 85 c9             	test   %rcx,%rcx
ffffffff812c60fc:	74 0a                	je     ffffffff812c6108 <rb_insert_color+0xe8>
ffffffff812c60fe:	48 89 f8             	mov    %rdi,%rax
ffffffff812c6101:	48 83 c8 01          	or     $0x1,%rax
ffffffff812c6105:	48 89 01             	mov    %rax,(%rcx)
				rb_set_parent_color(tmp, gparent, RB_BLACK);
			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
ffffffff812c6108:	31 c9                	xor    %ecx,%ecx
ffffffff812c610a:	e8 11 fd ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
};

void rb_insert_color(struct rb_node *node, struct rb_root *root)
{
	__rb_insert(node, root, dummy_rotate);
}
ffffffff812c610f:	5d                   	pop    %rbp
ffffffff812c6110:	c3                   	retq   

ffffffff812c6111 <__rb_insert_augmented>:
 * case, but this time with user-defined callbacks.
 */

void __rb_insert_augmented(struct rb_node *node, struct rb_root *root,
	void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
ffffffff812c6111:	55                   	push   %rbp
ffffffff812c6112:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c6115:	41 57                	push   %r15
ffffffff812c6117:	41 56                	push   %r14
ffffffff812c6119:	41 55                	push   %r13
ffffffff812c611b:	41 54                	push   %r12
ffffffff812c611d:	49 89 f6             	mov    %rsi,%r14
ffffffff812c6120:	53                   	push   %rbx
ffffffff812c6121:	49 89 fc             	mov    %rdi,%r12
ffffffff812c6124:	49 89 d5             	mov    %rdx,%r13
ffffffff812c6127:	48 83 ec 18          	sub    $0x18,%rsp
ffffffff812c612b:	48 8b 07             	mov    (%rdi),%rax
ffffffff812c612e:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
		 *
		 * If there is a black parent, we are done.
		 * Otherwise, take some corrective action as we don't
		 * want a red root or two consecutive red nodes.
		 */
		if (!parent) {
ffffffff812c6132:	48 83 7d c8 00       	cmpq   $0x0,-0x38(%rbp)
ffffffff812c6137:	75 0d                	jne    ffffffff812c6146 <__rb_insert_augmented+0x35>
ffffffff812c6139:	49 c7 04 24 01 00 00 	movq   $0x1,(%r12)
ffffffff812c6140:	00 
ffffffff812c6141:	e9 2e 01 00 00       	jmpq   ffffffff812c6274 <__rb_insert_augmented+0x163>
			rb_set_parent_color(node, NULL, RB_BLACK);
			break;
		} else if (rb_is_black(parent))
ffffffff812c6146:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812c614a:	48 8b 18             	mov    (%rax),%rbx
ffffffff812c614d:	f6 c3 01             	test   $0x1,%bl
ffffffff812c6150:	0f 85 1e 01 00 00    	jne    ffffffff812c6274 <__rb_insert_augmented+0x163>
			break;

		gparent = rb_red_parent(parent);

		tmp = gparent->rb_right;
ffffffff812c6156:	48 8b 43 08          	mov    0x8(%rbx),%rax
		if (parent != tmp) {	/* parent == gparent->rb_left */
ffffffff812c615a:	48 39 45 c8          	cmp    %rax,-0x38(%rbp)
ffffffff812c615e:	74 62                	je     ffffffff812c61c2 <__rb_insert_augmented+0xb1>
			if (tmp && rb_is_red(tmp)) {
ffffffff812c6160:	48 85 c0             	test   %rax,%rax
ffffffff812c6163:	74 05                	je     ffffffff812c616a <__rb_insert_augmented+0x59>
ffffffff812c6165:	f6 00 01             	testb  $0x1,(%rax)
ffffffff812c6168:	74 66                	je     ffffffff812c61d0 <__rb_insert_augmented+0xbf>
				parent = rb_parent(node);
				rb_set_parent_color(node, parent, RB_RED);
				continue;
			}

			tmp = parent->rb_right;
ffffffff812c616a:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812c616e:	4c 8b 78 08          	mov    0x8(%rax),%r15
			if (node == tmp) {
ffffffff812c6172:	4d 39 fc             	cmp    %r15,%r12
ffffffff812c6175:	75 3a                	jne    ffffffff812c61b1 <__rb_insert_augmented+0xa0>
				 *      n         p
				 *
				 * This still leaves us in violation of 4), the
				 * continuation into Case 3 will fix that.
				 */
				parent->rb_right = tmp = node->rb_left;
ffffffff812c6177:	49 8b 44 24 10       	mov    0x10(%r12),%rax
ffffffff812c617c:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
				node->rb_left = parent;
				if (tmp)
ffffffff812c6180:	48 85 c0             	test   %rax,%rax
				 *      n         p
				 *
				 * This still leaves us in violation of 4), the
				 * continuation into Case 3 will fix that.
				 */
				parent->rb_right = tmp = node->rb_left;
ffffffff812c6183:	48 89 46 08          	mov    %rax,0x8(%rsi)
				node->rb_left = parent;
ffffffff812c6187:	49 89 74 24 10       	mov    %rsi,0x10(%r12)
				if (tmp)
ffffffff812c618c:	74 0a                	je     ffffffff812c6198 <__rb_insert_augmented+0x87>
ffffffff812c618e:	48 89 f2             	mov    %rsi,%rdx
ffffffff812c6191:	48 83 ca 01          	or     $0x1,%rdx
ffffffff812c6195:	48 89 10             	mov    %rdx,(%rax)
ffffffff812c6198:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
					rb_set_parent_color(tmp, parent,
							    RB_BLACK);
				rb_set_parent_color(parent, node, RB_RED);
				augment_rotate(parent, node);
ffffffff812c619c:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c619f:	4c 89 20             	mov    %r12,(%rax)
ffffffff812c61a2:	48 89 c7             	mov    %rax,%rdi
ffffffff812c61a5:	41 ff d5             	callq  *%r13
				parent = node;
				tmp = node->rb_right;
ffffffff812c61a8:	4c 89 7d c8          	mov    %r15,-0x38(%rbp)
ffffffff812c61ac:	4d 8b 7c 24 08       	mov    0x8(%r12),%r15
			 *      p   U  -->  n   g
			 *     /                 \
			 *    n                   U
			 */
			gparent->rb_left = tmp;  /* == parent->rb_right */
			parent->rb_right = gparent;
ffffffff812c61b1:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
			 *       / \         / \
			 *      p   U  -->  n   g
			 *     /                 \
			 *    n                   U
			 */
			gparent->rb_left = tmp;  /* == parent->rb_right */
ffffffff812c61b5:	4c 89 7b 10          	mov    %r15,0x10(%rbx)
			parent->rb_right = gparent;
ffffffff812c61b9:	48 89 58 08          	mov    %rbx,0x8(%rax)
ffffffff812c61bd:	e9 88 00 00 00       	jmpq   ffffffff812c624a <__rb_insert_augmented+0x139>
				rb_set_parent_color(tmp, gparent, RB_BLACK);
			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
			augment_rotate(gparent, parent);
			break;
		} else {
			tmp = gparent->rb_left;
ffffffff812c61c2:	48 8b 43 10          	mov    0x10(%rbx),%rax
			if (tmp && rb_is_red(tmp)) {
ffffffff812c61c6:	48 85 c0             	test   %rax,%rax
ffffffff812c61c9:	74 2c                	je     ffffffff812c61f7 <__rb_insert_augmented+0xe6>
ffffffff812c61cb:	f6 00 01             	testb  $0x1,(%rax)
ffffffff812c61ce:	75 27                	jne    ffffffff812c61f7 <__rb_insert_augmented+0xe6>
ffffffff812c61d0:	48 89 da             	mov    %rbx,%rdx
	rb->__rb_parent_color |= RB_BLACK;
}

static inline struct rb_node *rb_red_parent(struct rb_node *red)
{
	return (struct rb_node *)red->__rb_parent_color;
ffffffff812c61d3:	49 89 dc             	mov    %rbx,%r12
ffffffff812c61d6:	48 83 ca 01          	or     $0x1,%rdx
ffffffff812c61da:	48 89 10             	mov    %rdx,(%rax)
ffffffff812c61dd:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812c61e1:	48 89 10             	mov    %rdx,(%rax)
			if (tmp && rb_is_red(tmp)) {
				/* Case 1 - color flips */
				rb_set_parent_color(tmp, gparent, RB_BLACK);
				rb_set_parent_color(parent, gparent, RB_BLACK);
				node = gparent;
				parent = rb_parent(node);
ffffffff812c61e4:	4c 8b 03             	mov    (%rbx),%r8
ffffffff812c61e7:	49 83 e0 fc          	and    $0xfffffffffffffffc,%r8
ffffffff812c61eb:	4c 89 45 c8          	mov    %r8,-0x38(%rbp)
ffffffff812c61ef:	4c 89 03             	mov    %r8,(%rbx)
ffffffff812c61f2:	e9 3b ff ff ff       	jmpq   ffffffff812c6132 <__rb_insert_augmented+0x21>
				rb_set_parent_color(node, parent, RB_RED);
				continue;
			}

			tmp = parent->rb_left;
ffffffff812c61f7:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812c61fb:	4c 8b 78 10          	mov    0x10(%rax),%r15
			if (node == tmp) {
ffffffff812c61ff:	4d 39 fc             	cmp    %r15,%r12
ffffffff812c6202:	75 3a                	jne    ffffffff812c623e <__rb_insert_augmented+0x12d>
				/* Case 2 - right rotate at parent */
				parent->rb_left = tmp = node->rb_right;
ffffffff812c6204:	49 8b 44 24 08       	mov    0x8(%r12),%rax
ffffffff812c6209:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
				node->rb_right = parent;
				if (tmp)
ffffffff812c620d:	48 85 c0             	test   %rax,%rax
			}

			tmp = parent->rb_left;
			if (node == tmp) {
				/* Case 2 - right rotate at parent */
				parent->rb_left = tmp = node->rb_right;
ffffffff812c6210:	48 89 41 10          	mov    %rax,0x10(%rcx)
				node->rb_right = parent;
ffffffff812c6214:	49 89 4c 24 08       	mov    %rcx,0x8(%r12)
				if (tmp)
ffffffff812c6219:	74 0a                	je     ffffffff812c6225 <__rb_insert_augmented+0x114>
ffffffff812c621b:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c621e:	48 83 ca 01          	or     $0x1,%rdx
ffffffff812c6222:	48 89 10             	mov    %rdx,(%rax)
ffffffff812c6225:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
					rb_set_parent_color(tmp, parent,
							    RB_BLACK);
				rb_set_parent_color(parent, node, RB_RED);
				augment_rotate(parent, node);
ffffffff812c6229:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c622c:	4c 89 20             	mov    %r12,(%rax)
ffffffff812c622f:	48 89 c7             	mov    %rax,%rdi
ffffffff812c6232:	41 ff d5             	callq  *%r13
				parent = node;
				tmp = node->rb_left;
ffffffff812c6235:	4c 89 7d c8          	mov    %r15,-0x38(%rbp)
ffffffff812c6239:	4d 8b 7c 24 10       	mov    0x10(%r12),%r15
			}

			/* Case 3 - left rotate at gparent */
			gparent->rb_right = tmp;  /* == parent->rb_left */
			parent->rb_left = gparent;
ffffffff812c623e:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
				parent = node;
				tmp = node->rb_left;
			}

			/* Case 3 - left rotate at gparent */
			gparent->rb_right = tmp;  /* == parent->rb_left */
ffffffff812c6242:	4c 89 7b 08          	mov    %r15,0x8(%rbx)
			parent->rb_left = gparent;
ffffffff812c6246:	48 89 58 10          	mov    %rbx,0x10(%rax)
			if (tmp)
ffffffff812c624a:	4d 85 ff             	test   %r15,%r15
ffffffff812c624d:	74 0a                	je     ffffffff812c6259 <__rb_insert_augmented+0x148>
ffffffff812c624f:	48 89 d8             	mov    %rbx,%rax
ffffffff812c6252:	48 83 c8 01          	or     $0x1,%rax
ffffffff812c6256:	49 89 07             	mov    %rax,(%r15)
				rb_set_parent_color(tmp, gparent, RB_BLACK);
			__rb_rotate_set_parents(gparent, parent, root, RB_RED);
ffffffff812c6259:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
ffffffff812c625d:	48 89 df             	mov    %rbx,%rdi
ffffffff812c6260:	31 c9                	xor    %ecx,%ecx
ffffffff812c6262:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c6265:	e8 b6 fb ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
			augment_rotate(gparent, parent);
ffffffff812c626a:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
ffffffff812c626e:	48 89 df             	mov    %rbx,%rdi
ffffffff812c6271:	41 ff d5             	callq  *%r13

void __rb_insert_augmented(struct rb_node *node, struct rb_root *root,
	void (*augment_rotate)(struct rb_node *old, struct rb_node *new))
{
	__rb_insert(node, root, augment_rotate);
}
ffffffff812c6274:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812c6278:	5b                   	pop    %rbx
ffffffff812c6279:	41 5c                	pop    %r12
ffffffff812c627b:	41 5d                	pop    %r13
ffffffff812c627d:	41 5e                	pop    %r14
ffffffff812c627f:	41 5f                	pop    %r15
ffffffff812c6281:	5d                   	pop    %rbp
ffffffff812c6282:	c3                   	retq   

ffffffff812c6283 <rb_replace_node>:
}
EXPORT_SYMBOL(rb_prev);

void rb_replace_node(struct rb_node *victim, struct rb_node *new,
		     struct rb_root *root)
{
ffffffff812c6283:	55                   	push   %rbp

static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
		  struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
ffffffff812c6284:	48 8b 0f             	mov    (%rdi),%rcx
ffffffff812c6287:	49 89 f8             	mov    %rdi,%r8
ffffffff812c628a:	48 89 f0             	mov    %rsi,%rax
ffffffff812c628d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c6290:	48 83 e1 fc          	and    $0xfffffffffffffffc,%rcx
ffffffff812c6294:	74 12                	je     ffffffff812c62a8 <rb_replace_node+0x25>
		if (parent->rb_left == old)
ffffffff812c6296:	48 3b 79 10          	cmp    0x10(%rcx),%rdi
ffffffff812c629a:	75 06                	jne    ffffffff812c62a2 <rb_replace_node+0x1f>
			parent->rb_left = new;
ffffffff812c629c:	48 89 71 10          	mov    %rsi,0x10(%rcx)
ffffffff812c62a0:	eb 09                	jmp    ffffffff812c62ab <rb_replace_node+0x28>
		else
			parent->rb_right = new;
ffffffff812c62a2:	48 89 71 08          	mov    %rsi,0x8(%rcx)
ffffffff812c62a6:	eb 03                	jmp    ffffffff812c62ab <rb_replace_node+0x28>
	} else
		root->rb_node = new;
ffffffff812c62a8:	48 89 32             	mov    %rsi,(%rdx)
	struct rb_node *parent = rb_parent(victim);

	/* Set the surrounding nodes to point to the replacement */
	__rb_change_child(victim, new, parent, root);
	if (victim->rb_left)
ffffffff812c62ab:	49 8b 48 10          	mov    0x10(%r8),%rcx
ffffffff812c62af:	48 85 c9             	test   %rcx,%rcx
ffffffff812c62b2:	74 0c                	je     ffffffff812c62c0 <rb_replace_node+0x3d>
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
ffffffff812c62b4:	48 8b 11             	mov    (%rcx),%rdx
ffffffff812c62b7:	83 e2 01             	and    $0x1,%edx
ffffffff812c62ba:	48 09 c2             	or     %rax,%rdx
ffffffff812c62bd:	48 89 11             	mov    %rdx,(%rcx)
		rb_set_parent(victim->rb_left, new);
	if (victim->rb_right)
ffffffff812c62c0:	49 8b 48 08          	mov    0x8(%r8),%rcx
ffffffff812c62c4:	48 85 c9             	test   %rcx,%rcx
ffffffff812c62c7:	74 0c                	je     ffffffff812c62d5 <rb_replace_node+0x52>
ffffffff812c62c9:	48 8b 11             	mov    (%rcx),%rdx
ffffffff812c62cc:	83 e2 01             	and    $0x1,%edx
ffffffff812c62cf:	48 09 c2             	or     %rax,%rdx
ffffffff812c62d2:	48 89 11             	mov    %rdx,(%rcx)
		rb_set_parent(victim->rb_right, new);

	/* Copy the pointers/colour from the victim to the replacement */
	*new = *victim;
ffffffff812c62d5:	b9 06 00 00 00       	mov    $0x6,%ecx
ffffffff812c62da:	48 89 c7             	mov    %rax,%rdi
ffffffff812c62dd:	4c 89 c6             	mov    %r8,%rsi
ffffffff812c62e0:	f3 a5                	rep movsl %ds:(%rsi),%es:(%rdi)
}
ffffffff812c62e2:	5d                   	pop    %rbp
ffffffff812c62e3:	c3                   	retq   

ffffffff812c62e4 <rb_first_postorder>:
}
EXPORT_SYMBOL(rb_next_postorder);

struct rb_node *rb_first_postorder(const struct rb_root *root)
{
	if (!root->rb_node)
ffffffff812c62e4:	48 8b 17             	mov    (%rdi),%rdx
		return (struct rb_node *)parent;
}
EXPORT_SYMBOL(rb_next_postorder);

struct rb_node *rb_first_postorder(const struct rb_root *root)
{
ffffffff812c62e7:	55                   	push   %rbp
ffffffff812c62e8:	31 c0                	xor    %eax,%eax
ffffffff812c62ea:	48 89 e5             	mov    %rsp,%rbp
	if (!root->rb_node)
ffffffff812c62ed:	48 85 d2             	test   %rdx,%rdx
ffffffff812c62f0:	74 15                	je     ffffffff812c6307 <rb_first_postorder+0x23>
EXPORT_SYMBOL(rb_replace_node);

static struct rb_node *rb_left_deepest_node(const struct rb_node *node)
{
	for (;;) {
		if (node->rb_left)
ffffffff812c62f2:	48 89 d0             	mov    %rdx,%rax
ffffffff812c62f5:	48 8b 52 10          	mov    0x10(%rdx),%rdx
ffffffff812c62f9:	48 85 d2             	test   %rdx,%rdx
ffffffff812c62fc:	75 f4                	jne    ffffffff812c62f2 <rb_first_postorder+0xe>
			node = node->rb_left;
		else if (node->rb_right)
ffffffff812c62fe:	48 8b 50 08          	mov    0x8(%rax),%rdx
ffffffff812c6302:	48 85 d2             	test   %rdx,%rdx
ffffffff812c6305:	75 eb                	jne    ffffffff812c62f2 <rb_first_postorder+0xe>
{
	if (!root->rb_node)
		return NULL;

	return rb_left_deepest_node(root->rb_node);
}
ffffffff812c6307:	5d                   	pop    %rbp
ffffffff812c6308:	c3                   	retq   

ffffffff812c6309 <rb_erase>:
	__rb_insert(node, root, dummy_rotate);
}
EXPORT_SYMBOL(rb_insert_color);

void rb_erase(struct rb_node *node, struct rb_root *root)
{
ffffffff812c6309:	55                   	push   %rbp
ffffffff812c630a:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c630d:	41 57                	push   %r15
ffffffff812c630f:	41 56                	push   %r14
ffffffff812c6311:	53                   	push   %rbx

static __always_inline struct rb_node *
__rb_erase_augmented(struct rb_node *node, struct rb_root *root,
		     const struct rb_augment_callbacks *augment)
{
	struct rb_node *child = node->rb_right, *tmp = node->rb_left;
ffffffff812c6312:	48 8b 57 10          	mov    0x10(%rdi),%rdx
ffffffff812c6316:	48 8b 47 08          	mov    0x8(%rdi),%rax
	struct rb_node *parent, *rebalance;
	unsigned long pc;

	if (!tmp) {
ffffffff812c631a:	48 85 d2             	test   %rdx,%rdx
ffffffff812c631d:	75 45                	jne    ffffffff812c6364 <rb_erase+0x5b>
		 *
		 * Note that if there is one child it must be red due to 5)
		 * and node must be black due to 4). We adjust colors locally
		 * so as to bypass __rb_erase_color() later on.
		 */
		pc = node->__rb_parent_color;
ffffffff812c631f:	48 8b 0f             	mov    (%rdi),%rcx
		parent = __rb_parent(pc);
ffffffff812c6322:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c6325:	48 83 e2 fc          	and    $0xfffffffffffffffc,%rdx

static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
		  struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
ffffffff812c6329:	48 85 d2             	test   %rdx,%rdx
		 * Note that if there is one child it must be red due to 5)
		 * and node must be black due to 4). We adjust colors locally
		 * so as to bypass __rb_erase_color() later on.
		 */
		pc = node->__rb_parent_color;
		parent = __rb_parent(pc);
ffffffff812c632c:	48 89 d3             	mov    %rdx,%rbx

static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
		  struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
ffffffff812c632f:	74 12                	je     ffffffff812c6343 <rb_erase+0x3a>
		if (parent->rb_left == old)
ffffffff812c6331:	48 3b 7a 10          	cmp    0x10(%rdx),%rdi
ffffffff812c6335:	75 06                	jne    ffffffff812c633d <rb_erase+0x34>
			parent->rb_left = new;
ffffffff812c6337:	48 89 42 10          	mov    %rax,0x10(%rdx)
ffffffff812c633b:	eb 09                	jmp    ffffffff812c6346 <rb_erase+0x3d>
		else
			parent->rb_right = new;
ffffffff812c633d:	48 89 42 08          	mov    %rax,0x8(%rdx)
ffffffff812c6341:	eb 03                	jmp    ffffffff812c6346 <rb_erase+0x3d>
	} else
		root->rb_node = new;
ffffffff812c6343:	48 89 06             	mov    %rax,(%rsi)
		 * so as to bypass __rb_erase_color() later on.
		 */
		pc = node->__rb_parent_color;
		parent = __rb_parent(pc);
		__rb_change_child(node, child, parent, root);
		if (child) {
ffffffff812c6346:	48 85 c0             	test   %rax,%rax
ffffffff812c6349:	74 08                	je     ffffffff812c6353 <rb_erase+0x4a>
			child->__rb_parent_color = pc;
ffffffff812c634b:	48 89 08             	mov    %rcx,(%rax)
ffffffff812c634e:	e9 44 02 00 00       	jmpq   ffffffff812c6597 <rb_erase+0x28e>
			rebalance = NULL;
		} else
			rebalance = __rb_is_black(pc) ? parent : NULL;
ffffffff812c6353:	80 e1 01             	and    $0x1,%cl
ffffffff812c6356:	0f 84 3b 02 00 00    	je     ffffffff812c6597 <rb_erase+0x28e>
	struct rb_node *rebalance;
	rebalance = __rb_erase_augmented(node, root, &dummy_callbacks);
	if (rebalance)
ffffffff812c635c:	48 85 d2             	test   %rdx,%rdx
ffffffff812c635f:	e9 c1 00 00 00       	jmpq   ffffffff812c6425 <rb_erase+0x11c>
		tmp = parent;
	} else if (!child) {
ffffffff812c6364:	48 85 c0             	test   %rax,%rax
ffffffff812c6367:	75 2c                	jne    ffffffff812c6395 <rb_erase+0x8c>
		/* Still case 1, but this time the child is node->rb_left */
		tmp->__rb_parent_color = pc = node->__rb_parent_color;
ffffffff812c6369:	48 8b 07             	mov    (%rdi),%rax
ffffffff812c636c:	48 89 02             	mov    %rax,(%rdx)

static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
		  struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
ffffffff812c636f:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax
ffffffff812c6373:	74 18                	je     ffffffff812c638d <rb_erase+0x84>
		if (parent->rb_left == old)
ffffffff812c6375:	48 3b 78 10          	cmp    0x10(%rax),%rdi
ffffffff812c6379:	75 09                	jne    ffffffff812c6384 <rb_erase+0x7b>
			parent->rb_left = new;
ffffffff812c637b:	48 89 50 10          	mov    %rdx,0x10(%rax)
ffffffff812c637f:	e9 13 02 00 00       	jmpq   ffffffff812c6597 <rb_erase+0x28e>
		else
			parent->rb_right = new;
ffffffff812c6384:	48 89 50 08          	mov    %rdx,0x8(%rax)
ffffffff812c6388:	e9 0a 02 00 00       	jmpq   ffffffff812c6597 <rb_erase+0x28e>
	} else
		root->rb_node = new;
ffffffff812c638d:	48 89 16             	mov    %rdx,(%rsi)
ffffffff812c6390:	e9 02 02 00 00       	jmpq   ffffffff812c6597 <rb_erase+0x28e>
		__rb_change_child(node, tmp, parent, root);
		rebalance = NULL;
		tmp = parent;
	} else {
		struct rb_node *successor = child, *child2;
		tmp = child->rb_left;
ffffffff812c6395:	48 8b 50 10          	mov    0x10(%rax),%rdx
		if (!tmp) {
ffffffff812c6399:	48 89 c3             	mov    %rax,%rbx
ffffffff812c639c:	48 85 d2             	test   %rdx,%rdx
ffffffff812c639f:	75 06                	jne    ffffffff812c63a7 <rb_erase+0x9e>
			 *  (x) (s)  ->  (x) (c)
			 *        \
			 *        (c)
			 */
			parent = successor;
			child2 = successor->rb_right;
ffffffff812c63a1:	4c 8b 48 08          	mov    0x8(%rax),%r9
ffffffff812c63a5:	eb 2c                	jmp    ffffffff812c63d3 <rb_erase+0xca>
			 *    (c)
			 */
			do {
				parent = successor;
				successor = tmp;
				tmp = tmp->rb_left;
ffffffff812c63a7:	48 8b 4a 10          	mov    0x10(%rdx),%rcx
			} while (tmp);
ffffffff812c63ab:	48 85 c9             	test   %rcx,%rcx
ffffffff812c63ae:	74 08                	je     ffffffff812c63b8 <rb_erase+0xaf>
ffffffff812c63b0:	48 89 d3             	mov    %rdx,%rbx
ffffffff812c63b3:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c63b6:	eb ef                	jmp    ffffffff812c63a7 <rb_erase+0x9e>
			parent->rb_left = child2 = successor->rb_right;
ffffffff812c63b8:	4c 8b 4a 08          	mov    0x8(%rdx),%r9
ffffffff812c63bc:	4c 89 4b 10          	mov    %r9,0x10(%rbx)
			successor->rb_right = child;
ffffffff812c63c0:	48 89 42 08          	mov    %rax,0x8(%rdx)
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
ffffffff812c63c4:	48 8b 08             	mov    (%rax),%rcx
ffffffff812c63c7:	83 e1 01             	and    $0x1,%ecx
ffffffff812c63ca:	48 09 d1             	or     %rdx,%rcx
ffffffff812c63cd:	48 89 08             	mov    %rcx,(%rax)
ffffffff812c63d0:	48 89 d0             	mov    %rdx,%rax
			rb_set_parent(child, successor);
			augment->copy(node, successor);
			augment->propagate(parent, successor);
		}

		successor->rb_left = tmp = node->rb_left;
ffffffff812c63d3:	48 8b 4f 10          	mov    0x10(%rdi),%rcx
ffffffff812c63d7:	48 89 48 10          	mov    %rcx,0x10(%rax)
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
ffffffff812c63db:	48 8b 11             	mov    (%rcx),%rdx
ffffffff812c63de:	83 e2 01             	and    $0x1,%edx
ffffffff812c63e1:	48 09 c2             	or     %rax,%rdx
ffffffff812c63e4:	48 89 11             	mov    %rdx,(%rcx)
		}

		successor->rb_left = tmp = node->rb_left;
		rb_set_parent(tmp, successor);

		pc = node->__rb_parent_color;
ffffffff812c63e7:	48 8b 17             	mov    (%rdi),%rdx

static inline void
__rb_change_child(struct rb_node *old, struct rb_node *new,
		  struct rb_node *parent, struct rb_root *root)
{
	if (parent) {
ffffffff812c63ea:	48 89 d1             	mov    %rdx,%rcx
ffffffff812c63ed:	48 83 e1 fc          	and    $0xfffffffffffffffc,%rcx
ffffffff812c63f1:	74 12                	je     ffffffff812c6405 <rb_erase+0xfc>
		if (parent->rb_left == old)
ffffffff812c63f3:	48 3b 79 10          	cmp    0x10(%rcx),%rdi
ffffffff812c63f7:	75 06                	jne    ffffffff812c63ff <rb_erase+0xf6>
			parent->rb_left = new;
ffffffff812c63f9:	48 89 41 10          	mov    %rax,0x10(%rcx)
ffffffff812c63fd:	eb 09                	jmp    ffffffff812c6408 <rb_erase+0xff>
		else
			parent->rb_right = new;
ffffffff812c63ff:	48 89 41 08          	mov    %rax,0x8(%rcx)
ffffffff812c6403:	eb 03                	jmp    ffffffff812c6408 <rb_erase+0xff>
	} else
		root->rb_node = new;
ffffffff812c6405:	48 89 06             	mov    %rax,(%rsi)
		rb_set_parent(tmp, successor);

		pc = node->__rb_parent_color;
		tmp = __rb_parent(pc);
		__rb_change_child(node, successor, tmp, root);
		if (child2) {
ffffffff812c6408:	4d 85 c9             	test   %r9,%r9
ffffffff812c640b:	74 0f                	je     ffffffff812c641c <rb_erase+0x113>
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c640d:	48 83 cb 01          	or     $0x1,%rbx

		pc = node->__rb_parent_color;
		tmp = __rb_parent(pc);
		__rb_change_child(node, successor, tmp, root);
		if (child2) {
			successor->__rb_parent_color = pc;
ffffffff812c6411:	48 89 10             	mov    %rdx,(%rax)
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c6414:	49 89 19             	mov    %rbx,(%r9)
ffffffff812c6417:	e9 7b 01 00 00       	jmpq   ffffffff812c6597 <rb_erase+0x28e>
		if (child2) {
			successor->__rb_parent_color = pc;
			rb_set_parent_color(child2, parent, RB_BLACK);
			rebalance = NULL;
		} else {
			unsigned long pc2 = successor->__rb_parent_color;
ffffffff812c641c:	48 8b 08             	mov    (%rax),%rcx
			successor->__rb_parent_color = pc;
ffffffff812c641f:	48 89 10             	mov    %rdx,(%rax)
			rebalance = __rb_is_black(pc2) ? parent : NULL;
ffffffff812c6422:	80 e1 01             	and    $0x1,%cl
ffffffff812c6425:	0f 84 6c 01 00 00    	je     ffffffff812c6597 <rb_erase+0x28e>
ffffffff812c642b:	49 89 f6             	mov    %rsi,%r14
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
ffffffff812c642e:	31 d2                	xor    %edx,%edx
		 * - node is black (or NULL on first iteration)
		 * - node is not the root (parent is not NULL)
		 * - All leaf paths going through parent and node have a
		 *   black node count that is 1 lower than other leaf paths.
		 */
		sibling = parent->rb_right;
ffffffff812c6430:	48 8b 43 08          	mov    0x8(%rbx),%rax
		if (node != sibling) {	/* node == parent->rb_left */
ffffffff812c6434:	48 39 d0             	cmp    %rdx,%rax
ffffffff812c6437:	0f 84 8a 00 00 00    	je     ffffffff812c64c7 <rb_erase+0x1be>
			if (rb_is_red(sibling)) {
ffffffff812c643d:	f6 00 01             	testb  $0x1,(%rax)
ffffffff812c6440:	75 29                	jne    ffffffff812c646b <rb_erase+0x162>
				 *    / \             / \
				 *   N   s    -->    p   Sr
				 *      / \         / \
				 *     Sl  Sr      N   Sl
				 */
				parent->rb_right = tmp1 = sibling->rb_left;
ffffffff812c6442:	4c 8b 78 10          	mov    0x10(%rax),%r15
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c6446:	48 89 da             	mov    %rbx,%rdx
				sibling->rb_left = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c6449:	48 89 c6             	mov    %rax,%rsi
ffffffff812c644c:	48 83 ca 01          	or     $0x1,%rdx
ffffffff812c6450:	31 c9                	xor    %ecx,%ecx
ffffffff812c6452:	48 89 df             	mov    %rbx,%rdi
				 *    / \             / \
				 *   N   s    -->    p   Sr
				 *      / \         / \
				 *     Sl  Sr      N   Sl
				 */
				parent->rb_right = tmp1 = sibling->rb_left;
ffffffff812c6455:	4c 89 7b 08          	mov    %r15,0x8(%rbx)
				sibling->rb_left = parent;
ffffffff812c6459:	48 89 58 10          	mov    %rbx,0x10(%rax)
ffffffff812c645d:	49 89 17             	mov    %rdx,(%r15)
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c6460:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c6463:	e8 b8 f9 ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
ffffffff812c6468:	4c 89 f8             	mov    %r15,%rax
			}
			tmp1 = sibling->rb_right;
ffffffff812c646b:	48 8b 50 08          	mov    0x8(%rax),%rdx
			if (!tmp1 || rb_is_black(tmp1)) {
ffffffff812c646f:	48 85 d2             	test   %rdx,%rdx
ffffffff812c6472:	74 05                	je     ffffffff812c6479 <rb_erase+0x170>
ffffffff812c6474:	f6 02 01             	testb  $0x1,(%rdx)
ffffffff812c6477:	74 37                	je     ffffffff812c64b0 <rb_erase+0x1a7>
				tmp2 = sibling->rb_left;
ffffffff812c6479:	48 8b 70 10          	mov    0x10(%rax),%rsi
				if (!tmp2 || rb_is_black(tmp2)) {
ffffffff812c647d:	48 85 f6             	test   %rsi,%rsi
ffffffff812c6480:	0f 84 8f 00 00 00    	je     ffffffff812c6515 <rb_erase+0x20c>
ffffffff812c6486:	f6 06 01             	testb  $0x1,(%rsi)
ffffffff812c6489:	0f 85 86 00 00 00    	jne    ffffffff812c6515 <rb_erase+0x20c>
				 *     / \             \
				 *    sl  Sr            s
				 *                       \
				 *                        Sr
				 */
				sibling->rb_left = tmp1 = tmp2->rb_right;
ffffffff812c648f:	48 8b 56 08          	mov    0x8(%rsi),%rdx
				tmp2->rb_right = sibling;
				parent->rb_right = tmp2;
				if (tmp1)
ffffffff812c6493:	48 85 d2             	test   %rdx,%rdx
				 *     / \             \
				 *    sl  Sr            s
				 *                       \
				 *                        Sr
				 */
				sibling->rb_left = tmp1 = tmp2->rb_right;
ffffffff812c6496:	48 89 50 10          	mov    %rdx,0x10(%rax)
				tmp2->rb_right = sibling;
ffffffff812c649a:	48 89 46 08          	mov    %rax,0x8(%rsi)
				parent->rb_right = tmp2;
ffffffff812c649e:	48 89 73 08          	mov    %rsi,0x8(%rbx)
				if (tmp1)
ffffffff812c64a2:	74 12                	je     ffffffff812c64b6 <rb_erase+0x1ad>
ffffffff812c64a4:	48 89 c1             	mov    %rax,%rcx
ffffffff812c64a7:	48 83 c9 01          	or     $0x1,%rcx
ffffffff812c64ab:	48 89 0a             	mov    %rcx,(%rdx)
ffffffff812c64ae:	eb 06                	jmp    ffffffff812c64b6 <rb_erase+0x1ad>
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_right;
			if (!tmp1 || rb_is_black(tmp1)) {
ffffffff812c64b0:	48 89 c6             	mov    %rax,%rsi
				__rb_rotate_set_parents(parent, sibling, root,
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_right;
ffffffff812c64b3:	48 89 d0             	mov    %rdx,%rax
			 *      / \             / \
			 *     N   S     -->   P   Sr
			 *        / \         / \
			 *      (sl) sr      N  (sl)
			 */
			parent->rb_right = tmp2 = sibling->rb_left;
ffffffff812c64b6:	48 8b 56 10          	mov    0x10(%rsi),%rdx
ffffffff812c64ba:	48 89 53 08          	mov    %rdx,0x8(%rbx)
			sibling->rb_left = parent;
ffffffff812c64be:	48 89 5e 10          	mov    %rbx,0x10(%rsi)
ffffffff812c64c2:	e9 a5 00 00 00       	jmpq   ffffffff812c656c <rb_erase+0x263>
			__rb_rotate_set_parents(parent, sibling, root,
						RB_BLACK);
			augment_rotate(parent, sibling);
			break;
		} else {
			sibling = parent->rb_left;
ffffffff812c64c7:	48 8b 43 10          	mov    0x10(%rbx),%rax
			if (rb_is_red(sibling)) {
ffffffff812c64cb:	f6 00 01             	testb  $0x1,(%rax)
ffffffff812c64ce:	75 29                	jne    ffffffff812c64f9 <rb_erase+0x1f0>
				/* Case 1 - right rotate at parent */
				parent->rb_left = tmp1 = sibling->rb_right;
ffffffff812c64d0:	4c 8b 78 08          	mov    0x8(%rax),%r15
ffffffff812c64d4:	48 89 da             	mov    %rbx,%rdx
				sibling->rb_right = parent;
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c64d7:	48 89 c6             	mov    %rax,%rsi
ffffffff812c64da:	48 83 ca 01          	or     $0x1,%rdx
ffffffff812c64de:	31 c9                	xor    %ecx,%ecx
ffffffff812c64e0:	48 89 df             	mov    %rbx,%rdi
			break;
		} else {
			sibling = parent->rb_left;
			if (rb_is_red(sibling)) {
				/* Case 1 - right rotate at parent */
				parent->rb_left = tmp1 = sibling->rb_right;
ffffffff812c64e3:	4c 89 7b 10          	mov    %r15,0x10(%rbx)
				sibling->rb_right = parent;
ffffffff812c64e7:	48 89 58 08          	mov    %rbx,0x8(%rax)
ffffffff812c64eb:	49 89 17             	mov    %rdx,(%r15)
				rb_set_parent_color(tmp1, parent, RB_BLACK);
				__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c64ee:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c64f1:	e8 2a f9 ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
ffffffff812c64f6:	4c 89 f8             	mov    %r15,%rax
			}
			tmp1 = sibling->rb_left;
ffffffff812c64f9:	48 8b 50 10          	mov    0x10(%rax),%rdx
			if (!tmp1 || rb_is_black(tmp1)) {
ffffffff812c64fd:	48 85 d2             	test   %rdx,%rdx
ffffffff812c6500:	74 05                	je     ffffffff812c6507 <rb_erase+0x1fe>
ffffffff812c6502:	f6 02 01             	testb  $0x1,(%rdx)
ffffffff812c6505:	74 53                	je     ffffffff812c655a <rb_erase+0x251>
				tmp2 = sibling->rb_right;
ffffffff812c6507:	48 8b 70 08          	mov    0x8(%rax),%rsi
				if (!tmp2 || rb_is_black(tmp2)) {
ffffffff812c650b:	48 85 f6             	test   %rsi,%rsi
ffffffff812c650e:	74 05                	je     ffffffff812c6515 <rb_erase+0x20c>
ffffffff812c6510:	f6 06 01             	testb  $0x1,(%rsi)
ffffffff812c6513:	74 24                	je     ffffffff812c6539 <rb_erase+0x230>
ffffffff812c6515:	48 89 18             	mov    %rbx,(%rax)
					/* Case 2 - sibling color flip */
					rb_set_parent_color(sibling, parent,
							    RB_RED);
					if (rb_is_red(parent))
ffffffff812c6518:	48 8b 03             	mov    (%rbx),%rax
ffffffff812c651b:	a8 01                	test   $0x1,%al
ffffffff812c651d:	75 09                	jne    ffffffff812c6528 <rb_erase+0x21f>
 *  parentheses and have some accompanying text comment.
 */

static inline void rb_set_black(struct rb_node *rb)
{
	rb->__rb_parent_color |= RB_BLACK;
ffffffff812c651f:	48 83 c8 01          	or     $0x1,%rax
ffffffff812c6523:	48 89 03             	mov    %rax,(%rbx)
ffffffff812c6526:	eb 6f                	jmp    ffffffff812c6597 <rb_erase+0x28e>
					if (rb_is_red(parent))
						rb_set_black(parent);
					else {
						node = parent;
						parent = rb_parent(node);
						if (parent)
ffffffff812c6528:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax
ffffffff812c652c:	74 69                	je     ffffffff812c6597 <rb_erase+0x28e>
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
ffffffff812c652e:	48 89 da             	mov    %rbx,%rdx
ffffffff812c6531:	48 89 c3             	mov    %rax,%rbx
ffffffff812c6534:	e9 f7 fe ff ff       	jmpq   ffffffff812c6430 <rb_erase+0x127>
							continue;
					}
					break;
				}
				/* Case 3 - right rotate at sibling */
				sibling->rb_right = tmp1 = tmp2->rb_left;
ffffffff812c6539:	48 8b 56 10          	mov    0x10(%rsi),%rdx
				tmp2->rb_left = sibling;
				parent->rb_left = tmp2;
				if (tmp1)
ffffffff812c653d:	48 85 d2             	test   %rdx,%rdx
							continue;
					}
					break;
				}
				/* Case 3 - right rotate at sibling */
				sibling->rb_right = tmp1 = tmp2->rb_left;
ffffffff812c6540:	48 89 50 08          	mov    %rdx,0x8(%rax)
				tmp2->rb_left = sibling;
ffffffff812c6544:	48 89 46 10          	mov    %rax,0x10(%rsi)
				parent->rb_left = tmp2;
ffffffff812c6548:	48 89 73 10          	mov    %rsi,0x10(%rbx)
				if (tmp1)
ffffffff812c654c:	74 12                	je     ffffffff812c6560 <rb_erase+0x257>
}

static inline void rb_set_parent_color(struct rb_node *rb,
				       struct rb_node *p, int color)
{
	rb->__rb_parent_color = (unsigned long)p | color;
ffffffff812c654e:	48 89 c1             	mov    %rax,%rcx
ffffffff812c6551:	48 83 c9 01          	or     $0x1,%rcx
ffffffff812c6555:	48 89 0a             	mov    %rcx,(%rdx)
ffffffff812c6558:	eb 06                	jmp    ffffffff812c6560 <rb_erase+0x257>
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_left;
			if (!tmp1 || rb_is_black(tmp1)) {
ffffffff812c655a:	48 89 c6             	mov    %rax,%rsi
				__rb_rotate_set_parents(parent, sibling, root,
							RB_RED);
				augment_rotate(parent, sibling);
				sibling = tmp1;
			}
			tmp1 = sibling->rb_left;
ffffffff812c655d:	48 89 d0             	mov    %rdx,%rax
				augment_rotate(sibling, tmp2);
				tmp1 = sibling;
				sibling = tmp2;
			}
			/* Case 4 - left rotate at parent + color flips */
			parent->rb_left = tmp2 = sibling->rb_right;
ffffffff812c6560:	48 8b 56 08          	mov    0x8(%rsi),%rdx
ffffffff812c6564:	48 89 53 10          	mov    %rdx,0x10(%rbx)
			sibling->rb_right = parent;
ffffffff812c6568:	48 89 5e 08          	mov    %rbx,0x8(%rsi)
ffffffff812c656c:	48 89 f1             	mov    %rsi,%rcx
ffffffff812c656f:	48 83 c9 01          	or     $0x1,%rcx
			rb_set_parent_color(tmp1, sibling, RB_BLACK);
			if (tmp2)
ffffffff812c6573:	48 85 d2             	test   %rdx,%rdx
ffffffff812c6576:	48 89 08             	mov    %rcx,(%rax)
ffffffff812c6579:	74 0c                	je     ffffffff812c6587 <rb_erase+0x27e>
#define rb_is_red(rb)      __rb_is_red((rb)->__rb_parent_color)
#define rb_is_black(rb)    __rb_is_black((rb)->__rb_parent_color)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->__rb_parent_color = rb_color(rb) | (unsigned long)p;
ffffffff812c657b:	48 8b 02             	mov    (%rdx),%rax
ffffffff812c657e:	83 e0 01             	and    $0x1,%eax
ffffffff812c6581:	48 09 d8             	or     %rbx,%rax
ffffffff812c6584:	48 89 02             	mov    %rax,(%rdx)
				rb_set_parent(tmp2, parent);
			__rb_rotate_set_parents(parent, sibling, root,
ffffffff812c6587:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812c658c:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c658f:	48 89 df             	mov    %rbx,%rdi
ffffffff812c6592:	e8 89 f8 ff ff       	callq  ffffffff812c5e20 <__rb_rotate_set_parents>
{
	struct rb_node *rebalance;
	rebalance = __rb_erase_augmented(node, root, &dummy_callbacks);
	if (rebalance)
		____rb_erase_color(rebalance, root, dummy_rotate);
}
ffffffff812c6597:	5b                   	pop    %rbx
ffffffff812c6598:	41 5e                	pop    %r14
ffffffff812c659a:	41 5f                	pop    %r15
ffffffff812c659c:	5d                   	pop    %rbp
ffffffff812c659d:	c3                   	retq   

ffffffff812c659e <seq_buf_print_seq>:
 * @s: the seq_buf descriptor that is the source.
 *
 * Returns zero on success, non zero otherwise
 */
int seq_buf_print_seq(struct seq_file *m, struct seq_buf *s)
{
ffffffff812c659e:	55                   	push   %rbp
	unsigned int len = seq_buf_used(s);

	return seq_write(m, s->buffer, len);
ffffffff812c659f:	48 8b 56 08          	mov    0x8(%rsi),%rdx
ffffffff812c65a3:	48 39 56 10          	cmp    %rdx,0x10(%rsi)
ffffffff812c65a7:	48 0f 46 56 10       	cmovbe 0x10(%rsi),%rdx
ffffffff812c65ac:	48 8b 36             	mov    (%rsi),%rsi
 * @s: the seq_buf descriptor that is the source.
 *
 * Returns zero on success, non zero otherwise
 */
int seq_buf_print_seq(struct seq_file *m, struct seq_buf *s)
{
ffffffff812c65af:	48 89 e5             	mov    %rsp,%rbp
	unsigned int len = seq_buf_used(s);

	return seq_write(m, s->buffer, len);
ffffffff812c65b2:	89 d2                	mov    %edx,%edx
ffffffff812c65b4:	e8 40 cd e5 ff       	callq  ffffffff811232f9 <seq_write>
}
ffffffff812c65b9:	5d                   	pop    %rbp
ffffffff812c65ba:	c3                   	retq   

ffffffff812c65bb <seq_buf_vprintf>:
 * Writes a vnprintf() format into the sequencce buffer.
 *
 * Returns zero on success, -1 on overflow.
 */
int seq_buf_vprintf(struct seq_buf *s, const char *fmt, va_list args)
{
ffffffff812c65bb:	55                   	push   %rbp
ffffffff812c65bc:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c65bf:	41 55                	push   %r13
ffffffff812c65c1:	41 54                	push   %r12
ffffffff812c65c3:	53                   	push   %rbx
ffffffff812c65c4:	51                   	push   %rcx
ffffffff812c65c5:	48 89 fb             	mov    %rdi,%rbx
	int len;

	WARN_ON(s->size == 0);
ffffffff812c65c8:	48 83 7f 08 00       	cmpq   $0x0,0x8(%rdi)
 * Writes a vnprintf() format into the sequencce buffer.
 *
 * Returns zero on success, -1 on overflow.
 */
int seq_buf_vprintf(struct seq_buf *s, const char *fmt, va_list args)
{
ffffffff812c65cd:	49 89 f4             	mov    %rsi,%r12
ffffffff812c65d0:	49 89 d5             	mov    %rdx,%r13
	int len;

	WARN_ON(s->size == 0);
ffffffff812c65d3:	75 11                	jne    ffffffff812c65e6 <seq_buf_vprintf+0x2b>
ffffffff812c65d5:	be 3c 00 00 00       	mov    $0x3c,%esi
ffffffff812c65da:	48 c7 c7 cd 6c 7b 81 	mov    $0xffffffff817b6ccd,%rdi
ffffffff812c65e1:	e8 cf fd d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>

	if (s->len < s->size) {
ffffffff812c65e6:	48 8b 7b 10          	mov    0x10(%rbx),%rdi
ffffffff812c65ea:	48 8b 73 08          	mov    0x8(%rbx),%rsi
ffffffff812c65ee:	48 39 f7             	cmp    %rsi,%rdi
ffffffff812c65f1:	73 25                	jae    ffffffff812c6618 <seq_buf_vprintf+0x5d>
		len = vsnprintf(s->buffer + s->len, s->size - s->len, fmt, args);
ffffffff812c65f3:	48 29 fe             	sub    %rdi,%rsi
ffffffff812c65f6:	48 03 3b             	add    (%rbx),%rdi
ffffffff812c65f9:	4c 89 e9             	mov    %r13,%rcx
ffffffff812c65fc:	4c 89 e2             	mov    %r12,%rdx
ffffffff812c65ff:	e8 bc 3d 00 00       	callq  ffffffff812ca3c0 <vsnprintf>
		if (s->len + len < s->size) {
ffffffff812c6604:	48 98                	cltq   
ffffffff812c6606:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812c660a:	48 3b 43 08          	cmp    0x8(%rbx),%rax
ffffffff812c660e:	73 08                	jae    ffffffff812c6618 <seq_buf_vprintf+0x5d>
			s->len += len;
ffffffff812c6610:	48 89 43 10          	mov    %rax,0x10(%rbx)
			return 0;
ffffffff812c6614:	31 c0                	xor    %eax,%eax
ffffffff812c6616:	eb 0e                	jmp    ffffffff812c6626 <seq_buf_vprintf+0x6b>
}

static inline void
seq_buf_set_overflow(struct seq_buf *s)
{
	s->len = s->size + 1;
ffffffff812c6618:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812c661c:	48 ff c0             	inc    %rax
ffffffff812c661f:	48 89 43 10          	mov    %rax,0x10(%rbx)
		}
	}
	seq_buf_set_overflow(s);
	return -1;
ffffffff812c6623:	83 c8 ff             	or     $0xffffffff,%eax
}
ffffffff812c6626:	5a                   	pop    %rdx
ffffffff812c6627:	5b                   	pop    %rbx
ffffffff812c6628:	41 5c                	pop    %r12
ffffffff812c662a:	41 5d                	pop    %r13
ffffffff812c662c:	5d                   	pop    %rbp
ffffffff812c662d:	c3                   	retq   

ffffffff812c662e <seq_buf_printf>:
 * Writes a printf() format into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow.
 */
int seq_buf_printf(struct seq_buf *s, const char *fmt, ...)
{
ffffffff812c662e:	55                   	push   %rbp
ffffffff812c662f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c6632:	48 83 ec 50          	sub    $0x50,%rsp
	va_list ap;
	int ret;

	va_start(ap, fmt);
ffffffff812c6636:	48 8d 45 10          	lea    0x10(%rbp),%rax
 * Writes a printf() format into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow.
 */
int seq_buf_printf(struct seq_buf *s, const char *fmt, ...)
{
ffffffff812c663a:	48 89 55 e0          	mov    %rdx,-0x20(%rbp)
	va_list ap;
	int ret;

	va_start(ap, fmt);
	ret = seq_buf_vprintf(s, fmt, ap);
ffffffff812c663e:	48 8d 55 b8          	lea    -0x48(%rbp),%rdx
 * Writes a printf() format into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow.
 */
int seq_buf_printf(struct seq_buf *s, const char *fmt, ...)
{
ffffffff812c6642:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
ffffffff812c6646:	4c 89 45 f0          	mov    %r8,-0x10(%rbp)
	va_list ap;
	int ret;

	va_start(ap, fmt);
ffffffff812c664a:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812c664e:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
 * Writes a printf() format into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow.
 */
int seq_buf_printf(struct seq_buf *s, const char *fmt, ...)
{
ffffffff812c6652:	4c 89 4d f8          	mov    %r9,-0x8(%rbp)
	va_list ap;
	int ret;

	va_start(ap, fmt);
ffffffff812c6656:	c7 45 b8 10 00 00 00 	movl   $0x10,-0x48(%rbp)
ffffffff812c665d:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	ret = seq_buf_vprintf(s, fmt, ap);
ffffffff812c6661:	e8 55 ff ff ff       	callq  ffffffff812c65bb <seq_buf_vprintf>
	va_end(ap);

	return ret;
}
ffffffff812c6666:	c9                   	leaveq 
ffffffff812c6667:	c3                   	retq   

ffffffff812c6668 <seq_buf_puts>:
 * Copy a simple string into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_puts(struct seq_buf *s, const char *str)
{
ffffffff812c6668:	55                   	push   %rbp
	unsigned int len = strlen(str);
ffffffff812c6669:	31 c0                	xor    %eax,%eax
 * Copy a simple string into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_puts(struct seq_buf *s, const char *str)
{
ffffffff812c666b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c666e:	41 55                	push   %r13
ffffffff812c6670:	41 54                	push   %r12
ffffffff812c6672:	53                   	push   %rbx
ffffffff812c6673:	51                   	push   %rcx
ffffffff812c6674:	49 89 fd             	mov    %rdi,%r13
	unsigned int len = strlen(str);
ffffffff812c6677:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812c667b:	48 89 f7             	mov    %rsi,%rdi
 * Copy a simple string into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_puts(struct seq_buf *s, const char *str)
{
ffffffff812c667e:	49 89 f4             	mov    %rsi,%r12
	unsigned int len = strlen(str);
ffffffff812c6681:	f2 ae                	repnz scas %es:(%rdi),%al

	WARN_ON(s->size == 0);
ffffffff812c6683:	49 83 7d 08 00       	cmpq   $0x0,0x8(%r13)
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_puts(struct seq_buf *s, const char *str)
{
	unsigned int len = strlen(str);
ffffffff812c6688:	48 89 c8             	mov    %rcx,%rax
ffffffff812c668b:	48 f7 d0             	not    %rax
ffffffff812c668e:	48 8d 58 ff          	lea    -0x1(%rax),%rbx

	WARN_ON(s->size == 0);
ffffffff812c6692:	75 11                	jne    ffffffff812c66a5 <seq_buf_puts+0x3d>
ffffffff812c6694:	be 90 00 00 00       	mov    $0x90,%esi
ffffffff812c6699:	48 c7 c7 cd 6c 7b 81 	mov    $0xffffffff817b6ccd,%rdi
ffffffff812c66a0:	e8 10 fd d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
 * Returns true if there's enough unused space in the seq_buf buffer
 * to fit the amount of new data according to @len.
 */
static bool seq_buf_can_fit(struct seq_buf *s, size_t len)
{
	return s->len + len <= s->size;
ffffffff812c66a5:	49 8b 55 10          	mov    0x10(%r13),%rdx
ffffffff812c66a9:	89 d8                	mov    %ebx,%eax
ffffffff812c66ab:	49 8b 4d 08          	mov    0x8(%r13),%rcx
{
	unsigned int len = strlen(str);

	WARN_ON(s->size == 0);

	if (seq_buf_can_fit(s, len)) {
ffffffff812c66af:	48 8d 34 10          	lea    (%rax,%rdx,1),%rsi
ffffffff812c66b3:	48 39 ce             	cmp    %rcx,%rsi
ffffffff812c66b6:	77 17                	ja     ffffffff812c66cf <seq_buf_puts+0x67>
		memcpy(s->buffer + s->len, str, len);
ffffffff812c66b8:	49 03 55 00          	add    0x0(%r13),%rdx
ffffffff812c66bc:	48 89 c1             	mov    %rax,%rcx
ffffffff812c66bf:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c66c2:	48 89 d7             	mov    %rdx,%rdi
ffffffff812c66c5:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
		s->len += len;
ffffffff812c66c7:	49 01 45 10          	add    %rax,0x10(%r13)
		return 0;
ffffffff812c66cb:	31 c0                	xor    %eax,%eax
ffffffff812c66cd:	eb 0a                	jmp    ffffffff812c66d9 <seq_buf_puts+0x71>
ffffffff812c66cf:	48 ff c1             	inc    %rcx
	}
	seq_buf_set_overflow(s);
	return -1;
ffffffff812c66d2:	83 c8 ff             	or     $0xffffffff,%eax
ffffffff812c66d5:	49 89 4d 10          	mov    %rcx,0x10(%r13)
}
ffffffff812c66d9:	5a                   	pop    %rdx
ffffffff812c66da:	5b                   	pop    %rbx
ffffffff812c66db:	41 5c                	pop    %r12
ffffffff812c66dd:	41 5d                	pop    %r13
ffffffff812c66df:	5d                   	pop    %rbp
ffffffff812c66e0:	c3                   	retq   

ffffffff812c66e1 <seq_buf_putc>:
 * Copy a single character into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_putc(struct seq_buf *s, unsigned char c)
{
ffffffff812c66e1:	55                   	push   %rbp
ffffffff812c66e2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c66e5:	41 54                	push   %r12
ffffffff812c66e7:	53                   	push   %rbx
	WARN_ON(s->size == 0);
ffffffff812c66e8:	48 83 7f 08 00       	cmpq   $0x0,0x8(%rdi)
 * Copy a single character into the sequence buffer.
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_putc(struct seq_buf *s, unsigned char c)
{
ffffffff812c66ed:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c66f0:	41 89 f4             	mov    %esi,%r12d
	WARN_ON(s->size == 0);
ffffffff812c66f3:	75 11                	jne    ffffffff812c6706 <seq_buf_putc+0x25>
ffffffff812c66f5:	be a6 00 00 00       	mov    $0xa6,%esi
ffffffff812c66fa:	48 c7 c7 cd 6c 7b 81 	mov    $0xffffffff817b6ccd,%rdi
ffffffff812c6701:	e8 af fc d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
 * Returns true if there's enough unused space in the seq_buf buffer
 * to fit the amount of new data according to @len.
 */
static bool seq_buf_can_fit(struct seq_buf *s, size_t len)
{
	return s->len + len <= s->size;
ffffffff812c6706:	48 8b 53 10          	mov    0x10(%rbx),%rdx
ffffffff812c670a:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812c670e:	48 8d 4a 01          	lea    0x1(%rdx),%rcx
 */
int seq_buf_putc(struct seq_buf *s, unsigned char c)
{
	WARN_ON(s->size == 0);

	if (seq_buf_can_fit(s, 1)) {
ffffffff812c6712:	48 39 c1             	cmp    %rax,%rcx
ffffffff812c6715:	77 0f                	ja     ffffffff812c6726 <seq_buf_putc+0x45>
		s->buffer[s->len++] = c;
ffffffff812c6717:	48 8b 03             	mov    (%rbx),%rax
ffffffff812c671a:	48 89 4b 10          	mov    %rcx,0x10(%rbx)
ffffffff812c671e:	44 88 24 10          	mov    %r12b,(%rax,%rdx,1)
		return 0;
ffffffff812c6722:	31 c0                	xor    %eax,%eax
ffffffff812c6724:	eb 0a                	jmp    ffffffff812c6730 <seq_buf_putc+0x4f>
ffffffff812c6726:	48 ff c0             	inc    %rax
ffffffff812c6729:	48 89 43 10          	mov    %rax,0x10(%rbx)
	}
	seq_buf_set_overflow(s);
	return -1;
ffffffff812c672d:	83 c8 ff             	or     $0xffffffff,%eax
}
ffffffff812c6730:	5b                   	pop    %rbx
ffffffff812c6731:	41 5c                	pop    %r12
ffffffff812c6733:	5d                   	pop    %rbp
ffffffff812c6734:	c3                   	retq   

ffffffff812c6735 <seq_buf_putmem>:
 * for such cases.
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_putmem(struct seq_buf *s, const void *mem, unsigned int len)
{
ffffffff812c6735:	55                   	push   %rbp
ffffffff812c6736:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c6739:	41 55                	push   %r13
ffffffff812c673b:	41 54                	push   %r12
ffffffff812c673d:	53                   	push   %rbx
ffffffff812c673e:	51                   	push   %rcx
ffffffff812c673f:	48 89 fb             	mov    %rdi,%rbx
	WARN_ON(s->size == 0);
ffffffff812c6742:	48 83 7f 08 00       	cmpq   $0x0,0x8(%rdi)
 * for such cases.
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_putmem(struct seq_buf *s, const void *mem, unsigned int len)
{
ffffffff812c6747:	49 89 f5             	mov    %rsi,%r13
ffffffff812c674a:	41 89 d4             	mov    %edx,%r12d
	WARN_ON(s->size == 0);
ffffffff812c674d:	75 11                	jne    ffffffff812c6760 <seq_buf_putmem+0x2b>
ffffffff812c674f:	be be 00 00 00       	mov    $0xbe,%esi
ffffffff812c6754:	48 c7 c7 cd 6c 7b 81 	mov    $0xffffffff817b6ccd,%rdi
ffffffff812c675b:	e8 55 fc d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
 * Returns true if there's enough unused space in the seq_buf buffer
 * to fit the amount of new data according to @len.
 */
static bool seq_buf_can_fit(struct seq_buf *s, size_t len)
{
	return s->len + len <= s->size;
ffffffff812c6760:	48 8b 43 10          	mov    0x10(%rbx),%rax
 */
int seq_buf_putmem(struct seq_buf *s, const void *mem, unsigned int len)
{
	WARN_ON(s->size == 0);

	if (seq_buf_can_fit(s, len)) {
ffffffff812c6764:	44 89 e2             	mov    %r12d,%edx
 * Returns true if there's enough unused space in the seq_buf buffer
 * to fit the amount of new data according to @len.
 */
static bool seq_buf_can_fit(struct seq_buf *s, size_t len)
{
	return s->len + len <= s->size;
ffffffff812c6767:	48 8b 4b 08          	mov    0x8(%rbx),%rcx
 */
int seq_buf_putmem(struct seq_buf *s, const void *mem, unsigned int len)
{
	WARN_ON(s->size == 0);

	if (seq_buf_can_fit(s, len)) {
ffffffff812c676b:	48 8d 34 02          	lea    (%rdx,%rax,1),%rsi
ffffffff812c676f:	48 39 ce             	cmp    %rcx,%rsi
ffffffff812c6772:	77 16                	ja     ffffffff812c678a <seq_buf_putmem+0x55>
		memcpy(s->buffer + s->len, mem, len);
ffffffff812c6774:	48 03 03             	add    (%rbx),%rax
ffffffff812c6777:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c677a:	48 89 d1             	mov    %rdx,%rcx
ffffffff812c677d:	48 89 c7             	mov    %rax,%rdi
		s->len += len;
		return 0;
ffffffff812c6780:	31 c0                	xor    %eax,%eax
int seq_buf_putmem(struct seq_buf *s, const void *mem, unsigned int len)
{
	WARN_ON(s->size == 0);

	if (seq_buf_can_fit(s, len)) {
		memcpy(s->buffer + s->len, mem, len);
ffffffff812c6782:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
		s->len += len;
ffffffff812c6784:	48 01 53 10          	add    %rdx,0x10(%rbx)
		return 0;
ffffffff812c6788:	eb 0a                	jmp    ffffffff812c6794 <seq_buf_putmem+0x5f>
ffffffff812c678a:	48 ff c1             	inc    %rcx
	}
	seq_buf_set_overflow(s);
	return -1;
ffffffff812c678d:	83 c8 ff             	or     $0xffffffff,%eax
ffffffff812c6790:	48 89 4b 10          	mov    %rcx,0x10(%rbx)
}
ffffffff812c6794:	5a                   	pop    %rdx
ffffffff812c6795:	5b                   	pop    %rbx
ffffffff812c6796:	41 5c                	pop    %r12
ffffffff812c6798:	41 5d                	pop    %r13
ffffffff812c679a:	5d                   	pop    %rbp
ffffffff812c679b:	c3                   	retq   

ffffffff812c679c <seq_buf_putmem_hex>:
 *
 * Returns zero on success, -1 on overflow
 */
int seq_buf_putmem_hex(struct seq_buf *s, const void *mem,
		       unsigned int len)
{
ffffffff812c679c:	55                   	push   %rbp
ffffffff812c679d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c67a0:	41 57                	push   %r15
ffffffff812c67a2:	41 56                	push   %r14
ffffffff812c67a4:	41 55                	push   %r13
ffffffff812c67a6:	41 54                	push   %r12
ffffffff812c67a8:	49 89 ff             	mov    %rdi,%r15
ffffffff812c67ab:	53                   	push   %rbx
ffffffff812c67ac:	49 89 f5             	mov    %rsi,%r13
ffffffff812c67af:	41 89 d4             	mov    %edx,%r12d
ffffffff812c67b2:	48 83 ec 28          	sub    $0x28,%rsp
	unsigned char hex[HEX_CHARS];
	const unsigned char *data = mem;
	unsigned int start_len;
	int i, j;

	WARN_ON(s->size == 0);
ffffffff812c67b6:	48 83 7f 08 00       	cmpq   $0x0,0x8(%rdi)
ffffffff812c67bb:	75 11                	jne    ffffffff812c67ce <seq_buf_putmem_hex+0x32>
ffffffff812c67bd:	be e0 00 00 00       	mov    $0xe0,%esi
ffffffff812c67c2:	48 c7 c7 cd 6c 7b 81 	mov    $0xffffffff817b6ccd,%rdi
ffffffff812c67c9:	e8 e7 fb d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>

	while (len) {
		start_len = min(len, HEX_CHARS - 1);
ffffffff812c67ce:	41 be 10 00 00 00    	mov    $0x10,%r14d
	unsigned int start_len;
	int i, j;

	WARN_ON(s->size == 0);

	while (len) {
ffffffff812c67d4:	45 85 e4             	test   %r12d,%r12d
ffffffff812c67d7:	0f 84 99 00 00 00    	je     ffffffff812c6876 <seq_buf_putmem_hex+0xda>
		start_len = min(len, HEX_CHARS - 1);
ffffffff812c67dd:	41 83 fc 10          	cmp    $0x10,%r12d
ffffffff812c67e1:	44 89 f7             	mov    %r14d,%edi
ffffffff812c67e4:	48 8d 45 bf          	lea    -0x41(%rbp),%rax
ffffffff812c67e8:	41 0f 46 fc          	cmovbe %r12d,%edi
#ifdef __BIG_ENDIAN
		for (i = 0, j = 0; i < start_len; i++) {
#else
		for (i = start_len-1, j = 0; i >= 0; i--) {
ffffffff812c67ec:	8d 5f ff             	lea    -0x1(%rdi),%ebx
ffffffff812c67ef:	48 89 c6             	mov    %rax,%rsi
#endif
			hex[j++] = hex_asc_hi(data[i]);
ffffffff812c67f2:	48 63 d3             	movslq %ebx,%rdx
	while (len) {
		start_len = min(len, HEX_CHARS - 1);
#ifdef __BIG_ENDIAN
		for (i = 0, j = 0; i < start_len; i++) {
#else
		for (i = start_len-1, j = 0; i >= 0; i--) {
ffffffff812c67f5:	ff cb                	dec    %ebx
ffffffff812c67f7:	48 83 c0 02          	add    $0x2,%rax
#endif
			hex[j++] = hex_asc_hi(data[i]);
ffffffff812c67fb:	41 8a 54 15 00       	mov    0x0(%r13,%rdx,1),%dl
ffffffff812c6800:	88 d1                	mov    %dl,%cl
			hex[j++] = hex_asc_lo(data[i]);
ffffffff812c6802:	83 e2 0f             	and    $0xf,%edx
#ifdef __BIG_ENDIAN
		for (i = 0, j = 0; i < start_len; i++) {
#else
		for (i = start_len-1, j = 0; i >= 0; i--) {
#endif
			hex[j++] = hex_asc_hi(data[i]);
ffffffff812c6805:	c0 e9 04             	shr    $0x4,%cl
			hex[j++] = hex_asc_lo(data[i]);
ffffffff812c6808:	8a 92 b0 32 64 81    	mov    -0x7e9bcd50(%rdx),%dl
#ifdef __BIG_ENDIAN
		for (i = 0, j = 0; i < start_len; i++) {
#else
		for (i = start_len-1, j = 0; i >= 0; i--) {
#endif
			hex[j++] = hex_asc_hi(data[i]);
ffffffff812c680e:	83 e1 0f             	and    $0xf,%ecx
ffffffff812c6811:	8a 89 b0 32 64 81    	mov    -0x7e9bcd50(%rcx),%cl
			hex[j++] = hex_asc_lo(data[i]);
ffffffff812c6817:	88 50 ff             	mov    %dl,-0x1(%rax)
#ifdef __BIG_ENDIAN
		for (i = 0, j = 0; i < start_len; i++) {
#else
		for (i = start_len-1, j = 0; i >= 0; i--) {
#endif
			hex[j++] = hex_asc_hi(data[i]);
ffffffff812c681a:	88 48 fe             	mov    %cl,-0x2(%rax)
	while (len) {
		start_len = min(len, HEX_CHARS - 1);
#ifdef __BIG_ENDIAN
		for (i = 0, j = 0; i < start_len; i++) {
#else
		for (i = start_len-1, j = 0; i >= 0; i--) {
ffffffff812c681d:	83 fb ff             	cmp    $0xffffffff,%ebx
ffffffff812c6820:	75 d0                	jne    ffffffff812c67f2 <seq_buf_putmem_hex+0x56>
ffffffff812c6822:	8d 54 3f fe          	lea    -0x2(%rdi,%rdi,1),%edx
ffffffff812c6826:	8d 42 02             	lea    0x2(%rdx),%eax
#endif
			hex[j++] = hex_asc_hi(data[i]);
			hex[j++] = hex_asc_lo(data[i]);
		}
		if (WARN_ON_ONCE(j == 0 || j/2 > len))
ffffffff812c6829:	89 c1                	mov    %eax,%ecx
ffffffff812c682b:	d1 f9                	sar    %ecx
ffffffff812c682d:	41 39 cc             	cmp    %ecx,%r12d
ffffffff812c6830:	72 23                	jb     ffffffff812c6855 <seq_buf_putmem_hex+0xb9>
			break;

		/* j increments twice per loop */
		len -= j / 2;
		hex[j++] = ' ';
ffffffff812c6832:	48 98                	cltq   

		seq_buf_putmem(s, hex, j);
ffffffff812c6834:	83 c2 03             	add    $0x3,%edx
ffffffff812c6837:	4c 89 ff             	mov    %r15,%rdi
		}
		if (WARN_ON_ONCE(j == 0 || j/2 > len))
			break;

		/* j increments twice per loop */
		len -= j / 2;
ffffffff812c683a:	41 29 cc             	sub    %ecx,%r12d
		hex[j++] = ' ';
ffffffff812c683d:	c6 44 05 bf 20       	movb   $0x20,-0x41(%rbp,%rax,1)

		seq_buf_putmem(s, hex, j);
ffffffff812c6842:	e8 ee fe ff ff       	callq  ffffffff812c6735 <seq_buf_putmem>
		if (seq_buf_has_overflowed(s))
ffffffff812c6847:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c684b:	49 39 47 10          	cmp    %rax,0x10(%r15)
ffffffff812c684f:	76 83                	jbe    ffffffff812c67d4 <seq_buf_putmem_hex+0x38>
			return -1;
ffffffff812c6851:	89 d8                	mov    %ebx,%eax
ffffffff812c6853:	eb 23                	jmp    ffffffff812c6878 <seq_buf_putmem_hex+0xdc>
		for (i = start_len-1, j = 0; i >= 0; i--) {
#endif
			hex[j++] = hex_asc_hi(data[i]);
			hex[j++] = hex_asc_lo(data[i]);
		}
		if (WARN_ON_ONCE(j == 0 || j/2 > len))
ffffffff812c6855:	80 3d 8e 0a 79 00 00 	cmpb   $0x0,0x790a8e(%rip)        # ffffffff81a572ea <__warned.16338>
ffffffff812c685c:	75 18                	jne    ffffffff812c6876 <seq_buf_putmem_hex+0xda>
ffffffff812c685e:	be ec 00 00 00       	mov    $0xec,%esi
ffffffff812c6863:	48 c7 c7 cd 6c 7b 81 	mov    $0xffffffff817b6ccd,%rdi
ffffffff812c686a:	e8 46 fb d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c686f:	c6 05 74 0a 79 00 01 	movb   $0x1,0x790a74(%rip)        # ffffffff81a572ea <__warned.16338>

		seq_buf_putmem(s, hex, j);
		if (seq_buf_has_overflowed(s))
			return -1;
	}
	return 0;
ffffffff812c6876:	31 c0                	xor    %eax,%eax
}
ffffffff812c6878:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c687c:	5b                   	pop    %rbx
ffffffff812c687d:	41 5c                	pop    %r12
ffffffff812c687f:	41 5d                	pop    %r13
ffffffff812c6881:	41 5e                	pop    %r14
ffffffff812c6883:	41 5f                	pop    %r15
ffffffff812c6885:	5d                   	pop    %rbp
ffffffff812c6886:	c3                   	retq   

ffffffff812c6887 <seq_buf_path>:
 * Write a path name into the sequence buffer.
 *
 * Returns the number of written bytes on success, -1 on overflow
 */
int seq_buf_path(struct seq_buf *s, const struct path *path, const char *esc)
{
ffffffff812c6887:	55                   	push   %rbp
ffffffff812c6888:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c688b:	41 57                	push   %r15
ffffffff812c688d:	41 56                	push   %r14
ffffffff812c688f:	41 55                	push   %r13
ffffffff812c6891:	41 54                	push   %r12
ffffffff812c6893:	49 89 f6             	mov    %rsi,%r14
ffffffff812c6896:	53                   	push   %rbx
ffffffff812c6897:	51                   	push   %rcx
ffffffff812c6898:	48 89 fb             	mov    %rdi,%rbx
 * Return the number of bytes available in the buffer, or zero if
 * there's no space.
 */
static inline size_t seq_buf_get_buf(struct seq_buf *s, char **bufp)
{
	WARN_ON(s->len > s->size + 1);
ffffffff812c689b:	48 8b 47 08          	mov    0x8(%rdi),%rax
ffffffff812c689f:	49 89 d7             	mov    %rdx,%r15
ffffffff812c68a2:	48 ff c0             	inc    %rax
ffffffff812c68a5:	48 39 47 10          	cmp    %rax,0x10(%rdi)
ffffffff812c68a9:	76 11                	jbe    ffffffff812c68bc <seq_buf_path+0x35>
ffffffff812c68ab:	be 53 00 00 00       	mov    $0x53,%esi
ffffffff812c68b0:	48 c7 c7 db 6c 7b 81 	mov    $0xffffffff817b6cdb,%rdi
ffffffff812c68b7:	e8 f9 fa d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>

	if (s->len < s->size) {
ffffffff812c68bc:	48 8b 43 10          	mov    0x10(%rbx),%rax
ffffffff812c68c0:	4c 8b 63 08          	mov    0x8(%rbx),%r12
ffffffff812c68c4:	4c 39 e0             	cmp    %r12,%rax
ffffffff812c68c7:	0f 83 83 00 00 00    	jae    ffffffff812c6950 <seq_buf_path+0xc9>
		*bufp = s->buffer + s->len;
		return s->size - s->len;
ffffffff812c68cd:	4c 89 e2             	mov    %r12,%rdx
static inline size_t seq_buf_get_buf(struct seq_buf *s, char **bufp)
{
	WARN_ON(s->len > s->size + 1);

	if (s->len < s->size) {
		*bufp = s->buffer + s->len;
ffffffff812c68d0:	49 89 c5             	mov    %rax,%r13
ffffffff812c68d3:	4c 03 2b             	add    (%rbx),%r13
		return s->size - s->len;
ffffffff812c68d6:	48 29 c2             	sub    %rax,%rdx
	char *buf;
	size_t size = seq_buf_get_buf(s, &buf);
	int res = -1;

	WARN_ON(s->size == 0);
ffffffff812c68d9:	4d 85 e4             	test   %r12,%r12
ffffffff812c68dc:	75 1c                	jne    ffffffff812c68fa <seq_buf_path+0x73>
ffffffff812c68de:	49 89 d4             	mov    %rdx,%r12
ffffffff812c68e1:	be 0a 01 00 00       	mov    $0x10a,%esi
ffffffff812c68e6:	48 c7 c7 cd 6c 7b 81 	mov    $0xffffffff817b6ccd,%rdi
ffffffff812c68ed:	e8 c3 fa d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>

	if (size) {
ffffffff812c68f2:	4d 85 e4             	test   %r12,%r12
ffffffff812c68f5:	74 61                	je     ffffffff812c6958 <seq_buf_path+0xd1>
ffffffff812c68f7:	4c 89 e2             	mov    %r12,%rdx
		char *p = d_path(path, buf, size);
ffffffff812c68fa:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c68fd:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c6900:	e8 9d 36 e5 ff       	callq  ffffffff81119fa2 <d_path>
		if (!IS_ERR(p)) {
ffffffff812c6905:	48 3d 00 f0 ff ff    	cmp    $0xfffffffffffff000,%rax
ffffffff812c690b:	77 4b                	ja     ffffffff812c6958 <seq_buf_path+0xd1>
			char *end = mangle_path(buf, p, esc);
ffffffff812c690d:	4c 89 fa             	mov    %r15,%rdx
ffffffff812c6910:	48 89 c6             	mov    %rax,%rsi
ffffffff812c6913:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c6916:	e8 2f c8 e5 ff       	callq  ffffffff8112314a <mangle_path>
			if (end)
ffffffff812c691b:	48 85 c0             	test   %rax,%rax
ffffffff812c691e:	74 38                	je     ffffffff812c6958 <seq_buf_path+0xd1>
				res = end - buf;
ffffffff812c6920:	4c 29 e8             	sub    %r13,%rax
 * by seq_buf_get.  To signal an error condition, or that the data
 * didn't fit in the available space, pass a negative @num value.
 */
static inline void seq_buf_commit(struct seq_buf *s, int num)
{
	if (num < 0) {
ffffffff812c6923:	85 c0                	test   %eax,%eax
ffffffff812c6925:	79 0a                	jns    ffffffff812c6931 <seq_buf_path+0xaa>
}

static inline void
seq_buf_set_overflow(struct seq_buf *s)
{
	s->len = s->size + 1;
ffffffff812c6927:	48 8b 4b 08          	mov    0x8(%rbx),%rcx
ffffffff812c692b:	48 8d 51 01          	lea    0x1(%rcx),%rdx
ffffffff812c692f:	eb 0f                	jmp    ffffffff812c6940 <seq_buf_path+0xb9>
{
	if (num < 0) {
		seq_buf_set_overflow(s);
	} else {
		/* num must be negative on overflow */
		BUG_ON(s->len + num > s->size);
ffffffff812c6931:	48 63 d0             	movslq %eax,%rdx
ffffffff812c6934:	48 03 53 10          	add    0x10(%rbx),%rdx
ffffffff812c6938:	48 3b 53 08          	cmp    0x8(%rbx),%rdx
ffffffff812c693c:	76 02                	jbe    ffffffff812c6940 <seq_buf_path+0xb9>
ffffffff812c693e:	0f 0b                	ud2    
		s->len += num;
ffffffff812c6940:	48 89 53 10          	mov    %rdx,0x10(%rbx)
		}
	}
	seq_buf_commit(s, res);

	return res;
}
ffffffff812c6944:	5a                   	pop    %rdx
ffffffff812c6945:	5b                   	pop    %rbx
ffffffff812c6946:	41 5c                	pop    %r12
ffffffff812c6948:	41 5d                	pop    %r13
ffffffff812c694a:	41 5e                	pop    %r14
ffffffff812c694c:	41 5f                	pop    %r15
ffffffff812c694e:	5d                   	pop    %rbp
ffffffff812c694f:	c3                   	retq   
	if (s->len < s->size) {
		*bufp = s->buffer + s->len;
		return s->size - s->len;
	}

	*bufp = NULL;
ffffffff812c6950:	45 31 ed             	xor    %r13d,%r13d
{
	char *buf;
	size_t size = seq_buf_get_buf(s, &buf);
	int res = -1;

	WARN_ON(s->size == 0);
ffffffff812c6953:	4d 85 e4             	test   %r12,%r12
ffffffff812c6956:	74 89                	je     ffffffff812c68e1 <seq_buf_path+0x5a>
ffffffff812c6958:	83 c8 ff             	or     $0xffffffff,%eax
ffffffff812c695b:	eb ca                	jmp    ffffffff812c6927 <seq_buf_path+0xa0>

ffffffff812c695d <seq_buf_to_user>:
{
	int len;
	int ret;

	if (!cnt)
		return 0;
ffffffff812c695d:	45 31 c9             	xor    %r9d,%r9d
int seq_buf_to_user(struct seq_buf *s, char __user *ubuf, int cnt)
{
	int len;
	int ret;

	if (!cnt)
ffffffff812c6960:	85 d2                	test   %edx,%edx
ffffffff812c6962:	74 62                	je     ffffffff812c69c6 <seq_buf_to_user+0x69>
		return 0;

	if (s->len <= s->readpos)
ffffffff812c6964:	48 8b 4f 10          	mov    0x10(%rdi),%rcx
ffffffff812c6968:	4c 8b 47 18          	mov    0x18(%rdi),%r8
		return -EBUSY;
ffffffff812c696c:	41 b9 f0 ff ff ff    	mov    $0xfffffff0,%r9d
	int ret;

	if (!cnt)
		return 0;

	if (s->len <= s->readpos)
ffffffff812c6972:	4c 39 c1             	cmp    %r8,%rcx
ffffffff812c6975:	76 4f                	jbe    ffffffff812c69c6 <seq_buf_to_user+0x69>
 * sequence (@s->len == @s->readpos).
 *
 * Returns -EFAULT if the copy to userspace fails.
 */
int seq_buf_to_user(struct seq_buf *s, char __user *ubuf, int cnt)
{
ffffffff812c6977:	55                   	push   %rbp
ffffffff812c6978:	48 89 f0             	mov    %rsi,%rax
ffffffff812c697b:	4c 89 c6             	mov    %r8,%rsi
ffffffff812c697e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c6981:	41 54                	push   %r12
ffffffff812c6983:	53                   	push   %rbx
		return 0;

	if (s->len <= s->readpos)
		return -EBUSY;

	len = seq_buf_used(s) - s->readpos;
ffffffff812c6984:	48 39 4f 08          	cmp    %rcx,0x8(%rdi)
ffffffff812c6988:	49 89 fc             	mov    %rdi,%r12
ffffffff812c698b:	48 0f 46 4f 08       	cmovbe 0x8(%rdi),%rcx
ffffffff812c6990:	44 29 c1             	sub    %r8d,%ecx
ffffffff812c6993:	39 ca                	cmp    %ecx,%edx
ffffffff812c6995:	89 cb                	mov    %ecx,%ebx
ffffffff812c6997:	0f 4e da             	cmovle %edx,%ebx
ffffffff812c699a:	48 03 37             	add    (%rdi),%rsi
ffffffff812c699d:	48 89 c7             	mov    %rax,%rdi
ffffffff812c69a0:	89 da                	mov    %ebx,%edx
ffffffff812c69a2:	e8 29 41 00 00       	callq  ffffffff812caad0 <_copy_to_user>
	if (cnt > len)
		cnt = len;
	ret = copy_to_user(ubuf, s->buffer + s->readpos, cnt);
	if (ret == cnt)
ffffffff812c69a7:	39 d8                	cmp    %ebx,%eax
		return -EFAULT;
ffffffff812c69a9:	41 b9 f2 ff ff ff    	mov    $0xfffffff2,%r9d

	len = seq_buf_used(s) - s->readpos;
	if (cnt > len)
		cnt = len;
	ret = copy_to_user(ubuf, s->buffer + s->readpos, cnt);
	if (ret == cnt)
ffffffff812c69af:	74 0d                	je     ffffffff812c69be <seq_buf_to_user+0x61>
		return -EFAULT;

	cnt -= ret;
ffffffff812c69b1:	29 c3                	sub    %eax,%ebx

	s->readpos += cnt;
ffffffff812c69b3:	48 63 c3             	movslq %ebx,%rax
ffffffff812c69b6:	49 01 44 24 18       	add    %rax,0x18(%r12)
	return cnt;
ffffffff812c69bb:	41 89 d9             	mov    %ebx,%r9d
}
ffffffff812c69be:	5b                   	pop    %rbx
ffffffff812c69bf:	44 89 c8             	mov    %r9d,%eax
ffffffff812c69c2:	41 5c                	pop    %r12
ffffffff812c69c4:	5d                   	pop    %rbp
ffffffff812c69c5:	c3                   	retq   
ffffffff812c69c6:	44 89 c8             	mov    %r9d,%eax
ffffffff812c69c9:	c3                   	retq   

ffffffff812c69ca <sha_init>:
/**
 * sha_init - initialize the vectors for a SHA1 digest
 * @buf: vector to initialize
 */
void sha_init(__u32 *buf)
{
ffffffff812c69ca:	55                   	push   %rbp
	buf[0] = 0x67452301;
ffffffff812c69cb:	c7 07 01 23 45 67    	movl   $0x67452301,(%rdi)
	buf[1] = 0xefcdab89;
ffffffff812c69d1:	c7 47 04 89 ab cd ef 	movl   $0xefcdab89,0x4(%rdi)
	buf[2] = 0x98badcfe;
ffffffff812c69d8:	c7 47 08 fe dc ba 98 	movl   $0x98badcfe,0x8(%rdi)
/**
 * sha_init - initialize the vectors for a SHA1 digest
 * @buf: vector to initialize
 */
void sha_init(__u32 *buf)
{
ffffffff812c69df:	48 89 e5             	mov    %rsp,%rbp
	buf[0] = 0x67452301;
	buf[1] = 0xefcdab89;
	buf[2] = 0x98badcfe;
	buf[3] = 0x10325476;
ffffffff812c69e2:	c7 47 0c 76 54 32 10 	movl   $0x10325476,0xc(%rdi)
	buf[4] = 0xc3d2e1f0;
ffffffff812c69e9:	c7 47 10 f0 e1 d2 c3 	movl   $0xc3d2e1f0,0x10(%rdi)
}
ffffffff812c69f0:	5d                   	pop    %rbp
ffffffff812c69f1:	c3                   	retq   

ffffffff812c69f2 <get_unaligned_be32>:
}

static inline __attribute_const__ __u32 __fswab32(__u32 val)
{
#ifdef __HAVE_BUILTIN_BSWAP32__
	return __builtin_bswap32(val);
ffffffff812c69f2:	8b 07                	mov    (%rdi),%eax
{
	return be16_to_cpup((__be16 *)p);
}

static inline u32 get_unaligned_be32(const void *p)
{
ffffffff812c69f4:	55                   	push   %rbp
ffffffff812c69f5:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c69f8:	0f c8                	bswap  %eax
	return be32_to_cpup((__be32 *)p);
}
ffffffff812c69fa:	5d                   	pop    %rbp
ffffffff812c69fb:	c3                   	retq   

ffffffff812c69fc <sha_transform>:
 * Note: If the hash is security sensitive, the caller should be sure
 * to clear the workspace. This is left to the caller to avoid
 * unnecessary clears between chained hashing operations.
 */
void sha_transform(__u32 *digest, const char *data, __u32 *array)
{
ffffffff812c69fc:	55                   	push   %rbp
ffffffff812c69fd:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c6a00:	41 57                	push   %r15
ffffffff812c6a02:	41 56                	push   %r14
ffffffff812c6a04:	41 55                	push   %r13
ffffffff812c6a06:	41 54                	push   %r12
ffffffff812c6a08:	53                   	push   %rbx
ffffffff812c6a09:	48 83 ec 20          	sub    $0x20,%rsp

	A = digest[0];
	B = digest[1];
	C = digest[2];
	D = digest[3];
	E = digest[4];
ffffffff812c6a0d:	8b 47 10             	mov    0x10(%rdi),%eax
{
	__u32 A, B, C, D, E;

	A = digest[0];
	B = digest[1];
	C = digest[2];
ffffffff812c6a10:	44 8b 67 08          	mov    0x8(%rdi),%r12d
 */
void sha_transform(__u32 *digest, const char *data, __u32 *array)
{
	__u32 A, B, C, D, E;

	A = digest[0];
ffffffff812c6a14:	44 8b 3f             	mov    (%rdi),%r15d
	B = digest[1];
	C = digest[2];
	D = digest[3];
ffffffff812c6a17:	44 8b 6f 0c          	mov    0xc(%rdi),%r13d
void sha_transform(__u32 *digest, const char *data, __u32 *array)
{
	__u32 A, B, C, D, E;

	A = digest[0];
	B = digest[1];
ffffffff812c6a1b:	8b 5f 04             	mov    0x4(%rdi),%ebx
 * Note: If the hash is security sensitive, the caller should be sure
 * to clear the workspace. This is left to the caller to avoid
 * unnecessary clears between chained hashing operations.
 */
void sha_transform(__u32 *digest, const char *data, __u32 *array)
{
ffffffff812c6a1e:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)
	C = digest[2];
	D = digest[3];
	E = digest[4];

	/* Round 1 - iterations 0-16 take their input from 'data' */
	T_0_15( 0, A, B, C, D, E);
ffffffff812c6a22:	48 89 f7             	mov    %rsi,%rdi
 * Note: If the hash is security sensitive, the caller should be sure
 * to clear the workspace. This is left to the caller to avoid
 * unnecessary clears between chained hashing operations.
 */
void sha_transform(__u32 *digest, const char *data, __u32 *array)
{
ffffffff812c6a25:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)

	A = digest[0];
	B = digest[1];
	C = digest[2];
	D = digest[3];
	E = digest[4];
ffffffff812c6a29:	89 45 c4             	mov    %eax,-0x3c(%rbp)

	/* Round 1 - iterations 0-16 take their input from 'data' */
	T_0_15( 0, A, B, C, D, E);
ffffffff812c6a2c:	e8 c1 ff ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6a31:	8b 55 c4             	mov    -0x3c(%rbp),%edx
ffffffff812c6a34:	44 89 e7             	mov    %r12d,%edi
ffffffff812c6a37:	45 89 f9             	mov    %r15d,%r9d
ffffffff812c6a3a:	41 c1 c1 05          	rol    $0x5,%r9d
ffffffff812c6a3e:	44 31 ef             	xor    %r13d,%edi
ffffffff812c6a41:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
ffffffff812c6a45:	21 df                	and    %ebx,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6a47:	c1 cb 02             	ror    $0x2,%ebx
ffffffff812c6a4a:	46 8d 84 0a 99 79 82 	lea    0x5a827999(%rdx,%r9,1),%r8d
ffffffff812c6a51:	5a 
ffffffff812c6a52:	44 31 ef             	xor    %r13d,%edi
ffffffff812c6a55:	89 01                	mov    %eax,(%rcx)
ffffffff812c6a57:	44 01 c7             	add    %r8d,%edi
ffffffff812c6a5a:	01 f8                	add    %edi,%eax
	T_0_15( 1, E, A, B, C, D);
ffffffff812c6a5c:	48 8d 7e 04          	lea    0x4(%rsi),%rdi
	C = digest[2];
	D = digest[3];
	E = digest[4];

	/* Round 1 - iterations 0-16 take their input from 'data' */
	T_0_15( 0, A, B, C, D, E);
ffffffff812c6a60:	89 45 c4             	mov    %eax,-0x3c(%rbp)
	T_0_15( 1, E, A, B, C, D);
ffffffff812c6a63:	e8 8a ff ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6a68:	44 89 e7             	mov    %r12d,%edi
ffffffff812c6a6b:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
ffffffff812c6a6f:	31 df                	xor    %ebx,%edi
ffffffff812c6a71:	44 21 ff             	and    %r15d,%edi
ffffffff812c6a74:	41 c1 cf 02          	ror    $0x2,%r15d
ffffffff812c6a78:	44 31 e7             	xor    %r12d,%edi
ffffffff812c6a7b:	89 41 04             	mov    %eax,0x4(%rcx)
ffffffff812c6a7e:	45 8d 84 3d 99 79 82 	lea    0x5a827999(%r13,%rdi,1),%r8d
ffffffff812c6a85:	5a 
	T_0_15( 2, D, E, A, B, C);
ffffffff812c6a86:	48 8d 7e 08          	lea    0x8(%rsi),%rdi
ffffffff812c6a8a:	44 01 c0             	add    %r8d,%eax
	D = digest[3];
	E = digest[4];

	/* Round 1 - iterations 0-16 take their input from 'data' */
	T_0_15( 0, A, B, C, D, E);
	T_0_15( 1, E, A, B, C, D);
ffffffff812c6a8d:	44 8b 45 c4          	mov    -0x3c(%rbp),%r8d
ffffffff812c6a91:	41 c1 c0 05          	rol    $0x5,%r8d
ffffffff812c6a95:	45 8d 2c 00          	lea    (%r8,%rax,1),%r13d
	T_0_15( 2, D, E, A, B, C);
ffffffff812c6a99:	e8 54 ff ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6a9e:	41 89 db             	mov    %ebx,%r11d
ffffffff812c6aa1:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
	T_0_15( 3, C, D, E, A, B);
ffffffff812c6aa5:	48 8d 7e 0c          	lea    0xc(%rsi),%rdi
ffffffff812c6aa9:	45 31 fb             	xor    %r15d,%r11d
ffffffff812c6aac:	44 23 5d c4          	and    -0x3c(%rbp),%r11d
	E = digest[4];

	/* Round 1 - iterations 0-16 take their input from 'data' */
	T_0_15( 0, A, B, C, D, E);
	T_0_15( 1, E, A, B, C, D);
	T_0_15( 2, D, E, A, B, C);
ffffffff812c6ab0:	89 42 08             	mov    %eax,0x8(%rdx)
ffffffff812c6ab3:	41 8d 84 04 99 79 82 	lea    0x5a827999(%r12,%rax,1),%eax
ffffffff812c6aba:	5a 
ffffffff812c6abb:	41 31 db             	xor    %ebx,%r11d
ffffffff812c6abe:	41 01 c3             	add    %eax,%r11d
ffffffff812c6ac1:	44 89 e8             	mov    %r13d,%eax
ffffffff812c6ac4:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6ac7:	44 01 d8             	add    %r11d,%eax
ffffffff812c6aca:	89 45 bc             	mov    %eax,-0x44(%rbp)
ffffffff812c6acd:	8b 45 c4             	mov    -0x3c(%rbp),%eax
ffffffff812c6ad0:	c1 c8 02             	ror    $0x2,%eax
ffffffff812c6ad3:	89 45 c4             	mov    %eax,-0x3c(%rbp)
	T_0_15( 3, C, D, E, A, B);
ffffffff812c6ad6:	e8 17 ff ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6adb:	8b 7d c4             	mov    -0x3c(%rbp),%edi
ffffffff812c6ade:	8d 9c 18 99 79 82 5a 	lea    0x5a827999(%rax,%rbx,1),%ebx
ffffffff812c6ae5:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
ffffffff812c6ae9:	44 31 ff             	xor    %r15d,%edi
ffffffff812c6aec:	89 42 0c             	mov    %eax,0xc(%rdx)
ffffffff812c6aef:	44 21 ef             	and    %r13d,%edi
ffffffff812c6af2:	41 c1 cd 02          	ror    $0x2,%r13d
ffffffff812c6af6:	44 31 ff             	xor    %r15d,%edi
ffffffff812c6af9:	44 89 6d c0          	mov    %r13d,-0x40(%rbp)
ffffffff812c6afd:	01 df                	add    %ebx,%edi
ffffffff812c6aff:	8b 5d bc             	mov    -0x44(%rbp),%ebx
ffffffff812c6b02:	c1 c3 05             	rol    $0x5,%ebx
ffffffff812c6b05:	01 fb                	add    %edi,%ebx
	T_0_15( 4, B, C, D, E, A);
ffffffff812c6b07:	48 8d 7e 10          	lea    0x10(%rsi),%rdi
ffffffff812c6b0b:	e8 e2 fe ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6b10:	44 8b 65 c4          	mov    -0x3c(%rbp),%r12d
ffffffff812c6b14:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
	T_0_15( 5, A, B, C, D, E);
ffffffff812c6b18:	48 8d 7e 14          	lea    0x14(%rsi),%rdi
ffffffff812c6b1c:	44 8b 5d bc          	mov    -0x44(%rbp),%r11d
	/* Round 1 - iterations 0-16 take their input from 'data' */
	T_0_15( 0, A, B, C, D, E);
	T_0_15( 1, E, A, B, C, D);
	T_0_15( 2, D, E, A, B, C);
	T_0_15( 3, C, D, E, A, B);
	T_0_15( 4, B, C, D, E, A);
ffffffff812c6b20:	45 31 ec             	xor    %r13d,%r12d
ffffffff812c6b23:	44 23 65 bc          	and    -0x44(%rbp),%r12d
ffffffff812c6b27:	89 42 10             	mov    %eax,0x10(%rdx)
ffffffff812c6b2a:	44 33 65 c4          	xor    -0x3c(%rbp),%r12d
ffffffff812c6b2e:	42 8d 84 38 99 79 82 	lea    0x5a827999(%rax,%r15,1),%eax
ffffffff812c6b35:	5a 
ffffffff812c6b36:	41 c1 cb 02          	ror    $0x2,%r11d
ffffffff812c6b3a:	45 89 df             	mov    %r11d,%r15d
ffffffff812c6b3d:	41 01 c4             	add    %eax,%r12d
ffffffff812c6b40:	89 d8                	mov    %ebx,%eax
ffffffff812c6b42:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6b45:	41 01 c4             	add    %eax,%r12d
	T_0_15( 5, A, B, C, D, E);
ffffffff812c6b48:	e8 a5 fe ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6b4d:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
ffffffff812c6b51:	45 31 dd             	xor    %r11d,%r13d
	T_0_15( 6, E, A, B, C, D);
ffffffff812c6b54:	48 8d 7e 18          	lea    0x18(%rsi),%rdi
	T_0_15( 0, A, B, C, D, E);
	T_0_15( 1, E, A, B, C, D);
	T_0_15( 2, D, E, A, B, C);
	T_0_15( 3, C, D, E, A, B);
	T_0_15( 4, B, C, D, E, A);
	T_0_15( 5, A, B, C, D, E);
ffffffff812c6b58:	41 21 dd             	and    %ebx,%r13d
ffffffff812c6b5b:	44 33 6d c0          	xor    -0x40(%rbp),%r13d
ffffffff812c6b5f:	c1 cb 02             	ror    $0x2,%ebx
ffffffff812c6b62:	89 42 14             	mov    %eax,0x14(%rdx)
ffffffff812c6b65:	8b 55 c4             	mov    -0x3c(%rbp),%edx
ffffffff812c6b68:	8d 84 10 99 79 82 5a 	lea    0x5a827999(%rax,%rdx,1),%eax
ffffffff812c6b6f:	41 01 c5             	add    %eax,%r13d
ffffffff812c6b72:	44 89 e0             	mov    %r12d,%eax
ffffffff812c6b75:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6b78:	41 01 c5             	add    %eax,%r13d
	T_0_15( 6, E, A, B, C, D);
ffffffff812c6b7b:	e8 72 fe ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6b80:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
ffffffff812c6b84:	41 31 db             	xor    %ebx,%r11d
	T_0_15( 7, D, E, A, B, C);
ffffffff812c6b87:	48 8d 7e 1c          	lea    0x1c(%rsi),%rdi
	T_0_15( 1, E, A, B, C, D);
	T_0_15( 2, D, E, A, B, C);
	T_0_15( 3, C, D, E, A, B);
	T_0_15( 4, B, C, D, E, A);
	T_0_15( 5, A, B, C, D, E);
	T_0_15( 6, E, A, B, C, D);
ffffffff812c6b8b:	45 21 e3             	and    %r12d,%r11d
ffffffff812c6b8e:	45 89 e1             	mov    %r12d,%r9d
ffffffff812c6b91:	45 31 fb             	xor    %r15d,%r11d
ffffffff812c6b94:	41 c1 c9 02          	ror    $0x2,%r9d
ffffffff812c6b98:	89 41 18             	mov    %eax,0x18(%rcx)
ffffffff812c6b9b:	8b 55 c0             	mov    -0x40(%rbp),%edx
ffffffff812c6b9e:	44 89 4d c4          	mov    %r9d,-0x3c(%rbp)
ffffffff812c6ba2:	8d 84 10 99 79 82 5a 	lea    0x5a827999(%rax,%rdx,1),%eax
ffffffff812c6ba9:	41 01 c3             	add    %eax,%r11d
ffffffff812c6bac:	44 89 e8             	mov    %r13d,%eax
ffffffff812c6baf:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6bb2:	44 01 d8             	add    %r11d,%eax
ffffffff812c6bb5:	89 45 c0             	mov    %eax,-0x40(%rbp)
	T_0_15( 7, D, E, A, B, C);
ffffffff812c6bb8:	e8 35 fe ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6bbd:	44 8b 65 c4          	mov    -0x3c(%rbp),%r12d
ffffffff812c6bc1:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
	T_0_15( 8, C, D, E, A, B);
ffffffff812c6bc5:	48 8d 7e 20          	lea    0x20(%rsi),%rdi
ffffffff812c6bc9:	45 89 e8             	mov    %r13d,%r8d
ffffffff812c6bcc:	41 c1 c8 02          	ror    $0x2,%r8d
	T_0_15( 2, D, E, A, B, C);
	T_0_15( 3, C, D, E, A, B);
	T_0_15( 4, B, C, D, E, A);
	T_0_15( 5, A, B, C, D, E);
	T_0_15( 6, E, A, B, C, D);
	T_0_15( 7, D, E, A, B, C);
ffffffff812c6bd0:	41 31 dc             	xor    %ebx,%r12d
ffffffff812c6bd3:	89 41 1c             	mov    %eax,0x1c(%rcx)
ffffffff812c6bd6:	42 8d 84 38 99 79 82 	lea    0x5a827999(%rax,%r15,1),%eax
ffffffff812c6bdd:	5a 
ffffffff812c6bde:	45 21 ec             	and    %r13d,%r12d
ffffffff812c6be1:	41 31 dc             	xor    %ebx,%r12d
ffffffff812c6be4:	41 01 c4             	add    %eax,%r12d
ffffffff812c6be7:	8b 45 c0             	mov    -0x40(%rbp),%eax
ffffffff812c6bea:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6bed:	41 01 c4             	add    %eax,%r12d
	T_0_15( 8, C, D, E, A, B);
ffffffff812c6bf0:	e8 fd fd ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6bf5:	44 8b 55 c4          	mov    -0x3c(%rbp),%r10d
ffffffff812c6bf9:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
	T_0_15( 9, B, C, D, E, A);
ffffffff812c6bfd:	48 8d 7e 24          	lea    0x24(%rsi),%rdi
ffffffff812c6c01:	44 8b 6d c0          	mov    -0x40(%rbp),%r13d
	T_0_15( 3, C, D, E, A, B);
	T_0_15( 4, B, C, D, E, A);
	T_0_15( 5, A, B, C, D, E);
	T_0_15( 6, E, A, B, C, D);
	T_0_15( 7, D, E, A, B, C);
	T_0_15( 8, C, D, E, A, B);
ffffffff812c6c05:	45 31 c2             	xor    %r8d,%r10d
ffffffff812c6c08:	44 23 55 c0          	and    -0x40(%rbp),%r10d
ffffffff812c6c0c:	89 41 20             	mov    %eax,0x20(%rcx)
ffffffff812c6c0f:	44 33 55 c4          	xor    -0x3c(%rbp),%r10d
ffffffff812c6c13:	8d 84 18 99 79 82 5a 	lea    0x5a827999(%rax,%rbx,1),%eax
ffffffff812c6c1a:	41 c1 cd 02          	ror    $0x2,%r13d
ffffffff812c6c1e:	41 01 c2             	add    %eax,%r10d
ffffffff812c6c21:	44 89 e0             	mov    %r12d,%eax
ffffffff812c6c24:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6c27:	44 01 d0             	add    %r10d,%eax
ffffffff812c6c2a:	89 45 bc             	mov    %eax,-0x44(%rbp)
	T_0_15( 9, B, C, D, E, A);
ffffffff812c6c2d:	e8 c0 fd ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6c32:	48 8b 5d d0          	mov    -0x30(%rbp),%rbx
ffffffff812c6c36:	8b 55 c4             	mov    -0x3c(%rbp),%edx
	T_0_15(10, A, B, C, D, E);
ffffffff812c6c39:	48 8d 7e 28          	lea    0x28(%rsi),%rdi
	T_0_15( 4, B, C, D, E, A);
	T_0_15( 5, A, B, C, D, E);
	T_0_15( 6, E, A, B, C, D);
	T_0_15( 7, D, E, A, B, C);
	T_0_15( 8, C, D, E, A, B);
	T_0_15( 9, B, C, D, E, A);
ffffffff812c6c3d:	89 43 24             	mov    %eax,0x24(%rbx)
ffffffff812c6c40:	44 89 c3             	mov    %r8d,%ebx
ffffffff812c6c43:	8d 84 10 99 79 82 5a 	lea    0x5a827999(%rax,%rdx,1),%eax
ffffffff812c6c4a:	44 31 eb             	xor    %r13d,%ebx
ffffffff812c6c4d:	44 21 e3             	and    %r12d,%ebx
ffffffff812c6c50:	41 c1 cc 02          	ror    $0x2,%r12d
ffffffff812c6c54:	44 31 c3             	xor    %r8d,%ebx
ffffffff812c6c57:	01 c3                	add    %eax,%ebx
ffffffff812c6c59:	8b 45 bc             	mov    -0x44(%rbp),%eax
ffffffff812c6c5c:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6c5f:	01 c3                	add    %eax,%ebx
	T_0_15(10, A, B, C, D, E);
ffffffff812c6c61:	e8 8c fd ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6c66:	45 89 e9             	mov    %r13d,%r9d
ffffffff812c6c69:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
	T_0_15(11, E, A, B, C, D);
ffffffff812c6c6d:	48 8d 7e 2c          	lea    0x2c(%rsi),%rdi
	T_0_15( 5, A, B, C, D, E);
	T_0_15( 6, E, A, B, C, D);
	T_0_15( 7, D, E, A, B, C);
	T_0_15( 8, C, D, E, A, B);
	T_0_15( 9, B, C, D, E, A);
	T_0_15(10, A, B, C, D, E);
ffffffff812c6c71:	45 31 e1             	xor    %r12d,%r9d
ffffffff812c6c74:	44 23 4d bc          	and    -0x44(%rbp),%r9d
ffffffff812c6c78:	44 8b 7d bc          	mov    -0x44(%rbp),%r15d
ffffffff812c6c7c:	89 41 28             	mov    %eax,0x28(%rcx)
ffffffff812c6c7f:	42 8d 84 00 99 79 82 	lea    0x5a827999(%rax,%r8,1),%eax
ffffffff812c6c86:	5a 
ffffffff812c6c87:	41 c1 cf 02          	ror    $0x2,%r15d
ffffffff812c6c8b:	45 31 e9             	xor    %r13d,%r9d
ffffffff812c6c8e:	41 01 c1             	add    %eax,%r9d
ffffffff812c6c91:	89 d8                	mov    %ebx,%eax
ffffffff812c6c93:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6c96:	44 01 c8             	add    %r9d,%eax
ffffffff812c6c99:	89 45 c4             	mov    %eax,-0x3c(%rbp)
	T_0_15(11, E, A, B, C, D);
ffffffff812c6c9c:	e8 51 fd ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6ca1:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
ffffffff812c6ca5:	45 89 e0             	mov    %r12d,%r8d
	T_0_15(12, D, E, A, B, C);
ffffffff812c6ca8:	48 8d 7e 30          	lea    0x30(%rsi),%rdi
	T_0_15( 6, E, A, B, C, D);
	T_0_15( 7, D, E, A, B, C);
	T_0_15( 8, C, D, E, A, B);
	T_0_15( 9, B, C, D, E, A);
	T_0_15(10, A, B, C, D, E);
	T_0_15(11, E, A, B, C, D);
ffffffff812c6cac:	45 31 f8             	xor    %r15d,%r8d
ffffffff812c6caf:	41 21 d8             	and    %ebx,%r8d
ffffffff812c6cb2:	c1 cb 02             	ror    $0x2,%ebx
ffffffff812c6cb5:	89 41 2c             	mov    %eax,0x2c(%rcx)
ffffffff812c6cb8:	41 8d 84 05 99 79 82 	lea    0x5a827999(%r13,%rax,1),%eax
ffffffff812c6cbf:	5a 
ffffffff812c6cc0:	45 31 e0             	xor    %r12d,%r8d
ffffffff812c6cc3:	41 01 c0             	add    %eax,%r8d
ffffffff812c6cc6:	8b 45 c4             	mov    -0x3c(%rbp),%eax
ffffffff812c6cc9:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6ccc:	45 8d 2c 00          	lea    (%r8,%rax,1),%r13d
	T_0_15(12, D, E, A, B, C);
ffffffff812c6cd0:	e8 1d fd ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6cd5:	45 89 fb             	mov    %r15d,%r11d
ffffffff812c6cd8:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
	T_0_15(13, C, D, E, A, B);
ffffffff812c6cdc:	48 8d 7e 34          	lea    0x34(%rsi),%rdi
	T_0_15( 7, D, E, A, B, C);
	T_0_15( 8, C, D, E, A, B);
	T_0_15( 9, B, C, D, E, A);
	T_0_15(10, A, B, C, D, E);
	T_0_15(11, E, A, B, C, D);
	T_0_15(12, D, E, A, B, C);
ffffffff812c6ce0:	41 31 db             	xor    %ebx,%r11d
ffffffff812c6ce3:	44 23 5d c4          	and    -0x3c(%rbp),%r11d
ffffffff812c6ce7:	89 42 30             	mov    %eax,0x30(%rdx)
ffffffff812c6cea:	41 8d 84 04 99 79 82 	lea    0x5a827999(%r12,%rax,1),%eax
ffffffff812c6cf1:	5a 
ffffffff812c6cf2:	45 31 fb             	xor    %r15d,%r11d
ffffffff812c6cf5:	41 01 c3             	add    %eax,%r11d
ffffffff812c6cf8:	44 89 e8             	mov    %r13d,%eax
ffffffff812c6cfb:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6cfe:	44 01 d8             	add    %r11d,%eax
ffffffff812c6d01:	89 45 c0             	mov    %eax,-0x40(%rbp)
ffffffff812c6d04:	8b 45 c4             	mov    -0x3c(%rbp),%eax
ffffffff812c6d07:	c1 c8 02             	ror    $0x2,%eax
ffffffff812c6d0a:	89 45 c4             	mov    %eax,-0x3c(%rbp)
	T_0_15(13, C, D, E, A, B);
ffffffff812c6d0d:	e8 e0 fc ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6d12:	44 8b 65 c4          	mov    -0x3c(%rbp),%r12d
ffffffff812c6d16:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
	T_0_15(14, B, C, D, E, A);
ffffffff812c6d1a:	48 8d 7e 38          	lea    0x38(%rsi),%rdi
	T_0_15( 8, C, D, E, A, B);
	T_0_15( 9, B, C, D, E, A);
	T_0_15(10, A, B, C, D, E);
	T_0_15(11, E, A, B, C, D);
	T_0_15(12, D, E, A, B, C);
	T_0_15(13, C, D, E, A, B);
ffffffff812c6d1e:	41 31 dc             	xor    %ebx,%r12d
ffffffff812c6d21:	89 42 34             	mov    %eax,0x34(%rdx)
ffffffff812c6d24:	41 8d 84 07 99 79 82 	lea    0x5a827999(%r15,%rax,1),%eax
ffffffff812c6d2b:	5a 
ffffffff812c6d2c:	45 21 ec             	and    %r13d,%r12d
ffffffff812c6d2f:	45 89 ef             	mov    %r13d,%r15d
ffffffff812c6d32:	41 31 dc             	xor    %ebx,%r12d
ffffffff812c6d35:	41 c1 cf 02          	ror    $0x2,%r15d
ffffffff812c6d39:	41 01 c4             	add    %eax,%r12d
ffffffff812c6d3c:	8b 45 c0             	mov    -0x40(%rbp),%eax
ffffffff812c6d3f:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6d42:	41 01 c4             	add    %eax,%r12d
	T_0_15(14, B, C, D, E, A);
ffffffff812c6d45:	e8 a8 fc ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6d4a:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
	T_0_15(15, A, B, C, D, E);
ffffffff812c6d4e:	48 8d 7e 3c          	lea    0x3c(%rsi),%rdi
	T_0_15( 9, B, C, D, E, A);
	T_0_15(10, A, B, C, D, E);
	T_0_15(11, E, A, B, C, D);
	T_0_15(12, D, E, A, B, C);
	T_0_15(13, C, D, E, A, B);
	T_0_15(14, B, C, D, E, A);
ffffffff812c6d52:	89 42 38             	mov    %eax,0x38(%rdx)
ffffffff812c6d55:	44 8b 55 c4          	mov    -0x3c(%rbp),%r10d
ffffffff812c6d59:	8d 84 03 99 79 82 5a 	lea    0x5a827999(%rbx,%rax,1),%eax
ffffffff812c6d60:	45 31 fa             	xor    %r15d,%r10d
ffffffff812c6d63:	44 23 55 c0          	and    -0x40(%rbp),%r10d
ffffffff812c6d67:	44 33 55 c4          	xor    -0x3c(%rbp),%r10d
ffffffff812c6d6b:	41 01 c2             	add    %eax,%r10d
ffffffff812c6d6e:	44 89 e0             	mov    %r12d,%eax
ffffffff812c6d71:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6d74:	41 8d 1c 02          	lea    (%r10,%rax,1),%ebx
ffffffff812c6d78:	8b 45 c0             	mov    -0x40(%rbp),%eax
ffffffff812c6d7b:	c1 c8 02             	ror    $0x2,%eax
ffffffff812c6d7e:	89 45 c0             	mov    %eax,-0x40(%rbp)
	T_0_15(15, A, B, C, D, E);
ffffffff812c6d81:	e8 6c fc ff ff       	callq  ffffffff812c69f2 <get_unaligned_be32>
ffffffff812c6d86:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
ffffffff812c6d8a:	44 8b 6d c0          	mov    -0x40(%rbp),%r13d
ffffffff812c6d8e:	44 89 e6             	mov    %r12d,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6d91:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6d95:	c1 ce 02             	ror    $0x2,%esi
ffffffff812c6d98:	89 41 3c             	mov    %eax,0x3c(%rcx)
ffffffff812c6d9b:	8b 4d c4             	mov    -0x3c(%rbp),%ecx
ffffffff812c6d9e:	45 31 fd             	xor    %r15d,%r13d
ffffffff812c6da1:	45 21 e5             	and    %r12d,%r13d

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
ffffffff812c6da4:	44 8b 65 c0          	mov    -0x40(%rbp),%r12d
	T_16_19(17, D, E, A, B, C);
ffffffff812c6da8:	44 8b 42 38          	mov    0x38(%rdx),%r8d
	T_0_15(10, A, B, C, D, E);
	T_0_15(11, E, A, B, C, D);
	T_0_15(12, D, E, A, B, C);
	T_0_15(13, C, D, E, A, B);
	T_0_15(14, B, C, D, E, A);
	T_0_15(15, A, B, C, D, E);
ffffffff812c6dac:	45 31 fd             	xor    %r15d,%r13d
ffffffff812c6daf:	8d 84 01 99 79 82 5a 	lea    0x5a827999(%rcx,%rax,1),%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6db6:	48 89 d1             	mov    %rdx,%rcx
ffffffff812c6db9:	41 01 c5             	add    %eax,%r13d
ffffffff812c6dbc:	89 d8                	mov    %ebx,%eax
ffffffff812c6dbe:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6dc1:	41 01 c5             	add    %eax,%r13d

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
ffffffff812c6dc4:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
ffffffff812c6dc8:	44 8b 48 34          	mov    0x34(%rax),%r9d
ffffffff812c6dcc:	8b 40 08             	mov    0x8(%rax),%eax
ffffffff812c6dcf:	44 89 cf             	mov    %r9d,%edi
ffffffff812c6dd2:	33 7a 20             	xor    0x20(%rdx),%edi
ffffffff812c6dd5:	31 c7                	xor    %eax,%edi
ffffffff812c6dd7:	33 3a                	xor    (%rdx),%edi
ffffffff812c6dd9:	41 31 f4             	xor    %esi,%r12d
ffffffff812c6ddc:	41 21 dc             	and    %ebx,%r12d
ffffffff812c6ddf:	44 33 65 c0          	xor    -0x40(%rbp),%r12d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6de3:	c1 cb 02             	ror    $0x2,%ebx
ffffffff812c6de6:	41 89 da             	mov    %ebx,%r10d
	T_16_19(17, D, E, A, B, C);
ffffffff812c6de9:	89 f3                	mov    %esi,%ebx
ffffffff812c6deb:	44 31 d3             	xor    %r10d,%ebx
	T_16_19(18, C, D, E, A, B);
ffffffff812c6dee:	45 89 d6             	mov    %r10d,%r14d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6df1:	d1 c7                	rol    %edi
	T_0_15(14, B, C, D, E, A);
	T_0_15(15, A, B, C, D, E);

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
	T_16_19(17, D, E, A, B, C);
ffffffff812c6df3:	44 21 eb             	and    %r13d,%ebx
	T_0_15(13, C, D, E, A, B);
	T_0_15(14, B, C, D, E, A);
	T_0_15(15, A, B, C, D, E);

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
ffffffff812c6df6:	89 3a                	mov    %edi,(%rdx)
ffffffff812c6df8:	41 8d bc 3f 99 79 82 	lea    0x5a827999(%r15,%rdi,1),%edi
ffffffff812c6dff:	5a 
	T_16_19(17, D, E, A, B, C);
ffffffff812c6e00:	44 8b 7a 0c          	mov    0xc(%rdx),%r15d
ffffffff812c6e04:	31 f3                	xor    %esi,%ebx
	T_0_15(13, C, D, E, A, B);
	T_0_15(14, B, C, D, E, A);
	T_0_15(15, A, B, C, D, E);

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
ffffffff812c6e06:	41 01 fc             	add    %edi,%r12d
ffffffff812c6e09:	44 89 ef             	mov    %r13d,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6e0c:	41 c1 cd 02          	ror    $0x2,%r13d
ffffffff812c6e10:	c1 c7 05             	rol    $0x5,%edi
	T_16_19(17, D, E, A, B, C);
	T_16_19(18, C, D, E, A, B);
ffffffff812c6e13:	45 31 ee             	xor    %r13d,%r14d
	T_16_19(19, B, C, D, E, A);
ffffffff812c6e16:	45 89 eb             	mov    %r13d,%r11d
	T_0_15(13, C, D, E, A, B);
	T_0_15(14, B, C, D, E, A);
	T_0_15(15, A, B, C, D, E);

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
ffffffff812c6e19:	41 01 fc             	add    %edi,%r12d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6e1c:	8b 7a 24             	mov    0x24(%rdx),%edi
	T_16_19(17, D, E, A, B, C);
	T_16_19(18, C, D, E, A, B);
ffffffff812c6e1f:	45 21 e6             	and    %r12d,%r14d
ffffffff812c6e22:	45 31 d6             	xor    %r10d,%r14d
ffffffff812c6e25:	44 31 c7             	xor    %r8d,%edi
ffffffff812c6e28:	44 31 ff             	xor    %r15d,%edi
ffffffff812c6e2b:	33 7a 04             	xor    0x4(%rdx),%edi
ffffffff812c6e2e:	d1 c7                	rol    %edi
	T_0_15(14, B, C, D, E, A);
	T_0_15(15, A, B, C, D, E);

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
	T_16_19(17, D, E, A, B, C);
ffffffff812c6e30:	89 7a 04             	mov    %edi,0x4(%rdx)
ffffffff812c6e33:	8b 55 c0             	mov    -0x40(%rbp),%edx
ffffffff812c6e36:	8d bc 3a 99 79 82 5a 	lea    0x5a827999(%rdx,%rdi,1),%edi
ffffffff812c6e3d:	01 fb                	add    %edi,%ebx
ffffffff812c6e3f:	44 89 e7             	mov    %r12d,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6e42:	41 c1 cc 02          	ror    $0x2,%r12d
ffffffff812c6e46:	c1 c7 05             	rol    $0x5,%edi
ffffffff812c6e49:	01 fb                	add    %edi,%ebx
	T_16_19(18, C, D, E, A, B);
ffffffff812c6e4b:	8b 79 3c             	mov    0x3c(%rcx),%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6e4e:	31 f8                	xor    %edi,%eax
ffffffff812c6e50:	33 41 28             	xor    0x28(%rcx),%eax
ffffffff812c6e53:	33 41 10             	xor    0x10(%rcx),%eax
ffffffff812c6e56:	d1 c0                	rol    %eax
ffffffff812c6e58:	89 41 08             	mov    %eax,0x8(%rcx)
ffffffff812c6e5b:	8d 84 06 99 79 82 5a 	lea    0x5a827999(%rsi,%rax,1),%eax
	T_16_19(19, B, C, D, E, A);
ffffffff812c6e62:	8b 31                	mov    (%rcx),%esi
	T_0_15(15, A, B, C, D, E);

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
	T_16_19(17, D, E, A, B, C);
	T_16_19(18, C, D, E, A, B);
ffffffff812c6e64:	41 01 c6             	add    %eax,%r14d
ffffffff812c6e67:	89 d8                	mov    %ebx,%eax
ffffffff812c6e69:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6e6c:	41 01 c6             	add    %eax,%r14d
ffffffff812c6e6f:	44 89 f8             	mov    %r15d,%eax
	T_16_19(19, B, C, D, E, A);

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
ffffffff812c6e72:	45 89 e7             	mov    %r12d,%r15d
ffffffff812c6e75:	31 f0                	xor    %esi,%eax
ffffffff812c6e77:	33 41 2c             	xor    0x2c(%rcx),%eax

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
	T_16_19(17, D, E, A, B, C);
	T_16_19(18, C, D, E, A, B);
	T_16_19(19, B, C, D, E, A);
ffffffff812c6e7a:	45 31 e3             	xor    %r12d,%r11d
ffffffff812c6e7d:	33 41 14             	xor    0x14(%rcx),%eax
ffffffff812c6e80:	41 21 db             	and    %ebx,%r11d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6e83:	c1 cb 02             	ror    $0x2,%ebx
ffffffff812c6e86:	45 31 eb             	xor    %r13d,%r11d

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
ffffffff812c6e89:	41 31 df             	xor    %ebx,%r15d
ffffffff812c6e8c:	45 31 f7             	xor    %r14d,%r15d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6e8f:	d1 c0                	rol    %eax

	/* Round 1 - tail. Input from 512-bit mixing array */
	T_16_19(16, E, A, B, C, D);
	T_16_19(17, D, E, A, B, C);
	T_16_19(18, C, D, E, A, B);
	T_16_19(19, B, C, D, E, A);
ffffffff812c6e91:	89 41 0c             	mov    %eax,0xc(%rcx)
ffffffff812c6e94:	41 8d 84 02 99 79 82 	lea    0x5a827999(%r10,%rax,1),%eax
ffffffff812c6e9b:	5a 
ffffffff812c6e9c:	41 01 c3             	add    %eax,%r11d
ffffffff812c6e9f:	44 89 f0             	mov    %r14d,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6ea2:	41 c1 ce 02          	ror    $0x2,%r14d
ffffffff812c6ea6:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c6ea9:	41 01 c3             	add    %eax,%r11d

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
ffffffff812c6eac:	8b 41 04             	mov    0x4(%rcx),%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6eaf:	44 8b 51 10          	mov    0x10(%rcx),%r10d
ffffffff812c6eb3:	41 31 c2             	xor    %eax,%r10d
ffffffff812c6eb6:	44 33 51 30          	xor    0x30(%rcx),%r10d
ffffffff812c6eba:	44 33 51 18          	xor    0x18(%rcx),%r10d
ffffffff812c6ebe:	41 d1 c2             	rol    %r10d
ffffffff812c6ec1:	44 89 51 10          	mov    %r10d,0x10(%rcx)
ffffffff812c6ec5:	47 8d 94 15 a1 eb d9 	lea    0x6ed9eba1(%r13,%r10,1),%r10d
ffffffff812c6ecc:	6e 
	T_20_39(21, E, A, B, C, D);
ffffffff812c6ecd:	41 89 dd             	mov    %ebx,%r13d
ffffffff812c6ed0:	45 31 f5             	xor    %r14d,%r13d
	T_16_19(17, D, E, A, B, C);
	T_16_19(18, C, D, E, A, B);
	T_16_19(19, B, C, D, E, A);

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
ffffffff812c6ed3:	45 01 d7             	add    %r10d,%r15d
ffffffff812c6ed6:	45 89 da             	mov    %r11d,%r10d
	T_20_39(21, E, A, B, C, D);
ffffffff812c6ed9:	45 31 dd             	xor    %r11d,%r13d
	T_16_19(17, D, E, A, B, C);
	T_16_19(18, C, D, E, A, B);
	T_16_19(19, B, C, D, E, A);

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
ffffffff812c6edc:	41 c1 c2 05          	rol    $0x5,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6ee0:	41 c1 cb 02          	ror    $0x2,%r11d
ffffffff812c6ee4:	45 01 d7             	add    %r10d,%r15d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6ee7:	44 8b 51 14          	mov    0x14(%rcx),%r10d
ffffffff812c6eeb:	45 31 ca             	xor    %r9d,%r10d
ffffffff812c6eee:	44 33 51 08          	xor    0x8(%rcx),%r10d
ffffffff812c6ef2:	44 33 51 1c          	xor    0x1c(%rcx),%r10d
ffffffff812c6ef6:	41 d1 c2             	rol    %r10d
	T_20_39(21, E, A, B, C, D);
ffffffff812c6ef9:	44 89 51 14          	mov    %r10d,0x14(%rcx)
ffffffff812c6efd:	47 8d 94 14 a1 eb d9 	lea    0x6ed9eba1(%r12,%r10,1),%r10d
ffffffff812c6f04:	6e 
	T_20_39(22, D, E, A, B, C);
ffffffff812c6f05:	45 89 f4             	mov    %r14d,%r12d
	T_16_19(18, C, D, E, A, B);
	T_16_19(19, B, C, D, E, A);

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
ffffffff812c6f08:	45 01 d5             	add    %r10d,%r13d
ffffffff812c6f0b:	45 89 fa             	mov    %r15d,%r10d
ffffffff812c6f0e:	41 c1 c2 05          	rol    $0x5,%r10d
ffffffff812c6f12:	45 01 d5             	add    %r10d,%r13d
ffffffff812c6f15:	44 8b 51 20          	mov    0x20(%rcx),%r10d
ffffffff812c6f19:	45 31 c2             	xor    %r8d,%r10d
ffffffff812c6f1c:	44 33 51 18          	xor    0x18(%rcx),%r10d
ffffffff812c6f20:	44 33 51 0c          	xor    0xc(%rcx),%r10d
	T_20_39(22, D, E, A, B, C);
ffffffff812c6f24:	45 31 dc             	xor    %r11d,%r12d
ffffffff812c6f27:	45 31 fc             	xor    %r15d,%r12d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6f2a:	41 c1 cf 02          	ror    $0x2,%r15d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6f2e:	41 d1 c2             	rol    %r10d
ffffffff812c6f31:	44 89 51 18          	mov    %r10d,0x18(%rcx)
ffffffff812c6f35:	46 8d 94 13 a1 eb d9 	lea    0x6ed9eba1(%rbx,%r10,1),%r10d
ffffffff812c6f3c:	6e 
	T_20_39(23, C, D, E, A, B);
ffffffff812c6f3d:	44 89 db             	mov    %r11d,%ebx
ffffffff812c6f40:	44 31 fb             	xor    %r15d,%ebx
	T_16_19(19, B, C, D, E, A);

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
ffffffff812c6f43:	45 01 d4             	add    %r10d,%r12d
ffffffff812c6f46:	45 89 ea             	mov    %r13d,%r10d
	T_20_39(23, C, D, E, A, B);
ffffffff812c6f49:	44 31 eb             	xor    %r13d,%ebx
	T_16_19(19, B, C, D, E, A);

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
ffffffff812c6f4c:	41 c1 c2 05          	rol    $0x5,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6f50:	41 c1 cd 02          	ror    $0x2,%r13d
ffffffff812c6f54:	45 01 d4             	add    %r10d,%r12d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6f57:	41 89 fa             	mov    %edi,%r10d
ffffffff812c6f5a:	44 33 51 24          	xor    0x24(%rcx),%r10d
ffffffff812c6f5e:	44 33 51 1c          	xor    0x1c(%rcx),%r10d
ffffffff812c6f62:	44 33 51 10          	xor    0x10(%rcx),%r10d
ffffffff812c6f66:	41 d1 c2             	rol    %r10d
	T_20_39(23, C, D, E, A, B);
ffffffff812c6f69:	44 89 51 1c          	mov    %r10d,0x1c(%rcx)
ffffffff812c6f6d:	47 8d 94 16 a1 eb d9 	lea    0x6ed9eba1(%r14,%r10,1),%r10d
ffffffff812c6f74:	6e 
	T_20_39(24, B, C, D, E, A);
ffffffff812c6f75:	45 89 fe             	mov    %r15d,%r14d
ffffffff812c6f78:	45 31 ee             	xor    %r13d,%r14d

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
	T_20_39(23, C, D, E, A, B);
ffffffff812c6f7b:	44 01 d3             	add    %r10d,%ebx
ffffffff812c6f7e:	45 89 e2             	mov    %r12d,%r10d
	T_20_39(24, B, C, D, E, A);
ffffffff812c6f81:	45 31 e6             	xor    %r12d,%r14d

	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
	T_20_39(23, C, D, E, A, B);
ffffffff812c6f84:	41 c1 c2 05          	rol    $0x5,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6f88:	41 c1 cc 02          	ror    $0x2,%r12d
ffffffff812c6f8c:	44 01 d3             	add    %r10d,%ebx
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6f8f:	44 8b 51 20          	mov    0x20(%rcx),%r10d
ffffffff812c6f93:	44 33 51 28          	xor    0x28(%rcx),%r10d
ffffffff812c6f97:	41 31 f2             	xor    %esi,%r10d
ffffffff812c6f9a:	44 33 51 14          	xor    0x14(%rcx),%r10d
ffffffff812c6f9e:	41 d1 c2             	rol    %r10d
	T_20_39(24, B, C, D, E, A);
ffffffff812c6fa1:	44 89 51 20          	mov    %r10d,0x20(%rcx)
ffffffff812c6fa5:	47 8d 94 13 a1 eb d9 	lea    0x6ed9eba1(%r11,%r10,1),%r10d
ffffffff812c6fac:	6e 
	T_20_39(25, A, B, C, D, E);
ffffffff812c6fad:	45 89 eb             	mov    %r13d,%r11d
	/* Round 2 */
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
	T_20_39(23, C, D, E, A, B);
	T_20_39(24, B, C, D, E, A);
ffffffff812c6fb0:	45 01 d6             	add    %r10d,%r14d
ffffffff812c6fb3:	41 89 da             	mov    %ebx,%r10d
ffffffff812c6fb6:	41 c1 c2 05          	rol    $0x5,%r10d
ffffffff812c6fba:	45 01 d6             	add    %r10d,%r14d
ffffffff812c6fbd:	44 8b 51 24          	mov    0x24(%rcx),%r10d
ffffffff812c6fc1:	44 33 51 2c          	xor    0x2c(%rcx),%r10d
ffffffff812c6fc5:	41 31 c2             	xor    %eax,%r10d
ffffffff812c6fc8:	44 33 51 18          	xor    0x18(%rcx),%r10d
ffffffff812c6fcc:	41 d1 c2             	rol    %r10d
	T_20_39(25, A, B, C, D, E);
ffffffff812c6fcf:	45 31 e3             	xor    %r12d,%r11d
ffffffff812c6fd2:	44 89 51 24          	mov    %r10d,0x24(%rcx)
ffffffff812c6fd6:	47 8d 94 17 a1 eb d9 	lea    0x6ed9eba1(%r15,%r10,1),%r10d
ffffffff812c6fdd:	6e 
ffffffff812c6fde:	41 31 db             	xor    %ebx,%r11d
	T_20_39(26, E, A, B, C, D);
ffffffff812c6fe1:	45 89 e7             	mov    %r12d,%r15d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c6fe4:	c1 cb 02             	ror    $0x2,%ebx
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
	T_20_39(23, C, D, E, A, B);
	T_20_39(24, B, C, D, E, A);
	T_20_39(25, A, B, C, D, E);
ffffffff812c6fe7:	45 01 d3             	add    %r10d,%r11d
ffffffff812c6fea:	45 89 f2             	mov    %r14d,%r10d
	T_20_39(26, E, A, B, C, D);
ffffffff812c6fed:	41 31 df             	xor    %ebx,%r15d
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
	T_20_39(23, C, D, E, A, B);
	T_20_39(24, B, C, D, E, A);
	T_20_39(25, A, B, C, D, E);
ffffffff812c6ff0:	41 c1 c2 05          	rol    $0x5,%r10d
	T_20_39(26, E, A, B, C, D);
ffffffff812c6ff4:	45 31 f7             	xor    %r14d,%r15d
ffffffff812c6ff7:	41 c1 ce 02          	ror    $0x2,%r14d
	T_20_39(20, A, B, C, D, E);
	T_20_39(21, E, A, B, C, D);
	T_20_39(22, D, E, A, B, C);
	T_20_39(23, C, D, E, A, B);
	T_20_39(24, B, C, D, E, A);
	T_20_39(25, A, B, C, D, E);
ffffffff812c6ffb:	45 01 d3             	add    %r10d,%r11d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c6ffe:	44 8b 51 28          	mov    0x28(%rcx),%r10d
ffffffff812c7002:	44 33 51 30          	xor    0x30(%rcx),%r10d
ffffffff812c7006:	44 33 51 08          	xor    0x8(%rcx),%r10d
ffffffff812c700a:	44 33 51 1c          	xor    0x1c(%rcx),%r10d
ffffffff812c700e:	41 d1 c2             	rol    %r10d
	T_20_39(26, E, A, B, C, D);
ffffffff812c7011:	44 89 51 28          	mov    %r10d,0x28(%rcx)
ffffffff812c7015:	47 8d ac 15 a1 eb d9 	lea    0x6ed9eba1(%r13,%r10,1),%r13d
ffffffff812c701c:	6e 
ffffffff812c701d:	44 8b 51 2c          	mov    0x2c(%rcx),%r10d
ffffffff812c7021:	45 01 ef             	add    %r13d,%r15d
ffffffff812c7024:	45 89 dd             	mov    %r11d,%r13d
ffffffff812c7027:	45 31 ca             	xor    %r9d,%r10d
ffffffff812c702a:	44 33 51 0c          	xor    0xc(%rcx),%r10d
ffffffff812c702e:	41 c1 c5 05          	rol    $0x5,%r13d
ffffffff812c7032:	44 33 51 20          	xor    0x20(%rcx),%r10d
ffffffff812c7036:	45 01 fd             	add    %r15d,%r13d
	T_20_39(27, D, E, A, B, C);
ffffffff812c7039:	41 89 df             	mov    %ebx,%r15d
ffffffff812c703c:	45 31 f7             	xor    %r14d,%r15d
ffffffff812c703f:	45 31 df             	xor    %r11d,%r15d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7042:	41 c1 cb 02          	ror    $0x2,%r11d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7046:	41 d1 c2             	rol    %r10d
ffffffff812c7049:	44 89 51 2c          	mov    %r10d,0x2c(%rcx)
ffffffff812c704d:	47 8d a4 14 a1 eb d9 	lea    0x6ed9eba1(%r12,%r10,1),%r12d
ffffffff812c7054:	6e 
ffffffff812c7055:	44 8b 51 30          	mov    0x30(%rcx),%r10d
ffffffff812c7059:	45 01 e7             	add    %r12d,%r15d
ffffffff812c705c:	45 89 ec             	mov    %r13d,%r12d
ffffffff812c705f:	45 31 c2             	xor    %r8d,%r10d
ffffffff812c7062:	44 33 51 10          	xor    0x10(%rcx),%r10d
ffffffff812c7066:	41 c1 c4 05          	rol    $0x5,%r12d
ffffffff812c706a:	44 33 51 24          	xor    0x24(%rcx),%r10d
ffffffff812c706e:	45 01 fc             	add    %r15d,%r12d
	T_20_39(28, C, D, E, A, B);
ffffffff812c7071:	45 89 f7             	mov    %r14d,%r15d
ffffffff812c7074:	45 31 df             	xor    %r11d,%r15d
ffffffff812c7077:	45 31 ef             	xor    %r13d,%r15d
ffffffff812c707a:	41 d1 c2             	rol    %r10d
ffffffff812c707d:	44 89 51 30          	mov    %r10d,0x30(%rcx)
ffffffff812c7081:	46 8d 94 13 a1 eb d9 	lea    0x6ed9eba1(%rbx,%r10,1),%r10d
ffffffff812c7088:	6e 
	T_20_39(29, B, C, D, E, A);
ffffffff812c7089:	8b 59 28             	mov    0x28(%rcx),%ebx
	T_20_39(23, C, D, E, A, B);
	T_20_39(24, B, C, D, E, A);
	T_20_39(25, A, B, C, D, E);
	T_20_39(26, E, A, B, C, D);
	T_20_39(27, D, E, A, B, C);
	T_20_39(28, C, D, E, A, B);
ffffffff812c708c:	45 01 d7             	add    %r10d,%r15d
ffffffff812c708f:	41 31 f9             	xor    %edi,%r9d
ffffffff812c7092:	44 33 49 14          	xor    0x14(%rcx),%r9d
ffffffff812c7096:	45 89 e2             	mov    %r12d,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7099:	41 c1 cd 02          	ror    $0x2,%r13d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c709d:	41 31 f0             	xor    %esi,%r8d
ffffffff812c70a0:	41 c1 c2 05          	rol    $0x5,%r10d
ffffffff812c70a4:	44 33 41 18          	xor    0x18(%rcx),%r8d
ffffffff812c70a8:	31 c7                	xor    %eax,%edi
ffffffff812c70aa:	45 01 d7             	add    %r10d,%r15d
	T_20_39(29, B, C, D, E, A);
ffffffff812c70ad:	45 89 da             	mov    %r11d,%r10d
ffffffff812c70b0:	33 79 1c             	xor    0x1c(%rcx),%edi
ffffffff812c70b3:	41 31 d9             	xor    %ebx,%r9d
ffffffff812c70b6:	45 31 ea             	xor    %r13d,%r10d
ffffffff812c70b9:	33 79 30             	xor    0x30(%rcx),%edi
ffffffff812c70bc:	41 d1 c1             	rol    %r9d
ffffffff812c70bf:	45 31 e2             	xor    %r12d,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c70c2:	41 c1 cc 02          	ror    $0x2,%r12d
ffffffff812c70c6:	44 89 49 34          	mov    %r9d,0x34(%rcx)
ffffffff812c70ca:	47 8d 8c 0e a1 eb d9 	lea    0x6ed9eba1(%r14,%r9,1),%r9d
ffffffff812c70d1:	6e 
	T_20_39(30, A, B, C, D, E);
ffffffff812c70d2:	44 8b 71 2c          	mov    0x2c(%rcx),%r14d
	T_20_39(24, B, C, D, E, A);
	T_20_39(25, A, B, C, D, E);
	T_20_39(26, E, A, B, C, D);
	T_20_39(27, D, E, A, B, C);
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
ffffffff812c70d6:	45 01 ca             	add    %r9d,%r10d
ffffffff812c70d9:	45 89 f9             	mov    %r15d,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c70dc:	d1 c7                	rol    %edi
ffffffff812c70de:	41 c1 c1 05          	rol    $0x5,%r9d
ffffffff812c70e2:	45 31 f0             	xor    %r14d,%r8d
ffffffff812c70e5:	45 01 ca             	add    %r9d,%r10d
ffffffff812c70e8:	41 d1 c0             	rol    %r8d
	T_20_39(30, A, B, C, D, E);
ffffffff812c70eb:	45 89 e9             	mov    %r13d,%r9d
ffffffff812c70ee:	44 89 41 38          	mov    %r8d,0x38(%rcx)
ffffffff812c70f2:	45 31 e1             	xor    %r12d,%r9d
ffffffff812c70f5:	47 8d 84 03 a1 eb d9 	lea    0x6ed9eba1(%r11,%r8,1),%r8d
ffffffff812c70fc:	6e 
ffffffff812c70fd:	45 31 f9             	xor    %r15d,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7100:	41 c1 cf 02          	ror    $0x2,%r15d
	T_20_39(31, E, A, B, C, D);
ffffffff812c7104:	89 79 3c             	mov    %edi,0x3c(%rcx)
	T_20_39(25, A, B, C, D, E);
	T_20_39(26, E, A, B, C, D);
	T_20_39(27, D, E, A, B, C);
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
ffffffff812c7107:	45 01 c1             	add    %r8d,%r9d
ffffffff812c710a:	45 89 d0             	mov    %r10d,%r8d
	T_20_39(31, E, A, B, C, D);
ffffffff812c710d:	41 8d bc 3d a1 eb d9 	lea    0x6ed9eba1(%r13,%rdi,1),%edi
ffffffff812c7114:	6e 
	T_20_39(25, A, B, C, D, E);
	T_20_39(26, E, A, B, C, D);
	T_20_39(27, D, E, A, B, C);
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
ffffffff812c7115:	41 c1 c0 05          	rol    $0x5,%r8d
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
ffffffff812c7119:	44 8b 69 34          	mov    0x34(%rcx),%r13d
	T_20_39(25, A, B, C, D, E);
	T_20_39(26, E, A, B, C, D);
	T_20_39(27, D, E, A, B, C);
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
ffffffff812c711d:	45 01 c1             	add    %r8d,%r9d
	T_20_39(31, E, A, B, C, D);
ffffffff812c7120:	45 89 e0             	mov    %r12d,%r8d
ffffffff812c7123:	45 31 f8             	xor    %r15d,%r8d
ffffffff812c7126:	45 31 d0             	xor    %r10d,%r8d
ffffffff812c7129:	41 01 f8             	add    %edi,%r8d
ffffffff812c712c:	44 89 cf             	mov    %r9d,%edi
ffffffff812c712f:	c1 c7 05             	rol    $0x5,%edi
ffffffff812c7132:	41 01 f8             	add    %edi,%r8d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7135:	33 71 08             	xor    0x8(%rcx),%esi
ffffffff812c7138:	33 41 0c             	xor    0xc(%rcx),%eax
ffffffff812c713b:	33 71 20             	xor    0x20(%rcx),%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c713e:	41 c1 ca 02          	ror    $0x2,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7142:	33 41 24             	xor    0x24(%rcx),%eax
	T_20_39(32, D, E, A, B, C);
ffffffff812c7145:	44 89 ff             	mov    %r15d,%edi
ffffffff812c7148:	44 31 d7             	xor    %r10d,%edi
ffffffff812c714b:	44 31 cf             	xor    %r9d,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c714e:	41 c1 c9 02          	ror    $0x2,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7152:	44 31 ee             	xor    %r13d,%esi
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
ffffffff812c7155:	45 89 cb             	mov    %r9d,%r11d
ffffffff812c7158:	d1 c6                	rol    %esi
	T_20_39(27, D, E, A, B, C);
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
ffffffff812c715a:	89 31                	mov    %esi,(%rcx)
ffffffff812c715c:	41 8d b4 34 a1 eb d9 	lea    0x6ed9eba1(%r12,%rsi,1),%esi
ffffffff812c7163:	6e 
	T_20_39(33, C, D, E, A, B);
ffffffff812c7164:	44 8b 61 38          	mov    0x38(%rcx),%r12d
	T_20_39(27, D, E, A, B, C);
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
ffffffff812c7168:	01 f7                	add    %esi,%edi
ffffffff812c716a:	44 89 c6             	mov    %r8d,%esi
ffffffff812c716d:	c1 c6 05             	rol    $0x5,%esi
ffffffff812c7170:	44 31 e0             	xor    %r12d,%eax
ffffffff812c7173:	01 f7                	add    %esi,%edi
ffffffff812c7175:	d1 c0                	rol    %eax
	T_20_39(33, C, D, E, A, B);
ffffffff812c7177:	44 89 d6             	mov    %r10d,%esi
ffffffff812c717a:	89 41 04             	mov    %eax,0x4(%rcx)
ffffffff812c717d:	44 31 ce             	xor    %r9d,%esi
ffffffff812c7180:	41 8d 84 07 a1 eb d9 	lea    0x6ed9eba1(%r15,%rax,1),%eax
ffffffff812c7187:	6e 
ffffffff812c7188:	44 31 c6             	xor    %r8d,%esi
	T_20_39(34, B, C, D, E, A);
ffffffff812c718b:	44 8b 79 3c          	mov    0x3c(%rcx),%r15d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c718f:	41 c1 c8 02          	ror    $0x2,%r8d
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
ffffffff812c7193:	01 c6                	add    %eax,%esi
ffffffff812c7195:	89 f8                	mov    %edi,%eax
	T_20_39(34, B, C, D, E, A);
ffffffff812c7197:	45 31 c3             	xor    %r8d,%r11d
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
ffffffff812c719a:	c1 c0 05             	rol    $0x5,%eax
	T_20_39(34, B, C, D, E, A);
ffffffff812c719d:	41 31 fb             	xor    %edi,%r11d
ffffffff812c71a0:	c1 cf 02             	ror    $0x2,%edi
	T_20_39(28, C, D, E, A, B);
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
ffffffff812c71a3:	01 c6                	add    %eax,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c71a5:	8b 41 08             	mov    0x8(%rcx),%eax
ffffffff812c71a8:	33 41 10             	xor    0x10(%rcx),%eax
ffffffff812c71ab:	31 d8                	xor    %ebx,%eax
ffffffff812c71ad:	44 31 f8             	xor    %r15d,%eax
ffffffff812c71b0:	d1 c0                	rol    %eax
	T_20_39(34, B, C, D, E, A);
ffffffff812c71b2:	89 41 08             	mov    %eax,0x8(%rcx)
ffffffff812c71b5:	41 8d 84 02 a1 eb d9 	lea    0x6ed9eba1(%r10,%rax,1),%eax
ffffffff812c71bc:	6e 
	T_20_39(35, A, B, C, D, E);
ffffffff812c71bd:	45 89 c2             	mov    %r8d,%r10d
	T_20_39(29, B, C, D, E, A);
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
ffffffff812c71c0:	41 01 c3             	add    %eax,%r11d
ffffffff812c71c3:	89 f0                	mov    %esi,%eax
ffffffff812c71c5:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c71c8:	41 01 c3             	add    %eax,%r11d
ffffffff812c71cb:	8b 41 0c             	mov    0xc(%rcx),%eax
ffffffff812c71ce:	33 41 14             	xor    0x14(%rcx),%eax
	T_20_39(35, A, B, C, D, E);
ffffffff812c71d1:	41 31 fa             	xor    %edi,%r10d
ffffffff812c71d4:	41 31 f2             	xor    %esi,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c71d7:	c1 ce 02             	ror    $0x2,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c71da:	44 31 f0             	xor    %r14d,%eax
ffffffff812c71dd:	33 01                	xor    (%rcx),%eax
ffffffff812c71df:	d1 c0                	rol    %eax
ffffffff812c71e1:	89 41 0c             	mov    %eax,0xc(%rcx)
ffffffff812c71e4:	41 8d 84 01 a1 eb d9 	lea    0x6ed9eba1(%r9,%rax,1),%eax
ffffffff812c71eb:	6e 
	T_20_39(36, E, A, B, C, D);
ffffffff812c71ec:	41 89 f9             	mov    %edi,%r9d
ffffffff812c71ef:	41 31 f1             	xor    %esi,%r9d
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
	T_20_39(35, A, B, C, D, E);
ffffffff812c71f2:	41 01 c2             	add    %eax,%r10d
ffffffff812c71f5:	44 89 d8             	mov    %r11d,%eax
	T_20_39(36, E, A, B, C, D);
ffffffff812c71f8:	45 31 d9             	xor    %r11d,%r9d
	T_20_39(30, A, B, C, D, E);
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
	T_20_39(35, A, B, C, D, E);
ffffffff812c71fb:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c71fe:	41 c1 cb 02          	ror    $0x2,%r11d
ffffffff812c7202:	41 01 c2             	add    %eax,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7205:	8b 41 10             	mov    0x10(%rcx),%eax
ffffffff812c7208:	33 41 18             	xor    0x18(%rcx),%eax
ffffffff812c720b:	33 41 30             	xor    0x30(%rcx),%eax
ffffffff812c720e:	33 41 04             	xor    0x4(%rcx),%eax
ffffffff812c7211:	d1 c0                	rol    %eax
	T_20_39(36, E, A, B, C, D);
ffffffff812c7213:	89 41 10             	mov    %eax,0x10(%rcx)
ffffffff812c7216:	41 8d 84 00 a1 eb d9 	lea    0x6ed9eba1(%r8,%rax,1),%eax
ffffffff812c721d:	6e 
	T_20_39(37, D, E, A, B, C);
ffffffff812c721e:	41 89 f0             	mov    %esi,%r8d
ffffffff812c7221:	45 31 d8             	xor    %r11d,%r8d
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
	T_20_39(35, A, B, C, D, E);
	T_20_39(36, E, A, B, C, D);
ffffffff812c7224:	41 01 c1             	add    %eax,%r9d
ffffffff812c7227:	44 89 d0             	mov    %r10d,%eax
	T_20_39(37, D, E, A, B, C);
ffffffff812c722a:	45 31 d0             	xor    %r10d,%r8d
	T_20_39(31, E, A, B, C, D);
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
	T_20_39(35, A, B, C, D, E);
	T_20_39(36, E, A, B, C, D);
ffffffff812c722d:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7230:	41 c1 ca 02          	ror    $0x2,%r10d
ffffffff812c7234:	41 01 c1             	add    %eax,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7237:	8b 41 14             	mov    0x14(%rcx),%eax
ffffffff812c723a:	33 41 1c             	xor    0x1c(%rcx),%eax
ffffffff812c723d:	44 31 e8             	xor    %r13d,%eax
ffffffff812c7240:	33 41 08             	xor    0x8(%rcx),%eax
ffffffff812c7243:	d1 c0                	rol    %eax
	T_20_39(37, D, E, A, B, C);
ffffffff812c7245:	89 41 14             	mov    %eax,0x14(%rcx)
ffffffff812c7248:	8d 84 07 a1 eb d9 6e 	lea    0x6ed9eba1(%rdi,%rax,1),%eax
	T_20_39(38, C, D, E, A, B);
ffffffff812c724f:	44 89 df             	mov    %r11d,%edi
	T_20_39(32, D, E, A, B, C);
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
	T_20_39(35, A, B, C, D, E);
	T_20_39(36, E, A, B, C, D);
	T_20_39(37, D, E, A, B, C);
ffffffff812c7252:	41 01 c0             	add    %eax,%r8d
ffffffff812c7255:	44 89 c8             	mov    %r9d,%eax
ffffffff812c7258:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c725b:	41 01 c0             	add    %eax,%r8d
ffffffff812c725e:	8b 41 18             	mov    0x18(%rcx),%eax
ffffffff812c7261:	33 41 20             	xor    0x20(%rcx),%eax
ffffffff812c7264:	44 31 e0             	xor    %r12d,%eax
ffffffff812c7267:	33 41 0c             	xor    0xc(%rcx),%eax
	T_20_39(38, C, D, E, A, B);
ffffffff812c726a:	44 31 d7             	xor    %r10d,%edi
ffffffff812c726d:	44 31 cf             	xor    %r9d,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7270:	41 c1 c9 02          	ror    $0x2,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7274:	d1 c0                	rol    %eax
ffffffff812c7276:	89 41 18             	mov    %eax,0x18(%rcx)
ffffffff812c7279:	8d 84 06 a1 eb d9 6e 	lea    0x6ed9eba1(%rsi,%rax,1),%eax
	T_20_39(39, B, C, D, E, A);
ffffffff812c7280:	44 89 d6             	mov    %r10d,%esi
ffffffff812c7283:	44 31 ce             	xor    %r9d,%esi
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
	T_20_39(35, A, B, C, D, E);
	T_20_39(36, E, A, B, C, D);
	T_20_39(37, D, E, A, B, C);
	T_20_39(38, C, D, E, A, B);
ffffffff812c7286:	01 c7                	add    %eax,%edi
ffffffff812c7288:	44 89 c0             	mov    %r8d,%eax
	T_20_39(39, B, C, D, E, A);
ffffffff812c728b:	44 31 c6             	xor    %r8d,%esi
	T_20_39(33, C, D, E, A, B);
	T_20_39(34, B, C, D, E, A);
	T_20_39(35, A, B, C, D, E);
	T_20_39(36, E, A, B, C, D);
	T_20_39(37, D, E, A, B, C);
	T_20_39(38, C, D, E, A, B);
ffffffff812c728e:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7291:	41 c1 c8 02          	ror    $0x2,%r8d
ffffffff812c7295:	01 c7                	add    %eax,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7297:	8b 41 1c             	mov    0x1c(%rcx),%eax
ffffffff812c729a:	33 41 24             	xor    0x24(%rcx),%eax
ffffffff812c729d:	44 31 f8             	xor    %r15d,%eax
ffffffff812c72a0:	33 41 10             	xor    0x10(%rcx),%eax
ffffffff812c72a3:	d1 c0                	rol    %eax
	T_20_39(39, B, C, D, E, A);
ffffffff812c72a5:	89 41 1c             	mov    %eax,0x1c(%rcx)
ffffffff812c72a8:	41 8d 84 03 a1 eb d9 	lea    0x6ed9eba1(%r11,%rax,1),%eax
ffffffff812c72af:	6e 
ffffffff812c72b0:	01 c6                	add    %eax,%esi
ffffffff812c72b2:	89 f8                	mov    %edi,%eax
ffffffff812c72b4:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c72b7:	01 c6                	add    %eax,%esi
ffffffff812c72b9:	8b 41 20             	mov    0x20(%rcx),%eax
ffffffff812c72bc:	31 d8                	xor    %ebx,%eax
ffffffff812c72be:	33 01                	xor    (%rcx),%eax
ffffffff812c72c0:	33 41 14             	xor    0x14(%rcx),%eax
ffffffff812c72c3:	d1 c0                	rol    %eax

	/* Round 3 */
	T_40_59(40, A, B, C, D, E);
ffffffff812c72c5:	89 41 20             	mov    %eax,0x20(%rcx)
ffffffff812c72c8:	45 8d 94 02 dc bc 1b 	lea    -0x70e44324(%r10,%rax,1),%r10d
ffffffff812c72cf:	8f 
ffffffff812c72d0:	89 f8                	mov    %edi,%eax
ffffffff812c72d2:	44 21 c0             	and    %r8d,%eax
ffffffff812c72d5:	45 8d 1c 02          	lea    (%r10,%rax,1),%r11d
ffffffff812c72d9:	41 89 fa             	mov    %edi,%r10d
ffffffff812c72dc:	89 f0                	mov    %esi,%eax
ffffffff812c72de:	45 31 c2             	xor    %r8d,%r10d
ffffffff812c72e1:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c72e4:	c1 cf 02             	ror    $0x2,%edi
ffffffff812c72e7:	45 21 ca             	and    %r9d,%r10d
ffffffff812c72ea:	45 01 da             	add    %r11d,%r10d
ffffffff812c72ed:	41 01 c2             	add    %eax,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c72f0:	8b 41 24             	mov    0x24(%rcx),%eax
ffffffff812c72f3:	44 31 f0             	xor    %r14d,%eax
ffffffff812c72f6:	33 41 04             	xor    0x4(%rcx),%eax
ffffffff812c72f9:	33 41 18             	xor    0x18(%rcx),%eax
ffffffff812c72fc:	d1 c0                	rol    %eax
ffffffff812c72fe:	33 59 30             	xor    0x30(%rcx),%ebx
ffffffff812c7301:	45 31 ee             	xor    %r13d,%r14d
	T_40_59(41, E, A, B, C, D);
ffffffff812c7304:	89 41 24             	mov    %eax,0x24(%rcx)
ffffffff812c7307:	33 59 08             	xor    0x8(%rcx),%ebx
ffffffff812c730a:	45 8d 8c 01 dc bc 1b 	lea    -0x70e44324(%r9,%rax,1),%r9d
ffffffff812c7311:	8f 
ffffffff812c7312:	89 f0                	mov    %esi,%eax
ffffffff812c7314:	33 59 1c             	xor    0x1c(%rcx),%ebx
ffffffff812c7317:	44 33 71 0c          	xor    0xc(%rcx),%r14d
ffffffff812c731b:	21 f8                	and    %edi,%eax
ffffffff812c731d:	45 8d 1c 01          	lea    (%r9,%rax,1),%r11d
ffffffff812c7321:	41 89 f1             	mov    %esi,%r9d
ffffffff812c7324:	44 89 d0             	mov    %r10d,%eax
ffffffff812c7327:	41 31 f9             	xor    %edi,%r9d
ffffffff812c732a:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c732d:	c1 ce 02             	ror    $0x2,%esi
ffffffff812c7330:	45 21 c1             	and    %r8d,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7333:	d1 c3                	rol    %ebx
ffffffff812c7335:	45 01 d9             	add    %r11d,%r9d
	T_40_59(42, D, E, A, B, C);
ffffffff812c7338:	45 8d 84 18 dc bc 1b 	lea    -0x70e44324(%r8,%rbx,1),%r8d
ffffffff812c733f:	8f 
ffffffff812c7340:	89 59 28             	mov    %ebx,0x28(%rcx)
	T_20_39(38, C, D, E, A, B);
	T_20_39(39, B, C, D, E, A);

	/* Round 3 */
	T_40_59(40, A, B, C, D, E);
	T_40_59(41, E, A, B, C, D);
ffffffff812c7343:	41 01 c1             	add    %eax,%r9d
	T_40_59(42, D, E, A, B, C);
ffffffff812c7346:	44 89 d0             	mov    %r10d,%eax
	T_40_59(43, C, D, E, A, B);
ffffffff812c7349:	8b 59 20             	mov    0x20(%rcx),%ebx
	T_20_39(39, B, C, D, E, A);

	/* Round 3 */
	T_40_59(40, A, B, C, D, E);
	T_40_59(41, E, A, B, C, D);
	T_40_59(42, D, E, A, B, C);
ffffffff812c734c:	21 f0                	and    %esi,%eax
ffffffff812c734e:	45 8d 1c 00          	lea    (%r8,%rax,1),%r11d
ffffffff812c7352:	45 89 d0             	mov    %r10d,%r8d
ffffffff812c7355:	44 89 c8             	mov    %r9d,%eax
ffffffff812c7358:	41 31 f0             	xor    %esi,%r8d
ffffffff812c735b:	41 31 de             	xor    %ebx,%r14d
ffffffff812c735e:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c7361:	41 21 f8             	and    %edi,%r8d
ffffffff812c7364:	41 d1 c6             	rol    %r14d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7367:	41 c1 ca 02          	ror    $0x2,%r10d
ffffffff812c736b:	45 01 d8             	add    %r11d,%r8d
	T_40_59(43, C, D, E, A, B);
ffffffff812c736e:	42 8d bc 37 dc bc 1b 	lea    -0x70e44324(%rdi,%r14,1),%edi
ffffffff812c7375:	8f 
ffffffff812c7376:	44 89 71 2c          	mov    %r14d,0x2c(%rcx)
	T_20_39(39, B, C, D, E, A);

	/* Round 3 */
	T_40_59(40, A, B, C, D, E);
	T_40_59(41, E, A, B, C, D);
	T_40_59(42, D, E, A, B, C);
ffffffff812c737a:	41 01 c0             	add    %eax,%r8d
	T_40_59(43, C, D, E, A, B);
ffffffff812c737d:	44 89 c8             	mov    %r9d,%eax
	T_40_59(44, B, C, D, E, A);
ffffffff812c7380:	44 8b 71 24          	mov    0x24(%rcx),%r14d

	/* Round 3 */
	T_40_59(40, A, B, C, D, E);
	T_40_59(41, E, A, B, C, D);
	T_40_59(42, D, E, A, B, C);
	T_40_59(43, C, D, E, A, B);
ffffffff812c7384:	44 21 d0             	and    %r10d,%eax
ffffffff812c7387:	44 8d 1c 07          	lea    (%rdi,%rax,1),%r11d
ffffffff812c738b:	44 89 cf             	mov    %r9d,%edi
ffffffff812c738e:	44 89 c0             	mov    %r8d,%eax
ffffffff812c7391:	44 31 d7             	xor    %r10d,%edi
ffffffff812c7394:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c7397:	41 c1 c9 02          	ror    $0x2,%r9d
ffffffff812c739b:	21 f7                	and    %esi,%edi
ffffffff812c739d:	44 01 df             	add    %r11d,%edi
ffffffff812c73a0:	01 c7                	add    %eax,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c73a2:	8b 41 30             	mov    0x30(%rcx),%eax
ffffffff812c73a5:	44 31 e0             	xor    %r12d,%eax
ffffffff812c73a8:	33 41 10             	xor    0x10(%rcx),%eax
ffffffff812c73ab:	44 31 f0             	xor    %r14d,%eax
ffffffff812c73ae:	45 31 fd             	xor    %r15d,%r13d
ffffffff812c73b1:	44 33 69 14          	xor    0x14(%rcx),%r13d
ffffffff812c73b5:	d1 c0                	rol    %eax
ffffffff812c73b7:	44 33 69 28          	xor    0x28(%rcx),%r13d
ffffffff812c73bb:	44 33 21             	xor    (%rcx),%r12d
	T_40_59(44, B, C, D, E, A);
ffffffff812c73be:	89 41 30             	mov    %eax,0x30(%rcx)
ffffffff812c73c1:	8d b4 06 dc bc 1b 8f 	lea    -0x70e44324(%rsi,%rax,1),%esi
ffffffff812c73c8:	44 89 c0             	mov    %r8d,%eax
ffffffff812c73cb:	44 21 c8             	and    %r9d,%eax
ffffffff812c73ce:	44 33 61 18          	xor    0x18(%rcx),%r12d
ffffffff812c73d2:	44 33 79 04          	xor    0x4(%rcx),%r15d
ffffffff812c73d6:	44 8d 1c 06          	lea    (%rsi,%rax,1),%r11d
ffffffff812c73da:	44 89 c6             	mov    %r8d,%esi
ffffffff812c73dd:	89 f8                	mov    %edi,%eax
ffffffff812c73df:	44 31 ce             	xor    %r9d,%esi
ffffffff812c73e2:	41 d1 c5             	rol    %r13d
ffffffff812c73e5:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c73e8:	44 21 d6             	and    %r10d,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c73eb:	41 c1 c8 02          	ror    $0x2,%r8d
	T_40_59(45, A, B, C, D, E);
ffffffff812c73ef:	47 8d 94 2a dc bc 1b 	lea    -0x70e44324(%r10,%r13,1),%r10d
ffffffff812c73f6:	8f 
	/* Round 3 */
	T_40_59(40, A, B, C, D, E);
	T_40_59(41, E, A, B, C, D);
	T_40_59(42, D, E, A, B, C);
	T_40_59(43, C, D, E, A, B);
	T_40_59(44, B, C, D, E, A);
ffffffff812c73f7:	44 01 de             	add    %r11d,%esi
	T_40_59(45, A, B, C, D, E);
ffffffff812c73fa:	44 89 69 34          	mov    %r13d,0x34(%rcx)
	T_40_59(46, E, A, B, C, D);
ffffffff812c73fe:	44 8b 69 2c          	mov    0x2c(%rcx),%r13d
	/* Round 3 */
	T_40_59(40, A, B, C, D, E);
	T_40_59(41, E, A, B, C, D);
	T_40_59(42, D, E, A, B, C);
	T_40_59(43, C, D, E, A, B);
	T_40_59(44, B, C, D, E, A);
ffffffff812c7402:	01 c6                	add    %eax,%esi
	T_40_59(45, A, B, C, D, E);
ffffffff812c7404:	89 f8                	mov    %edi,%eax
ffffffff812c7406:	44 21 c0             	and    %r8d,%eax
ffffffff812c7409:	45 8d 1c 02          	lea    (%r10,%rax,1),%r11d
ffffffff812c740d:	41 89 fa             	mov    %edi,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7410:	45 31 ec             	xor    %r13d,%r12d
ffffffff812c7413:	45 31 c2             	xor    %r8d,%r10d
ffffffff812c7416:	89 f0                	mov    %esi,%eax
ffffffff812c7418:	41 d1 c4             	rol    %r12d
ffffffff812c741b:	45 21 ca             	and    %r9d,%r10d
ffffffff812c741e:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7421:	c1 cf 02             	ror    $0x2,%edi
ffffffff812c7424:	45 01 da             	add    %r11d,%r10d
	T_40_59(46, E, A, B, C, D);
ffffffff812c7427:	47 8d 8c 21 dc bc 1b 	lea    -0x70e44324(%r9,%r12,1),%r9d
ffffffff812c742e:	8f 
ffffffff812c742f:	44 89 61 38          	mov    %r12d,0x38(%rcx)
	T_40_59(40, A, B, C, D, E);
	T_40_59(41, E, A, B, C, D);
	T_40_59(42, D, E, A, B, C);
	T_40_59(43, C, D, E, A, B);
	T_40_59(44, B, C, D, E, A);
	T_40_59(45, A, B, C, D, E);
ffffffff812c7433:	41 01 c2             	add    %eax,%r10d
	T_40_59(46, E, A, B, C, D);
ffffffff812c7436:	89 f0                	mov    %esi,%eax
	T_40_59(47, D, E, A, B, C);
ffffffff812c7438:	44 8b 61 30          	mov    0x30(%rcx),%r12d
	T_40_59(41, E, A, B, C, D);
	T_40_59(42, D, E, A, B, C);
	T_40_59(43, C, D, E, A, B);
	T_40_59(44, B, C, D, E, A);
	T_40_59(45, A, B, C, D, E);
	T_40_59(46, E, A, B, C, D);
ffffffff812c743c:	21 f8                	and    %edi,%eax
ffffffff812c743e:	45 8d 1c 01          	lea    (%r9,%rax,1),%r11d
ffffffff812c7442:	41 89 f1             	mov    %esi,%r9d
ffffffff812c7445:	44 89 d0             	mov    %r10d,%eax
ffffffff812c7448:	41 31 f9             	xor    %edi,%r9d
ffffffff812c744b:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c744e:	c1 ce 02             	ror    $0x2,%esi
ffffffff812c7451:	45 21 c1             	and    %r8d,%r9d
ffffffff812c7454:	45 01 d9             	add    %r11d,%r9d
ffffffff812c7457:	41 01 c1             	add    %eax,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c745a:	44 33 79 1c          	xor    0x1c(%rcx),%r15d
	T_40_59(47, D, E, A, B, C);
ffffffff812c745e:	44 89 d0             	mov    %r10d,%eax
ffffffff812c7461:	21 f0                	and    %esi,%eax
ffffffff812c7463:	45 31 e7             	xor    %r12d,%r15d
ffffffff812c7466:	41 d1 c7             	rol    %r15d
ffffffff812c7469:	47 8d 84 38 dc bc 1b 	lea    -0x70e44324(%r8,%r15,1),%r8d
ffffffff812c7470:	8f 
ffffffff812c7471:	44 89 79 3c          	mov    %r15d,0x3c(%rcx)
	T_40_59(48, C, D, E, A, B);
ffffffff812c7475:	44 8b 79 34          	mov    0x34(%rcx),%r15d
	T_40_59(42, D, E, A, B, C);
	T_40_59(43, C, D, E, A, B);
	T_40_59(44, B, C, D, E, A);
	T_40_59(45, A, B, C, D, E);
	T_40_59(46, E, A, B, C, D);
	T_40_59(47, D, E, A, B, C);
ffffffff812c7479:	45 8d 1c 00          	lea    (%r8,%rax,1),%r11d
ffffffff812c747d:	45 89 d0             	mov    %r10d,%r8d
ffffffff812c7480:	44 89 c8             	mov    %r9d,%eax
ffffffff812c7483:	41 31 f0             	xor    %esi,%r8d
ffffffff812c7486:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7489:	41 c1 ca 02          	ror    $0x2,%r10d
ffffffff812c748d:	41 21 f8             	and    %edi,%r8d
ffffffff812c7490:	45 01 d8             	add    %r11d,%r8d
ffffffff812c7493:	41 01 c0             	add    %eax,%r8d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7496:	8b 01                	mov    (%rcx),%eax
ffffffff812c7498:	33 41 08             	xor    0x8(%rcx),%eax
ffffffff812c749b:	31 d8                	xor    %ebx,%eax
ffffffff812c749d:	44 31 f8             	xor    %r15d,%eax
ffffffff812c74a0:	d1 c0                	rol    %eax
	T_40_59(48, C, D, E, A, B);
ffffffff812c74a2:	89 01                	mov    %eax,(%rcx)
ffffffff812c74a4:	8d bc 07 dc bc 1b 8f 	lea    -0x70e44324(%rdi,%rax,1),%edi
ffffffff812c74ab:	44 89 c8             	mov    %r9d,%eax
ffffffff812c74ae:	44 21 d0             	and    %r10d,%eax
ffffffff812c74b1:	44 8d 1c 07          	lea    (%rdi,%rax,1),%r11d
ffffffff812c74b5:	44 89 cf             	mov    %r9d,%edi
ffffffff812c74b8:	44 89 c0             	mov    %r8d,%eax
ffffffff812c74bb:	44 31 d7             	xor    %r10d,%edi
ffffffff812c74be:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c74c1:	41 c1 c9 02          	ror    $0x2,%r9d
ffffffff812c74c5:	21 f7                	and    %esi,%edi
ffffffff812c74c7:	44 01 df             	add    %r11d,%edi
ffffffff812c74ca:	01 c7                	add    %eax,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c74cc:	8b 41 04             	mov    0x4(%rcx),%eax
ffffffff812c74cf:	33 41 0c             	xor    0xc(%rcx),%eax
ffffffff812c74d2:	44 31 f0             	xor    %r14d,%eax
ffffffff812c74d5:	33 41 38             	xor    0x38(%rcx),%eax
ffffffff812c74d8:	d1 c0                	rol    %eax
	T_40_59(49, B, C, D, E, A);
ffffffff812c74da:	89 41 04             	mov    %eax,0x4(%rcx)
ffffffff812c74dd:	8d b4 06 dc bc 1b 8f 	lea    -0x70e44324(%rsi,%rax,1),%esi
ffffffff812c74e4:	44 89 c0             	mov    %r8d,%eax
ffffffff812c74e7:	44 21 c8             	and    %r9d,%eax
ffffffff812c74ea:	44 8d 1c 06          	lea    (%rsi,%rax,1),%r11d
ffffffff812c74ee:	44 89 c6             	mov    %r8d,%esi
ffffffff812c74f1:	89 f8                	mov    %edi,%eax
ffffffff812c74f3:	44 31 ce             	xor    %r9d,%esi
ffffffff812c74f6:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c74f9:	41 c1 c8 02          	ror    $0x2,%r8d
ffffffff812c74fd:	44 21 d6             	and    %r10d,%esi
ffffffff812c7500:	44 01 de             	add    %r11d,%esi
ffffffff812c7503:	01 c6                	add    %eax,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7505:	8b 41 08             	mov    0x8(%rcx),%eax
ffffffff812c7508:	33 41 10             	xor    0x10(%rcx),%eax
ffffffff812c750b:	33 41 28             	xor    0x28(%rcx),%eax
ffffffff812c750e:	33 41 3c             	xor    0x3c(%rcx),%eax
ffffffff812c7511:	d1 c0                	rol    %eax
	T_40_59(50, A, B, C, D, E);
ffffffff812c7513:	89 41 08             	mov    %eax,0x8(%rcx)
ffffffff812c7516:	45 8d 94 02 dc bc 1b 	lea    -0x70e44324(%r10,%rax,1),%r10d
ffffffff812c751d:	8f 
ffffffff812c751e:	89 f8                	mov    %edi,%eax
ffffffff812c7520:	44 21 c0             	and    %r8d,%eax
ffffffff812c7523:	45 8d 1c 02          	lea    (%r10,%rax,1),%r11d
ffffffff812c7527:	41 89 fa             	mov    %edi,%r10d
ffffffff812c752a:	89 f0                	mov    %esi,%eax
ffffffff812c752c:	45 31 c2             	xor    %r8d,%r10d
ffffffff812c752f:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7532:	c1 cf 02             	ror    $0x2,%edi
ffffffff812c7535:	45 21 ca             	and    %r9d,%r10d
ffffffff812c7538:	45 01 da             	add    %r11d,%r10d
ffffffff812c753b:	41 01 c2             	add    %eax,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c753e:	8b 41 0c             	mov    0xc(%rcx),%eax
ffffffff812c7541:	33 41 14             	xor    0x14(%rcx),%eax
ffffffff812c7544:	44 31 e8             	xor    %r13d,%eax
ffffffff812c7547:	33 01                	xor    (%rcx),%eax
ffffffff812c7549:	d1 c0                	rol    %eax
	T_40_59(51, E, A, B, C, D);
ffffffff812c754b:	89 41 0c             	mov    %eax,0xc(%rcx)
ffffffff812c754e:	45 8d 8c 01 dc bc 1b 	lea    -0x70e44324(%r9,%rax,1),%r9d
ffffffff812c7555:	8f 
ffffffff812c7556:	89 f0                	mov    %esi,%eax
ffffffff812c7558:	21 f8                	and    %edi,%eax
ffffffff812c755a:	45 8d 1c 01          	lea    (%r9,%rax,1),%r11d
ffffffff812c755e:	41 89 f1             	mov    %esi,%r9d
ffffffff812c7561:	44 89 d0             	mov    %r10d,%eax
ffffffff812c7564:	41 31 f9             	xor    %edi,%r9d
ffffffff812c7567:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c756a:	c1 ce 02             	ror    $0x2,%esi
ffffffff812c756d:	45 21 c1             	and    %r8d,%r9d
ffffffff812c7570:	45 01 d9             	add    %r11d,%r9d
ffffffff812c7573:	41 01 c1             	add    %eax,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7576:	8b 41 10             	mov    0x10(%rcx),%eax
ffffffff812c7579:	33 41 18             	xor    0x18(%rcx),%eax
ffffffff812c757c:	44 31 e0             	xor    %r12d,%eax
ffffffff812c757f:	33 41 04             	xor    0x4(%rcx),%eax
ffffffff812c7582:	d1 c0                	rol    %eax
	T_40_59(52, D, E, A, B, C);
ffffffff812c7584:	89 41 10             	mov    %eax,0x10(%rcx)
ffffffff812c7587:	45 8d 84 00 dc bc 1b 	lea    -0x70e44324(%r8,%rax,1),%r8d
ffffffff812c758e:	8f 
ffffffff812c758f:	44 89 d0             	mov    %r10d,%eax
ffffffff812c7592:	21 f0                	and    %esi,%eax
ffffffff812c7594:	45 8d 1c 00          	lea    (%r8,%rax,1),%r11d
ffffffff812c7598:	45 89 d0             	mov    %r10d,%r8d
ffffffff812c759b:	44 89 c8             	mov    %r9d,%eax
ffffffff812c759e:	41 31 f0             	xor    %esi,%r8d
ffffffff812c75a1:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c75a4:	41 21 f8             	and    %edi,%r8d
ffffffff812c75a7:	45 01 d8             	add    %r11d,%r8d
ffffffff812c75aa:	41 01 c0             	add    %eax,%r8d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c75ad:	41 c1 ca 02          	ror    $0x2,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c75b1:	8b 41 14             	mov    0x14(%rcx),%eax
ffffffff812c75b4:	33 41 1c             	xor    0x1c(%rcx),%eax
ffffffff812c75b7:	44 31 f8             	xor    %r15d,%eax
ffffffff812c75ba:	33 41 08             	xor    0x8(%rcx),%eax
ffffffff812c75bd:	d1 c0                	rol    %eax
	T_40_59(53, C, D, E, A, B);
ffffffff812c75bf:	89 41 14             	mov    %eax,0x14(%rcx)
ffffffff812c75c2:	8d bc 07 dc bc 1b 8f 	lea    -0x70e44324(%rdi,%rax,1),%edi
ffffffff812c75c9:	44 89 c8             	mov    %r9d,%eax
ffffffff812c75cc:	44 21 d0             	and    %r10d,%eax
ffffffff812c75cf:	44 8d 1c 07          	lea    (%rdi,%rax,1),%r11d
ffffffff812c75d3:	44 89 cf             	mov    %r9d,%edi
ffffffff812c75d6:	44 89 c0             	mov    %r8d,%eax
ffffffff812c75d9:	44 31 d7             	xor    %r10d,%edi
ffffffff812c75dc:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c75df:	41 c1 c9 02          	ror    $0x2,%r9d
ffffffff812c75e3:	21 f7                	and    %esi,%edi
ffffffff812c75e5:	44 01 df             	add    %r11d,%edi
ffffffff812c75e8:	01 c7                	add    %eax,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c75ea:	8b 41 18             	mov    0x18(%rcx),%eax
ffffffff812c75ed:	31 d8                	xor    %ebx,%eax
ffffffff812c75ef:	33 41 38             	xor    0x38(%rcx),%eax
ffffffff812c75f2:	33 41 0c             	xor    0xc(%rcx),%eax
ffffffff812c75f5:	d1 c0                	rol    %eax
	T_40_59(54, B, C, D, E, A);
ffffffff812c75f7:	89 41 18             	mov    %eax,0x18(%rcx)
ffffffff812c75fa:	8d b4 06 dc bc 1b 8f 	lea    -0x70e44324(%rsi,%rax,1),%esi
ffffffff812c7601:	44 89 c0             	mov    %r8d,%eax
ffffffff812c7604:	44 21 c8             	and    %r9d,%eax
ffffffff812c7607:	44 8d 1c 06          	lea    (%rsi,%rax,1),%r11d
ffffffff812c760b:	44 89 c6             	mov    %r8d,%esi
ffffffff812c760e:	89 f8                	mov    %edi,%eax
ffffffff812c7610:	44 31 ce             	xor    %r9d,%esi
ffffffff812c7613:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7616:	41 c1 c8 02          	ror    $0x2,%r8d
ffffffff812c761a:	44 21 d6             	and    %r10d,%esi
ffffffff812c761d:	44 01 de             	add    %r11d,%esi
ffffffff812c7620:	01 c6                	add    %eax,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7622:	8b 41 1c             	mov    0x1c(%rcx),%eax
ffffffff812c7625:	44 31 f0             	xor    %r14d,%eax
ffffffff812c7628:	33 41 3c             	xor    0x3c(%rcx),%eax
ffffffff812c762b:	33 41 10             	xor    0x10(%rcx),%eax
ffffffff812c762e:	d1 c0                	rol    %eax
	T_40_59(55, A, B, C, D, E);
ffffffff812c7630:	89 41 1c             	mov    %eax,0x1c(%rcx)
ffffffff812c7633:	45 8d 94 02 dc bc 1b 	lea    -0x70e44324(%r10,%rax,1),%r10d
ffffffff812c763a:	8f 
ffffffff812c763b:	89 f8                	mov    %edi,%eax
ffffffff812c763d:	44 21 c0             	and    %r8d,%eax
ffffffff812c7640:	45 8d 1c 02          	lea    (%r10,%rax,1),%r11d
ffffffff812c7644:	41 89 fa             	mov    %edi,%r10d
ffffffff812c7647:	89 f0                	mov    %esi,%eax
ffffffff812c7649:	45 31 c2             	xor    %r8d,%r10d
ffffffff812c764c:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c764f:	45 21 ca             	and    %r9d,%r10d
ffffffff812c7652:	45 01 da             	add    %r11d,%r10d
ffffffff812c7655:	41 01 c2             	add    %eax,%r10d
ffffffff812c7658:	33 59 28             	xor    0x28(%rcx),%ebx
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c765b:	c1 cf 02             	ror    $0x2,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c765e:	33 19                	xor    (%rcx),%ebx
ffffffff812c7660:	45 31 ee             	xor    %r13d,%r14d
	T_40_59(56, E, A, B, C, D);
ffffffff812c7663:	89 f0                	mov    %esi,%eax
ffffffff812c7665:	33 59 14             	xor    0x14(%rcx),%ebx
ffffffff812c7668:	44 33 71 04          	xor    0x4(%rcx),%r14d
ffffffff812c766c:	21 f8                	and    %edi,%eax
ffffffff812c766e:	44 33 71 18          	xor    0x18(%rcx),%r14d
ffffffff812c7672:	d1 c3                	rol    %ebx
ffffffff812c7674:	45 8d 8c 19 dc bc 1b 	lea    -0x70e44324(%r9,%rbx,1),%r9d
ffffffff812c767b:	8f 
ffffffff812c767c:	41 d1 c6             	rol    %r14d
ffffffff812c767f:	89 59 20             	mov    %ebx,0x20(%rcx)
	T_40_59(57, D, E, A, B, C);
ffffffff812c7682:	44 89 71 24          	mov    %r14d,0x24(%rcx)
	T_40_59(51, E, A, B, C, D);
	T_40_59(52, D, E, A, B, C);
	T_40_59(53, C, D, E, A, B);
	T_40_59(54, B, C, D, E, A);
	T_40_59(55, A, B, C, D, E);
	T_40_59(56, E, A, B, C, D);
ffffffff812c7686:	45 8d 1c 01          	lea    (%r9,%rax,1),%r11d
ffffffff812c768a:	41 89 f1             	mov    %esi,%r9d
ffffffff812c768d:	44 89 d0             	mov    %r10d,%eax
ffffffff812c7690:	41 31 f9             	xor    %edi,%r9d
ffffffff812c7693:	c1 c0 05             	rol    $0x5,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7696:	c1 ce 02             	ror    $0x2,%esi
ffffffff812c7699:	45 21 c1             	and    %r8d,%r9d
	T_40_59(57, D, E, A, B, C);
ffffffff812c769c:	47 8d 84 30 dc bc 1b 	lea    -0x70e44324(%r8,%r14,1),%r8d
ffffffff812c76a3:	8f 
	T_40_59(58, C, D, E, A, B);
ffffffff812c76a4:	44 8b 71 1c          	mov    0x1c(%rcx),%r14d
	T_40_59(51, E, A, B, C, D);
	T_40_59(52, D, E, A, B, C);
	T_40_59(53, C, D, E, A, B);
	T_40_59(54, B, C, D, E, A);
	T_40_59(55, A, B, C, D, E);
	T_40_59(56, E, A, B, C, D);
ffffffff812c76a8:	45 01 d9             	add    %r11d,%r9d
ffffffff812c76ab:	41 01 c1             	add    %eax,%r9d
	T_40_59(57, D, E, A, B, C);
ffffffff812c76ae:	44 89 d0             	mov    %r10d,%eax
ffffffff812c76b1:	21 f0                	and    %esi,%eax
ffffffff812c76b3:	45 8d 1c 00          	lea    (%r8,%rax,1),%r11d
ffffffff812c76b7:	45 89 d0             	mov    %r10d,%r8d
ffffffff812c76ba:	44 89 c8             	mov    %r9d,%eax
ffffffff812c76bd:	41 31 f0             	xor    %esi,%r8d
ffffffff812c76c0:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c76c3:	41 c1 ca 02          	ror    $0x2,%r10d
ffffffff812c76c7:	41 21 f8             	and    %edi,%r8d
ffffffff812c76ca:	45 01 d8             	add    %r11d,%r8d
ffffffff812c76cd:	41 01 c0             	add    %eax,%r8d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c76d0:	8b 41 28             	mov    0x28(%rcx),%eax
ffffffff812c76d3:	44 31 e0             	xor    %r12d,%eax
ffffffff812c76d6:	33 41 08             	xor    0x8(%rcx),%eax
ffffffff812c76d9:	44 31 f0             	xor    %r14d,%eax
ffffffff812c76dc:	d1 c0                	rol    %eax
	T_40_59(58, C, D, E, A, B);
ffffffff812c76de:	89 41 28             	mov    %eax,0x28(%rcx)
ffffffff812c76e1:	8d bc 07 dc bc 1b 8f 	lea    -0x70e44324(%rdi,%rax,1),%edi
ffffffff812c76e8:	44 89 c8             	mov    %r9d,%eax
ffffffff812c76eb:	44 21 d0             	and    %r10d,%eax
ffffffff812c76ee:	44 8d 1c 07          	lea    (%rdi,%rax,1),%r11d
ffffffff812c76f2:	44 89 cf             	mov    %r9d,%edi
ffffffff812c76f5:	44 89 c0             	mov    %r8d,%eax
ffffffff812c76f8:	44 31 d7             	xor    %r10d,%edi
ffffffff812c76fb:	21 f7                	and    %esi,%edi
ffffffff812c76fd:	44 01 df             	add    %r11d,%edi
ffffffff812c7700:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c7703:	45 31 fd             	xor    %r15d,%r13d
ffffffff812c7706:	44 33 69 0c          	xor    0xc(%rcx),%r13d
ffffffff812c770a:	44 33 61 38          	xor    0x38(%rcx),%r12d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c770e:	41 c1 c9 02          	ror    $0x2,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7712:	44 33 69 20          	xor    0x20(%rcx),%r13d
ffffffff812c7716:	44 33 61 10          	xor    0x10(%rcx),%r12d
ffffffff812c771a:	01 c7                	add    %eax,%edi
	T_40_59(59, B, C, D, E, A);
ffffffff812c771c:	44 89 c0             	mov    %r8d,%eax
ffffffff812c771f:	44 21 c8             	and    %r9d,%eax
ffffffff812c7722:	41 d1 c5             	rol    %r13d
ffffffff812c7725:	44 89 69 2c          	mov    %r13d,0x2c(%rcx)
ffffffff812c7729:	46 8d 9c 2e dc bc 1b 	lea    -0x70e44324(%rsi,%r13,1),%r11d
ffffffff812c7730:	8f 

	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
ffffffff812c7731:	44 8b 69 24          	mov    0x24(%rcx),%r13d
	T_40_59(54, B, C, D, E, A);
	T_40_59(55, A, B, C, D, E);
	T_40_59(56, E, A, B, C, D);
	T_40_59(57, D, E, A, B, C);
	T_40_59(58, C, D, E, A, B);
	T_40_59(59, B, C, D, E, A);
ffffffff812c7735:	44 89 c6             	mov    %r8d,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7738:	41 c1 c8 02          	ror    $0x2,%r8d
ffffffff812c773c:	44 31 ce             	xor    %r9d,%esi
ffffffff812c773f:	41 01 c3             	add    %eax,%r11d
ffffffff812c7742:	44 21 d6             	and    %r10d,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7745:	45 31 ec             	xor    %r13d,%r12d
ffffffff812c7748:	41 8d 04 33          	lea    (%r11,%rsi,1),%eax
ffffffff812c774c:	41 d1 c4             	rol    %r12d
ffffffff812c774f:	89 fe                	mov    %edi,%esi

	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
ffffffff812c7751:	44 89 61 30          	mov    %r12d,0x30(%rcx)
	T_40_59(54, B, C, D, E, A);
	T_40_59(55, A, B, C, D, E);
	T_40_59(56, E, A, B, C, D);
	T_40_59(57, D, E, A, B, C);
	T_40_59(58, C, D, E, A, B);
	T_40_59(59, B, C, D, E, A);
ffffffff812c7755:	c1 c6 05             	rol    $0x5,%esi
ffffffff812c7758:	44 33 79 3c          	xor    0x3c(%rcx),%r15d
ffffffff812c775c:	01 c6                	add    %eax,%esi
ffffffff812c775e:	44 33 79 14          	xor    0x14(%rcx),%r15d

	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
ffffffff812c7762:	44 89 c8             	mov    %r9d,%eax
ffffffff812c7765:	44 33 79 28          	xor    0x28(%rcx),%r15d
ffffffff812c7769:	44 31 c0             	xor    %r8d,%eax
ffffffff812c776c:	47 8d 94 22 d6 c1 62 	lea    -0x359d3e2a(%r10,%r12,1),%r10d
ffffffff812c7773:	ca 
ffffffff812c7774:	31 f8                	xor    %edi,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7776:	c1 cf 02             	ror    $0x2,%edi
ffffffff812c7779:	44 01 d0             	add    %r10d,%eax
ffffffff812c777c:	41 89 f2             	mov    %esi,%r10d
ffffffff812c777f:	41 c1 c2 05          	rol    $0x5,%r10d
ffffffff812c7783:	41 01 c2             	add    %eax,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7786:	41 d1 c7             	rol    %r15d
	T_60_79(61, E, A, B, C, D);
ffffffff812c7789:	44 89 c0             	mov    %r8d,%eax
ffffffff812c778c:	47 8d 9c 39 d6 c1 62 	lea    -0x359d3e2a(%r9,%r15,1),%r11d
ffffffff812c7793:	ca 
ffffffff812c7794:	31 f8                	xor    %edi,%eax
ffffffff812c7796:	44 8b 49 38          	mov    0x38(%rcx),%r9d
ffffffff812c779a:	31 f0                	xor    %esi,%eax
ffffffff812c779c:	44 89 79 34          	mov    %r15d,0x34(%rcx)
	T_60_79(62, D, E, A, B, C);
ffffffff812c77a0:	44 8b 79 2c          	mov    0x2c(%rcx),%r15d
	T_40_59(58, C, D, E, A, B);
	T_40_59(59, B, C, D, E, A);

	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
	T_60_79(61, E, A, B, C, D);
ffffffff812c77a4:	44 01 d8             	add    %r11d,%eax
ffffffff812c77a7:	45 89 d3             	mov    %r10d,%r11d
ffffffff812c77aa:	41 c1 c3 05          	rol    $0x5,%r11d
ffffffff812c77ae:	41 01 c3             	add    %eax,%r11d
ffffffff812c77b1:	44 33 09             	xor    (%rcx),%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c77b4:	c1 ce 02             	ror    $0x2,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c77b7:	44 33 49 18          	xor    0x18(%rcx),%r9d
	T_60_79(62, D, E, A, B, C);
ffffffff812c77bb:	89 f8                	mov    %edi,%eax
ffffffff812c77bd:	31 f0                	xor    %esi,%eax
ffffffff812c77bf:	44 31 d0             	xor    %r10d,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c77c2:	41 c1 ca 02          	ror    $0x2,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c77c6:	45 31 f9             	xor    %r15d,%r9d
ffffffff812c77c9:	41 d1 c1             	rol    %r9d
ffffffff812c77cc:	44 89 49 38          	mov    %r9d,0x38(%rcx)
ffffffff812c77d0:	47 8d 8c 08 d6 c1 62 	lea    -0x359d3e2a(%r8,%r9,1),%r9d
ffffffff812c77d7:	ca 
ffffffff812c77d8:	44 8b 41 3c          	mov    0x3c(%rcx),%r8d
ffffffff812c77dc:	44 33 41 04          	xor    0x4(%rcx),%r8d
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
ffffffff812c77e0:	8b 59 38             	mov    0x38(%rcx),%ebx
	T_40_59(59, B, C, D, E, A);

	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
ffffffff812c77e3:	44 01 c8             	add    %r9d,%eax
ffffffff812c77e6:	45 89 d9             	mov    %r11d,%r9d
ffffffff812c77e9:	41 c1 c1 05          	rol    $0x5,%r9d
ffffffff812c77ed:	41 01 c1             	add    %eax,%r9d
	T_60_79(63, C, D, E, A, B);
ffffffff812c77f0:	89 f0                	mov    %esi,%eax
ffffffff812c77f2:	45 31 f0             	xor    %r14d,%r8d
ffffffff812c77f5:	44 33 41 30          	xor    0x30(%rcx),%r8d
ffffffff812c77f9:	44 31 d0             	xor    %r10d,%eax
ffffffff812c77fc:	44 31 d8             	xor    %r11d,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c77ff:	41 c1 cb 02          	ror    $0x2,%r11d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7803:	41 d1 c0             	rol    %r8d
ffffffff812c7806:	42 8d bc 07 d6 c1 62 	lea    -0x359d3e2a(%rdi,%r8,1),%edi
ffffffff812c780d:	ca 
ffffffff812c780e:	44 89 41 3c          	mov    %r8d,0x3c(%rcx)
	T_60_79(64, B, C, D, E, A);
ffffffff812c7812:	45 89 d0             	mov    %r10d,%r8d
ffffffff812c7815:	45 31 d8             	xor    %r11d,%r8d
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
ffffffff812c7818:	44 8b 61 3c          	mov    0x3c(%rcx),%r12d

	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
ffffffff812c781c:	01 f8                	add    %edi,%eax
ffffffff812c781e:	44 89 cf             	mov    %r9d,%edi
	T_60_79(64, B, C, D, E, A);
ffffffff812c7821:	45 31 c8             	xor    %r9d,%r8d

	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
ffffffff812c7824:	c1 c7 05             	rol    $0x5,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7827:	41 c1 c9 02          	ror    $0x2,%r9d
ffffffff812c782b:	01 c7                	add    %eax,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c782d:	8b 01                	mov    (%rcx),%eax
ffffffff812c782f:	33 41 08             	xor    0x8(%rcx),%eax
ffffffff812c7832:	33 41 20             	xor    0x20(%rcx),%eax
ffffffff812c7835:	33 41 34             	xor    0x34(%rcx),%eax
ffffffff812c7838:	d1 c0                	rol    %eax
	T_60_79(64, B, C, D, E, A);
ffffffff812c783a:	89 01                	mov    %eax,(%rcx)
ffffffff812c783c:	8d 84 06 d6 c1 62 ca 	lea    -0x359d3e2a(%rsi,%rax,1),%eax
	T_60_79(65, A, B, C, D, E);
ffffffff812c7843:	44 89 de             	mov    %r11d,%esi
	/* Round 4 */
	T_60_79(60, A, B, C, D, E);
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
ffffffff812c7846:	41 01 c0             	add    %eax,%r8d
ffffffff812c7849:	89 f8                	mov    %edi,%eax
ffffffff812c784b:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c784e:	41 01 c0             	add    %eax,%r8d
ffffffff812c7851:	8b 41 04             	mov    0x4(%rcx),%eax
ffffffff812c7854:	33 41 0c             	xor    0xc(%rcx),%eax
	T_60_79(65, A, B, C, D, E);
ffffffff812c7857:	44 31 ce             	xor    %r9d,%esi
ffffffff812c785a:	31 fe                	xor    %edi,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c785c:	c1 cf 02             	ror    $0x2,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c785f:	44 31 e8             	xor    %r13d,%eax
ffffffff812c7862:	31 d8                	xor    %ebx,%eax
ffffffff812c7864:	d1 c0                	rol    %eax
ffffffff812c7866:	45 8d 94 02 d6 c1 62 	lea    -0x359d3e2a(%r10,%rax,1),%r10d
ffffffff812c786d:	ca 
ffffffff812c786e:	89 41 04             	mov    %eax,0x4(%rcx)
	T_60_79(66, E, A, B, C, D);
ffffffff812c7871:	44 89 c8             	mov    %r9d,%eax
ffffffff812c7874:	31 f8                	xor    %edi,%eax
	T_60_79(60, A, B, C, D, E);
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
ffffffff812c7876:	44 01 d6             	add    %r10d,%esi
ffffffff812c7879:	45 89 c2             	mov    %r8d,%r10d
	T_60_79(66, E, A, B, C, D);
ffffffff812c787c:	44 31 c0             	xor    %r8d,%eax
	T_60_79(60, A, B, C, D, E);
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
ffffffff812c787f:	41 c1 c2 05          	rol    $0x5,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7883:	41 c1 c8 02          	ror    $0x2,%r8d
ffffffff812c7887:	41 01 f2             	add    %esi,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c788a:	8b 71 08             	mov    0x8(%rcx),%esi
ffffffff812c788d:	33 71 10             	xor    0x10(%rcx),%esi
ffffffff812c7890:	33 71 28             	xor    0x28(%rcx),%esi
ffffffff812c7893:	44 31 e6             	xor    %r12d,%esi
ffffffff812c7896:	d1 c6                	rol    %esi
	T_60_79(66, E, A, B, C, D);
ffffffff812c7898:	89 71 08             	mov    %esi,0x8(%rcx)
ffffffff812c789b:	41 8d b4 33 d6 c1 62 	lea    -0x359d3e2a(%r11,%rsi,1),%esi
ffffffff812c78a2:	ca 
	T_60_79(67, D, E, A, B, C);
ffffffff812c78a3:	41 89 fb             	mov    %edi,%r11d
ffffffff812c78a6:	45 31 c3             	xor    %r8d,%r11d
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
ffffffff812c78a9:	01 f0                	add    %esi,%eax
ffffffff812c78ab:	44 89 d6             	mov    %r10d,%esi
	T_60_79(67, D, E, A, B, C);
ffffffff812c78ae:	45 31 d3             	xor    %r10d,%r11d
	T_60_79(61, E, A, B, C, D);
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
ffffffff812c78b1:	c1 c6 05             	rol    $0x5,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c78b4:	41 c1 ca 02          	ror    $0x2,%r10d
ffffffff812c78b8:	01 f0                	add    %esi,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c78ba:	8b 71 0c             	mov    0xc(%rcx),%esi
ffffffff812c78bd:	33 71 14             	xor    0x14(%rcx),%esi
ffffffff812c78c0:	44 31 fe             	xor    %r15d,%esi
ffffffff812c78c3:	33 31                	xor    (%rcx),%esi
ffffffff812c78c5:	d1 c6                	rol    %esi
	T_60_79(67, D, E, A, B, C);
ffffffff812c78c7:	89 71 0c             	mov    %esi,0xc(%rcx)
ffffffff812c78ca:	41 8d b4 31 d6 c1 62 	lea    -0x359d3e2a(%r9,%rsi,1),%esi
ffffffff812c78d1:	ca 
	T_60_79(68, C, D, E, A, B);
ffffffff812c78d2:	45 89 c1             	mov    %r8d,%r9d
	T_60_79(62, D, E, A, B, C);
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
ffffffff812c78d5:	41 01 f3             	add    %esi,%r11d
ffffffff812c78d8:	89 c6                	mov    %eax,%esi
ffffffff812c78da:	c1 c6 05             	rol    $0x5,%esi
ffffffff812c78dd:	41 01 f3             	add    %esi,%r11d
ffffffff812c78e0:	8b 71 10             	mov    0x10(%rcx),%esi
ffffffff812c78e3:	33 71 18             	xor    0x18(%rcx),%esi
ffffffff812c78e6:	33 71 30             	xor    0x30(%rcx),%esi
ffffffff812c78e9:	33 71 04             	xor    0x4(%rcx),%esi
	T_60_79(68, C, D, E, A, B);
ffffffff812c78ec:	45 31 d1             	xor    %r10d,%r9d
ffffffff812c78ef:	41 31 c1             	xor    %eax,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c78f2:	c1 c8 02             	ror    $0x2,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c78f5:	d1 c6                	rol    %esi
ffffffff812c78f7:	89 71 10             	mov    %esi,0x10(%rcx)
ffffffff812c78fa:	8d b4 37 d6 c1 62 ca 	lea    -0x359d3e2a(%rdi,%rsi,1),%esi
	T_60_79(69, B, C, D, E, A);
ffffffff812c7901:	44 89 d7             	mov    %r10d,%edi
ffffffff812c7904:	31 c7                	xor    %eax,%edi
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
ffffffff812c7906:	41 01 f1             	add    %esi,%r9d
ffffffff812c7909:	44 89 de             	mov    %r11d,%esi
	T_60_79(69, B, C, D, E, A);
ffffffff812c790c:	44 31 df             	xor    %r11d,%edi
	T_60_79(63, C, D, E, A, B);
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
ffffffff812c790f:	c1 c6 05             	rol    $0x5,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7912:	41 c1 cb 02          	ror    $0x2,%r11d
ffffffff812c7916:	41 01 f1             	add    %esi,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7919:	8b 71 14             	mov    0x14(%rcx),%esi
ffffffff812c791c:	44 31 f6             	xor    %r14d,%esi
ffffffff812c791f:	33 71 34             	xor    0x34(%rcx),%esi
ffffffff812c7922:	45 31 ee             	xor    %r13d,%r14d
ffffffff812c7925:	33 71 08             	xor    0x8(%rcx),%esi
ffffffff812c7928:	45 31 e6             	xor    %r12d,%r14d
ffffffff812c792b:	44 33 71 10          	xor    0x10(%rcx),%r14d
ffffffff812c792f:	d1 c6                	rol    %esi
ffffffff812c7931:	41 d1 c6             	rol    %r14d
	T_60_79(69, B, C, D, E, A);
ffffffff812c7934:	89 71 14             	mov    %esi,0x14(%rcx)
ffffffff812c7937:	41 8d b4 30 d6 c1 62 	lea    -0x359d3e2a(%r8,%rsi,1),%esi
ffffffff812c793e:	ca 
	T_60_79(70, A, B, C, D, E);
ffffffff812c793f:	41 89 c0             	mov    %eax,%r8d
ffffffff812c7942:	45 31 d8             	xor    %r11d,%r8d
	T_60_79(71, E, A, B, C, D);
ffffffff812c7945:	42 8d 84 30 d6 c1 62 	lea    -0x359d3e2a(%rax,%r14,1),%eax
ffffffff812c794c:	ca 
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
ffffffff812c794d:	01 f7                	add    %esi,%edi
ffffffff812c794f:	44 89 ce             	mov    %r9d,%esi
	T_60_79(70, A, B, C, D, E);
ffffffff812c7952:	45 31 c8             	xor    %r9d,%r8d
	T_60_79(64, B, C, D, E, A);
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
ffffffff812c7955:	c1 c6 05             	rol    $0x5,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7958:	41 c1 c9 02          	ror    $0x2,%r9d
ffffffff812c795c:	01 f7                	add    %esi,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c795e:	8b 71 18             	mov    0x18(%rcx),%esi
ffffffff812c7961:	33 71 20             	xor    0x20(%rcx),%esi
ffffffff812c7964:	31 de                	xor    %ebx,%esi
ffffffff812c7966:	33 71 0c             	xor    0xc(%rcx),%esi
ffffffff812c7969:	d1 c6                	rol    %esi
	T_60_79(70, A, B, C, D, E);
ffffffff812c796b:	89 71 18             	mov    %esi,0x18(%rcx)
ffffffff812c796e:	41 8d b4 32 d6 c1 62 	lea    -0x359d3e2a(%r10,%rsi,1),%esi
ffffffff812c7975:	ca 
	T_60_79(71, E, A, B, C, D);
ffffffff812c7976:	44 89 71 1c          	mov    %r14d,0x1c(%rcx)
	T_60_79(72, D, E, A, B, C);
ffffffff812c797a:	44 8b 71 14          	mov    0x14(%rcx),%r14d
ffffffff812c797e:	45 89 ca             	mov    %r9d,%r10d
	T_60_79(65, A, B, C, D, E);
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
	T_60_79(70, A, B, C, D, E);
ffffffff812c7981:	41 01 f0             	add    %esi,%r8d
ffffffff812c7984:	89 fe                	mov    %edi,%esi
ffffffff812c7986:	c1 c6 05             	rol    $0x5,%esi
ffffffff812c7989:	41 01 f0             	add    %esi,%r8d
	T_60_79(71, E, A, B, C, D);
ffffffff812c798c:	44 89 de             	mov    %r11d,%esi
ffffffff812c798f:	44 31 ce             	xor    %r9d,%esi
ffffffff812c7992:	45 31 fd             	xor    %r15d,%r13d
ffffffff812c7995:	44 33 69 04          	xor    0x4(%rcx),%r13d
ffffffff812c7999:	31 fe                	xor    %edi,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c799b:	c1 cf 02             	ror    $0x2,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c799e:	44 33 69 18          	xor    0x18(%rcx),%r13d
ffffffff812c79a2:	01 c6                	add    %eax,%esi
ffffffff812c79a4:	44 89 c0             	mov    %r8d,%eax
	T_60_79(72, D, E, A, B, C);
ffffffff812c79a7:	41 31 fa             	xor    %edi,%r10d
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
ffffffff812c79aa:	c1 c0 05             	rol    $0x5,%eax
	T_60_79(72, D, E, A, B, C);
ffffffff812c79ad:	45 31 c2             	xor    %r8d,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c79b0:	41 c1 c8 02          	ror    $0x2,%r8d
	T_60_79(66, E, A, B, C, D);
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
ffffffff812c79b4:	01 c6                	add    %eax,%esi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c79b6:	8b 41 20             	mov    0x20(%rcx),%eax
ffffffff812c79b9:	33 41 28             	xor    0x28(%rcx),%eax
ffffffff812c79bc:	33 01                	xor    (%rcx),%eax
ffffffff812c79be:	41 d1 c5             	rol    %r13d
	T_60_79(72, D, E, A, B, C);
	T_60_79(73, C, D, E, A, B);
ffffffff812c79c1:	47 8d 8c 29 d6 c1 62 	lea    -0x359d3e2a(%r9,%r13,1),%r9d
ffffffff812c79c8:	ca 
ffffffff812c79c9:	44 31 f0             	xor    %r14d,%eax
ffffffff812c79cc:	d1 c0                	rol    %eax
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
ffffffff812c79ce:	89 41 20             	mov    %eax,0x20(%rcx)
ffffffff812c79d1:	41 8d 84 03 d6 c1 62 	lea    -0x359d3e2a(%r11,%rax,1),%eax
ffffffff812c79d8:	ca 
	T_60_79(73, C, D, E, A, B);
ffffffff812c79d9:	44 89 69 24          	mov    %r13d,0x24(%rcx)
	T_60_79(74, B, C, D, E, A);
ffffffff812c79dd:	44 8b 69 1c          	mov    0x1c(%rcx),%r13d
ffffffff812c79e1:	45 89 c3             	mov    %r8d,%r11d
	T_60_79(67, D, E, A, B, C);
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
ffffffff812c79e4:	41 01 c2             	add    %eax,%r10d
ffffffff812c79e7:	89 f0                	mov    %esi,%eax
ffffffff812c79e9:	c1 c0 05             	rol    $0x5,%eax
ffffffff812c79ec:	41 01 c2             	add    %eax,%r10d
	T_60_79(73, C, D, E, A, B);
ffffffff812c79ef:	89 f8                	mov    %edi,%eax
ffffffff812c79f1:	44 31 c0             	xor    %r8d,%eax
ffffffff812c79f4:	31 f0                	xor    %esi,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c79f6:	c1 ce 02             	ror    $0x2,%esi
ffffffff812c79f9:	44 01 c8             	add    %r9d,%eax
ffffffff812c79fc:	45 89 d1             	mov    %r10d,%r9d
	T_60_79(74, B, C, D, E, A);
ffffffff812c79ff:	41 31 f3             	xor    %esi,%r11d
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
	T_60_79(73, C, D, E, A, B);
ffffffff812c7a02:	41 c1 c1 05          	rol    $0x5,%r9d
	T_60_79(74, B, C, D, E, A);
ffffffff812c7a06:	45 31 d3             	xor    %r10d,%r11d
	T_60_79(68, C, D, E, A, B);
	T_60_79(69, B, C, D, E, A);
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
	T_60_79(73, C, D, E, A, B);
ffffffff812c7a09:	44 01 c8             	add    %r9d,%eax
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7a0c:	44 8b 49 28          	mov    0x28(%rcx),%r9d
ffffffff812c7a10:	44 33 49 30          	xor    0x30(%rcx),%r9d
ffffffff812c7a14:	44 33 49 08          	xor    0x8(%rcx),%r9d
ffffffff812c7a18:	45 31 e9             	xor    %r13d,%r9d
ffffffff812c7a1b:	41 d1 c1             	rol    %r9d
	T_60_79(74, B, C, D, E, A);
ffffffff812c7a1e:	42 8d bc 0f d6 c1 62 	lea    -0x359d3e2a(%rdi,%r9,1),%edi
ffffffff812c7a25:	ca 
ffffffff812c7a26:	44 89 49 28          	mov    %r9d,0x28(%rcx)
ffffffff812c7a2a:	41 01 fb             	add    %edi,%r11d
ffffffff812c7a2d:	44 33 79 34          	xor    0x34(%rcx),%r15d
ffffffff812c7a31:	89 c7                	mov    %eax,%edi
ffffffff812c7a33:	44 33 79 0c          	xor    0xc(%rcx),%r15d
ffffffff812c7a37:	c1 c7 05             	rol    $0x5,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7a3a:	41 c1 ca 02          	ror    $0x2,%r10d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7a3e:	44 33 79 20          	xor    0x20(%rcx),%r15d
ffffffff812c7a42:	41 01 fb             	add    %edi,%r11d
	T_60_79(75, A, B, C, D, E);
ffffffff812c7a45:	89 f7                	mov    %esi,%edi
ffffffff812c7a47:	44 31 d7             	xor    %r10d,%edi
	T_60_79(76, E, A, B, C, D);
ffffffff812c7a4a:	45 89 d1             	mov    %r10d,%r9d
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
	T_60_79(73, C, D, E, A, B);
	T_60_79(74, B, C, D, E, A);
	T_60_79(75, A, B, C, D, E);
ffffffff812c7a4d:	31 c7                	xor    %eax,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7a4f:	c1 c8 02             	ror    $0x2,%eax
	T_60_79(76, E, A, B, C, D);
ffffffff812c7a52:	41 31 c1             	xor    %eax,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7a55:	41 d1 c7             	rol    %r15d
ffffffff812c7a58:	45 31 d9             	xor    %r11d,%r9d
	T_60_79(70, A, B, C, D, E);
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
	T_60_79(73, C, D, E, A, B);
	T_60_79(74, B, C, D, E, A);
	T_60_79(75, A, B, C, D, E);
ffffffff812c7a5b:	47 8d 84 38 d6 c1 62 	lea    -0x359d3e2a(%r8,%r15,1),%r8d
ffffffff812c7a62:	ca 
ffffffff812c7a63:	44 89 79 2c          	mov    %r15d,0x2c(%rcx)
ffffffff812c7a67:	44 01 c7             	add    %r8d,%edi
ffffffff812c7a6a:	45 89 d8             	mov    %r11d,%r8d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7a6d:	41 c1 cb 02          	ror    $0x2,%r11d
ffffffff812c7a71:	41 c1 c0 05          	rol    $0x5,%r8d
ffffffff812c7a75:	44 01 c7             	add    %r8d,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7a78:	44 8b 41 30          	mov    0x30(%rcx),%r8d
ffffffff812c7a7c:	41 31 d8             	xor    %ebx,%r8d
ffffffff812c7a7f:	44 33 41 10          	xor    0x10(%rcx),%r8d
	T_60_79(76, E, A, B, C, D);
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
	T_60_79(79, B, C, D, E, A);

	digest[0] += A;
ffffffff812c7a83:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
ffffffff812c7a87:	44 33 41 24          	xor    0x24(%rcx),%r8d
ffffffff812c7a8b:	41 d1 c0             	rol    %r8d
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
	T_60_79(73, C, D, E, A, B);
	T_60_79(74, B, C, D, E, A);
	T_60_79(75, A, B, C, D, E);
	T_60_79(76, E, A, B, C, D);
ffffffff812c7a8e:	42 8d b4 06 d6 c1 62 	lea    -0x359d3e2a(%rsi,%r8,1),%esi
ffffffff812c7a95:	ca 
ffffffff812c7a96:	44 89 41 30          	mov    %r8d,0x30(%rcx)
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
ffffffff812c7a9a:	45 89 d8             	mov    %r11d,%r8d
	T_60_79(71, E, A, B, C, D);
	T_60_79(72, D, E, A, B, C);
	T_60_79(73, C, D, E, A, B);
	T_60_79(74, B, C, D, E, A);
	T_60_79(75, A, B, C, D, E);
	T_60_79(76, E, A, B, C, D);
ffffffff812c7a9d:	41 01 f1             	add    %esi,%r9d
ffffffff812c7aa0:	89 fe                	mov    %edi,%esi
ffffffff812c7aa2:	c1 c6 05             	rol    $0x5,%esi
ffffffff812c7aa5:	41 01 f1             	add    %esi,%r9d
ffffffff812c7aa8:	8b 71 34             	mov    0x34(%rcx),%esi
ffffffff812c7aab:	44 31 e6             	xor    %r12d,%esi
ffffffff812c7aae:	41 31 f6             	xor    %esi,%r14d
ffffffff812c7ab1:	44 33 71 28          	xor    0x28(%rcx),%r14d
	T_60_79(77, D, E, A, B, C);
ffffffff812c7ab5:	89 c6                	mov    %eax,%esi
ffffffff812c7ab7:	44 31 de             	xor    %r11d,%esi
ffffffff812c7aba:	31 fe                	xor    %edi,%esi
ffffffff812c7abc:	41 d1 c6             	rol    %r14d
ffffffff812c7abf:	47 8d 94 32 d6 c1 62 	lea    -0x359d3e2a(%r10,%r14,1),%r10d
ffffffff812c7ac6:	ca 
ffffffff812c7ac7:	44 89 71 34          	mov    %r14d,0x34(%rcx)
ffffffff812c7acb:	44 01 d6             	add    %r10d,%esi
ffffffff812c7ace:	45 89 ca             	mov    %r9d,%r10d
ffffffff812c7ad1:	41 c1 c2 05          	rol    $0x5,%r10d
ffffffff812c7ad5:	41 01 f2             	add    %esi,%r10d
ffffffff812c7ad8:	44 33 61 04          	xor    0x4(%rcx),%r12d
ffffffff812c7adc:	33 19                	xor    (%rcx),%ebx
ffffffff812c7ade:	33 59 18             	xor    0x18(%rcx),%ebx
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7ae1:	c1 cf 02             	ror    $0x2,%edi
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7ae4:	33 59 2c             	xor    0x2c(%rcx),%ebx
	T_60_79(78, C, D, E, A, B);
ffffffff812c7ae7:	41 31 f8             	xor    %edi,%r8d
ffffffff812c7aea:	45 31 c8             	xor    %r9d,%r8d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 ror32(__u32 word, unsigned int shift)
{
	return (word >> shift) | (word << (32 - shift));
ffffffff812c7aed:	41 c1 c9 02          	ror    $0x2,%r9d
 * @word: value to rotate
 * @shift: bits to roll
 */
static inline __u32 rol32(__u32 word, unsigned int shift)
{
	return (word << shift) | (word >> (32 - shift));
ffffffff812c7af1:	45 31 e5             	xor    %r12d,%r13d
ffffffff812c7af4:	44 33 69 30          	xor    0x30(%rcx),%r13d
ffffffff812c7af8:	d1 c3                	rol    %ebx
ffffffff812c7afa:	89 59 38             	mov    %ebx,0x38(%rcx)
ffffffff812c7afd:	8d b4 18 d6 c1 62 ca 	lea    -0x359d3e2a(%rax,%rbx,1),%esi
ffffffff812c7b04:	41 d1 c5             	rol    %r13d
	T_60_79(79, B, C, D, E, A);
ffffffff812c7b07:	44 89 69 3c          	mov    %r13d,0x3c(%rcx)

	digest[0] += A;
ffffffff812c7b0b:	44 03 1a             	add    (%rdx),%r11d
	T_60_79(73, C, D, E, A, B);
	T_60_79(74, B, C, D, E, A);
	T_60_79(75, A, B, C, D, E);
	T_60_79(76, E, A, B, C, D);
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
ffffffff812c7b0e:	41 8d 04 30          	lea    (%r8,%rsi,1),%eax
ffffffff812c7b12:	44 89 d6             	mov    %r10d,%esi
	T_60_79(79, B, C, D, E, A);

	digest[0] += A;
ffffffff812c7b15:	89 fa                	mov    %edi,%edx
ffffffff812c7b17:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
	T_60_79(73, C, D, E, A, B);
	T_60_79(74, B, C, D, E, A);
	T_60_79(75, A, B, C, D, E);
	T_60_79(76, E, A, B, C, D);
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
ffffffff812c7b1b:	c1 c6 05             	rol    $0x5,%esi
	T_60_79(79, B, C, D, E, A);

	digest[0] += A;
ffffffff812c7b1e:	44 31 ca             	xor    %r9d,%edx
	T_60_79(73, C, D, E, A, B);
	T_60_79(74, B, C, D, E, A);
	T_60_79(75, A, B, C, D, E);
	T_60_79(76, E, A, B, C, D);
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
ffffffff812c7b21:	01 f0                	add    %esi,%eax
	T_60_79(79, B, C, D, E, A);

	digest[0] += A;
ffffffff812c7b23:	44 31 d2             	xor    %r10d,%edx
	digest[1] += B;
	digest[2] += C;
ffffffff812c7b26:	41 c1 ca 02          	ror    $0x2,%r10d
	T_60_79(76, E, A, B, C, D);
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
	T_60_79(79, B, C, D, E, A);

	digest[0] += A;
ffffffff812c7b2a:	43 8d b4 2b d6 c1 62 	lea    -0x359d3e2a(%r11,%r13,1),%esi
ffffffff812c7b31:	ca 
	digest[1] += B;
ffffffff812c7b32:	01 41 04             	add    %eax,0x4(%rcx)
	digest[2] += C;
ffffffff812c7b35:	44 01 51 08          	add    %r10d,0x8(%rcx)
	T_60_79(76, E, A, B, C, D);
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
	T_60_79(79, B, C, D, E, A);

	digest[0] += A;
ffffffff812c7b39:	01 f2                	add    %esi,%edx
ffffffff812c7b3b:	89 c6                	mov    %eax,%esi
	digest[1] += B;
	digest[2] += C;
	digest[3] += D;
ffffffff812c7b3d:	44 01 49 0c          	add    %r9d,0xc(%rcx)
	T_60_79(76, E, A, B, C, D);
	T_60_79(77, D, E, A, B, C);
	T_60_79(78, C, D, E, A, B);
	T_60_79(79, B, C, D, E, A);

	digest[0] += A;
ffffffff812c7b41:	c1 c6 05             	rol    $0x5,%esi
ffffffff812c7b44:	01 f2                	add    %esi,%edx
ffffffff812c7b46:	89 11                	mov    %edx,(%rcx)
	digest[1] += B;
	digest[2] += C;
	digest[3] += D;
	digest[4] += E;
ffffffff812c7b48:	01 79 10             	add    %edi,0x10(%rcx)
}
ffffffff812c7b4b:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812c7b4f:	5b                   	pop    %rbx
ffffffff812c7b50:	41 5c                	pop    %r12
ffffffff812c7b52:	41 5d                	pop    %r13
ffffffff812c7b54:	41 5e                	pop    %r14
ffffffff812c7b56:	41 5f                	pop    %r15
ffffffff812c7b58:	5d                   	pop    %rbp
ffffffff812c7b59:	c3                   	retq   

ffffffff812c7b5a <show_mem>:
#include <linux/mm.h>
#include <linux/quicklist.h>
#include <linux/cma.h>

void show_mem(unsigned int filter)
{
ffffffff812c7b5a:	55                   	push   %rbp
	pg_data_t *pgdat;
	unsigned long total = 0, reserved = 0, highmem = 0;

	printk("Mem-Info:\n");
ffffffff812c7b5b:	31 c0                	xor    %eax,%eax
#include <linux/mm.h>
#include <linux/quicklist.h>
#include <linux/cma.h>

void show_mem(unsigned int filter)
{
ffffffff812c7b5d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c7b60:	41 54                	push   %r12
ffffffff812c7b62:	53                   	push   %rbx
ffffffff812c7b63:	89 fb                	mov    %edi,%ebx
	pg_data_t *pgdat;
	unsigned long total = 0, reserved = 0, highmem = 0;

	printk("Mem-Info:\n");
ffffffff812c7b65:	48 c7 c7 f3 6c 7b 81 	mov    $0xffffffff817b6cf3,%rdi
#include <linux/cma.h>

void show_mem(unsigned int filter)
{
	pg_data_t *pgdat;
	unsigned long total = 0, reserved = 0, highmem = 0;
ffffffff812c7b6c:	45 31 e4             	xor    %r12d,%r12d

	printk("Mem-Info:\n");
ffffffff812c7b6f:	e8 1e b4 16 00       	callq  ffffffff81432f92 <printk>
	show_free_areas(filter);
ffffffff812c7b74:	89 df                	mov    %ebx,%edi
#include <linux/cma.h>

void show_mem(unsigned int filter)
{
	pg_data_t *pgdat;
	unsigned long total = 0, reserved = 0, highmem = 0;
ffffffff812c7b76:	31 db                	xor    %ebx,%ebx

	printk("Mem-Info:\n");
	show_free_areas(filter);
ffffffff812c7b78:	e8 7d 08 e1 ff       	callq  ffffffff810d83fa <show_free_areas>

	for_each_online_pgdat(pgdat) {
ffffffff812c7b7d:	e8 88 e8 e1 ff       	callq  ffffffff810e640a <first_online_pgdat>
ffffffff812c7b82:	48 85 c0             	test   %rax,%rax
ffffffff812c7b85:	74 36                	je     ffffffff812c7bbd <show_mem+0x63>
ffffffff812c7b87:	48 8d 50 70          	lea    0x70(%rax),%rdx
ffffffff812c7b8b:	48 8d b0 70 18 00 00 	lea    0x1870(%rax),%rsi
ffffffff812c7b92:	48 8b 0a             	mov    (%rdx),%rcx
		int zoneid;

		pgdat_resize_lock(pgdat, &flags);
		for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
			struct zone *zone = &pgdat->node_zones[zoneid];
			if (!populated_zone(zone))
ffffffff812c7b95:	48 85 c9             	test   %rcx,%rcx
ffffffff812c7b98:	74 0d                	je     ffffffff812c7ba7 <show_mem+0x4d>
				continue;

			total += zone->present_pages;
ffffffff812c7b9a:	49 01 cc             	add    %rcx,%r12
ffffffff812c7b9d:	48 01 d9             	add    %rbx,%rcx
			reserved += zone->present_pages - zone->managed_pages;
ffffffff812c7ba0:	48 2b 4a f0          	sub    -0x10(%rdx),%rcx
ffffffff812c7ba4:	48 89 cb             	mov    %rcx,%rbx
ffffffff812c7ba7:	48 81 c2 00 06 00 00 	add    $0x600,%rdx
	for_each_online_pgdat(pgdat) {
		unsigned long flags;
		int zoneid;

		pgdat_resize_lock(pgdat, &flags);
		for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
ffffffff812c7bae:	48 39 d6             	cmp    %rdx,%rsi
ffffffff812c7bb1:	75 df                	jne    ffffffff812c7b92 <show_mem+0x38>
	unsigned long total = 0, reserved = 0, highmem = 0;

	printk("Mem-Info:\n");
	show_free_areas(filter);

	for_each_online_pgdat(pgdat) {
ffffffff812c7bb3:	48 89 c7             	mov    %rax,%rdi
ffffffff812c7bb6:	e8 5c e8 e1 ff       	callq  ffffffff810e6417 <next_online_pgdat>
ffffffff812c7bbb:	eb c5                	jmp    ffffffff812c7b82 <show_mem+0x28>
				highmem += zone->present_pages;
		}
		pgdat_resize_unlock(pgdat, &flags);
	}

	printk("%lu pages RAM\n", total);
ffffffff812c7bbd:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c7bc0:	48 c7 c7 fe 6c 7b 81 	mov    $0xffffffff817b6cfe,%rdi
ffffffff812c7bc7:	31 c0                	xor    %eax,%eax
ffffffff812c7bc9:	e8 c4 b3 16 00       	callq  ffffffff81432f92 <printk>
	printk("%lu pages HighMem/MovableOnly\n", highmem);
ffffffff812c7bce:	31 f6                	xor    %esi,%esi
ffffffff812c7bd0:	48 c7 c7 0d 6d 7b 81 	mov    $0xffffffff817b6d0d,%rdi
ffffffff812c7bd7:	31 c0                	xor    %eax,%eax
ffffffff812c7bd9:	e8 b4 b3 16 00       	callq  ffffffff81432f92 <printk>
#ifdef CONFIG_CMA
	printk("%lu pages reserved\n", (reserved - totalcma_pages));
	printk("%lu pages cma reserved\n", totalcma_pages);
#else
	printk("%lu pages reserved\n", reserved);
ffffffff812c7bde:	48 89 de             	mov    %rbx,%rsi
ffffffff812c7be1:	48 c7 c7 89 b5 79 81 	mov    $0xffffffff8179b589,%rdi
ffffffff812c7be8:	31 c0                	xor    %eax,%eax
ffffffff812c7bea:	e8 a3 b3 16 00       	callq  ffffffff81432f92 <printk>
		quicklist_total_size());
#endif
#ifdef CONFIG_MEMORY_FAILURE
	printk("%lu pages hwpoisoned\n", atomic_long_read(&num_poisoned_pages));
#endif
}
ffffffff812c7bef:	5b                   	pop    %rbx
ffffffff812c7bf0:	41 5c                	pop    %r12
ffffffff812c7bf2:	5d                   	pop    %rbp
ffffffff812c7bf3:	c3                   	retq   

ffffffff812c7bf4 <strcpy>:
 * @dest: Where to copy the string to
 * @src: Where to copy the string from
 */
#undef strcpy
char *strcpy(char *dest, const char *src)
{
ffffffff812c7bf4:	55                   	push   %rbp
ffffffff812c7bf5:	48 89 f8             	mov    %rdi,%rax
	char *tmp = dest;

	while ((*dest++ = *src++) != '\0')
ffffffff812c7bf8:	31 d2                	xor    %edx,%edx
 * @dest: Where to copy the string to
 * @src: Where to copy the string from
 */
#undef strcpy
char *strcpy(char *dest, const char *src)
{
ffffffff812c7bfa:	48 89 e5             	mov    %rsp,%rbp
	char *tmp = dest;

	while ((*dest++ = *src++) != '\0')
ffffffff812c7bfd:	8a 0c 16             	mov    (%rsi,%rdx,1),%cl
ffffffff812c7c00:	88 0c 10             	mov    %cl,(%rax,%rdx,1)
ffffffff812c7c03:	48 ff c2             	inc    %rdx
ffffffff812c7c06:	84 c9                	test   %cl,%cl
ffffffff812c7c08:	75 f3                	jne    ffffffff812c7bfd <strcpy+0x9>
		/* nothing */;
	return tmp;
}
ffffffff812c7c0a:	5d                   	pop    %rbp
ffffffff812c7c0b:	c3                   	retq   

ffffffff812c7c0c <strncpy>:
 * In the case where the length of @src is less than  that  of
 * count, the remainder of @dest will be padded with %NUL.
 *
 */
char *strncpy(char *dest, const char *src, size_t count)
{
ffffffff812c7c0c:	55                   	push   %rbp
ffffffff812c7c0d:	48 89 f8             	mov    %rdi,%rax
	char *tmp = dest;

	while (count) {
ffffffff812c7c10:	31 c9                	xor    %ecx,%ecx
 * In the case where the length of @src is less than  that  of
 * count, the remainder of @dest will be padded with %NUL.
 *
 */
char *strncpy(char *dest, const char *src, size_t count)
{
ffffffff812c7c12:	48 89 e5             	mov    %rsp,%rbp
	char *tmp = dest;

	while (count) {
ffffffff812c7c15:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812c7c18:	74 14                	je     ffffffff812c7c2e <strncpy+0x22>
		if ((*tmp = *src) != 0)
ffffffff812c7c1a:	40 8a 3e             	mov    (%rsi),%dil
			src++;
ffffffff812c7c1d:	40 80 ff 01          	cmp    $0x1,%dil
char *strncpy(char *dest, const char *src, size_t count)
{
	char *tmp = dest;

	while (count) {
		if ((*tmp = *src) != 0)
ffffffff812c7c21:	40 88 3c 08          	mov    %dil,(%rax,%rcx,1)
			src++;
ffffffff812c7c25:	48 83 de ff          	sbb    $0xffffffffffffffff,%rsi
ffffffff812c7c29:	48 ff c1             	inc    %rcx
ffffffff812c7c2c:	eb e7                	jmp    ffffffff812c7c15 <strncpy+0x9>
		tmp++;
		count--;
	}
	return dest;
}
ffffffff812c7c2e:	5d                   	pop    %rbp
ffffffff812c7c2f:	c3                   	retq   

ffffffff812c7c30 <strcat>:
 * @dest: The string to be appended to
 * @src: The string to append to it
 */
#undef strcat
char *strcat(char *dest, const char *src)
{
ffffffff812c7c30:	55                   	push   %rbp
ffffffff812c7c31:	48 89 f8             	mov    %rdi,%rax
	char *tmp = dest;

	while (*dest)
ffffffff812c7c34:	48 89 f9             	mov    %rdi,%rcx
 * @dest: The string to be appended to
 * @src: The string to append to it
 */
#undef strcat
char *strcat(char *dest, const char *src)
{
ffffffff812c7c37:	48 89 e5             	mov    %rsp,%rbp
	char *tmp = dest;

	while (*dest)
ffffffff812c7c3a:	80 39 00             	cmpb   $0x0,(%rcx)
ffffffff812c7c3d:	74 05                	je     ffffffff812c7c44 <strcat+0x14>
		dest++;
ffffffff812c7c3f:	48 ff c1             	inc    %rcx
ffffffff812c7c42:	eb f6                	jmp    ffffffff812c7c3a <strcat+0xa>
ffffffff812c7c44:	31 d2                	xor    %edx,%edx
	while ((*dest++ = *src++) != '\0')
ffffffff812c7c46:	40 8a 3c 16          	mov    (%rsi,%rdx,1),%dil
ffffffff812c7c4a:	40 88 3c 11          	mov    %dil,(%rcx,%rdx,1)
ffffffff812c7c4e:	48 ff c2             	inc    %rdx
ffffffff812c7c51:	40 84 ff             	test   %dil,%dil
ffffffff812c7c54:	75 f0                	jne    ffffffff812c7c46 <strcat+0x16>
		;
	return tmp;
}
ffffffff812c7c56:	5d                   	pop    %rbp
ffffffff812c7c57:	c3                   	retq   

ffffffff812c7c58 <strcmp>:
 * @cs: One string
 * @ct: Another string
 */
#undef strcmp
int strcmp(const char *cs, const char *ct)
{
ffffffff812c7c58:	55                   	push   %rbp
ffffffff812c7c59:	31 c0                	xor    %eax,%eax
ffffffff812c7c5b:	48 89 e5             	mov    %rsp,%rbp
	unsigned char c1, c2;

	while (1) {
		c1 = *cs++;
ffffffff812c7c5e:	8a 14 07             	mov    (%rdi,%rax,1),%dl
		c2 = *ct++;
		if (c1 != c2)
ffffffff812c7c61:	3a 14 06             	cmp    (%rsi,%rax,1),%dl
ffffffff812c7c64:	74 07                	je     ffffffff812c7c6d <strcmp+0x15>
			return c1 < c2 ? -1 : 1;
ffffffff812c7c66:	19 c0                	sbb    %eax,%eax
ffffffff812c7c68:	83 c8 01             	or     $0x1,%eax
ffffffff812c7c6b:	eb 09                	jmp    ffffffff812c7c76 <strcmp+0x1e>
ffffffff812c7c6d:	48 ff c0             	inc    %rax
		if (!c1)
ffffffff812c7c70:	84 d2                	test   %dl,%dl
ffffffff812c7c72:	75 ea                	jne    ffffffff812c7c5e <strcmp+0x6>
			break;
	}
	return 0;
ffffffff812c7c74:	31 c0                	xor    %eax,%eax
}
ffffffff812c7c76:	5d                   	pop    %rbp
ffffffff812c7c77:	c3                   	retq   

ffffffff812c7c78 <strncmp>:
 * @cs: One string
 * @ct: Another string
 * @count: The maximum number of bytes to compare
 */
int strncmp(const char *cs, const char *ct, size_t count)
{
ffffffff812c7c78:	55                   	push   %rbp
	unsigned char c1, c2;

	while (count) {
ffffffff812c7c79:	31 c0                	xor    %eax,%eax
 * @cs: One string
 * @ct: Another string
 * @count: The maximum number of bytes to compare
 */
int strncmp(const char *cs, const char *ct, size_t count)
{
ffffffff812c7c7b:	48 89 e5             	mov    %rsp,%rbp
	unsigned char c1, c2;

	while (count) {
ffffffff812c7c7e:	48 39 c2             	cmp    %rax,%rdx
ffffffff812c7c81:	74 16                	je     ffffffff812c7c99 <strncmp+0x21>
		c1 = *cs++;
ffffffff812c7c83:	8a 0c 07             	mov    (%rdi,%rax,1),%cl
		c2 = *ct++;
		if (c1 != c2)
ffffffff812c7c86:	3a 0c 06             	cmp    (%rsi,%rax,1),%cl
ffffffff812c7c89:	74 07                	je     ffffffff812c7c92 <strncmp+0x1a>
			return c1 < c2 ? -1 : 1;
ffffffff812c7c8b:	19 c0                	sbb    %eax,%eax
ffffffff812c7c8d:	83 c8 01             	or     $0x1,%eax
ffffffff812c7c90:	eb 09                	jmp    ffffffff812c7c9b <strncmp+0x23>
ffffffff812c7c92:	48 ff c0             	inc    %rax
		if (!c1)
ffffffff812c7c95:	84 c9                	test   %cl,%cl
ffffffff812c7c97:	75 e5                	jne    ffffffff812c7c7e <strncmp+0x6>
			break;
		count--;
	}
	return 0;
ffffffff812c7c99:	31 c0                	xor    %eax,%eax
}
ffffffff812c7c9b:	5d                   	pop    %rbp
ffffffff812c7c9c:	c3                   	retq   

ffffffff812c7c9d <strchr>:
 * strchr - Find the first occurrence of a character in a string
 * @s: The string to be searched
 * @c: The character to search for
 */
char *strchr(const char *s, int c)
{
ffffffff812c7c9d:	55                   	push   %rbp
ffffffff812c7c9e:	48 89 e5             	mov    %rsp,%rbp
	for (; *s != (char)c; ++s)
ffffffff812c7ca1:	8a 07                	mov    (%rdi),%al
ffffffff812c7ca3:	40 38 f0             	cmp    %sil,%al
ffffffff812c7ca6:	74 09                	je     ffffffff812c7cb1 <strchr+0x14>
		if (*s == '\0')
ffffffff812c7ca8:	84 c0                	test   %al,%al
ffffffff812c7caa:	74 0a                	je     ffffffff812c7cb6 <strchr+0x19>
 * @s: The string to be searched
 * @c: The character to search for
 */
char *strchr(const char *s, int c)
{
	for (; *s != (char)c; ++s)
ffffffff812c7cac:	48 ff c7             	inc    %rdi
ffffffff812c7caf:	eb f0                	jmp    ffffffff812c7ca1 <strchr+0x4>
ffffffff812c7cb1:	48 89 f8             	mov    %rdi,%rax
ffffffff812c7cb4:	eb 02                	jmp    ffffffff812c7cb8 <strchr+0x1b>
		if (*s == '\0')
			return NULL;
ffffffff812c7cb6:	31 c0                	xor    %eax,%eax
	return (char *)s;
}
ffffffff812c7cb8:	5d                   	pop    %rbp
ffffffff812c7cb9:	c3                   	retq   

ffffffff812c7cba <strchrnul>:
 *
 * Returns pointer to first occurrence of 'c' in s. If c is not found, then
 * return a pointer to the null byte at the end of s.
 */
char *strchrnul(const char *s, int c)
{
ffffffff812c7cba:	55                   	push   %rbp
ffffffff812c7cbb:	48 89 f8             	mov    %rdi,%rax
ffffffff812c7cbe:	48 89 e5             	mov    %rsp,%rbp
	while (*s && *s != (char)c)
ffffffff812c7cc1:	8a 10                	mov    (%rax),%dl
ffffffff812c7cc3:	84 d2                	test   %dl,%dl
ffffffff812c7cc5:	74 0a                	je     ffffffff812c7cd1 <strchrnul+0x17>
ffffffff812c7cc7:	40 38 f2             	cmp    %sil,%dl
ffffffff812c7cca:	74 05                	je     ffffffff812c7cd1 <strchrnul+0x17>
		s++;
ffffffff812c7ccc:	48 ff c0             	inc    %rax
ffffffff812c7ccf:	eb f0                	jmp    ffffffff812c7cc1 <strchrnul+0x7>
	return (char *)s;
}
ffffffff812c7cd1:	5d                   	pop    %rbp
ffffffff812c7cd2:	c3                   	retq   

ffffffff812c7cd3 <strrchr>:
 * strrchr - Find the last occurrence of a character in a string
 * @s: The string to be searched
 * @c: The character to search for
 */
char *strrchr(const char *s, int c)
{
ffffffff812c7cd3:	55                   	push   %rbp
	const char *last = NULL;
ffffffff812c7cd4:	31 c0                	xor    %eax,%eax
 * strrchr - Find the last occurrence of a character in a string
 * @s: The string to be searched
 * @c: The character to search for
 */
char *strrchr(const char *s, int c)
{
ffffffff812c7cd6:	48 89 e5             	mov    %rsp,%rbp
	const char *last = NULL;
	do {
		if (*s == (char)c)
ffffffff812c7cd9:	8a 17                	mov    (%rdi),%dl
			last = s;
ffffffff812c7cdb:	40 38 f2             	cmp    %sil,%dl
ffffffff812c7cde:	48 0f 44 c7          	cmove  %rdi,%rax
	} while (*s++);
ffffffff812c7ce2:	48 ff c7             	inc    %rdi
ffffffff812c7ce5:	84 d2                	test   %dl,%dl
ffffffff812c7ce7:	75 f0                	jne    ffffffff812c7cd9 <strrchr+0x6>
	return (char *)last;
}
ffffffff812c7ce9:	5d                   	pop    %rbp
ffffffff812c7cea:	c3                   	retq   

ffffffff812c7ceb <strnchr>:
 * @s: The string to be searched
 * @count: The number of characters to be searched
 * @c: The character to search for
 */
char *strnchr(const char *s, size_t count, int c)
{
ffffffff812c7ceb:	55                   	push   %rbp
ffffffff812c7cec:	48 01 fe             	add    %rdi,%rsi
ffffffff812c7cef:	48 89 e5             	mov    %rsp,%rbp
	for (; count-- && *s != '\0'; ++s)
ffffffff812c7cf2:	48 39 f7             	cmp    %rsi,%rdi
ffffffff812c7cf5:	74 14                	je     ffffffff812c7d0b <strnchr+0x20>
ffffffff812c7cf7:	8a 07                	mov    (%rdi),%al
ffffffff812c7cf9:	84 c0                	test   %al,%al
ffffffff812c7cfb:	74 0e                	je     ffffffff812c7d0b <strnchr+0x20>
		if (*s == (char)c)
ffffffff812c7cfd:	38 d0                	cmp    %dl,%al
ffffffff812c7cff:	74 05                	je     ffffffff812c7d06 <strnchr+0x1b>
 * @count: The number of characters to be searched
 * @c: The character to search for
 */
char *strnchr(const char *s, size_t count, int c)
{
	for (; count-- && *s != '\0'; ++s)
ffffffff812c7d01:	48 ff c7             	inc    %rdi
ffffffff812c7d04:	eb ec                	jmp    ffffffff812c7cf2 <strnchr+0x7>
ffffffff812c7d06:	48 89 f8             	mov    %rdi,%rax
ffffffff812c7d09:	eb 02                	jmp    ffffffff812c7d0d <strnchr+0x22>
		if (*s == (char)c)
			return (char *)s;
	return NULL;
ffffffff812c7d0b:	31 c0                	xor    %eax,%eax
}
ffffffff812c7d0d:	5d                   	pop    %rbp
ffffffff812c7d0e:	c3                   	retq   

ffffffff812c7d0f <skip_spaces>:
 * @str: The string to be stripped.
 *
 * Returns a pointer to the first non-whitespace character in @str.
 */
char *skip_spaces(const char *str)
{
ffffffff812c7d0f:	55                   	push   %rbp
ffffffff812c7d10:	48 89 f8             	mov    %rdi,%rax
ffffffff812c7d13:	48 89 e5             	mov    %rsp,%rbp
	while (isspace(*str))
ffffffff812c7d16:	0f b6 10             	movzbl (%rax),%edx
ffffffff812c7d19:	f6 82 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rdx)
ffffffff812c7d20:	74 05                	je     ffffffff812c7d27 <skip_spaces+0x18>
		++str;
ffffffff812c7d22:	48 ff c0             	inc    %rax
ffffffff812c7d25:	eb ef                	jmp    ffffffff812c7d16 <skip_spaces+0x7>
	return (char *)str;
}
ffffffff812c7d27:	5d                   	pop    %rbp
ffffffff812c7d28:	c3                   	retq   

ffffffff812c7d29 <strlen>:
/**
 * strlen - Find the length of a string
 * @s: The string to be sized
 */
size_t strlen(const char *s)
{
ffffffff812c7d29:	55                   	push   %rbp
	const char *sc;

	for (sc = s; *sc != '\0'; ++sc)
ffffffff812c7d2a:	48 89 f8             	mov    %rdi,%rax
/**
 * strlen - Find the length of a string
 * @s: The string to be sized
 */
size_t strlen(const char *s)
{
ffffffff812c7d2d:	48 89 e5             	mov    %rsp,%rbp
	const char *sc;

	for (sc = s; *sc != '\0'; ++sc)
ffffffff812c7d30:	80 38 00             	cmpb   $0x0,(%rax)
ffffffff812c7d33:	74 05                	je     ffffffff812c7d3a <strlen+0x11>
ffffffff812c7d35:	48 ff c0             	inc    %rax
ffffffff812c7d38:	eb f6                	jmp    ffffffff812c7d30 <strlen+0x7>
		/* nothing */;
	return sc - s;
ffffffff812c7d3a:	48 29 f8             	sub    %rdi,%rax
}
ffffffff812c7d3d:	5d                   	pop    %rbp
ffffffff812c7d3e:	c3                   	retq   

ffffffff812c7d3f <strim>:
 * Note that the first trailing whitespace is replaced with a %NUL-terminator
 * in the given string @s. Returns a pointer to the first non-whitespace
 * character in @s.
 */
char *strim(char *s)
{
ffffffff812c7d3f:	55                   	push   %rbp
	size_t size;
	char *end;

	size = strlen(s);
ffffffff812c7d40:	31 c0                	xor    %eax,%eax
ffffffff812c7d42:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
 * Note that the first trailing whitespace is replaced with a %NUL-terminator
 * in the given string @s. Returns a pointer to the first non-whitespace
 * character in @s.
 */
char *strim(char *s)
{
ffffffff812c7d46:	48 89 fa             	mov    %rdi,%rdx
	size_t size;
	char *end;

	size = strlen(s);
ffffffff812c7d49:	f2 ae                	repnz scas %es:(%rdi),%al
 * Note that the first trailing whitespace is replaced with a %NUL-terminator
 * in the given string @s. Returns a pointer to the first non-whitespace
 * character in @s.
 */
char *strim(char *s)
{
ffffffff812c7d4b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c7d4e:	48 89 d0             	mov    %rdx,%rax
	size_t size;
	char *end;

	size = strlen(s);
ffffffff812c7d51:	48 f7 d1             	not    %rcx
	if (!size)
ffffffff812c7d54:	48 83 f9 01          	cmp    $0x1,%rcx
ffffffff812c7d58:	74 29                	je     ffffffff812c7d83 <strim+0x44>
		return s;

	end = s + size - 1;
ffffffff812c7d5a:	48 8d 44 0a fe       	lea    -0x2(%rdx,%rcx,1),%rax
	while (end >= s && isspace(*end))
ffffffff812c7d5f:	48 39 d0             	cmp    %rdx,%rax
ffffffff812c7d62:	73 0e                	jae    ffffffff812c7d72 <strim+0x33>
ffffffff812c7d64:	48 89 d7             	mov    %rdx,%rdi
		end--;
	*(end + 1) = '\0';
ffffffff812c7d67:	c6 40 01 00          	movb   $0x0,0x1(%rax)

	return skip_spaces(s);
ffffffff812c7d6b:	e8 9f ff ff ff       	callq  ffffffff812c7d0f <skip_spaces>
ffffffff812c7d70:	eb 11                	jmp    ffffffff812c7d83 <strim+0x44>
	size = strlen(s);
	if (!size)
		return s;

	end = s + size - 1;
	while (end >= s && isspace(*end))
ffffffff812c7d72:	0f b6 08             	movzbl (%rax),%ecx
ffffffff812c7d75:	f6 81 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rcx)
ffffffff812c7d7c:	74 e6                	je     ffffffff812c7d64 <strim+0x25>
		end--;
ffffffff812c7d7e:	48 ff c8             	dec    %rax
ffffffff812c7d81:	eb dc                	jmp    ffffffff812c7d5f <strim+0x20>
	*(end + 1) = '\0';

	return skip_spaces(s);
}
ffffffff812c7d83:	5d                   	pop    %rbp
ffffffff812c7d84:	c3                   	retq   

ffffffff812c7d85 <strnlen>:
 * strnlen - Find the length of a length-limited string
 * @s: The string to be sized
 * @count: The maximum number of bytes to search
 */
size_t strnlen(const char *s, size_t count)
{
ffffffff812c7d85:	55                   	push   %rbp
ffffffff812c7d86:	48 01 fe             	add    %rdi,%rsi
	const char *sc;

	for (sc = s; count-- && *sc != '\0'; ++sc)
ffffffff812c7d89:	48 89 f8             	mov    %rdi,%rax
 * strnlen - Find the length of a length-limited string
 * @s: The string to be sized
 * @count: The maximum number of bytes to search
 */
size_t strnlen(const char *s, size_t count)
{
ffffffff812c7d8c:	48 89 e5             	mov    %rsp,%rbp
	const char *sc;

	for (sc = s; count-- && *sc != '\0'; ++sc)
ffffffff812c7d8f:	48 39 f0             	cmp    %rsi,%rax
ffffffff812c7d92:	74 0a                	je     ffffffff812c7d9e <strnlen+0x19>
ffffffff812c7d94:	80 38 00             	cmpb   $0x0,(%rax)
ffffffff812c7d97:	74 05                	je     ffffffff812c7d9e <strnlen+0x19>
ffffffff812c7d99:	48 ff c0             	inc    %rax
ffffffff812c7d9c:	eb f1                	jmp    ffffffff812c7d8f <strnlen+0xa>
		/* nothing */;
	return sc - s;
ffffffff812c7d9e:	48 29 f8             	sub    %rdi,%rax
}
ffffffff812c7da1:	5d                   	pop    %rbp
ffffffff812c7da2:	c3                   	retq   

ffffffff812c7da3 <strspn>:
 * strspn - Calculate the length of the initial substring of @s which only contain letters in @accept
 * @s: The string to be searched
 * @accept: The string to search for
 */
size_t strspn(const char *s, const char *accept)
{
ffffffff812c7da3:	55                   	push   %rbp
	const char *p;
	const char *a;
	size_t count = 0;
ffffffff812c7da4:	31 c0                	xor    %eax,%eax
 * strspn - Calculate the length of the initial substring of @s which only contain letters in @accept
 * @s: The string to be searched
 * @accept: The string to search for
 */
size_t strspn(const char *s, const char *accept)
{
ffffffff812c7da6:	48 89 e5             	mov    %rsp,%rbp
	const char *p;
	const char *a;
	size_t count = 0;

	for (p = s; *p != '\0'; ++p) {
ffffffff812c7da9:	8a 0c 07             	mov    (%rdi,%rax,1),%cl
ffffffff812c7dac:	84 c9                	test   %cl,%cl
ffffffff812c7dae:	74 1a                	je     ffffffff812c7dca <strspn+0x27>
ffffffff812c7db0:	48 89 f2             	mov    %rsi,%rdx
		for (a = accept; *a != '\0'; ++a) {
ffffffff812c7db3:	44 8a 02             	mov    (%rdx),%r8b
ffffffff812c7db6:	45 84 c0             	test   %r8b,%r8b
ffffffff812c7db9:	74 0f                	je     ffffffff812c7dca <strspn+0x27>
			if (*p == *a)
ffffffff812c7dbb:	44 38 c1             	cmp    %r8b,%cl
ffffffff812c7dbe:	74 05                	je     ffffffff812c7dc5 <strspn+0x22>
	const char *p;
	const char *a;
	size_t count = 0;

	for (p = s; *p != '\0'; ++p) {
		for (a = accept; *a != '\0'; ++a) {
ffffffff812c7dc0:	48 ff c2             	inc    %rdx
ffffffff812c7dc3:	eb ee                	jmp    ffffffff812c7db3 <strspn+0x10>
			if (*p == *a)
				break;
		}
		if (*a == '\0')
			return count;
		++count;
ffffffff812c7dc5:	48 ff c0             	inc    %rax
ffffffff812c7dc8:	eb df                	jmp    ffffffff812c7da9 <strspn+0x6>
	}
	return count;
}
ffffffff812c7dca:	5d                   	pop    %rbp
ffffffff812c7dcb:	c3                   	retq   

ffffffff812c7dcc <strcspn>:
 * strcspn - Calculate the length of the initial substring of @s which does not contain letters in @reject
 * @s: The string to be searched
 * @reject: The string to avoid
 */
size_t strcspn(const char *s, const char *reject)
{
ffffffff812c7dcc:	55                   	push   %rbp
	const char *p;
	const char *r;
	size_t count = 0;
ffffffff812c7dcd:	31 c0                	xor    %eax,%eax
 * strcspn - Calculate the length of the initial substring of @s which does not contain letters in @reject
 * @s: The string to be searched
 * @reject: The string to avoid
 */
size_t strcspn(const char *s, const char *reject)
{
ffffffff812c7dcf:	48 89 e5             	mov    %rsp,%rbp
	const char *p;
	const char *r;
	size_t count = 0;

	for (p = s; *p != '\0'; ++p) {
ffffffff812c7dd2:	8a 0c 07             	mov    (%rdi,%rax,1),%cl
ffffffff812c7dd5:	84 c9                	test   %cl,%cl
ffffffff812c7dd7:	74 1a                	je     ffffffff812c7df3 <strcspn+0x27>
ffffffff812c7dd9:	48 89 f2             	mov    %rsi,%rdx
		for (r = reject; *r != '\0'; ++r) {
ffffffff812c7ddc:	44 8a 02             	mov    (%rdx),%r8b
ffffffff812c7ddf:	45 84 c0             	test   %r8b,%r8b
ffffffff812c7de2:	74 0a                	je     ffffffff812c7dee <strcspn+0x22>
			if (*p == *r)
ffffffff812c7de4:	44 38 c1             	cmp    %r8b,%cl
ffffffff812c7de7:	74 0a                	je     ffffffff812c7df3 <strcspn+0x27>
	const char *p;
	const char *r;
	size_t count = 0;

	for (p = s; *p != '\0'; ++p) {
		for (r = reject; *r != '\0'; ++r) {
ffffffff812c7de9:	48 ff c2             	inc    %rdx
ffffffff812c7dec:	eb ee                	jmp    ffffffff812c7ddc <strcspn+0x10>
			if (*p == *r)
				return count;
		}
		++count;
ffffffff812c7dee:	48 ff c0             	inc    %rax
ffffffff812c7df1:	eb df                	jmp    ffffffff812c7dd2 <strcspn+0x6>
	}
	return count;
}
ffffffff812c7df3:	5d                   	pop    %rbp
ffffffff812c7df4:	c3                   	retq   

ffffffff812c7df5 <strpbrk>:
 * strpbrk - Find the first occurrence of a set of characters
 * @cs: The string to be searched
 * @ct: The characters to search for
 */
char *strpbrk(const char *cs, const char *ct)
{
ffffffff812c7df5:	55                   	push   %rbp
ffffffff812c7df6:	48 89 e5             	mov    %rsp,%rbp
	const char *sc1, *sc2;

	for (sc1 = cs; *sc1 != '\0'; ++sc1) {
ffffffff812c7df9:	8a 17                	mov    (%rdi),%dl
ffffffff812c7dfb:	84 d2                	test   %dl,%dl
ffffffff812c7dfd:	74 1c                	je     ffffffff812c7e1b <strpbrk+0x26>
ffffffff812c7dff:	48 89 f0             	mov    %rsi,%rax
		for (sc2 = ct; *sc2 != '\0'; ++sc2) {
ffffffff812c7e02:	8a 08                	mov    (%rax),%cl
ffffffff812c7e04:	84 c9                	test   %cl,%cl
ffffffff812c7e06:	74 09                	je     ffffffff812c7e11 <strpbrk+0x1c>
			if (*sc1 == *sc2)
ffffffff812c7e08:	38 ca                	cmp    %cl,%dl
ffffffff812c7e0a:	74 0a                	je     ffffffff812c7e16 <strpbrk+0x21>
char *strpbrk(const char *cs, const char *ct)
{
	const char *sc1, *sc2;

	for (sc1 = cs; *sc1 != '\0'; ++sc1) {
		for (sc2 = ct; *sc2 != '\0'; ++sc2) {
ffffffff812c7e0c:	48 ff c0             	inc    %rax
ffffffff812c7e0f:	eb f1                	jmp    ffffffff812c7e02 <strpbrk+0xd>
 */
char *strpbrk(const char *cs, const char *ct)
{
	const char *sc1, *sc2;

	for (sc1 = cs; *sc1 != '\0'; ++sc1) {
ffffffff812c7e11:	48 ff c7             	inc    %rdi
ffffffff812c7e14:	eb e3                	jmp    ffffffff812c7df9 <strpbrk+0x4>
ffffffff812c7e16:	48 89 f8             	mov    %rdi,%rax
ffffffff812c7e19:	eb 02                	jmp    ffffffff812c7e1d <strpbrk+0x28>
		for (sc2 = ct; *sc2 != '\0'; ++sc2) {
			if (*sc1 == *sc2)
				return (char *)sc1;
		}
	}
	return NULL;
ffffffff812c7e1b:	31 c0                	xor    %eax,%eax
}
ffffffff812c7e1d:	5d                   	pop    %rbp
ffffffff812c7e1e:	c3                   	retq   

ffffffff812c7e1f <strsep>:
 * of that name. In fact, it was stolen from glibc2 and de-fancy-fied.
 * Same semantics, slimmer shape. ;)
 */
char *strsep(char **s, const char *ct)
{
	char *sbegin = *s;
ffffffff812c7e1f:	4c 8b 0f             	mov    (%rdi),%r9
	char *end;

	if (sbegin == NULL)
ffffffff812c7e22:	4d 85 c9             	test   %r9,%r9
ffffffff812c7e25:	74 22                	je     ffffffff812c7e49 <strsep+0x2a>
 * It returns empty tokens, too, behaving exactly like the libc function
 * of that name. In fact, it was stolen from glibc2 and de-fancy-fied.
 * Same semantics, slimmer shape. ;)
 */
char *strsep(char **s, const char *ct)
{
ffffffff812c7e27:	55                   	push   %rbp
ffffffff812c7e28:	49 89 f8             	mov    %rdi,%r8
	char *end;

	if (sbegin == NULL)
		return NULL;

	end = strpbrk(sbegin, ct);
ffffffff812c7e2b:	4c 89 cf             	mov    %r9,%rdi
 * It returns empty tokens, too, behaving exactly like the libc function
 * of that name. In fact, it was stolen from glibc2 and de-fancy-fied.
 * Same semantics, slimmer shape. ;)
 */
char *strsep(char **s, const char *ct)
{
ffffffff812c7e2e:	48 89 e5             	mov    %rsp,%rbp
	char *end;

	if (sbegin == NULL)
		return NULL;

	end = strpbrk(sbegin, ct);
ffffffff812c7e31:	e8 bf ff ff ff       	callq  ffffffff812c7df5 <strpbrk>
	if (end)
ffffffff812c7e36:	48 85 c0             	test   %rax,%rax
ffffffff812c7e39:	74 06                	je     ffffffff812c7e41 <strsep+0x22>
		*end++ = '\0';
ffffffff812c7e3b:	c6 00 00             	movb   $0x0,(%rax)
ffffffff812c7e3e:	48 ff c0             	inc    %rax
	*s = end;
ffffffff812c7e41:	49 89 00             	mov    %rax,(%r8)
	return sbegin;
ffffffff812c7e44:	4c 89 c8             	mov    %r9,%rax
}
ffffffff812c7e47:	5d                   	pop    %rbp
ffffffff812c7e48:	c3                   	retq   
ffffffff812c7e49:	31 c0                	xor    %eax,%eax
ffffffff812c7e4b:	c3                   	retq   

ffffffff812c7e4c <sysfs_streq>:
 * NUL and newline-then-NUL as equivalent string terminations.  It's
 * geared for use with sysfs input strings, which generally terminate
 * with newlines but are compared against values without newlines.
 */
bool sysfs_streq(const char *s1, const char *s2)
{
ffffffff812c7e4c:	55                   	push   %rbp
ffffffff812c7e4d:	48 89 e5             	mov    %rsp,%rbp
	while (*s1 && *s1 == *s2) {
ffffffff812c7e50:	8a 17                	mov    (%rdi),%dl
ffffffff812c7e52:	84 d2                	test   %dl,%dl
ffffffff812c7e54:	74 0c                	je     ffffffff812c7e62 <sysfs_streq+0x16>
ffffffff812c7e56:	3a 16                	cmp    (%rsi),%dl
ffffffff812c7e58:	75 08                	jne    ffffffff812c7e62 <sysfs_streq+0x16>
		s1++;
ffffffff812c7e5a:	48 ff c7             	inc    %rdi
		s2++;
ffffffff812c7e5d:	48 ff c6             	inc    %rsi
ffffffff812c7e60:	eb ee                	jmp    ffffffff812c7e50 <sysfs_streq+0x4>
	}

	if (*s1 == *s2)
ffffffff812c7e62:	8a 0e                	mov    (%rsi),%cl
		return true;
ffffffff812c7e64:	b0 01                	mov    $0x1,%al
	while (*s1 && *s1 == *s2) {
		s1++;
		s2++;
	}

	if (*s1 == *s2)
ffffffff812c7e66:	38 d1                	cmp    %dl,%cl
ffffffff812c7e68:	74 1c                	je     ffffffff812c7e86 <sysfs_streq+0x3a>
		return true;
	if (!*s1 && *s2 == '\n' && !s2[1])
		return true;
	if (*s1 == '\n' && !s1[1] && !*s2)
		return true;
	return false;
ffffffff812c7e6a:	31 c0                	xor    %eax,%eax
		s2++;
	}

	if (*s1 == *s2)
		return true;
	if (!*s1 && *s2 == '\n' && !s2[1])
ffffffff812c7e6c:	84 d2                	test   %dl,%dl
ffffffff812c7e6e:	75 0b                	jne    ffffffff812c7e7b <sysfs_streq+0x2f>
ffffffff812c7e70:	80 f9 0a             	cmp    $0xa,%cl
ffffffff812c7e73:	75 11                	jne    ffffffff812c7e86 <sysfs_streq+0x3a>
ffffffff812c7e75:	80 7e 01 00          	cmpb   $0x0,0x1(%rsi)
ffffffff812c7e79:	eb 08                	jmp    ffffffff812c7e83 <sysfs_streq+0x37>
		return true;
	if (*s1 == '\n' && !s1[1] && !*s2)
ffffffff812c7e7b:	80 fa 0a             	cmp    $0xa,%dl
ffffffff812c7e7e:	75 06                	jne    ffffffff812c7e86 <sysfs_streq+0x3a>
ffffffff812c7e80:	0a 4f 01             	or     0x1(%rdi),%cl
ffffffff812c7e83:	0f 94 c0             	sete   %al
		return true;
	return false;
}
ffffffff812c7e86:	5d                   	pop    %rbp
ffffffff812c7e87:	c3                   	retq   

ffffffff812c7e88 <strtobool>:
 * Otherwise it will return -EINVAL.  Value pointed to by res is
 * updated upon finding a match.
 */
int strtobool(const char *s, bool *res)
{
	switch (s[0]) {
ffffffff812c7e88:	8a 07                	mov    (%rdi),%al
 * This routine returns 0 iff the first character is one of 'Yy1Nn0'.
 * Otherwise it will return -EINVAL.  Value pointed to by res is
 * updated upon finding a match.
 */
int strtobool(const char *s, bool *res)
{
ffffffff812c7e8a:	55                   	push   %rbp
ffffffff812c7e8b:	48 89 e5             	mov    %rsp,%rbp
	switch (s[0]) {
ffffffff812c7e8e:	3c 4e                	cmp    $0x4e,%al
ffffffff812c7e90:	74 20                	je     ffffffff812c7eb2 <strtobool+0x2a>
ffffffff812c7e92:	7f 08                	jg     ffffffff812c7e9c <strtobool+0x14>
ffffffff812c7e94:	3c 30                	cmp    $0x30,%al
ffffffff812c7e96:	74 1a                	je     ffffffff812c7eb2 <strtobool+0x2a>
ffffffff812c7e98:	3c 31                	cmp    $0x31,%al
ffffffff812c7e9a:	eb 0a                	jmp    ffffffff812c7ea6 <strtobool+0x1e>
ffffffff812c7e9c:	3c 6e                	cmp    $0x6e,%al
ffffffff812c7e9e:	74 12                	je     ffffffff812c7eb2 <strtobool+0x2a>
ffffffff812c7ea0:	3c 79                	cmp    $0x79,%al
ffffffff812c7ea2:	74 09                	je     ffffffff812c7ead <strtobool+0x25>
ffffffff812c7ea4:	3c 59                	cmp    $0x59,%al
	case 'N':
	case '0':
		*res = false;
		break;
	default:
		return -EINVAL;
ffffffff812c7ea6:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
 * Otherwise it will return -EINVAL.  Value pointed to by res is
 * updated upon finding a match.
 */
int strtobool(const char *s, bool *res)
{
	switch (s[0]) {
ffffffff812c7eab:	75 0a                	jne    ffffffff812c7eb7 <strtobool+0x2f>
	case 'y':
	case 'Y':
	case '1':
		*res = true;
ffffffff812c7ead:	c6 06 01             	movb   $0x1,(%rsi)
ffffffff812c7eb0:	eb 03                	jmp    ffffffff812c7eb5 <strtobool+0x2d>
		break;
	case 'n':
	case 'N':
	case '0':
		*res = false;
ffffffff812c7eb2:	c6 06 00             	movb   $0x0,(%rsi)
		break;
	default:
		return -EINVAL;
	}
	return 0;
ffffffff812c7eb5:	31 c0                	xor    %eax,%eax
}
ffffffff812c7eb7:	5d                   	pop    %rbp
ffffffff812c7eb8:	c3                   	retq   

ffffffff812c7eb9 <memcmp>:
 * @ct: Another area of memory
 * @count: The size of the area.
 */
#undef memcmp
__visible int memcmp(const void *cs, const void *ct, size_t count)
{
ffffffff812c7eb9:	55                   	push   %rbp
	const unsigned char *su1, *su2;
	int res = 0;

	for (su1 = cs, su2 = ct; 0 < count; ++su1, ++su2, count--)
ffffffff812c7eba:	31 c9                	xor    %ecx,%ecx
 * @ct: Another area of memory
 * @count: The size of the area.
 */
#undef memcmp
__visible int memcmp(const void *cs, const void *ct, size_t count)
{
ffffffff812c7ebc:	48 89 e5             	mov    %rsp,%rbp
	const unsigned char *su1, *su2;
	int res = 0;

	for (su1 = cs, su2 = ct; 0 < count; ++su1, ++su2, count--)
ffffffff812c7ebf:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812c7ec2:	74 13                	je     ffffffff812c7ed7 <memcmp+0x1e>
		if ((res = *su1 - *su2) != 0)
ffffffff812c7ec4:	0f b6 04 0f          	movzbl (%rdi,%rcx,1),%eax
ffffffff812c7ec8:	44 0f b6 04 0e       	movzbl (%rsi,%rcx,1),%r8d
ffffffff812c7ecd:	48 ff c1             	inc    %rcx
ffffffff812c7ed0:	44 29 c0             	sub    %r8d,%eax
ffffffff812c7ed3:	74 ea                	je     ffffffff812c7ebf <memcmp+0x6>
ffffffff812c7ed5:	eb 02                	jmp    ffffffff812c7ed9 <memcmp+0x20>
ffffffff812c7ed7:	31 c0                	xor    %eax,%eax
			break;
	return res;
}
ffffffff812c7ed9:	5d                   	pop    %rbp
ffffffff812c7eda:	c3                   	retq   

ffffffff812c7edb <memscan>:
 *
 * returns the address of the first occurrence of @c, or 1 byte past
 * the area if @c is not found
 */
void *memscan(void *addr, int c, size_t size)
{
ffffffff812c7edb:	55                   	push   %rbp
ffffffff812c7edc:	48 89 f8             	mov    %rdi,%rax
ffffffff812c7edf:	48 01 fa             	add    %rdi,%rdx
ffffffff812c7ee2:	48 89 e5             	mov    %rsp,%rbp
	unsigned char *p = addr;

	while (size) {
ffffffff812c7ee5:	48 39 d0             	cmp    %rdx,%rax
ffffffff812c7ee8:	74 0c                	je     ffffffff812c7ef6 <memscan+0x1b>
		if (*p == c)
ffffffff812c7eea:	0f b6 08             	movzbl (%rax),%ecx
ffffffff812c7eed:	39 f1                	cmp    %esi,%ecx
ffffffff812c7eef:	74 05                	je     ffffffff812c7ef6 <memscan+0x1b>
			return (void *)p;
		p++;
ffffffff812c7ef1:	48 ff c0             	inc    %rax
ffffffff812c7ef4:	eb ef                	jmp    ffffffff812c7ee5 <memscan+0xa>
		size--;
	}
  	return (void *)p;
}
ffffffff812c7ef6:	5d                   	pop    %rbp
ffffffff812c7ef7:	c3                   	retq   

ffffffff812c7ef8 <strstr>:
 * strstr - Find the first substring in a %NUL terminated string
 * @s1: The string to be searched
 * @s2: The string to search for
 */
char *strstr(const char *s1, const char *s2)
{
ffffffff812c7ef8:	55                   	push   %rbp
	size_t l1, l2;

	l2 = strlen(s2);
ffffffff812c7ef9:	49 83 c9 ff          	or     $0xffffffffffffffff,%r9
ffffffff812c7efd:	45 31 c0             	xor    %r8d,%r8d
ffffffff812c7f00:	44 88 c0             	mov    %r8b,%al
ffffffff812c7f03:	4c 89 c9             	mov    %r9,%rcx
 * strstr - Find the first substring in a %NUL terminated string
 * @s1: The string to be searched
 * @s2: The string to search for
 */
char *strstr(const char *s1, const char *s2)
{
ffffffff812c7f06:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c7f09:	41 55                	push   %r13
ffffffff812c7f0b:	41 54                	push   %r12
ffffffff812c7f0d:	53                   	push   %rbx
ffffffff812c7f0e:	48 89 fb             	mov    %rdi,%rbx
	size_t l1, l2;

	l2 = strlen(s2);
ffffffff812c7f11:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c7f14:	f2 ae                	repnz scas %es:(%rdi),%al
	if (!l2)
		return (char *)s1;
ffffffff812c7f16:	48 89 d8             	mov    %rbx,%rax
 */
char *strstr(const char *s1, const char *s2)
{
	size_t l1, l2;

	l2 = strlen(s2);
ffffffff812c7f19:	48 f7 d1             	not    %rcx
	if (!l2)
ffffffff812c7f1c:	4c 01 c9             	add    %r9,%rcx
ffffffff812c7f1f:	74 45                	je     ffffffff812c7f66 <strstr+0x6e>
ffffffff812c7f21:	49 89 cb             	mov    %rcx,%r11
		return (char *)s1;
	l1 = strlen(s1);
ffffffff812c7f24:	48 89 df             	mov    %rbx,%rdi
ffffffff812c7f27:	4c 89 c9             	mov    %r9,%rcx
ffffffff812c7f2a:	44 88 c0             	mov    %r8b,%al
ffffffff812c7f2d:	49 89 f4             	mov    %rsi,%r12
ffffffff812c7f30:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c7f32:	48 f7 d1             	not    %rcx
ffffffff812c7f35:	49 01 c9             	add    %rcx,%r9
ffffffff812c7f38:	4d 89 cd             	mov    %r9,%r13
ffffffff812c7f3b:	4c 89 c8             	mov    %r9,%rax
ffffffff812c7f3e:	4c 29 e8             	sub    %r13,%rax
	while (l1 >= l2) {
ffffffff812c7f41:	4d 39 dd             	cmp    %r11,%r13
ffffffff812c7f44:	4c 8d 14 03          	lea    (%rbx,%rax,1),%r10
ffffffff812c7f48:	72 1a                	jb     ffffffff812c7f64 <strstr+0x6c>
		l1--;
		if (!memcmp(s1, s2, l2))
ffffffff812c7f4a:	4c 89 da             	mov    %r11,%rdx
ffffffff812c7f4d:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c7f50:	4c 89 d7             	mov    %r10,%rdi
	l2 = strlen(s2);
	if (!l2)
		return (char *)s1;
	l1 = strlen(s1);
	while (l1 >= l2) {
		l1--;
ffffffff812c7f53:	49 ff cd             	dec    %r13
		if (!memcmp(s1, s2, l2))
ffffffff812c7f56:	e8 5e ff ff ff       	callq  ffffffff812c7eb9 <memcmp>
ffffffff812c7f5b:	85 c0                	test   %eax,%eax
ffffffff812c7f5d:	75 dc                	jne    ffffffff812c7f3b <strstr+0x43>
ffffffff812c7f5f:	4c 89 d0             	mov    %r10,%rax
ffffffff812c7f62:	eb 02                	jmp    ffffffff812c7f66 <strstr+0x6e>
			return (char *)s1;
		s1++;
	}
	return NULL;
ffffffff812c7f64:	31 c0                	xor    %eax,%eax
}
ffffffff812c7f66:	5b                   	pop    %rbx
ffffffff812c7f67:	41 5c                	pop    %r12
ffffffff812c7f69:	41 5d                	pop    %r13
ffffffff812c7f6b:	5d                   	pop    %rbp
ffffffff812c7f6c:	c3                   	retq   

ffffffff812c7f6d <strnstr>:
 */
char *strnstr(const char *s1, const char *s2, size_t len)
{
	size_t l2;

	l2 = strlen(s2);
ffffffff812c7f6d:	31 c0                	xor    %eax,%eax
 * @s1: The string to be searched
 * @s2: The string to search for
 * @len: the maximum number of characters to search
 */
char *strnstr(const char *s1, const char *s2, size_t len)
{
ffffffff812c7f6f:	49 89 f8             	mov    %rdi,%r8
	size_t l2;

	l2 = strlen(s2);
ffffffff812c7f72:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812c7f76:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c7f79:	f2 ae                	repnz scas %es:(%rdi),%al
	if (!l2)
		return (char *)s1;
ffffffff812c7f7b:	4c 89 c0             	mov    %r8,%rax
 */
char *strnstr(const char *s1, const char *s2, size_t len)
{
	size_t l2;

	l2 = strlen(s2);
ffffffff812c7f7e:	48 f7 d1             	not    %rcx
	if (!l2)
ffffffff812c7f81:	48 ff c9             	dec    %rcx
ffffffff812c7f84:	74 49                	je     ffffffff812c7fcf <strnstr+0x62>
 * @s1: The string to be searched
 * @s2: The string to search for
 * @len: the maximum number of characters to search
 */
char *strnstr(const char *s1, const char *s2, size_t len)
{
ffffffff812c7f86:	55                   	push   %rbp
ffffffff812c7f87:	49 89 c9             	mov    %rcx,%r9
ffffffff812c7f8a:	49 89 d3             	mov    %rdx,%r11
ffffffff812c7f8d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c7f90:	41 55                	push   %r13
ffffffff812c7f92:	49 89 f5             	mov    %rsi,%r13
ffffffff812c7f95:	41 54                	push   %r12
ffffffff812c7f97:	49 89 d4             	mov    %rdx,%r12
ffffffff812c7f9a:	53                   	push   %rbx
ffffffff812c7f9b:	4c 89 c3             	mov    %r8,%rbx
ffffffff812c7f9e:	4c 89 e0             	mov    %r12,%rax
ffffffff812c7fa1:	4c 29 d8             	sub    %r11,%rax
	size_t l2;

	l2 = strlen(s2);
	if (!l2)
		return (char *)s1;
	while (len >= l2) {
ffffffff812c7fa4:	4d 39 cb             	cmp    %r9,%r11
ffffffff812c7fa7:	4c 8d 14 03          	lea    (%rbx,%rax,1),%r10
ffffffff812c7fab:	72 1a                	jb     ffffffff812c7fc7 <strnstr+0x5a>
		len--;
		if (!memcmp(s1, s2, l2))
ffffffff812c7fad:	4c 89 ca             	mov    %r9,%rdx
ffffffff812c7fb0:	4c 89 ee             	mov    %r13,%rsi
ffffffff812c7fb3:	4c 89 d7             	mov    %r10,%rdi

	l2 = strlen(s2);
	if (!l2)
		return (char *)s1;
	while (len >= l2) {
		len--;
ffffffff812c7fb6:	49 ff cb             	dec    %r11
		if (!memcmp(s1, s2, l2))
ffffffff812c7fb9:	e8 fb fe ff ff       	callq  ffffffff812c7eb9 <memcmp>
ffffffff812c7fbe:	85 c0                	test   %eax,%eax
ffffffff812c7fc0:	75 dc                	jne    ffffffff812c7f9e <strnstr+0x31>
ffffffff812c7fc2:	4c 89 d0             	mov    %r10,%rax
ffffffff812c7fc5:	eb 02                	jmp    ffffffff812c7fc9 <strnstr+0x5c>
			return (char *)s1;
		s1++;
	}
	return NULL;
ffffffff812c7fc7:	31 c0                	xor    %eax,%eax
}
ffffffff812c7fc9:	5b                   	pop    %rbx
ffffffff812c7fca:	41 5c                	pop    %r12
ffffffff812c7fcc:	41 5d                	pop    %r13
ffffffff812c7fce:	5d                   	pop    %rbp
ffffffff812c7fcf:	c3                   	retq   

ffffffff812c7fd0 <memchr>:
 *
 * returns the address of the first occurrence of @c, or %NULL
 * if @c is not found
 */
void *memchr(const void *s, int c, size_t n)
{
ffffffff812c7fd0:	55                   	push   %rbp
ffffffff812c7fd1:	48 01 fa             	add    %rdi,%rdx
	const unsigned char *p = s;
	while (n-- != 0) {
        	if ((unsigned char)c == *p++) {
ffffffff812c7fd4:	40 0f b6 f6          	movzbl %sil,%esi
 *
 * returns the address of the first occurrence of @c, or %NULL
 * if @c is not found
 */
void *memchr(const void *s, int c, size_t n)
{
ffffffff812c7fd8:	48 89 e5             	mov    %rsp,%rbp
	const unsigned char *p = s;
	while (n-- != 0) {
ffffffff812c7fdb:	48 39 d7             	cmp    %rdx,%rdi
ffffffff812c7fde:	74 10                	je     ffffffff812c7ff0 <memchr+0x20>
        	if ((unsigned char)c == *p++) {
ffffffff812c7fe0:	0f b6 0f             	movzbl (%rdi),%ecx
ffffffff812c7fe3:	48 8d 47 01          	lea    0x1(%rdi),%rax
ffffffff812c7fe7:	39 ce                	cmp    %ecx,%esi
ffffffff812c7fe9:	74 09                	je     ffffffff812c7ff4 <memchr+0x24>
ffffffff812c7feb:	48 89 c7             	mov    %rax,%rdi
ffffffff812c7fee:	eb eb                	jmp    ffffffff812c7fdb <memchr+0xb>
			return (void *)(p - 1);
		}
	}
	return NULL;
ffffffff812c7ff0:	31 c0                	xor    %eax,%eax
ffffffff812c7ff2:	eb 03                	jmp    ffffffff812c7ff7 <memchr+0x27>
ffffffff812c7ff4:	48 89 f8             	mov    %rdi,%rax
}
ffffffff812c7ff7:	5d                   	pop    %rbp
ffffffff812c7ff8:	c3                   	retq   

ffffffff812c7ff9 <strlcpy>:
 * NUL-terminated string that fits in the buffer (unless,
 * of course, the buffer size is zero). It does not pad
 * out the result like strncpy() does.
 */
size_t strlcpy(char *dest, const char *src, size_t size)
{
ffffffff812c7ff9:	55                   	push   %rbp
	size_t ret = strlen(src);
ffffffff812c7ffa:	31 c0                	xor    %eax,%eax
ffffffff812c7ffc:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
 * NUL-terminated string that fits in the buffer (unless,
 * of course, the buffer size is zero). It does not pad
 * out the result like strncpy() does.
 */
size_t strlcpy(char *dest, const char *src, size_t size)
{
ffffffff812c8000:	49 89 f8             	mov    %rdi,%r8
	size_t ret = strlen(src);
ffffffff812c8003:	48 89 f7             	mov    %rsi,%rdi
ffffffff812c8006:	f2 ae                	repnz scas %es:(%rdi),%al
 * NUL-terminated string that fits in the buffer (unless,
 * of course, the buffer size is zero). It does not pad
 * out the result like strncpy() does.
 */
size_t strlcpy(char *dest, const char *src, size_t size)
{
ffffffff812c8008:	48 89 e5             	mov    %rsp,%rbp
	size_t ret = strlen(src);
ffffffff812c800b:	48 89 c8             	mov    %rcx,%rax
ffffffff812c800e:	48 f7 d0             	not    %rax
ffffffff812c8011:	48 ff c8             	dec    %rax

	if (size) {
ffffffff812c8014:	48 85 d2             	test   %rdx,%rdx
ffffffff812c8017:	74 1b                	je     ffffffff812c8034 <strlcpy+0x3b>
		size_t len = (ret >= size) ? size - 1 : ret;
ffffffff812c8019:	48 8d 4a ff          	lea    -0x1(%rdx),%rcx
ffffffff812c801d:	48 39 d0             	cmp    %rdx,%rax
		memcpy(dest, src, len);
ffffffff812c8020:	4c 89 c7             	mov    %r8,%rdi
size_t strlcpy(char *dest, const char *src, size_t size)
{
	size_t ret = strlen(src);

	if (size) {
		size_t len = (ret >= size) ? size - 1 : ret;
ffffffff812c8023:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c8026:	48 0f 42 d0          	cmovb  %rax,%rdx
		memcpy(dest, src, len);
ffffffff812c802a:	48 89 d1             	mov    %rdx,%rcx
ffffffff812c802d:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
		dest[len] = '\0';
ffffffff812c802f:	41 c6 04 10 00       	movb   $0x0,(%r8,%rdx,1)
	}
	return ret;
}
ffffffff812c8034:	5d                   	pop    %rbp
ffffffff812c8035:	c3                   	retq   

ffffffff812c8036 <memzero_explicit>:
 *
 * memzero_explicit() doesn't need an arch-specific version as
 * it just invokes the one of memset() implicitly.
 */
void memzero_explicit(void *s, size_t count)
{
ffffffff812c8036:	55                   	push   %rbp
	memset(s, 0, count);
ffffffff812c8037:	31 c0                	xor    %eax,%eax
ffffffff812c8039:	48 89 f1             	mov    %rsi,%rcx
 *
 * memzero_explicit() doesn't need an arch-specific version as
 * it just invokes the one of memset() implicitly.
 */
void memzero_explicit(void *s, size_t count)
{
ffffffff812c803c:	48 89 fa             	mov    %rdi,%rdx
ffffffff812c803f:	48 89 e5             	mov    %rsp,%rbp
	memset(s, 0, count);
ffffffff812c8042:	f3 aa                	rep stos %al,%es:(%rdi)
	barrier_data(s);
}
ffffffff812c8044:	5d                   	pop    %rbp
ffffffff812c8045:	c3                   	retq   

ffffffff812c8046 <strncat>:
 *
 * Note that in contrast to strncpy(), strncat() ensures the result is
 * terminated.
 */
char *strncat(char *dest, const char *src, size_t count)
{
ffffffff812c8046:	55                   	push   %rbp
	char *tmp = dest;

	if (count) {
ffffffff812c8047:	48 85 d2             	test   %rdx,%rdx
 *
 * Note that in contrast to strncpy(), strncat() ensures the result is
 * terminated.
 */
char *strncat(char *dest, const char *src, size_t count)
{
ffffffff812c804a:	48 89 f8             	mov    %rdi,%rax
ffffffff812c804d:	49 89 f8             	mov    %rdi,%r8
ffffffff812c8050:	48 89 e5             	mov    %rsp,%rbp
	char *tmp = dest;

	if (count) {
ffffffff812c8053:	74 2b                	je     ffffffff812c8080 <strncat+0x3a>
		while (*dest)
ffffffff812c8055:	41 80 38 00          	cmpb   $0x0,(%r8)
ffffffff812c8059:	74 05                	je     ffffffff812c8060 <strncat+0x1a>
			dest++;
ffffffff812c805b:	49 ff c0             	inc    %r8
ffffffff812c805e:	eb f5                	jmp    ffffffff812c8055 <strncat+0xf>
char *strncat(char *dest, const char *src, size_t count)
{
	char *tmp = dest;

	if (count) {
		while (*dest)
ffffffff812c8060:	31 c9                	xor    %ecx,%ecx
			dest++;
		while ((*dest++ = *src++) != 0) {
ffffffff812c8062:	40 8a 3c 0e          	mov    (%rsi,%rcx,1),%dil
ffffffff812c8066:	4d 8d 0c 08          	lea    (%r8,%rcx,1),%r9
ffffffff812c806a:	40 84 ff             	test   %dil,%dil
ffffffff812c806d:	41 88 3c 08          	mov    %dil,(%r8,%rcx,1)
ffffffff812c8071:	74 0d                	je     ffffffff812c8080 <strncat+0x3a>
ffffffff812c8073:	48 ff c1             	inc    %rcx
			if (--count == 0) {
ffffffff812c8076:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812c8079:	75 e7                	jne    ffffffff812c8062 <strncat+0x1c>
				*dest = '\0';
ffffffff812c807b:	41 c6 41 01 00       	movb   $0x0,0x1(%r9)
				break;
			}
		}
	}
	return tmp;
}
ffffffff812c8080:	5d                   	pop    %rbp
ffffffff812c8081:	c3                   	retq   

ffffffff812c8082 <strlcat>:
 * @src: The string to append to it
 * @count: The size of the destination buffer.
 */
size_t strlcat(char *dest, const char *src, size_t count)
{
	size_t dsize = strlen(dest);
ffffffff812c8082:	49 83 c8 ff          	or     $0xffffffffffffffff,%r8
 * @dest: The string to be appended to
 * @src: The string to append to it
 * @count: The size of the destination buffer.
 */
size_t strlcat(char *dest, const char *src, size_t count)
{
ffffffff812c8086:	55                   	push   %rbp
	size_t dsize = strlen(dest);
ffffffff812c8087:	31 c0                	xor    %eax,%eax
ffffffff812c8089:	4c 89 c1             	mov    %r8,%rcx
 * @dest: The string to be appended to
 * @src: The string to append to it
 * @count: The size of the destination buffer.
 */
size_t strlcat(char *dest, const char *src, size_t count)
{
ffffffff812c808c:	49 89 fa             	mov    %rdi,%r10
	size_t dsize = strlen(dest);
ffffffff812c808f:	f2 ae                	repnz scas %es:(%rdi),%al
	size_t len = strlen(src);
ffffffff812c8091:	48 89 f7             	mov    %rsi,%rdi
 * @dest: The string to be appended to
 * @src: The string to append to it
 * @count: The size of the destination buffer.
 */
size_t strlcat(char *dest, const char *src, size_t count)
{
ffffffff812c8094:	48 89 e5             	mov    %rsp,%rbp
	size_t dsize = strlen(dest);
ffffffff812c8097:	48 f7 d1             	not    %rcx
ffffffff812c809a:	4e 8d 0c 01          	lea    (%rcx,%r8,1),%r9
	size_t len = strlen(src);
ffffffff812c809e:	4c 89 c1             	mov    %r8,%rcx
ffffffff812c80a1:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812c80a3:	48 89 c8             	mov    %rcx,%rax
ffffffff812c80a6:	48 f7 d0             	not    %rax
ffffffff812c80a9:	49 01 c0             	add    %rax,%r8
	size_t res = dsize + len;

	/* This would be a bug */
	BUG_ON(dsize >= count);
ffffffff812c80ac:	49 39 d1             	cmp    %rdx,%r9
 */
size_t strlcat(char *dest, const char *src, size_t count)
{
	size_t dsize = strlen(dest);
	size_t len = strlen(src);
	size_t res = dsize + len;
ffffffff812c80af:	4b 8d 04 01          	lea    (%r9,%r8,1),%rax

	/* This would be a bug */
	BUG_ON(dsize >= count);
ffffffff812c80b3:	72 02                	jb     ffffffff812c80b7 <strlcat+0x35>
ffffffff812c80b5:	0f 0b                	ud2    

	dest += dsize;
	count -= dsize;
ffffffff812c80b7:	4c 29 ca             	sub    %r9,%rdx
	size_t res = dsize + len;

	/* This would be a bug */
	BUG_ON(dsize >= count);

	dest += dsize;
ffffffff812c80ba:	4d 01 ca             	add    %r9,%r10
	count -= dsize;
	if (len >= count)
		len = count-1;
ffffffff812c80bd:	48 8d 4a ff          	lea    -0x1(%rdx),%rcx
ffffffff812c80c1:	49 39 d0             	cmp    %rdx,%r8
	memcpy(dest, src, len);
ffffffff812c80c4:	4c 89 d7             	mov    %r10,%rdi
	BUG_ON(dsize >= count);

	dest += dsize;
	count -= dsize;
	if (len >= count)
		len = count-1;
ffffffff812c80c7:	4c 0f 43 c1          	cmovae %rcx,%r8
	memcpy(dest, src, len);
ffffffff812c80cb:	4c 89 c1             	mov    %r8,%rcx
ffffffff812c80ce:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
	dest[len] = 0;
ffffffff812c80d0:	43 c6 04 02 00       	movb   $0x0,(%r10,%r8,1)
	return res;
}
ffffffff812c80d5:	5d                   	pop    %rbp
ffffffff812c80d6:	c3                   	retq   

ffffffff812c80d7 <strcasecmp>:
EXPORT_SYMBOL(strncasecmp);
#endif

#ifndef __HAVE_ARCH_STRCASECMP
int strcasecmp(const char *s1, const char *s2)
{
ffffffff812c80d7:	55                   	push   %rbp
ffffffff812c80d8:	31 c9                	xor    %ecx,%ecx
ffffffff812c80da:	48 89 e5             	mov    %rsp,%rbp
	int c1, c2;

	do {
		c1 = tolower(*s1++);
ffffffff812c80dd:	44 0f b6 04 0f       	movzbl (%rdi,%rcx,1),%r8d
		c2 = tolower(*s2++);
ffffffff812c80e2:	44 0f b6 0c 0e       	movzbl (%rsi,%rcx,1),%r9d
#define toascii(c) (((unsigned char)(c))&0x7f)

static inline unsigned char __tolower(unsigned char c)
{
	if (isupper(c))
		c -= 'A'-'a';
ffffffff812c80e7:	41 f6 80 60 ef 63 81 	testb  $0x1,-0x7e9c10a0(%r8)
ffffffff812c80ee:	01 
ffffffff812c80ef:	41 8d 50 20          	lea    0x20(%r8),%edx
int strcasecmp(const char *s1, const char *s2)
{
	int c1, c2;

	do {
		c1 = tolower(*s1++);
ffffffff812c80f3:	4c 89 c0             	mov    %r8,%rax
ffffffff812c80f6:	45 8d 41 20          	lea    0x20(%r9),%r8d
ffffffff812c80fa:	0f 45 c2             	cmovne %edx,%eax
ffffffff812c80fd:	41 f6 81 60 ef 63 81 	testb  $0x1,-0x7e9c10a0(%r9)
ffffffff812c8104:	01 
		c2 = tolower(*s2++);
ffffffff812c8105:	4c 89 ca             	mov    %r9,%rdx
int strcasecmp(const char *s1, const char *s2)
{
	int c1, c2;

	do {
		c1 = tolower(*s1++);
ffffffff812c8108:	0f b6 c0             	movzbl %al,%eax
ffffffff812c810b:	41 0f 45 d0          	cmovne %r8d,%edx
ffffffff812c810f:	48 ff c1             	inc    %rcx
		c2 = tolower(*s2++);
ffffffff812c8112:	0f b6 d2             	movzbl %dl,%edx
	} while (c1 == c2 && c1 != 0);
ffffffff812c8115:	39 d0                	cmp    %edx,%eax
ffffffff812c8117:	75 04                	jne    ffffffff812c811d <strcasecmp+0x46>
ffffffff812c8119:	85 c0                	test   %eax,%eax
ffffffff812c811b:	75 c0                	jne    ffffffff812c80dd <strcasecmp+0x6>
	return c1 - c2;
ffffffff812c811d:	29 d0                	sub    %edx,%eax
}
ffffffff812c811f:	5d                   	pop    %rbp
ffffffff812c8120:	c3                   	retq   

ffffffff812c8121 <strncasecmp>:
 * @s1: One string
 * @s2: The other string
 * @len: the maximum number of characters to compare
 */
int strncasecmp(const char *s1, const char *s2, size_t len)
{
ffffffff812c8121:	55                   	push   %rbp
ffffffff812c8122:	31 c0                	xor    %eax,%eax
	/* Yes, Virginia, it had better be unsigned */
	unsigned char c1, c2;

	if (!len)
ffffffff812c8124:	48 85 d2             	test   %rdx,%rdx
 * @s1: One string
 * @s2: The other string
 * @len: the maximum number of characters to compare
 */
int strncasecmp(const char *s1, const char *s2, size_t len)
{
ffffffff812c8127:	48 89 e5             	mov    %rsp,%rbp
	/* Yes, Virginia, it had better be unsigned */
	unsigned char c1, c2;

	if (!len)
ffffffff812c812a:	74 55                	je     ffffffff812c8181 <strncasecmp+0x60>
ffffffff812c812c:	45 31 c0             	xor    %r8d,%r8d
		return 0;

	do {
		c1 = *s1++;
ffffffff812c812f:	42 8a 04 07          	mov    (%rdi,%r8,1),%al
		c2 = *s2++;
ffffffff812c8133:	42 8a 0c 06          	mov    (%rsi,%r8,1),%cl
		if (!c1 || !c2)
ffffffff812c8137:	84 c0                	test   %al,%al
ffffffff812c8139:	74 3e                	je     ffffffff812c8179 <strncasecmp+0x58>
ffffffff812c813b:	84 c9                	test   %cl,%cl
ffffffff812c813d:	74 3a                	je     ffffffff812c8179 <strncasecmp+0x58>
			break;
		if (c1 == c2)
ffffffff812c813f:	38 c8                	cmp    %cl,%al
ffffffff812c8141:	74 2c                	je     ffffffff812c816f <strncasecmp+0x4e>
#define isascii(c) (((unsigned char)(c))<=0x7f)
#define toascii(c) (((unsigned char)(c))&0x7f)

static inline unsigned char __tolower(unsigned char c)
{
	if (isupper(c))
ffffffff812c8143:	44 0f b6 d0          	movzbl %al,%r10d
		c -= 'A'-'a';
ffffffff812c8147:	44 8d 48 20          	lea    0x20(%rax),%r9d
ffffffff812c814b:	41 f6 82 60 ef 63 81 	testb  $0x1,-0x7e9c10a0(%r10)
ffffffff812c8152:	01 
#define isascii(c) (((unsigned char)(c))<=0x7f)
#define toascii(c) (((unsigned char)(c))&0x7f)

static inline unsigned char __tolower(unsigned char c)
{
	if (isupper(c))
ffffffff812c8153:	44 0f b6 d1          	movzbl %cl,%r10d
		c -= 'A'-'a';
ffffffff812c8157:	41 0f 45 c1          	cmovne %r9d,%eax
ffffffff812c815b:	41 f6 82 60 ef 63 81 	testb  $0x1,-0x7e9c10a0(%r10)
ffffffff812c8162:	01 
ffffffff812c8163:	44 8d 49 20          	lea    0x20(%rcx),%r9d
ffffffff812c8167:	41 0f 45 c9          	cmovne %r9d,%ecx
			continue;
		c1 = tolower(c1);
		c2 = tolower(c2);
		if (c1 != c2)
ffffffff812c816b:	38 c1                	cmp    %al,%cl
ffffffff812c816d:	75 0a                	jne    ffffffff812c8179 <strncasecmp+0x58>
ffffffff812c816f:	49 ff c0             	inc    %r8
			break;
	} while (--len);
ffffffff812c8172:	4c 39 c2             	cmp    %r8,%rdx
ffffffff812c8175:	75 b8                	jne    ffffffff812c812f <strncasecmp+0xe>
ffffffff812c8177:	88 c8                	mov    %cl,%al
	return (int)c1 - (int)c2;
ffffffff812c8179:	0f b6 c0             	movzbl %al,%eax
ffffffff812c817c:	0f b6 c9             	movzbl %cl,%ecx
ffffffff812c817f:	29 c8                	sub    %ecx,%eax
}
ffffffff812c8181:	5d                   	pop    %rbp
ffffffff812c8182:	c3                   	retq   

ffffffff812c8183 <memchr_inv>:
 *
 * returns the address of the first character other than @c, or %NULL
 * if the whole buffer contains just @c.
 */
void *memchr_inv(const void *start, int c, size_t bytes)
{
ffffffff812c8183:	55                   	push   %rbp
	u8 value = c;
	u64 value64;
	unsigned int words, prefix;

	if (bytes <= 16)
ffffffff812c8184:	48 83 fa 10          	cmp    $0x10,%rdx
 *
 * returns the address of the first character other than @c, or %NULL
 * if the whole buffer contains just @c.
 */
void *memchr_inv(const void *start, int c, size_t bytes)
{
ffffffff812c8188:	48 89 e5             	mov    %rsp,%rbp
	u8 value = c;
	u64 value64;
	unsigned int words, prefix;

	if (bytes <= 16)
ffffffff812c818b:	77 18                	ja     ffffffff812c81a5 <memchr_inv+0x22>
EXPORT_SYMBOL(memchr);
#endif

static void *check_bytes8(const u8 *start, u8 value, unsigned int bytes)
{
	while (bytes) {
ffffffff812c818d:	85 d2                	test   %edx,%edx
ffffffff812c818f:	0f 84 93 00 00 00    	je     ffffffff812c8228 <memchr_inv+0xa5>
		if (*start != value)
ffffffff812c8195:	40 3a 37             	cmp    (%rdi),%sil
ffffffff812c8198:	0f 85 8e 00 00 00    	jne    ffffffff812c822c <memchr_inv+0xa9>
			return (void *)start;
		start++;
ffffffff812c819e:	48 ff c7             	inc    %rdi
		bytes--;
ffffffff812c81a1:	ff ca                	dec    %edx
ffffffff812c81a3:	eb e8                	jmp    ffffffff812c818d <memchr_inv+0xa>
	value64 |= value64 << 16;
	value64 |= value64 << 32;
#endif

	prefix = (unsigned long)start % 8;
	if (prefix) {
ffffffff812c81a5:	89 f9                	mov    %edi,%ecx
ffffffff812c81a7:	83 e1 07             	and    $0x7,%ecx
ffffffff812c81aa:	74 2b                	je     ffffffff812c81d7 <memchr_inv+0x54>
		u8 *r;

		prefix = 8 - prefix;
ffffffff812c81ac:	b8 08 00 00 00       	mov    $0x8,%eax
ffffffff812c81b1:	29 c8                	sub    %ecx,%eax
ffffffff812c81b3:	89 c1                	mov    %eax,%ecx
ffffffff812c81b5:	41 89 c0             	mov    %eax,%r8d
ffffffff812c81b8:	48 89 f8             	mov    %rdi,%rax
#endif

static void *check_bytes8(const u8 *start, u8 value, unsigned int bytes)
{
	while (bytes) {
		if (*start != value)
ffffffff812c81bb:	40 3a 30             	cmp    (%rax),%sil
ffffffff812c81be:	75 0a                	jne    ffffffff812c81ca <memchr_inv+0x47>
			return (void *)start;
		start++;
ffffffff812c81c0:	48 ff c0             	inc    %rax
EXPORT_SYMBOL(memchr);
#endif

static void *check_bytes8(const u8 *start, u8 value, unsigned int bytes)
{
	while (bytes) {
ffffffff812c81c3:	41 ff c8             	dec    %r8d
ffffffff812c81c6:	75 f3                	jne    ffffffff812c81bb <memchr_inv+0x38>
ffffffff812c81c8:	eb 05                	jmp    ffffffff812c81cf <memchr_inv+0x4c>
	if (prefix) {
		u8 *r;

		prefix = 8 - prefix;
		r = check_bytes8(start, value, prefix);
		if (r)
ffffffff812c81ca:	48 85 c0             	test   %rax,%rax
ffffffff812c81cd:	75 60                	jne    ffffffff812c822f <memchr_inv+0xac>
			return r;
		start += prefix;
ffffffff812c81cf:	89 c8                	mov    %ecx,%eax
ffffffff812c81d1:	48 01 c7             	add    %rax,%rdi
		bytes -= prefix;
ffffffff812c81d4:	48 29 c2             	sub    %rax,%rdx
	unsigned int words, prefix;

	if (bytes <= 16)
		return check_bytes8(start, value, bytes);

	value64 = value;
ffffffff812c81d7:	44 0f b6 c6          	movzbl %sil,%r8d
#if defined(CONFIG_ARCH_HAS_FAST_MULTIPLIER) && BITS_PER_LONG == 64
	value64 *= 0x0101010101010101;
ffffffff812c81db:	48 b9 01 01 01 01 01 	movabs $0x101010101010101,%rcx
ffffffff812c81e2:	01 01 01 
			return r;
		start += prefix;
		bytes -= prefix;
	}

	words = bytes / 8;
ffffffff812c81e5:	48 89 d0             	mov    %rdx,%rax
	if (bytes <= 16)
		return check_bytes8(start, value, bytes);

	value64 = value;
#if defined(CONFIG_ARCH_HAS_FAST_MULTIPLIER) && BITS_PER_LONG == 64
	value64 *= 0x0101010101010101;
ffffffff812c81e8:	49 0f af c8          	imul   %r8,%rcx
			return r;
		start += prefix;
		bytes -= prefix;
	}

	words = bytes / 8;
ffffffff812c81ec:	48 c1 e8 03          	shr    $0x3,%rax

	while (words) {
ffffffff812c81f0:	85 c0                	test   %eax,%eax
ffffffff812c81f2:	74 1f                	je     ffffffff812c8213 <memchr_inv+0x90>
		if (*(u64 *)start != value64)
ffffffff812c81f4:	48 3b 0f             	cmp    (%rdi),%rcx
ffffffff812c81f7:	4c 8d 47 08          	lea    0x8(%rdi),%r8
ffffffff812c81fb:	74 0f                	je     ffffffff812c820c <memchr_inv+0x89>
#endif

static void *check_bytes8(const u8 *start, u8 value, unsigned int bytes)
{
	while (bytes) {
		if (*start != value)
ffffffff812c81fd:	40 3a 37             	cmp    (%rdi),%sil
ffffffff812c8200:	75 2a                	jne    ffffffff812c822c <memchr_inv+0xa9>
			return (void *)start;
		start++;
ffffffff812c8202:	48 ff c7             	inc    %rdi
EXPORT_SYMBOL(memchr);
#endif

static void *check_bytes8(const u8 *start, u8 value, unsigned int bytes)
{
	while (bytes) {
ffffffff812c8205:	49 39 f8             	cmp    %rdi,%r8
ffffffff812c8208:	75 f3                	jne    ffffffff812c81fd <memchr_inv+0x7a>
ffffffff812c820a:	eb 1c                	jmp    ffffffff812c8228 <memchr_inv+0xa5>
	words = bytes / 8;

	while (words) {
		if (*(u64 *)start != value64)
			return check_bytes8(start, value, 8);
		start += 8;
ffffffff812c820c:	4c 89 c7             	mov    %r8,%rdi
		words--;
ffffffff812c820f:	ff c8                	dec    %eax
ffffffff812c8211:	eb dd                	jmp    ffffffff812c81f0 <memchr_inv+0x6d>
ffffffff812c8213:	83 e2 07             	and    $0x7,%edx
ffffffff812c8216:	48 01 fa             	add    %rdi,%rdx
EXPORT_SYMBOL(memchr);
#endif

static void *check_bytes8(const u8 *start, u8 value, unsigned int bytes)
{
	while (bytes) {
ffffffff812c8219:	48 39 d7             	cmp    %rdx,%rdi
ffffffff812c821c:	74 0a                	je     ffffffff812c8228 <memchr_inv+0xa5>
		if (*start != value)
ffffffff812c821e:	40 3a 37             	cmp    (%rdi),%sil
ffffffff812c8221:	75 09                	jne    ffffffff812c822c <memchr_inv+0xa9>
			return (void *)start;
		start++;
ffffffff812c8223:	48 ff c7             	inc    %rdi
ffffffff812c8226:	eb f1                	jmp    ffffffff812c8219 <memchr_inv+0x96>
		bytes--;
	}
	return NULL;
ffffffff812c8228:	31 c0                	xor    %eax,%eax
ffffffff812c822a:	eb 03                	jmp    ffffffff812c822f <memchr_inv+0xac>
#endif

static void *check_bytes8(const u8 *start, u8 value, unsigned int bytes)
{
	while (bytes) {
		if (*start != value)
ffffffff812c822c:	48 89 f8             	mov    %rdi,%rax
		start += 8;
		words--;
	}

	return check_bytes8(start, value, bytes % 8);
}
ffffffff812c822f:	5d                   	pop    %rbp
ffffffff812c8230:	c3                   	retq   

ffffffff812c8231 <timerqueue_add>:
 *
 * Adds the timer node to the timerqueue, sorted by the
 * node's expires value.
 */
void timerqueue_add(struct timerqueue_head *head, struct timerqueue_node *node)
{
ffffffff812c8231:	55                   	push   %rbp
ffffffff812c8232:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c8235:	41 54                	push   %r12
ffffffff812c8237:	53                   	push   %rbx
	struct rb_node **p = &head->head.rb_node;
	struct rb_node *parent = NULL;
	struct timerqueue_node  *ptr;

	/* Make sure we don't add nodes that are already added */
	WARN_ON_ONCE(!RB_EMPTY_NODE(&node->node));
ffffffff812c8238:	48 3b 36             	cmp    (%rsi),%rsi
 *
 * Adds the timer node to the timerqueue, sorted by the
 * node's expires value.
 */
void timerqueue_add(struct timerqueue_head *head, struct timerqueue_node *node)
{
ffffffff812c823b:	49 89 fc             	mov    %rdi,%r12
ffffffff812c823e:	48 89 f3             	mov    %rsi,%rbx
	struct rb_node **p = &head->head.rb_node;
	struct rb_node *parent = NULL;
	struct timerqueue_node  *ptr;

	/* Make sure we don't add nodes that are already added */
	WARN_ON_ONCE(!RB_EMPTY_NODE(&node->node));
ffffffff812c8241:	74 21                	je     ffffffff812c8264 <timerqueue_add+0x33>
ffffffff812c8243:	80 3d a2 f0 78 00 00 	cmpb   $0x0,0x78f0a2(%rip)        # ffffffff81a572ec <__warned.6655>
ffffffff812c824a:	75 18                	jne    ffffffff812c8264 <timerqueue_add+0x33>
ffffffff812c824c:	be 2e 00 00 00       	mov    $0x2e,%esi
ffffffff812c8251:	48 c7 c7 39 6d 7b 81 	mov    $0xffffffff817b6d39,%rdi
ffffffff812c8258:	e8 58 e1 d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c825d:	c6 05 88 f0 78 00 01 	movb   $0x1,0x78f088(%rip)        # ffffffff81a572ec <__warned.6655>
 * Adds the timer node to the timerqueue, sorted by the
 * node's expires value.
 */
void timerqueue_add(struct timerqueue_head *head, struct timerqueue_node *node)
{
	struct rb_node **p = &head->head.rb_node;
ffffffff812c8264:	4c 89 e0             	mov    %r12,%rax
	struct rb_node *parent = NULL;
ffffffff812c8267:	31 c9                	xor    %ecx,%ecx
	struct timerqueue_node  *ptr;

	/* Make sure we don't add nodes that are already added */
	WARN_ON_ONCE(!RB_EMPTY_NODE(&node->node));

	while (*p) {
ffffffff812c8269:	48 8b 10             	mov    (%rax),%rdx
ffffffff812c826c:	48 85 d2             	test   %rdx,%rdx
ffffffff812c826f:	74 19                	je     ffffffff812c828a <timerqueue_add+0x59>
		parent = *p;
		ptr = rb_entry(parent, struct timerqueue_node, node);
		if (node->expires.tv64 < ptr->expires.tv64)
			p = &(*p)->rb_left;
ffffffff812c8271:	48 8b 72 18          	mov    0x18(%rdx),%rsi
ffffffff812c8275:	48 39 73 18          	cmp    %rsi,0x18(%rbx)
ffffffff812c8279:	48 8d 4a 10          	lea    0x10(%rdx),%rcx
ffffffff812c827d:	48 8d 42 08          	lea    0x8(%rdx),%rax
ffffffff812c8281:	48 0f 4c c1          	cmovl  %rcx,%rax
 *
 * Adds the timer node to the timerqueue, sorted by the
 * node's expires value.
 */
void timerqueue_add(struct timerqueue_head *head, struct timerqueue_node *node)
{
ffffffff812c8285:	48 89 d1             	mov    %rdx,%rcx
ffffffff812c8288:	eb df                	jmp    ffffffff812c8269 <timerqueue_add+0x38>
			    struct rb_root *root);

static inline void rb_link_node(struct rb_node * node, struct rb_node * parent,
				struct rb_node ** rb_link)
{
	node->__rb_parent_color = (unsigned long)parent;
ffffffff812c828a:	48 89 0b             	mov    %rcx,(%rbx)
	node->rb_left = node->rb_right = NULL;
ffffffff812c828d:	48 c7 43 08 00 00 00 	movq   $0x0,0x8(%rbx)
ffffffff812c8294:	00 
			p = &(*p)->rb_left;
		else
			p = &(*p)->rb_right;
	}
	rb_link_node(&node->node, parent, p);
	rb_insert_color(&node->node, &head->head);
ffffffff812c8295:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c8298:	48 c7 43 10 00 00 00 	movq   $0x0,0x10(%rbx)
ffffffff812c829f:	00 
ffffffff812c82a0:	48 89 df             	mov    %rbx,%rdi

	*rb_link = node;
ffffffff812c82a3:	48 89 18             	mov    %rbx,(%rax)
ffffffff812c82a6:	e8 75 dd ff ff       	callq  ffffffff812c6020 <rb_insert_color>

	if (!head->next || node->expires.tv64 < head->next->expires.tv64)
ffffffff812c82ab:	49 8b 44 24 08       	mov    0x8(%r12),%rax
ffffffff812c82b0:	48 85 c0             	test   %rax,%rax
ffffffff812c82b3:	74 0a                	je     ffffffff812c82bf <timerqueue_add+0x8e>
ffffffff812c82b5:	48 8b 40 18          	mov    0x18(%rax),%rax
ffffffff812c82b9:	48 39 43 18          	cmp    %rax,0x18(%rbx)
ffffffff812c82bd:	7d 05                	jge    ffffffff812c82c4 <timerqueue_add+0x93>
		head->next = node;
ffffffff812c82bf:	49 89 5c 24 08       	mov    %rbx,0x8(%r12)
}
ffffffff812c82c4:	5b                   	pop    %rbx
ffffffff812c82c5:	41 5c                	pop    %r12
ffffffff812c82c7:	5d                   	pop    %rbp
ffffffff812c82c8:	c3                   	retq   

ffffffff812c82c9 <timerqueue_iterate_next>:
 */
struct timerqueue_node *timerqueue_iterate_next(struct timerqueue_node *node)
{
	struct rb_node *next;

	if (!node)
ffffffff812c82c9:	48 85 ff             	test   %rdi,%rdi
ffffffff812c82cc:	74 0b                	je     ffffffff812c82d9 <timerqueue_iterate_next+0x10>
 * Provides the timer that is after the given node. This is used, when
 * necessary, to iterate through the list of timers in a timer list
 * without modifying the list.
 */
struct timerqueue_node *timerqueue_iterate_next(struct timerqueue_node *node)
{
ffffffff812c82ce:	55                   	push   %rbp
ffffffff812c82cf:	48 89 e5             	mov    %rsp,%rbp
	struct rb_node *next;

	if (!node)
		return NULL;
	next = rb_next(&node->node);
ffffffff812c82d2:	e8 d9 da ff ff       	callq  ffffffff812c5db0 <rb_next>
	if (!next)
		return NULL;
	return container_of(next, struct timerqueue_node, node);
}
ffffffff812c82d7:	5d                   	pop    %rbp
ffffffff812c82d8:	c3                   	retq   
ffffffff812c82d9:	31 c0                	xor    %eax,%eax
ffffffff812c82db:	c3                   	retq   

ffffffff812c82dc <timerqueue_del>:
 * @node: timer node to be removed
 *
 * Removes the timer node from the timerqueue.
 */
void timerqueue_del(struct timerqueue_head *head, struct timerqueue_node *node)
{
ffffffff812c82dc:	55                   	push   %rbp
ffffffff812c82dd:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c82e0:	41 54                	push   %r12
ffffffff812c82e2:	53                   	push   %rbx
	WARN_ON_ONCE(RB_EMPTY_NODE(&node->node));
ffffffff812c82e3:	48 39 36             	cmp    %rsi,(%rsi)
 * @node: timer node to be removed
 *
 * Removes the timer node from the timerqueue.
 */
void timerqueue_del(struct timerqueue_head *head, struct timerqueue_node *node)
{
ffffffff812c82e6:	49 89 fc             	mov    %rdi,%r12
ffffffff812c82e9:	48 89 f3             	mov    %rsi,%rbx
	WARN_ON_ONCE(RB_EMPTY_NODE(&node->node));
ffffffff812c82ec:	75 21                	jne    ffffffff812c830f <timerqueue_del+0x33>
ffffffff812c82ee:	80 3d f6 ef 78 00 00 	cmpb   $0x0,0x78eff6(%rip)        # ffffffff81a572eb <__warned.6677>
ffffffff812c82f5:	75 18                	jne    ffffffff812c830f <timerqueue_del+0x33>
ffffffff812c82f7:	be 4a 00 00 00       	mov    $0x4a,%esi
ffffffff812c82fc:	48 c7 c7 39 6d 7b 81 	mov    $0xffffffff817b6d39,%rdi
ffffffff812c8303:	e8 ad e0 d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812c8308:	c6 05 dc ef 78 00 01 	movb   $0x1,0x78efdc(%rip)        # ffffffff81a572eb <__warned.6677>

	/* update next pointer */
	if (head->next == node) {
ffffffff812c830f:	49 3b 5c 24 08       	cmp    0x8(%r12),%rbx
ffffffff812c8314:	75 0d                	jne    ffffffff812c8323 <timerqueue_del+0x47>
		struct rb_node *rbn = rb_next(&node->node);
ffffffff812c8316:	48 89 df             	mov    %rbx,%rdi
ffffffff812c8319:	e8 92 da ff ff       	callq  ffffffff812c5db0 <rb_next>

		head->next = rbn ?
ffffffff812c831e:	49 89 44 24 08       	mov    %rax,0x8(%r12)
			rb_entry(rbn, struct timerqueue_node, node) : NULL;
	}
	rb_erase(&node->node, &head->head);
ffffffff812c8323:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c8326:	48 89 df             	mov    %rbx,%rdi
ffffffff812c8329:	e8 db df ff ff       	callq  ffffffff812c6309 <rb_erase>
	RB_CLEAR_NODE(&node->node);
ffffffff812c832e:	48 89 1b             	mov    %rbx,(%rbx)
}
ffffffff812c8331:	5b                   	pop    %rbx
ffffffff812c8332:	41 5c                	pop    %r12
ffffffff812c8334:	5d                   	pop    %rbp
ffffffff812c8335:	c3                   	retq   

ffffffff812c8336 <skip_atoi>:
}
EXPORT_SYMBOL(simple_strtoll);

static noinline_for_stack
int skip_atoi(const char **s)
{
ffffffff812c8336:	55                   	push   %rbp
	int i = 0;
ffffffff812c8337:	31 c0                	xor    %eax,%eax
}
EXPORT_SYMBOL(simple_strtoll);

static noinline_for_stack
int skip_atoi(const char **s)
{
ffffffff812c8339:	48 89 e5             	mov    %rsp,%rbp
	int i = 0;

	do {
		i = i*10 + *((*s)++) - '0';
ffffffff812c833c:	48 8b 17             	mov    (%rdi),%rdx
ffffffff812c833f:	6b c0 0a             	imul   $0xa,%eax,%eax
ffffffff812c8342:	48 8d 4a 01          	lea    0x1(%rdx),%rcx
ffffffff812c8346:	48 89 0f             	mov    %rcx,(%rdi)
ffffffff812c8349:	0f be 0a             	movsbl (%rdx),%ecx
	} while (isdigit(**s));
ffffffff812c834c:	0f b6 52 01          	movzbl 0x1(%rdx),%edx
int skip_atoi(const char **s)
{
	int i = 0;

	do {
		i = i*10 + *((*s)++) - '0';
ffffffff812c8350:	8d 44 08 d0          	lea    -0x30(%rax,%rcx,1),%eax
	} while (isdigit(**s));
ffffffff812c8354:	f6 82 60 ef 63 81 04 	testb  $0x4,-0x7e9c10a0(%rdx)
ffffffff812c835b:	75 df                	jne    ffffffff812c833c <skip_atoi+0x6>

	return i;
}
ffffffff812c835d:	5d                   	pop    %rbp
ffffffff812c835e:	c3                   	retq   

ffffffff812c835f <put_dec_trunc8>:
 * one of them accounted for in buf. This is needed by ip4_string
 * below. All other callers pass a non-zero value of r.
*/
static noinline_for_stack
char *put_dec_trunc8(char *buf, unsigned r)
{
ffffffff812c835f:	55                   	push   %rbp
	unsigned q;

	/* 1 <= r < 10^8 */
	if (r < 100)
ffffffff812c8360:	83 fe 63             	cmp    $0x63,%esi
 * one of them accounted for in buf. This is needed by ip4_string
 * below. All other callers pass a non-zero value of r.
*/
static noinline_for_stack
char *put_dec_trunc8(char *buf, unsigned r)
{
ffffffff812c8363:	48 89 e5             	mov    %rsp,%rbp
	unsigned q;

	/* 1 <= r < 10^8 */
	if (r < 100)
ffffffff812c8366:	76 73                	jbe    ffffffff812c83db <put_dec_trunc8+0x7c>
		goto out_r;

	/* 100 <= r < 10^8 */
	q = (r * (u64)0x28f5c29) >> 32;
ffffffff812c8368:	89 f0                	mov    %esi,%eax
ffffffff812c836a:	48 69 c0 29 5c 8f 02 	imul   $0x28f5c29,%rax,%rax
ffffffff812c8371:	48 c1 e8 20          	shr    $0x20,%rax
	*((u16 *)buf) = decpair[r - 100*q];
ffffffff812c8375:	6b d0 64             	imul   $0x64,%eax,%edx
ffffffff812c8378:	29 d6                	sub    %edx,%esi
	buf += 2;

	/* 1 <= q < 10^6 */
	if (q < 100)
ffffffff812c837a:	48 83 f8 63          	cmp    $0x63,%rax
	if (r < 100)
		goto out_r;

	/* 100 <= r < 10^8 */
	q = (r * (u64)0x28f5c29) >> 32;
	*((u16 *)buf) = decpair[r - 100*q];
ffffffff812c837e:	66 8b 94 36 80 f4 63 	mov    -0x7e9c0b80(%rsi,%rsi,1),%dx
ffffffff812c8385:	81 
ffffffff812c8386:	66 89 17             	mov    %dx,(%rdi)
	buf += 2;

	/* 1 <= q < 10^6 */
	if (q < 100)
ffffffff812c8389:	77 08                	ja     ffffffff812c8393 <put_dec_trunc8+0x34>
		goto out_r;

	/* 100 <= r < 10^8 */
	q = (r * (u64)0x28f5c29) >> 32;
	*((u16 *)buf) = decpair[r - 100*q];
	buf += 2;
ffffffff812c838b:	48 83 c7 02          	add    $0x2,%rdi
ffffffff812c838f:	89 c6                	mov    %eax,%esi
ffffffff812c8391:	eb 48                	jmp    ffffffff812c83db <put_dec_trunc8+0x7c>
	/* 1 <= q < 10^6 */
	if (q < 100)
		goto out_q;

	/*  100 <= q < 10^6 */
	r = (q * (u64)0x28f5c29) >> 32;
ffffffff812c8393:	48 69 d0 29 5c 8f 02 	imul   $0x28f5c29,%rax,%rdx
ffffffff812c839a:	48 c1 ea 20          	shr    $0x20,%rdx
	*((u16 *)buf) = decpair[q - 100*r];
ffffffff812c839e:	6b ca 64             	imul   $0x64,%edx,%ecx
	/* 1 <= q < 10^6 */
	if (q < 100)
		goto out_q;

	/*  100 <= q < 10^6 */
	r = (q * (u64)0x28f5c29) >> 32;
ffffffff812c83a1:	89 d6                	mov    %edx,%esi
	*((u16 *)buf) = decpair[q - 100*r];
ffffffff812c83a3:	29 c8                	sub    %ecx,%eax
	buf += 2;

	/* 1 <= r < 10^4 */
	if (r < 100)
ffffffff812c83a5:	48 83 fa 63          	cmp    $0x63,%rdx
	if (q < 100)
		goto out_q;

	/*  100 <= q < 10^6 */
	r = (q * (u64)0x28f5c29) >> 32;
	*((u16 *)buf) = decpair[q - 100*r];
ffffffff812c83a9:	66 8b 84 00 80 f4 63 	mov    -0x7e9c0b80(%rax,%rax,1),%ax
ffffffff812c83b0:	81 
ffffffff812c83b1:	66 89 47 02          	mov    %ax,0x2(%rdi)
	buf += 2;

	/* 1 <= r < 10^4 */
	if (r < 100)
ffffffff812c83b5:	77 06                	ja     ffffffff812c83bd <put_dec_trunc8+0x5e>
		goto out_q;

	/*  100 <= q < 10^6 */
	r = (q * (u64)0x28f5c29) >> 32;
	*((u16 *)buf) = decpair[q - 100*r];
	buf += 2;
ffffffff812c83b7:	48 83 c7 04          	add    $0x4,%rdi
ffffffff812c83bb:	eb 1e                	jmp    ffffffff812c83db <put_dec_trunc8+0x7c>
	/* 1 <= r < 10^4 */
	if (r < 100)
		goto out_r;

	/* 100 <= r < 10^4 */
	q = (r * 0x147b) >> 19;
ffffffff812c83bd:	69 f2 7b 14 00 00    	imul   $0x147b,%edx,%esi
	*((u16 *)buf) = decpair[r - 100*q];
	buf += 2;
ffffffff812c83c3:	48 83 c7 06          	add    $0x6,%rdi
	/* 1 <= r < 10^4 */
	if (r < 100)
		goto out_r;

	/* 100 <= r < 10^4 */
	q = (r * 0x147b) >> 19;
ffffffff812c83c7:	c1 ee 13             	shr    $0x13,%esi
	*((u16 *)buf) = decpair[r - 100*q];
ffffffff812c83ca:	6b c6 64             	imul   $0x64,%esi,%eax
ffffffff812c83cd:	29 c2                	sub    %eax,%edx
ffffffff812c83cf:	66 8b 84 12 80 f4 63 	mov    -0x7e9c0b80(%rdx,%rdx,1),%ax
ffffffff812c83d6:	81 
ffffffff812c83d7:	66 89 47 fe          	mov    %ax,-0x2(%rdi)
out_q:
	/* 1 <= q < 100 */
	r = q;
out_r:
	/* 1 <= r < 100 */
	*((u16 *)buf) = decpair[r];
ffffffff812c83db:	89 f0                	mov    %esi,%eax
	buf += r < 10 ? 1 : 2;
ffffffff812c83dd:	83 fe 0a             	cmp    $0xa,%esi
out_q:
	/* 1 <= q < 100 */
	r = q;
out_r:
	/* 1 <= r < 100 */
	*((u16 *)buf) = decpair[r];
ffffffff812c83e0:	66 8b 84 00 80 f4 63 	mov    -0x7e9c0b80(%rax,%rax,1),%ax
ffffffff812c83e7:	81 
	buf += r < 10 ? 1 : 2;
	return buf;
}
ffffffff812c83e8:	5d                   	pop    %rbp
out_q:
	/* 1 <= q < 100 */
	r = q;
out_r:
	/* 1 <= r < 100 */
	*((u16 *)buf) = decpair[r];
ffffffff812c83e9:	66 89 07             	mov    %ax,(%rdi)
	buf += r < 10 ? 1 : 2;
ffffffff812c83ec:	48 19 c0             	sbb    %rax,%rax
	return buf;
ffffffff812c83ef:	48 8d 44 07 02       	lea    0x2(%rdi,%rax,1),%rax
}
ffffffff812c83f4:	c3                   	retq   

ffffffff812c83f5 <put_dec_full8>:
char *put_dec_full8(char *buf, unsigned r)
{
	unsigned q;

	/* 0 <= r < 10^8 */
	q = (r * (u64)0x28f5c29) >> 32;
ffffffff812c83f5:	89 f2                	mov    %esi,%edx
}

#if BITS_PER_LONG == 64 && BITS_PER_LONG_LONG == 64
static noinline_for_stack
char *put_dec_full8(char *buf, unsigned r)
{
ffffffff812c83f7:	55                   	push   %rbp
	unsigned q;

	/* 0 <= r < 10^8 */
	q = (r * (u64)0x28f5c29) >> 32;
ffffffff812c83f8:	48 69 d2 29 5c 8f 02 	imul   $0x28f5c29,%rdx,%rdx
}

#if BITS_PER_LONG == 64 && BITS_PER_LONG_LONG == 64
static noinline_for_stack
char *put_dec_full8(char *buf, unsigned r)
{
ffffffff812c83ff:	48 89 e5             	mov    %rsp,%rbp

	/* 0 <= q < 100 */
	*((u16 *)buf) = decpair[q];
	buf += 2;
	return buf;
}
ffffffff812c8402:	5d                   	pop    %rbp
char *put_dec_full8(char *buf, unsigned r)
{
	unsigned q;

	/* 0 <= r < 10^8 */
	q = (r * (u64)0x28f5c29) >> 32;
ffffffff812c8403:	48 c1 ea 20          	shr    $0x20,%rdx
	*((u16 *)buf) = decpair[r - 100*q];
ffffffff812c8407:	6b c2 64             	imul   $0x64,%edx,%eax
ffffffff812c840a:	29 c6                	sub    %eax,%esi
ffffffff812c840c:	66 8b 84 36 80 f4 63 	mov    -0x7e9c0b80(%rsi,%rsi,1),%ax
ffffffff812c8413:	81 
ffffffff812c8414:	66 89 07             	mov    %ax,(%rdi)
	buf += 2;

	/* 0 <= q < 10^6 */
	r = (q * (u64)0x28f5c29) >> 32;
ffffffff812c8417:	48 69 c2 29 5c 8f 02 	imul   $0x28f5c29,%rdx,%rax
ffffffff812c841e:	48 c1 e8 20          	shr    $0x20,%rax
	*((u16 *)buf) = decpair[q - 100*r];
ffffffff812c8422:	6b c8 64             	imul   $0x64,%eax,%ecx
ffffffff812c8425:	29 ca                	sub    %ecx,%edx
ffffffff812c8427:	66 8b 94 12 80 f4 63 	mov    -0x7e9c0b80(%rdx,%rdx,1),%dx
ffffffff812c842e:	81 
ffffffff812c842f:	66 89 57 02          	mov    %dx,0x2(%rdi)
	buf += 2;

	/* 0 <= r < 10^4 */
	q = (r * 0x147b) >> 19;
ffffffff812c8433:	69 d0 7b 14 00 00    	imul   $0x147b,%eax,%edx
ffffffff812c8439:	c1 ea 13             	shr    $0x13,%edx
	*((u16 *)buf) = decpair[r - 100*q];
ffffffff812c843c:	6b ca 64             	imul   $0x64,%edx,%ecx
ffffffff812c843f:	29 c8                	sub    %ecx,%eax
ffffffff812c8441:	66 8b 84 00 80 f4 63 	mov    -0x7e9c0b80(%rax,%rax,1),%ax
ffffffff812c8448:	81 
ffffffff812c8449:	66 89 47 04          	mov    %ax,0x4(%rdi)
	buf += 2;

	/* 0 <= q < 100 */
	*((u16 *)buf) = decpair[q];
ffffffff812c844d:	66 8b 84 12 80 f4 63 	mov    -0x7e9c0b80(%rdx,%rdx,1),%ax
ffffffff812c8454:	81 
ffffffff812c8455:	66 89 47 06          	mov    %ax,0x6(%rdi)
	buf += 2;
ffffffff812c8459:	48 8d 47 08          	lea    0x8(%rdi),%rax
	return buf;
}
ffffffff812c845d:	c3                   	retq   

ffffffff812c845e <put_dec>:

static noinline_for_stack
char *put_dec(char *buf, unsigned long long n)
{
ffffffff812c845e:	55                   	push   %rbp
	if (n >= 100*1000*1000)
ffffffff812c845f:	48 81 fe ff e0 f5 05 	cmp    $0x5f5e0ff,%rsi
	return buf;
}

static noinline_for_stack
char *put_dec(char *buf, unsigned long long n)
{
ffffffff812c8466:	49 89 f0             	mov    %rsi,%r8
ffffffff812c8469:	48 89 e5             	mov    %rsp,%rbp
	if (n >= 100*1000*1000)
ffffffff812c846c:	76 50                	jbe    ffffffff812c84be <put_dec+0x60>
		buf = put_dec_full8(buf, do_div(n, 100*1000*1000));
ffffffff812c846e:	41 ba 00 e1 f5 05    	mov    $0x5f5e100,%r10d
ffffffff812c8474:	48 89 f0             	mov    %rsi,%rax
ffffffff812c8477:	31 d2                	xor    %edx,%edx
ffffffff812c8479:	49 f7 f2             	div    %r10
ffffffff812c847c:	49 89 c1             	mov    %rax,%r9
ffffffff812c847f:	89 d6                	mov    %edx,%esi
ffffffff812c8481:	e8 6f ff ff ff       	callq  ffffffff812c83f5 <put_dec_full8>
	/* 1 <= n <= 1.6e11 */
	if (n >= 100*1000*1000)
ffffffff812c8486:	49 81 f9 ff e0 f5 05 	cmp    $0x5f5e0ff,%r9

static noinline_for_stack
char *put_dec(char *buf, unsigned long long n)
{
	if (n >= 100*1000*1000)
		buf = put_dec_full8(buf, do_div(n, 100*1000*1000));
ffffffff812c848d:	48 89 c7             	mov    %rax,%rdi
	/* 1 <= n <= 1.6e11 */
	if (n >= 100*1000*1000)
ffffffff812c8490:	76 29                	jbe    ffffffff812c84bb <put_dec+0x5d>
		buf = put_dec_full8(buf, do_div(n, 100*1000*1000));
ffffffff812c8492:	48 b9 00 00 c1 6f f2 	movabs $0x2386f26fc10000,%rcx
ffffffff812c8499:	86 23 00 
ffffffff812c849c:	4c 89 c0             	mov    %r8,%rax
ffffffff812c849f:	31 d2                	xor    %edx,%edx
ffffffff812c84a1:	48 f7 f1             	div    %rcx
ffffffff812c84a4:	31 d2                	xor    %edx,%edx
ffffffff812c84a6:	49 89 c0             	mov    %rax,%r8
ffffffff812c84a9:	4c 89 c8             	mov    %r9,%rax
ffffffff812c84ac:	49 f7 f2             	div    %r10
ffffffff812c84af:	89 d6                	mov    %edx,%esi
ffffffff812c84b1:	e8 3f ff ff ff       	callq  ffffffff812c83f5 <put_dec_full8>
ffffffff812c84b6:	48 89 c7             	mov    %rax,%rdi
ffffffff812c84b9:	eb 03                	jmp    ffffffff812c84be <put_dec+0x60>
ffffffff812c84bb:	4d 89 c8             	mov    %r9,%r8
	/* 1 <= n < 1e8 */
	return put_dec_trunc8(buf, n);
ffffffff812c84be:	44 89 c6             	mov    %r8d,%esi
ffffffff812c84c1:	e8 99 fe ff ff       	callq  ffffffff812c835f <put_dec_trunc8>
}
ffffffff812c84c6:	5d                   	pop    %rbp
ffffffff812c84c7:	c3                   	retq   

ffffffff812c84c8 <ip4_string>:
	return string(buf, end, mac_addr, spec);
}

static noinline_for_stack
char *ip4_string(char *p, const u8 *addr, const char *fmt)
{
ffffffff812c84c8:	55                   	push   %rbp
ffffffff812c84c9:	49 89 f8             	mov    %rdi,%r8
ffffffff812c84cc:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c84cf:	41 55                	push   %r13
ffffffff812c84d1:	41 54                	push   %r12
ffffffff812c84d3:	53                   	push   %rbx
ffffffff812c84d4:	49 89 f4             	mov    %rsi,%r12
ffffffff812c84d7:	48 83 ec 10          	sub    $0x10,%rsp
	int i;
	bool leading_zeros = (fmt[0] == 'i');
	int index;
	int step;

	switch (fmt[2]) {
ffffffff812c84db:	8a 42 02             	mov    0x2(%rdx),%al

static noinline_for_stack
char *ip4_string(char *p, const u8 *addr, const char *fmt)
{
	int i;
	bool leading_zeros = (fmt[0] == 'i');
ffffffff812c84de:	8a 1a                	mov    (%rdx),%bl
	int index;
	int step;

	switch (fmt[2]) {
ffffffff812c84e0:	3c 68                	cmp    $0x68,%al
ffffffff812c84e2:	74 0f                	je     ffffffff812c84f3 <ip4_string+0x2b>
ffffffff812c84e4:	3c 6c                	cmp    $0x6c,%al
ffffffff812c84e6:	74 0b                	je     ffffffff812c84f3 <ip4_string+0x2b>
		break;
	case 'n':
	case 'b':
	default:
		index = 0;
		step = 1;
ffffffff812c84e8:	41 bb 01 00 00 00    	mov    $0x1,%r11d
		step = -1;
		break;
	case 'n':
	case 'b':
	default:
		index = 0;
ffffffff812c84ee:	45 31 d2             	xor    %r10d,%r10d
ffffffff812c84f1:	eb 0a                	jmp    ffffffff812c84fd <ip4_string+0x35>
#ifdef __BIG_ENDIAN
		index = 0;
		step = 1;
#else
		index = 3;
		step = -1;
ffffffff812c84f3:	41 83 cb ff          	or     $0xffffffff,%r11d
	case 'h':
#ifdef __BIG_ENDIAN
		index = 0;
		step = 1;
#else
		index = 3;
ffffffff812c84f7:	41 ba 03 00 00 00    	mov    $0x3,%r10d
		step = 1;
		break;
	}
	for (i = 0; i < 4; i++) {
		char temp[4] __aligned(2);	/* hold each IP quad in reverse order */
		int digits = put_dec_trunc8(temp, addr[index]) - temp;
ffffffff812c84fd:	4c 8d 6d e4          	lea    -0x1c(%rbp),%r13
	default:
		index = 0;
		step = 1;
		break;
	}
	for (i = 0; i < 4; i++) {
ffffffff812c8501:	45 31 c9             	xor    %r9d,%r9d
		char temp[4] __aligned(2);	/* hold each IP quad in reverse order */
		int digits = put_dec_trunc8(temp, addr[index]) - temp;
ffffffff812c8504:	49 63 c2             	movslq %r10d,%rax
ffffffff812c8507:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c850a:	41 0f b6 34 04       	movzbl (%r12,%rax,1),%esi
ffffffff812c850f:	e8 4b fe ff ff       	callq  ffffffff812c835f <put_dec_trunc8>
ffffffff812c8514:	4c 29 e8             	sub    %r13,%rax
		if (leading_zeros) {
			if (digits < 3)
ffffffff812c8517:	80 fb 69             	cmp    $0x69,%bl
		step = 1;
		break;
	}
	for (i = 0; i < 4; i++) {
		char temp[4] __aligned(2);	/* hold each IP quad in reverse order */
		int digits = put_dec_trunc8(temp, addr[index]) - temp;
ffffffff812c851a:	89 c2                	mov    %eax,%edx
		if (leading_zeros) {
			if (digits < 3)
ffffffff812c851c:	75 1c                	jne    ffffffff812c853a <ip4_string+0x72>
ffffffff812c851e:	83 f8 02             	cmp    $0x2,%eax
ffffffff812c8521:	7f 17                	jg     ffffffff812c853a <ip4_string+0x72>
				*p++ = '0';
			if (digits < 2)
ffffffff812c8523:	83 f8 02             	cmp    $0x2,%eax
	for (i = 0; i < 4; i++) {
		char temp[4] __aligned(2);	/* hold each IP quad in reverse order */
		int digits = put_dec_trunc8(temp, addr[index]) - temp;
		if (leading_zeros) {
			if (digits < 3)
				*p++ = '0';
ffffffff812c8526:	41 c6 00 30          	movb   $0x30,(%r8)
			if (digits < 2)
ffffffff812c852a:	75 05                	jne    ffffffff812c8531 <ip4_string+0x69>
	for (i = 0; i < 4; i++) {
		char temp[4] __aligned(2);	/* hold each IP quad in reverse order */
		int digits = put_dec_trunc8(temp, addr[index]) - temp;
		if (leading_zeros) {
			if (digits < 3)
				*p++ = '0';
ffffffff812c852c:	49 ff c0             	inc    %r8
ffffffff812c852f:	eb 09                	jmp    ffffffff812c853a <ip4_string+0x72>
			if (digits < 2)
				*p++ = '0';
ffffffff812c8531:	41 c6 40 01 30       	movb   $0x30,0x1(%r8)
ffffffff812c8536:	49 83 c0 02          	add    $0x2,%r8
ffffffff812c853a:	4c 89 c1             	mov    %r8,%rcx
		}
		/* reverse the digits in the quad */
		while (digits--)
ffffffff812c853d:	ff ca                	dec    %edx
ffffffff812c853f:	83 fa ff             	cmp    $0xffffffff,%edx
ffffffff812c8542:	74 11                	je     ffffffff812c8555 <ip4_string+0x8d>
			*p++ = temp[digits];
ffffffff812c8544:	48 63 f2             	movslq %edx,%rsi
ffffffff812c8547:	48 ff c1             	inc    %rcx
ffffffff812c854a:	40 8a 74 35 e4       	mov    -0x1c(%rbp,%rsi,1),%sil
ffffffff812c854f:	40 88 71 ff          	mov    %sil,-0x1(%rcx)
ffffffff812c8553:	eb e8                	jmp    ffffffff812c853d <ip4_string+0x75>
ffffffff812c8555:	89 c0                	mov    %eax,%eax
ffffffff812c8557:	49 01 c0             	add    %rax,%r8
		if (i < 3)
ffffffff812c855a:	41 83 f9 03          	cmp    $0x3,%r9d
ffffffff812c855e:	74 07                	je     ffffffff812c8567 <ip4_string+0x9f>
			*p++ = '.';
ffffffff812c8560:	41 c6 00 2e          	movb   $0x2e,(%r8)
ffffffff812c8564:	49 ff c0             	inc    %r8
	default:
		index = 0;
		step = 1;
		break;
	}
	for (i = 0; i < 4; i++) {
ffffffff812c8567:	41 ff c1             	inc    %r9d
		/* reverse the digits in the quad */
		while (digits--)
			*p++ = temp[digits];
		if (i < 3)
			*p++ = '.';
		index += step;
ffffffff812c856a:	45 01 da             	add    %r11d,%r10d
	default:
		index = 0;
		step = 1;
		break;
	}
	for (i = 0; i < 4; i++) {
ffffffff812c856d:	41 83 f9 04          	cmp    $0x4,%r9d
ffffffff812c8571:	75 91                	jne    ffffffff812c8504 <ip4_string+0x3c>
			*p++ = temp[digits];
		if (i < 3)
			*p++ = '.';
		index += step;
	}
	*p = '\0';
ffffffff812c8573:	41 c6 00 00          	movb   $0x0,(%r8)

	return p;
}
ffffffff812c8577:	4c 89 c0             	mov    %r8,%rax
ffffffff812c857a:	5a                   	pop    %rdx
ffffffff812c857b:	59                   	pop    %rcx
ffffffff812c857c:	5b                   	pop    %rbx
ffffffff812c857d:	41 5c                	pop    %r12
ffffffff812c857f:	41 5d                	pop    %r13
ffffffff812c8581:	5d                   	pop    %rbp
ffffffff812c8582:	c3                   	retq   

ffffffff812c8583 <simple_strtoull>:
 * @base: The number base to use
 *
 * This function is obsolete. Please use kstrtoull instead.
 */
unsigned long long simple_strtoull(const char *cp, char **endp, unsigned int base)
{
ffffffff812c8583:	55                   	push   %rbp
ffffffff812c8584:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c8587:	41 54                	push   %r12
ffffffff812c8589:	53                   	push   %rbx
ffffffff812c858a:	48 89 f3             	mov    %rsi,%rbx
	unsigned long long result;
	unsigned int rv;

	cp = _parse_integer_fixup_radix(cp, &base);
ffffffff812c858d:	48 8d 75 dc          	lea    -0x24(%rbp),%rsi
 * @base: The number base to use
 *
 * This function is obsolete. Please use kstrtoull instead.
 */
unsigned long long simple_strtoull(const char *cp, char **endp, unsigned int base)
{
ffffffff812c8591:	48 83 ec 20          	sub    $0x20,%rsp
ffffffff812c8595:	89 55 dc             	mov    %edx,-0x24(%rbp)
	unsigned long long result;
	unsigned int rv;

	cp = _parse_integer_fixup_radix(cp, &base);
ffffffff812c8598:	e8 60 b6 00 00       	callq  ffffffff812d3bfd <_parse_integer_fixup_radix>
	rv = _parse_integer(cp, base, &result);
ffffffff812c859d:	8b 75 dc             	mov    -0x24(%rbp),%esi
ffffffff812c85a0:	48 8d 55 e8          	lea    -0x18(%rbp),%rdx
ffffffff812c85a4:	48 89 c7             	mov    %rax,%rdi
unsigned long long simple_strtoull(const char *cp, char **endp, unsigned int base)
{
	unsigned long long result;
	unsigned int rv;

	cp = _parse_integer_fixup_radix(cp, &base);
ffffffff812c85a7:	49 89 c4             	mov    %rax,%r12
	rv = _parse_integer(cp, base, &result);
ffffffff812c85aa:	e8 aa b6 00 00       	callq  ffffffff812d3c59 <_parse_integer>
	/* FIXME */
	cp += (rv & ~KSTRTOX_OVERFLOW);
ffffffff812c85af:	25 ff ff ff 7f       	and    $0x7fffffff,%eax
ffffffff812c85b4:	4c 01 e0             	add    %r12,%rax

	if (endp)
ffffffff812c85b7:	48 85 db             	test   %rbx,%rbx
ffffffff812c85ba:	74 03                	je     ffffffff812c85bf <simple_strtoull+0x3c>
		*endp = (char *)cp;
ffffffff812c85bc:	48 89 03             	mov    %rax,(%rbx)

	return result;
}
ffffffff812c85bf:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
ffffffff812c85c3:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812c85c7:	5b                   	pop    %rbx
ffffffff812c85c8:	41 5c                	pop    %r12
ffffffff812c85ca:	5d                   	pop    %rbp
ffffffff812c85cb:	c3                   	retq   

ffffffff812c85cc <simple_strtoul>:
 * @base: The number base to use
 *
 * This function is obsolete. Please use kstrtoul instead.
 */
unsigned long simple_strtoul(const char *cp, char **endp, unsigned int base)
{
ffffffff812c85cc:	55                   	push   %rbp
ffffffff812c85cd:	48 89 e5             	mov    %rsp,%rbp
	return simple_strtoull(cp, endp, base);
ffffffff812c85d0:	e8 ae ff ff ff       	callq  ffffffff812c8583 <simple_strtoull>
}
ffffffff812c85d5:	5d                   	pop    %rbp
ffffffff812c85d6:	c3                   	retq   

ffffffff812c85d7 <format_decode>:
 * @precision: precision of a number
 * @qualifier: qualifier of a number (long, size_t, ...)
 */
static noinline_for_stack
int format_decode(const char *fmt, struct printf_spec *spec)
{
ffffffff812c85d7:	55                   	push   %rbp
ffffffff812c85d8:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c85db:	41 54                	push   %r12
ffffffff812c85dd:	53                   	push   %rbx
ffffffff812c85de:	49 89 fc             	mov    %rdi,%r12
ffffffff812c85e1:	48 89 f3             	mov    %rsi,%rbx
ffffffff812c85e4:	48 83 ec 10          	sub    $0x10,%rsp
	const char *start = fmt;

	/* we finished early by reading the field width */
	if (spec->type == FORMAT_TYPE_WIDTH) {
ffffffff812c85e8:	8a 06                	mov    (%rsi),%al
 * @precision: precision of a number
 * @qualifier: qualifier of a number (long, size_t, ...)
 */
static noinline_for_stack
int format_decode(const char *fmt, struct printf_spec *spec)
{
ffffffff812c85ea:	48 89 7d e8          	mov    %rdi,-0x18(%rbp)
	const char *start = fmt;

	/* we finished early by reading the field width */
	if (spec->type == FORMAT_TYPE_WIDTH) {
ffffffff812c85ee:	3c 01                	cmp    $0x1,%al
ffffffff812c85f0:	75 18                	jne    ffffffff812c860a <format_decode+0x33>
		if (spec->field_width < 0) {
ffffffff812c85f2:	66 8b 46 04          	mov    0x4(%rsi),%ax
ffffffff812c85f6:	66 85 c0             	test   %ax,%ax
ffffffff812c85f9:	79 0a                	jns    ffffffff812c8605 <format_decode+0x2e>
			spec->field_width = -spec->field_width;
			spec->flags |= LEFT;
ffffffff812c85fb:	80 4e 01 02          	orb    $0x2,0x1(%rsi)
	const char *start = fmt;

	/* we finished early by reading the field width */
	if (spec->type == FORMAT_TYPE_WIDTH) {
		if (spec->field_width < 0) {
			spec->field_width = -spec->field_width;
ffffffff812c85ff:	f7 d8                	neg    %eax
ffffffff812c8601:	66 89 46 04          	mov    %ax,0x4(%rsi)
			spec->flags |= LEFT;
		}
		spec->type = FORMAT_TYPE_NONE;
ffffffff812c8605:	c6 03 00             	movb   $0x0,(%rbx)
		goto precision;
ffffffff812c8608:	eb 63                	jmp    ffffffff812c866d <format_decode+0x96>
	}

	/* we finished early by reading the precision */
	if (spec->type == FORMAT_TYPE_PRECISION) {
ffffffff812c860a:	3c 02                	cmp    $0x2,%al
ffffffff812c860c:	75 15                	jne    ffffffff812c8623 <format_decode+0x4c>
		if (spec->precision < 0)
ffffffff812c860e:	66 83 7e 06 00       	cmpw   $0x0,0x6(%rsi)
ffffffff812c8613:	79 06                	jns    ffffffff812c861b <format_decode+0x44>
			spec->precision = 0;
ffffffff812c8615:	66 c7 46 06 00 00    	movw   $0x0,0x6(%rsi)

		spec->type = FORMAT_TYPE_NONE;
ffffffff812c861b:	c6 03 00             	movb   $0x0,(%rbx)
		goto qualifier;
ffffffff812c861e:	e9 99 00 00 00       	jmpq   ffffffff812c86bc <format_decode+0xe5>
	}

	/* By default */
	spec->type = FORMAT_TYPE_NONE;
ffffffff812c8623:	c6 06 00             	movb   $0x0,(%rsi)

	for (; *fmt ; ++fmt) {
ffffffff812c8626:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
ffffffff812c862a:	8a 10                	mov    (%rax),%dl
ffffffff812c862c:	84 d2                	test   %dl,%dl
ffffffff812c862e:	0f 84 f0 01 00 00    	je     ffffffff812c8824 <format_decode+0x24d>
		if (*fmt == '%')
ffffffff812c8634:	80 fa 25             	cmp    $0x25,%dl
ffffffff812c8637:	0f 84 f1 01 00 00    	je     ffffffff812c882e <format_decode+0x257>
	}

	/* By default */
	spec->type = FORMAT_TYPE_NONE;

	for (; *fmt ; ++fmt) {
ffffffff812c863d:	48 ff c0             	inc    %rax
ffffffff812c8640:	48 89 45 e8          	mov    %rax,-0x18(%rbp)
ffffffff812c8644:	eb e0                	jmp    ffffffff812c8626 <format_decode+0x4f>
		if (!found)
			break;
	}

	/* get field width */
	spec->field_width = -1;
ffffffff812c8646:	66 c7 43 04 ff ff    	movw   $0xffff,0x4(%rbx)

	if (isdigit(*fmt))
ffffffff812c864c:	0f b6 48 01          	movzbl 0x1(%rax),%ecx
ffffffff812c8650:	f6 81 60 ef 63 81 04 	testb  $0x4,-0x7e9c10a0(%rcx)
ffffffff812c8657:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c865a:	0f 84 27 02 00 00    	je     ffffffff812c8887 <format_decode+0x2b0>
		spec->field_width = skip_atoi(&fmt);
ffffffff812c8660:	48 8d 7d e8          	lea    -0x18(%rbp),%rdi
ffffffff812c8664:	e8 cd fc ff ff       	callq  ffffffff812c8336 <skip_atoi>
ffffffff812c8669:	66 89 43 04          	mov    %ax,0x4(%rbx)
	}

precision:
	/* get the precision */
	spec->precision = -1;
	if (*fmt == '.') {
ffffffff812c866d:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
		return ++fmt - start;
	}

precision:
	/* get the precision */
	spec->precision = -1;
ffffffff812c8671:	66 c7 43 06 ff ff    	movw   $0xffff,0x6(%rbx)
	if (*fmt == '.') {
ffffffff812c8677:	80 38 2e             	cmpb   $0x2e,(%rax)
ffffffff812c867a:	75 40                	jne    ffffffff812c86bc <format_decode+0xe5>
		++fmt;
ffffffff812c867c:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812c8680:	48 89 55 e8          	mov    %rdx,-0x18(%rbp)
		if (isdigit(*fmt)) {
ffffffff812c8684:	0f b6 48 01          	movzbl 0x1(%rax),%ecx
ffffffff812c8688:	f6 81 60 ef 63 81 04 	testb  $0x4,-0x7e9c10a0(%rcx)
ffffffff812c868f:	74 1a                	je     ffffffff812c86ab <format_decode+0xd4>
			spec->precision = skip_atoi(&fmt);
ffffffff812c8691:	48 8d 7d e8          	lea    -0x18(%rbp),%rdi
ffffffff812c8695:	e8 9c fc ff ff       	callq  ffffffff812c8336 <skip_atoi>
ffffffff812c869a:	ba 00 00 00 00       	mov    $0x0,%edx
ffffffff812c869f:	66 85 c0             	test   %ax,%ax
ffffffff812c86a2:	0f 48 c2             	cmovs  %edx,%eax
ffffffff812c86a5:	66 89 43 06          	mov    %ax,0x6(%rbx)
ffffffff812c86a9:	eb 11                	jmp    ffffffff812c86bc <format_decode+0xe5>
			if (spec->precision < 0)
				spec->precision = 0;
		} else if (*fmt == '*') {
ffffffff812c86ab:	80 f9 2a             	cmp    $0x2a,%cl
ffffffff812c86ae:	75 0c                	jne    ffffffff812c86bc <format_decode+0xe5>
			/* it's the next argument */
			spec->type = FORMAT_TYPE_PRECISION;
ffffffff812c86b0:	c6 03 02             	movb   $0x2,(%rbx)
			return ++fmt - start;
ffffffff812c86b3:	48 83 c0 02          	add    $0x2,%rax
ffffffff812c86b7:	e9 68 01 00 00       	jmpq   ffffffff812c8824 <format_decode+0x24d>
	}

qualifier:
	/* get the conversion qualifier */
	spec->qualifier = -1;
	if (*fmt == 'h' || _tolower(*fmt) == 'l' ||
ffffffff812c86bc:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
		}
	}

qualifier:
	/* get the conversion qualifier */
	spec->qualifier = -1;
ffffffff812c86c0:	c6 43 03 ff          	movb   $0xff,0x3(%rbx)
	if (*fmt == 'h' || _tolower(*fmt) == 'l' ||
ffffffff812c86c4:	8a 10                	mov    (%rax),%dl
ffffffff812c86c6:	80 fa 68             	cmp    $0x68,%dl
ffffffff812c86c9:	74 1c                	je     ffffffff812c86e7 <format_decode+0x110>
 * Fast implementation of tolower() for internal usage. Do not use in your
 * code.
 */
static inline char _tolower(const char c)
{
	return c | 0x20;
ffffffff812c86cb:	88 d1                	mov    %dl,%cl
ffffffff812c86cd:	83 c9 20             	or     $0x20,%ecx
	    _tolower(*fmt) == 'z' || *fmt == 't') {
ffffffff812c86d0:	80 f9 6c             	cmp    $0x6c,%cl
ffffffff812c86d3:	40 0f 94 c6          	sete   %sil
ffffffff812c86d7:	80 f9 7a             	cmp    $0x7a,%cl
ffffffff812c86da:	0f 94 c1             	sete   %cl
ffffffff812c86dd:	40 08 ce             	or     %cl,%sil
ffffffff812c86e0:	75 05                	jne    ffffffff812c86e7 <format_decode+0x110>
ffffffff812c86e2:	80 fa 74             	cmp    $0x74,%dl
ffffffff812c86e5:	75 35                	jne    ffffffff812c871c <format_decode+0x145>
		spec->qualifier = *fmt++;
ffffffff812c86e7:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812c86eb:	48 89 55 e8          	mov    %rdx,-0x18(%rbp)
ffffffff812c86ef:	0f b6 30             	movzbl (%rax),%esi
ffffffff812c86f2:	40 88 73 03          	mov    %sil,0x3(%rbx)
		if (unlikely(spec->qualifier == *fmt)) {
ffffffff812c86f6:	0f be 48 01          	movsbl 0x1(%rax),%ecx
ffffffff812c86fa:	39 ce                	cmp    %ecx,%esi
ffffffff812c86fc:	75 1e                	jne    ffffffff812c871c <format_decode+0x145>
			if (spec->qualifier == 'l') {
ffffffff812c86fe:	40 80 fe 6c          	cmp    $0x6c,%sil
ffffffff812c8702:	75 06                	jne    ffffffff812c870a <format_decode+0x133>
				spec->qualifier = 'L';
ffffffff812c8704:	c6 43 03 4c          	movb   $0x4c,0x3(%rbx)
ffffffff812c8708:	eb 0a                	jmp    ffffffff812c8714 <format_decode+0x13d>
				++fmt;
			} else if (spec->qualifier == 'h') {
ffffffff812c870a:	40 80 fe 68          	cmp    $0x68,%sil
ffffffff812c870e:	75 0c                	jne    ffffffff812c871c <format_decode+0x145>
				spec->qualifier = 'H';
ffffffff812c8710:	c6 43 03 48          	movb   $0x48,0x3(%rbx)
				++fmt;
ffffffff812c8714:	48 83 c0 02          	add    $0x2,%rax
ffffffff812c8718:	48 89 45 e8          	mov    %rax,-0x18(%rbp)
		}
	}

	/* default base */
	spec->base = 10;
	switch (*fmt) {
ffffffff812c871c:	48 8b 4d e8          	mov    -0x18(%rbp),%rcx
			}
		}
	}

	/* default base */
	spec->base = 10;
ffffffff812c8720:	c6 43 02 0a          	movb   $0xa,0x2(%rbx)
	switch (*fmt) {
ffffffff812c8724:	8a 01                	mov    (%rcx),%al
ffffffff812c8726:	3c 6e                	cmp    $0x6e,%al
ffffffff812c8728:	74 6d                	je     ffffffff812c8797 <format_decode+0x1c0>
ffffffff812c872a:	7f 1d                	jg     ffffffff812c8749 <format_decode+0x172>
ffffffff812c872c:	3c 63                	cmp    $0x63,%al
ffffffff812c872e:	74 3d                	je     ffffffff812c876d <format_decode+0x196>
ffffffff812c8730:	7f 0d                	jg     ffffffff812c873f <format_decode+0x168>
ffffffff812c8732:	3c 25                	cmp    $0x25,%al
ffffffff812c8734:	74 47                	je     ffffffff812c877d <format_decode+0x1a6>
ffffffff812c8736:	3c 58                	cmp    $0x58,%al
ffffffff812c8738:	74 51                	je     ffffffff812c878b <format_decode+0x1b4>
ffffffff812c873a:	e9 82 00 00 00       	jmpq   ffffffff812c87c1 <format_decode+0x1ea>
ffffffff812c873f:	3c 64                	cmp    $0x64,%al
ffffffff812c8741:	74 4e                	je     ffffffff812c8791 <format_decode+0x1ba>
ffffffff812c8743:	3c 69                	cmp    $0x69,%al
ffffffff812c8745:	74 4a                	je     ffffffff812c8791 <format_decode+0x1ba>
ffffffff812c8747:	eb 78                	jmp    ffffffff812c87c1 <format_decode+0x1ea>
ffffffff812c8749:	3c 73                	cmp    $0x73,%al
ffffffff812c874b:	74 28                	je     ffffffff812c8775 <format_decode+0x19e>
ffffffff812c874d:	7f 10                	jg     ffffffff812c875f <format_decode+0x188>
ffffffff812c874f:	3c 6f                	cmp    $0x6f,%al
ffffffff812c8751:	74 32                	je     ffffffff812c8785 <format_decode+0x1ae>
ffffffff812c8753:	3c 70                	cmp    $0x70,%al
ffffffff812c8755:	75 6a                	jne    ffffffff812c87c1 <format_decode+0x1ea>
	case 's':
		spec->type = FORMAT_TYPE_STR;
		return ++fmt - start;

	case 'p':
		spec->type = FORMAT_TYPE_PTR;
ffffffff812c8757:	c6 03 05             	movb   $0x5,(%rbx)
ffffffff812c875a:	e9 c1 00 00 00       	jmpq   ffffffff812c8820 <format_decode+0x249>
		}
	}

	/* default base */
	spec->base = 10;
	switch (*fmt) {
ffffffff812c875f:	3c 75                	cmp    $0x75,%al
ffffffff812c8761:	74 66                	je     ffffffff812c87c9 <format_decode+0x1f2>
ffffffff812c8763:	3c 78                	cmp    $0x78,%al
ffffffff812c8765:	75 5a                	jne    ffffffff812c87c1 <format_decode+0x1ea>
	case 'o':
		spec->base = 8;
		break;

	case 'x':
		spec->flags |= SMALL;
ffffffff812c8767:	80 4b 01 20          	orb    $0x20,0x1(%rbx)
ffffffff812c876b:	eb 1e                	jmp    ffffffff812c878b <format_decode+0x1b4>

	/* default base */
	spec->base = 10;
	switch (*fmt) {
	case 'c':
		spec->type = FORMAT_TYPE_CHAR;
ffffffff812c876d:	c6 03 03             	movb   $0x3,(%rbx)
ffffffff812c8770:	e9 ab 00 00 00       	jmpq   ffffffff812c8820 <format_decode+0x249>
		return ++fmt - start;

	case 's':
		spec->type = FORMAT_TYPE_STR;
ffffffff812c8775:	c6 03 04             	movb   $0x4,(%rbx)
ffffffff812c8778:	e9 a3 00 00 00       	jmpq   ffffffff812c8820 <format_decode+0x249>
	case 'p':
		spec->type = FORMAT_TYPE_PTR;
		return ++fmt - start;

	case '%':
		spec->type = FORMAT_TYPE_PERCENT_CHAR;
ffffffff812c877d:	c6 03 06             	movb   $0x6,(%rbx)
ffffffff812c8780:	e9 9b 00 00 00       	jmpq   ffffffff812c8820 <format_decode+0x249>
		return ++fmt - start;

	/* integer number formats - set up the flags and "break" */
	case 'o':
		spec->base = 8;
ffffffff812c8785:	c6 43 02 08          	movb   $0x8,0x2(%rbx)
		break;
ffffffff812c8789:	eb 3e                	jmp    ffffffff812c87c9 <format_decode+0x1f2>

	case 'x':
		spec->flags |= SMALL;

	case 'X':
		spec->base = 16;
ffffffff812c878b:	c6 43 02 10          	movb   $0x10,0x2(%rbx)
		break;
ffffffff812c878f:	eb 38                	jmp    ffffffff812c87c9 <format_decode+0x1f2>

	case 'd':
	case 'i':
		spec->flags |= SIGN;
ffffffff812c8791:	80 4b 01 01          	orb    $0x1,0x1(%rbx)
ffffffff812c8795:	eb 32                	jmp    ffffffff812c87c9 <format_decode+0x1f2>
		/*
		 * Since %n poses a greater security risk than utility, treat
		 * it as an invalid format specifier. Warn about its use so
		 * that new instances don't get added.
		 */
		WARN_ONCE(1, "Please remove ignored %%n in '%s'\n", fmt);
ffffffff812c8797:	80 3d 50 eb 78 00 00 	cmpb   $0x0,0x78eb50(%rip)        # ffffffff81a572ee <__warned.45999>
ffffffff812c879e:	75 21                	jne    ffffffff812c87c1 <format_decode+0x1ea>
ffffffff812c87a0:	48 c7 c2 4a 6d 7b 81 	mov    $0xffffffff817b6d4a,%rdx
ffffffff812c87a7:	be ef 06 00 00       	mov    $0x6ef,%esi
ffffffff812c87ac:	48 c7 c7 6d 6d 7b 81 	mov    $0xffffffff817b6d6d,%rdi
ffffffff812c87b3:	31 c0                	xor    %eax,%eax
ffffffff812c87b5:	e8 7c db d9 ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
ffffffff812c87ba:	c6 05 2d eb 78 00 01 	movb   $0x1,0x78eb2d(%rip)        # ffffffff81a572ee <__warned.45999>
		/* Fall-through */

	default:
		spec->type = FORMAT_TYPE_INVALID;
ffffffff812c87c1:	c6 03 07             	movb   $0x7,(%rbx)
		return fmt - start;
ffffffff812c87c4:	8b 45 e8             	mov    -0x18(%rbp),%eax
ffffffff812c87c7:	eb 5b                	jmp    ffffffff812c8824 <format_decode+0x24d>
	}

	if (spec->qualifier == 'L')
ffffffff812c87c9:	8a 53 03             	mov    0x3(%rbx),%dl
ffffffff812c87cc:	80 fa 4c             	cmp    $0x4c,%dl
ffffffff812c87cf:	75 05                	jne    ffffffff812c87d6 <format_decode+0x1ff>
		spec->type = FORMAT_TYPE_LONG_LONG;
ffffffff812c87d1:	c6 03 08             	movb   $0x8,(%rbx)
ffffffff812c87d4:	eb 4a                	jmp    ffffffff812c8820 <format_decode+0x249>
	else if (spec->qualifier == 'l') {
ffffffff812c87d6:	80 fa 6c             	cmp    $0x6c,%dl
ffffffff812c87d9:	75 0b                	jne    ffffffff812c87e6 <format_decode+0x20f>
		BUILD_BUG_ON(FORMAT_TYPE_ULONG + SIGN != FORMAT_TYPE_LONG);
		spec->type = FORMAT_TYPE_ULONG + (spec->flags & SIGN);
ffffffff812c87db:	8a 43 01             	mov    0x1(%rbx),%al
ffffffff812c87de:	83 e0 01             	and    $0x1,%eax
ffffffff812c87e1:	83 c0 09             	add    $0x9,%eax
ffffffff812c87e4:	eb 38                	jmp    ffffffff812c881e <format_decode+0x247>
	} else if (_tolower(spec->qualifier) == 'z') {
ffffffff812c87e6:	88 d0                	mov    %dl,%al
ffffffff812c87e8:	83 c8 20             	or     $0x20,%eax
ffffffff812c87eb:	3c 7a                	cmp    $0x7a,%al
ffffffff812c87ed:	75 05                	jne    ffffffff812c87f4 <format_decode+0x21d>
		spec->type = FORMAT_TYPE_SIZE_T;
ffffffff812c87ef:	c6 03 11             	movb   $0x11,(%rbx)
ffffffff812c87f2:	eb 2c                	jmp    ffffffff812c8820 <format_decode+0x249>
	} else if (spec->qualifier == 't') {
ffffffff812c87f4:	80 fa 74             	cmp    $0x74,%dl
ffffffff812c87f7:	75 05                	jne    ffffffff812c87fe <format_decode+0x227>
		spec->type = FORMAT_TYPE_PTRDIFF;
ffffffff812c87f9:	c6 03 12             	movb   $0x12,(%rbx)
ffffffff812c87fc:	eb 22                	jmp    ffffffff812c8820 <format_decode+0x249>
	} else if (spec->qualifier == 'H') {
ffffffff812c87fe:	80 fa 48             	cmp    $0x48,%dl
ffffffff812c8801:	8a 43 01             	mov    0x1(%rbx),%al
ffffffff812c8804:	75 08                	jne    ffffffff812c880e <format_decode+0x237>
		BUILD_BUG_ON(FORMAT_TYPE_UBYTE + SIGN != FORMAT_TYPE_BYTE);
		spec->type = FORMAT_TYPE_UBYTE + (spec->flags & SIGN);
ffffffff812c8806:	83 e0 01             	and    $0x1,%eax
ffffffff812c8809:	83 c0 0b             	add    $0xb,%eax
ffffffff812c880c:	eb 10                	jmp    ffffffff812c881e <format_decode+0x247>
	} else if (spec->qualifier == 'h') {
		BUILD_BUG_ON(FORMAT_TYPE_USHORT + SIGN != FORMAT_TYPE_SHORT);
		spec->type = FORMAT_TYPE_USHORT + (spec->flags & SIGN);
ffffffff812c880e:	83 e0 01             	and    $0x1,%eax
	} else if (spec->qualifier == 't') {
		spec->type = FORMAT_TYPE_PTRDIFF;
	} else if (spec->qualifier == 'H') {
		BUILD_BUG_ON(FORMAT_TYPE_UBYTE + SIGN != FORMAT_TYPE_BYTE);
		spec->type = FORMAT_TYPE_UBYTE + (spec->flags & SIGN);
	} else if (spec->qualifier == 'h') {
ffffffff812c8811:	80 fa 68             	cmp    $0x68,%dl
ffffffff812c8814:	75 05                	jne    ffffffff812c881b <format_decode+0x244>
		BUILD_BUG_ON(FORMAT_TYPE_USHORT + SIGN != FORMAT_TYPE_SHORT);
		spec->type = FORMAT_TYPE_USHORT + (spec->flags & SIGN);
ffffffff812c8816:	83 c0 0d             	add    $0xd,%eax
ffffffff812c8819:	eb 03                	jmp    ffffffff812c881e <format_decode+0x247>
	} else {
		BUILD_BUG_ON(FORMAT_TYPE_UINT + SIGN != FORMAT_TYPE_INT);
		spec->type = FORMAT_TYPE_UINT + (spec->flags & SIGN);
ffffffff812c881b:	83 c0 0f             	add    $0xf,%eax
ffffffff812c881e:	88 03                	mov    %al,(%rbx)
	}

	return ++fmt - start;
ffffffff812c8820:	48 8d 41 01          	lea    0x1(%rcx),%rax
}
ffffffff812c8824:	5a                   	pop    %rdx
	} else {
		BUILD_BUG_ON(FORMAT_TYPE_UINT + SIGN != FORMAT_TYPE_INT);
		spec->type = FORMAT_TYPE_UINT + (spec->flags & SIGN);
	}

	return ++fmt - start;
ffffffff812c8825:	44 29 e0             	sub    %r12d,%eax
}
ffffffff812c8828:	59                   	pop    %rcx
ffffffff812c8829:	5b                   	pop    %rbx
ffffffff812c882a:	41 5c                	pop    %r12
ffffffff812c882c:	5d                   	pop    %rbp
ffffffff812c882d:	c3                   	retq   
		if (*fmt == '%')
			break;
	}

	/* Return the current non-format string */
	if (fmt != start || !*fmt)
ffffffff812c882e:	49 39 c4             	cmp    %rax,%r12
ffffffff812c8831:	75 f1                	jne    ffffffff812c8824 <format_decode+0x24d>
		return fmt - start;

	/* Process flags */
	spec->flags = 0;
ffffffff812c8833:	c6 43 01 00          	movb   $0x0,0x1(%rbx)

	while (1) { /* this also skips first '%' */
		bool found = true;

		++fmt;
ffffffff812c8837:	48 8b 45 e8          	mov    -0x18(%rbp),%rax
ffffffff812c883b:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812c883f:	48 89 55 e8          	mov    %rdx,-0x18(%rbp)

		switch (*fmt) {
ffffffff812c8843:	8a 50 01             	mov    0x1(%rax),%dl
ffffffff812c8846:	80 fa 2b             	cmp    $0x2b,%dl
ffffffff812c8849:	74 30                	je     ffffffff812c887b <format_decode+0x2a4>
ffffffff812c884b:	7f 14                	jg     ffffffff812c8861 <format_decode+0x28a>
ffffffff812c884d:	80 fa 20             	cmp    $0x20,%dl
ffffffff812c8850:	74 2f                	je     ffffffff812c8881 <format_decode+0x2aa>
ffffffff812c8852:	80 fa 23             	cmp    $0x23,%dl
ffffffff812c8855:	0f 85 eb fd ff ff    	jne    ffffffff812c8646 <format_decode+0x6f>
		case '-': spec->flags |= LEFT;    break;
		case '+': spec->flags |= PLUS;    break;
		case ' ': spec->flags |= SPACE;   break;
		case '#': spec->flags |= SPECIAL; break;
ffffffff812c885b:	80 4b 01 40          	orb    $0x40,0x1(%rbx)
ffffffff812c885f:	eb d6                	jmp    ffffffff812c8837 <format_decode+0x260>
	while (1) { /* this also skips first '%' */
		bool found = true;

		++fmt;

		switch (*fmt) {
ffffffff812c8861:	80 fa 2d             	cmp    $0x2d,%dl
ffffffff812c8864:	74 0f                	je     ffffffff812c8875 <format_decode+0x29e>
ffffffff812c8866:	80 fa 30             	cmp    $0x30,%dl
ffffffff812c8869:	0f 85 d7 fd ff ff    	jne    ffffffff812c8646 <format_decode+0x6f>
		case '-': spec->flags |= LEFT;    break;
		case '+': spec->flags |= PLUS;    break;
		case ' ': spec->flags |= SPACE;   break;
		case '#': spec->flags |= SPECIAL; break;
		case '0': spec->flags |= ZEROPAD; break;
ffffffff812c886f:	80 4b 01 10          	orb    $0x10,0x1(%rbx)
ffffffff812c8873:	eb c2                	jmp    ffffffff812c8837 <format_decode+0x260>
		bool found = true;

		++fmt;

		switch (*fmt) {
		case '-': spec->flags |= LEFT;    break;
ffffffff812c8875:	80 4b 01 02          	orb    $0x2,0x1(%rbx)
ffffffff812c8879:	eb bc                	jmp    ffffffff812c8837 <format_decode+0x260>
		case '+': spec->flags |= PLUS;    break;
ffffffff812c887b:	80 4b 01 04          	orb    $0x4,0x1(%rbx)
ffffffff812c887f:	eb b6                	jmp    ffffffff812c8837 <format_decode+0x260>
		case ' ': spec->flags |= SPACE;   break;
ffffffff812c8881:	80 4b 01 08          	orb    $0x8,0x1(%rbx)
ffffffff812c8885:	eb b0                	jmp    ffffffff812c8837 <format_decode+0x260>
	/* get field width */
	spec->field_width = -1;

	if (isdigit(*fmt))
		spec->field_width = skip_atoi(&fmt);
	else if (*fmt == '*') {
ffffffff812c8887:	80 fa 2a             	cmp    $0x2a,%dl
ffffffff812c888a:	0f 85 dd fd ff ff    	jne    ffffffff812c866d <format_decode+0x96>
		/* it's the next argument */
		spec->type = FORMAT_TYPE_WIDTH;
ffffffff812c8890:	c6 03 01             	movb   $0x1,(%rbx)
ffffffff812c8893:	e9 1b fe ff ff       	jmpq   ffffffff812c86b3 <format_decode+0xdc>

ffffffff812c8898 <simple_strtoll>:
 * @base: The number base to use
 *
 * This function is obsolete. Please use kstrtoll instead.
 */
long long simple_strtoll(const char *cp, char **endp, unsigned int base)
{
ffffffff812c8898:	55                   	push   %rbp
	if (*cp == '-')
ffffffff812c8899:	80 3f 2d             	cmpb   $0x2d,(%rdi)
 * @base: The number base to use
 *
 * This function is obsolete. Please use kstrtoll instead.
 */
long long simple_strtoll(const char *cp, char **endp, unsigned int base)
{
ffffffff812c889c:	48 89 e5             	mov    %rsp,%rbp
	if (*cp == '-')
ffffffff812c889f:	75 0d                	jne    ffffffff812c88ae <simple_strtoll+0x16>
		return -simple_strtoull(cp + 1, endp, base);
ffffffff812c88a1:	48 ff c7             	inc    %rdi
ffffffff812c88a4:	e8 da fc ff ff       	callq  ffffffff812c8583 <simple_strtoull>
ffffffff812c88a9:	48 f7 d8             	neg    %rax
ffffffff812c88ac:	eb 05                	jmp    ffffffff812c88b3 <simple_strtoll+0x1b>

	return simple_strtoull(cp, endp, base);
ffffffff812c88ae:	e8 d0 fc ff ff       	callq  ffffffff812c8583 <simple_strtoull>
}
ffffffff812c88b3:	5d                   	pop    %rbp
ffffffff812c88b4:	c3                   	retq   

ffffffff812c88b5 <vsscanf>:
 * @buf:	input buffer
 * @fmt:	format of buffer
 * @args:	arguments
 */
int vsscanf(const char *buf, const char *fmt, va_list args)
{
ffffffff812c88b5:	55                   	push   %rbp
ffffffff812c88b6:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c88b9:	41 57                	push   %r15
ffffffff812c88bb:	41 56                	push   %r14
ffffffff812c88bd:	41 55                	push   %r13
ffffffff812c88bf:	41 54                	push   %r12
ffffffff812c88c1:	49 89 d7             	mov    %rdx,%r15
ffffffff812c88c4:	53                   	push   %rbx
	const char *str = buf;
ffffffff812c88c5:	49 89 fd             	mov    %rdi,%r13
 * @buf:	input buffer
 * @fmt:	format of buffer
 * @args:	arguments
 */
int vsscanf(const char *buf, const char *fmt, va_list args)
{
ffffffff812c88c8:	48 83 ec 38          	sub    $0x38,%rsp
ffffffff812c88cc:	48 89 7d a8          	mov    %rdi,-0x58(%rbp)
ffffffff812c88d0:	48 89 75 b8          	mov    %rsi,-0x48(%rbp)
	const char *str = buf;
	char *next;
	char digit;
	int num = 0;
ffffffff812c88d4:	c7 45 b4 00 00 00 00 	movl   $0x0,-0x4c(%rbp)
		unsigned long long u;
	} val;
	s16 field_width;
	bool is_sign;

	while (*fmt) {
ffffffff812c88db:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812c88df:	0f b6 07             	movzbl (%rdi),%eax
ffffffff812c88e2:	84 c0                	test   %al,%al
ffffffff812c88e4:	0f 84 e5 04 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
		/* skip any white space in format */
		/* white space in format matchs any amount of
		 * white space, including none, in the input.
		 */
		if (isspace(*fmt)) {
ffffffff812c88ea:	f6 80 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rax)
ffffffff812c88f1:	74 1b                	je     ffffffff812c890e <vsscanf+0x59>
			fmt = skip_spaces(++fmt);
ffffffff812c88f3:	48 ff c7             	inc    %rdi
ffffffff812c88f6:	48 89 7d b8          	mov    %rdi,-0x48(%rbp)
ffffffff812c88fa:	e8 10 f4 ff ff       	callq  ffffffff812c7d0f <skip_spaces>
			str = skip_spaces(str);
ffffffff812c88ff:	4c 89 ef             	mov    %r13,%rdi
		/* skip any white space in format */
		/* white space in format matchs any amount of
		 * white space, including none, in the input.
		 */
		if (isspace(*fmt)) {
			fmt = skip_spaces(++fmt);
ffffffff812c8902:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
			str = skip_spaces(str);
ffffffff812c8906:	e8 04 f4 ff ff       	callq  ffffffff812c7d0f <skip_spaces>
ffffffff812c890b:	49 89 c5             	mov    %rax,%r13
		}

		/* anything that is not a conversion must match exactly */
		if (*fmt != '%' && *fmt) {
ffffffff812c890e:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812c8912:	8a 10                	mov    (%rax),%dl
ffffffff812c8914:	80 fa 25             	cmp    $0x25,%dl
ffffffff812c8917:	0f 95 c1             	setne  %cl
ffffffff812c891a:	84 d2                	test   %dl,%dl
ffffffff812c891c:	40 0f 95 c6          	setne  %sil
ffffffff812c8920:	40 20 f1             	and    %sil,%cl
ffffffff812c8923:	88 4d b3             	mov    %cl,-0x4d(%rbp)
ffffffff812c8926:	74 13                	je     ffffffff812c893b <vsscanf+0x86>
			if (*fmt++ != *str++)
ffffffff812c8928:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812c892c:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
ffffffff812c8930:	41 8a 4d 00          	mov    0x0(%r13),%cl
ffffffff812c8934:	38 08                	cmp    %cl,(%rax)
ffffffff812c8936:	e9 8c 02 00 00       	jmpq   ffffffff812c8bc7 <vsscanf+0x312>
				break;
			continue;
		}

		if (!*fmt)
ffffffff812c893b:	84 d2                	test   %dl,%dl
ffffffff812c893d:	0f 84 8c 04 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
			break;
		++fmt;
ffffffff812c8943:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812c8947:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)

		/* skip this conversion.
		 * advance both strings to next white space
		 */
		if (*fmt == '*') {
ffffffff812c894b:	0f b6 40 01          	movzbl 0x1(%rax),%eax
ffffffff812c894f:	3c 2a                	cmp    $0x2a,%al
ffffffff812c8951:	75 57                	jne    ffffffff812c89aa <vsscanf+0xf5>
			if (!*str)
ffffffff812c8953:	41 80 7d 00 00       	cmpb   $0x0,0x0(%r13)
ffffffff812c8958:	0f 84 71 04 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
				break;
			while (!isspace(*fmt) && *fmt != '%' && *fmt)
ffffffff812c895e:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812c8962:	0f b6 30             	movzbl (%rax),%esi
ffffffff812c8965:	f6 86 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rsi)
ffffffff812c896c:	48 89 f2             	mov    %rsi,%rdx
ffffffff812c896f:	75 26                	jne    ffffffff812c8997 <vsscanf+0xe2>
ffffffff812c8971:	40 80 fe 25          	cmp    $0x25,%sil
ffffffff812c8975:	40 0f 95 c6          	setne  %sil
ffffffff812c8979:	84 d2                	test   %dl,%dl
ffffffff812c897b:	0f 95 c2             	setne  %dl
ffffffff812c897e:	40 84 d6             	test   %dl,%sil
ffffffff812c8981:	74 14                	je     ffffffff812c8997 <vsscanf+0xe2>
				fmt++;
ffffffff812c8983:	48 ff c0             	inc    %rax
ffffffff812c8986:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812c898a:	eb d2                	jmp    ffffffff812c895e <vsscanf+0xa9>
			while (!isspace(*str) && *str)
ffffffff812c898c:	84 d2                	test   %dl,%dl
ffffffff812c898e:	0f 84 47 ff ff ff    	je     ffffffff812c88db <vsscanf+0x26>
				str++;
ffffffff812c8994:	49 ff c5             	inc    %r13
		if (*fmt == '*') {
			if (!*str)
				break;
			while (!isspace(*fmt) && *fmt != '%' && *fmt)
				fmt++;
			while (!isspace(*str) && *str)
ffffffff812c8997:	41 0f b6 55 00       	movzbl 0x0(%r13),%edx
ffffffff812c899c:	f6 82 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rdx)
ffffffff812c89a3:	74 e7                	je     ffffffff812c898c <vsscanf+0xd7>
ffffffff812c89a5:	e9 31 ff ff ff       	jmpq   ffffffff812c88db <vsscanf+0x26>
				str++;
			continue;
		}

		/* get field width */
		field_width = -1;
ffffffff812c89aa:	48 83 cb ff          	or     $0xffffffffffffffff,%rbx
		if (isdigit(*fmt)) {
ffffffff812c89ae:	f6 80 60 ef 63 81 04 	testb  $0x4,-0x7e9c10a0(%rax)
ffffffff812c89b5:	74 16                	je     ffffffff812c89cd <vsscanf+0x118>
			field_width = skip_atoi(&fmt);
ffffffff812c89b7:	48 8d 7d b8          	lea    -0x48(%rbp),%rdi
ffffffff812c89bb:	e8 76 f9 ff ff       	callq  ffffffff812c8336 <skip_atoi>
			if (field_width <= 0)
ffffffff812c89c0:	66 85 c0             	test   %ax,%ax
		}

		/* get field width */
		field_width = -1;
		if (isdigit(*fmt)) {
			field_width = skip_atoi(&fmt);
ffffffff812c89c3:	48 0f bf d8          	movswq %ax,%rbx
			if (field_width <= 0)
ffffffff812c89c7:	0f 8e 02 04 00 00    	jle    ffffffff812c8dcf <vsscanf+0x51a>
				break;
		}

		/* get conversion qualifier */
		qualifier = -1;
		if (*fmt == 'h' || _tolower(*fmt) == 'l' ||
ffffffff812c89cd:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812c89d1:	8a 10                	mov    (%rax),%dl
ffffffff812c89d3:	80 fa 68             	cmp    $0x68,%dl
ffffffff812c89d6:	74 10                	je     ffffffff812c89e8 <vsscanf+0x133>
ffffffff812c89d8:	83 ca 20             	or     $0x20,%edx
ffffffff812c89db:	80 fa 7a             	cmp    $0x7a,%dl
ffffffff812c89de:	74 08                	je     ffffffff812c89e8 <vsscanf+0x133>
ffffffff812c89e0:	80 fa 6c             	cmp    $0x6c,%dl
			if (field_width <= 0)
				break;
		}

		/* get conversion qualifier */
		qualifier = -1;
ffffffff812c89e3:	41 b4 ff             	mov    $0xff,%r12b
		if (*fmt == 'h' || _tolower(*fmt) == 'l' ||
ffffffff812c89e6:	75 3a                	jne    ffffffff812c8a22 <vsscanf+0x16d>
		    _tolower(*fmt) == 'z') {
			qualifier = *fmt++;
ffffffff812c89e8:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812c89ec:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
ffffffff812c89f0:	0f b6 30             	movzbl (%rax),%esi
			if (unlikely(qualifier == *fmt)) {
ffffffff812c89f3:	0f be 50 01          	movsbl 0x1(%rax),%edx

		/* get conversion qualifier */
		qualifier = -1;
		if (*fmt == 'h' || _tolower(*fmt) == 'l' ||
		    _tolower(*fmt) == 'z') {
			qualifier = *fmt++;
ffffffff812c89f7:	41 89 f4             	mov    %esi,%r12d
			if (unlikely(qualifier == *fmt)) {
ffffffff812c89fa:	39 d6                	cmp    %edx,%esi
ffffffff812c89fc:	75 24                	jne    ffffffff812c8a22 <vsscanf+0x16d>
				if (qualifier == 'h') {
ffffffff812c89fe:	40 80 fe 68          	cmp    $0x68,%sil
ffffffff812c8a02:	75 0d                	jne    ffffffff812c8a11 <vsscanf+0x15c>
					qualifier = 'H';
					fmt++;
ffffffff812c8a04:	48 83 c0 02          	add    $0x2,%rax
		if (*fmt == 'h' || _tolower(*fmt) == 'l' ||
		    _tolower(*fmt) == 'z') {
			qualifier = *fmt++;
			if (unlikely(qualifier == *fmt)) {
				if (qualifier == 'h') {
					qualifier = 'H';
ffffffff812c8a08:	41 b4 48             	mov    $0x48,%r12b
					fmt++;
ffffffff812c8a0b:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812c8a0f:	eb 11                	jmp    ffffffff812c8a22 <vsscanf+0x16d>
				} else if (qualifier == 'l') {
ffffffff812c8a11:	40 80 fe 6c          	cmp    $0x6c,%sil
ffffffff812c8a15:	75 0b                	jne    ffffffff812c8a22 <vsscanf+0x16d>
					qualifier = 'L';
					fmt++;
ffffffff812c8a17:	48 83 c0 02          	add    $0x2,%rax
			if (unlikely(qualifier == *fmt)) {
				if (qualifier == 'h') {
					qualifier = 'H';
					fmt++;
				} else if (qualifier == 'l') {
					qualifier = 'L';
ffffffff812c8a1b:	41 b4 4c             	mov    $0x4c,%r12b
					fmt++;
ffffffff812c8a1e:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
				}
			}
		}

		if (!*fmt)
ffffffff812c8a22:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812c8a26:	8a 10                	mov    (%rax),%dl
ffffffff812c8a28:	84 d2                	test   %dl,%dl
ffffffff812c8a2a:	0f 84 9f 03 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
			break;

		if (*fmt == 'n') {
ffffffff812c8a30:	80 fa 6e             	cmp    $0x6e,%dl
ffffffff812c8a33:	75 37                	jne    ffffffff812c8a6c <vsscanf+0x1b7>
			/* return number of characters read so far */
			*va_arg(args, int *) = str - buf;
ffffffff812c8a35:	41 8b 17             	mov    (%r15),%edx
ffffffff812c8a38:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812c8a3b:	77 0e                	ja     ffffffff812c8a4b <vsscanf+0x196>
ffffffff812c8a3d:	89 d0                	mov    %edx,%eax
ffffffff812c8a3f:	83 c2 08             	add    $0x8,%edx
ffffffff812c8a42:	49 03 47 10          	add    0x10(%r15),%rax
ffffffff812c8a46:	41 89 17             	mov    %edx,(%r15)
ffffffff812c8a49:	eb 0c                	jmp    ffffffff812c8a57 <vsscanf+0x1a2>
ffffffff812c8a4b:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c8a4f:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812c8a53:	49 89 57 08          	mov    %rdx,0x8(%r15)
ffffffff812c8a57:	4c 89 ea             	mov    %r13,%rdx
ffffffff812c8a5a:	48 2b 55 a8          	sub    -0x58(%rbp),%rdx
ffffffff812c8a5e:	48 8b 00             	mov    (%rax),%rax
ffffffff812c8a61:	89 10                	mov    %edx,(%rax)
			++fmt;
ffffffff812c8a63:	48 ff 45 b8          	incq   -0x48(%rbp)
			continue;
ffffffff812c8a67:	e9 6f fe ff ff       	jmpq   ffffffff812c88db <vsscanf+0x26>
		}

		if (!*str)
ffffffff812c8a6c:	41 80 7d 00 00       	cmpb   $0x0,0x0(%r13)
ffffffff812c8a71:	0f 84 58 03 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
			break;

		base = 10;
		is_sign = false;

		switch (*fmt++) {
ffffffff812c8a77:	48 8d 50 01          	lea    0x1(%rax),%rdx
		}

		if (!*str)
			break;

		base = 10;
ffffffff812c8a7b:	c7 45 c4 0a 00 00 00 	movl   $0xa,-0x3c(%rbp)
		is_sign = false;

		switch (*fmt++) {
ffffffff812c8a82:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
ffffffff812c8a86:	8a 00                	mov    (%rax),%al
ffffffff812c8a88:	3c 69                	cmp    $0x69,%al
ffffffff812c8a8a:	0f 84 2b 01 00 00    	je     ffffffff812c8bbb <vsscanf+0x306>
ffffffff812c8a90:	7f 2d                	jg     ffffffff812c8abf <vsscanf+0x20a>
ffffffff812c8a92:	3c 58                	cmp    $0x58,%al
ffffffff812c8a94:	0f 84 15 01 00 00    	je     ffffffff812c8baf <vsscanf+0x2fa>
ffffffff812c8a9a:	7f 12                	jg     ffffffff812c8aae <vsscanf+0x1f9>
ffffffff812c8a9c:	3c 25                	cmp    $0x25,%al
ffffffff812c8a9e:	0f 85 2b 03 00 00    	jne    ffffffff812c8dcf <vsscanf+0x51a>
			is_sign = true;
		case 'u':
			break;
		case '%':
			/* looking for '%' in str */
			if (*str++ != '%')
ffffffff812c8aa4:	41 80 7d 00 25       	cmpb   $0x25,0x0(%r13)
ffffffff812c8aa9:	e9 19 01 00 00       	jmpq   ffffffff812c8bc7 <vsscanf+0x312>
			break;

		base = 10;
		is_sign = false;

		switch (*fmt++) {
ffffffff812c8aae:	3c 63                	cmp    $0x63,%al
ffffffff812c8ab0:	74 3f                	je     ffffffff812c8af1 <vsscanf+0x23c>
ffffffff812c8ab2:	3c 64                	cmp    $0x64,%al
ffffffff812c8ab4:	0f 84 08 01 00 00    	je     ffffffff812c8bc2 <vsscanf+0x30d>
ffffffff812c8aba:	e9 10 03 00 00       	jmpq   ffffffff812c8dcf <vsscanf+0x51a>
ffffffff812c8abf:	3c 73                	cmp    $0x73,%al
ffffffff812c8ac1:	74 7a                	je     ffffffff812c8b3d <vsscanf+0x288>
ffffffff812c8ac3:	7f 14                	jg     ffffffff812c8ad9 <vsscanf+0x224>
ffffffff812c8ac5:	3c 6f                	cmp    $0x6f,%al
ffffffff812c8ac7:	0f 85 02 03 00 00    	jne    ffffffff812c8dcf <vsscanf+0x51a>
			*s = '\0';
			num++;
		}
		continue;
		case 'o':
			base = 8;
ffffffff812c8acd:	c7 45 c4 08 00 00 00 	movl   $0x8,-0x3c(%rbp)
ffffffff812c8ad4:	e9 dd 00 00 00       	jmpq   ffffffff812c8bb6 <vsscanf+0x301>

		if (!*str)
			break;

		base = 10;
		is_sign = false;
ffffffff812c8ad9:	45 31 f6             	xor    %r14d,%r14d

		switch (*fmt++) {
ffffffff812c8adc:	3c 75                	cmp    $0x75,%al
ffffffff812c8ade:	0f 84 f1 00 00 00    	je     ffffffff812c8bd5 <vsscanf+0x320>
ffffffff812c8ae4:	3c 78                	cmp    $0x78,%al
ffffffff812c8ae6:	0f 84 c3 00 00 00    	je     ffffffff812c8baf <vsscanf+0x2fa>
ffffffff812c8aec:	e9 de 02 00 00       	jmpq   ffffffff812c8dcf <vsscanf+0x51a>
		case 'c':
		{
			char *s = (char *)va_arg(args, char*);
ffffffff812c8af1:	41 8b 17             	mov    (%r15),%edx
ffffffff812c8af4:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812c8af7:	77 0e                	ja     ffffffff812c8b07 <vsscanf+0x252>
ffffffff812c8af9:	89 d0                	mov    %edx,%eax
ffffffff812c8afb:	83 c2 08             	add    $0x8,%edx
ffffffff812c8afe:	49 03 47 10          	add    0x10(%r15),%rax
ffffffff812c8b02:	41 89 17             	mov    %edx,(%r15)
ffffffff812c8b05:	eb 0c                	jmp    ffffffff812c8b13 <vsscanf+0x25e>
ffffffff812c8b07:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c8b0b:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812c8b0f:	49 89 57 08          	mov    %rdx,0x8(%r15)
			if (field_width == -1)
ffffffff812c8b13:	66 83 fb ff          	cmp    $0xffff,%bx
		is_sign = false;

		switch (*fmt++) {
		case 'c':
		{
			char *s = (char *)va_arg(args, char*);
ffffffff812c8b17:	48 8b 00             	mov    (%rax),%rax
			if (field_width == -1)
ffffffff812c8b1a:	0f 84 a1 02 00 00    	je     ffffffff812c8dc1 <vsscanf+0x50c>
				field_width = 1;
			do {
				*s++ = *str++;
ffffffff812c8b20:	49 ff c5             	inc    %r13
ffffffff812c8b23:	41 8a 55 ff          	mov    -0x1(%r13),%dl
ffffffff812c8b27:	ff cb                	dec    %ebx
ffffffff812c8b29:	48 ff c0             	inc    %rax
			} while (--field_width > 0 && *str);
ffffffff812c8b2c:	66 85 db             	test   %bx,%bx
		{
			char *s = (char *)va_arg(args, char*);
			if (field_width == -1)
				field_width = 1;
			do {
				*s++ = *str++;
ffffffff812c8b2f:	88 50 ff             	mov    %dl,-0x1(%rax)
			} while (--field_width > 0 && *str);
ffffffff812c8b32:	7e 52                	jle    ffffffff812c8b86 <vsscanf+0x2d1>
ffffffff812c8b34:	41 80 7d 00 00       	cmpb   $0x0,0x0(%r13)
ffffffff812c8b39:	75 e5                	jne    ffffffff812c8b20 <vsscanf+0x26b>
ffffffff812c8b3b:	eb 49                	jmp    ffffffff812c8b86 <vsscanf+0x2d1>
			num++;
		}
		continue;
		case 's':
		{
			char *s = (char *)va_arg(args, char *);
ffffffff812c8b3d:	41 8b 17             	mov    (%r15),%edx
ffffffff812c8b40:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812c8b43:	77 0e                	ja     ffffffff812c8b53 <vsscanf+0x29e>
ffffffff812c8b45:	89 d0                	mov    %edx,%eax
ffffffff812c8b47:	83 c2 08             	add    $0x8,%edx
ffffffff812c8b4a:	49 03 47 10          	add    0x10(%r15),%rax
ffffffff812c8b4e:	41 89 17             	mov    %edx,(%r15)
ffffffff812c8b51:	eb 0c                	jmp    ffffffff812c8b5f <vsscanf+0x2aa>
ffffffff812c8b53:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c8b57:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812c8b5b:	49 89 57 08          	mov    %rdx,0x8(%r15)
ffffffff812c8b5f:	4c 8b 20             	mov    (%rax),%r12
			if (field_width == -1)
				field_width = SHRT_MAX;
ffffffff812c8b62:	66 83 fb ff          	cmp    $0xffff,%bx
ffffffff812c8b66:	b8 ff 7f 00 00       	mov    $0x7fff,%eax
			/* first, skip leading white space in buffer */
			str = skip_spaces(str);
ffffffff812c8b6b:	4c 89 ef             	mov    %r13,%rdi
		continue;
		case 's':
		{
			char *s = (char *)va_arg(args, char *);
			if (field_width == -1)
				field_width = SHRT_MAX;
ffffffff812c8b6e:	0f 44 d8             	cmove  %eax,%ebx
			/* first, skip leading white space in buffer */
			str = skip_spaces(str);
ffffffff812c8b71:	e8 99 f1 ff ff       	callq  ffffffff812c7d0f <skip_spaces>
ffffffff812c8b76:	49 89 c5             	mov    %rax,%r13

			/* now copy until next white space */
			while (*str && !isspace(*str) && field_width--)
ffffffff812c8b79:	41 8a 45 00          	mov    0x0(%r13),%al
ffffffff812c8b7d:	84 c0                	test   %al,%al
ffffffff812c8b7f:	75 0d                	jne    ffffffff812c8b8e <vsscanf+0x2d9>
				*s++ = *str++;
			*s = '\0';
ffffffff812c8b81:	41 c6 04 24 00       	movb   $0x0,(%r12)
			num++;
ffffffff812c8b86:	ff 45 b4             	incl   -0x4c(%rbp)
		}
		continue;
ffffffff812c8b89:	e9 4d fd ff ff       	jmpq   ffffffff812c88db <vsscanf+0x26>
				field_width = SHRT_MAX;
			/* first, skip leading white space in buffer */
			str = skip_spaces(str);

			/* now copy until next white space */
			while (*str && !isspace(*str) && field_width--)
ffffffff812c8b8e:	0f b6 d0             	movzbl %al,%edx
ffffffff812c8b91:	f6 82 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rdx)
ffffffff812c8b98:	75 e7                	jne    ffffffff812c8b81 <vsscanf+0x2cc>
ffffffff812c8b9a:	ff cb                	dec    %ebx
ffffffff812c8b9c:	66 83 fb ff          	cmp    $0xffff,%bx
ffffffff812c8ba0:	74 df                	je     ffffffff812c8b81 <vsscanf+0x2cc>
				*s++ = *str++;
ffffffff812c8ba2:	49 ff c4             	inc    %r12
ffffffff812c8ba5:	49 ff c5             	inc    %r13
ffffffff812c8ba8:	41 88 44 24 ff       	mov    %al,-0x1(%r12)
ffffffff812c8bad:	eb ca                	jmp    ffffffff812c8b79 <vsscanf+0x2c4>
		case 'o':
			base = 8;
			break;
		case 'x':
		case 'X':
			base = 16;
ffffffff812c8baf:	c7 45 c4 10 00 00 00 	movl   $0x10,-0x3c(%rbp)

		if (!*str)
			break;

		base = 10;
		is_sign = false;
ffffffff812c8bb6:	45 31 f6             	xor    %r14d,%r14d
			base = 8;
			break;
		case 'x':
		case 'X':
			base = 16;
			break;
ffffffff812c8bb9:	eb 1a                	jmp    ffffffff812c8bd5 <vsscanf+0x320>
		case 'i':
			base = 0;
ffffffff812c8bbb:	c7 45 c4 00 00 00 00 	movl   $0x0,-0x3c(%rbp)
		case 'd':
			is_sign = true;
ffffffff812c8bc2:	41 b6 01             	mov    $0x1,%r14b
ffffffff812c8bc5:	eb 0e                	jmp    ffffffff812c8bd5 <vsscanf+0x320>
		case 'u':
			break;
		case '%':
			/* looking for '%' in str */
			if (*str++ != '%')
ffffffff812c8bc7:	0f 85 02 02 00 00    	jne    ffffffff812c8dcf <vsscanf+0x51a>
ffffffff812c8bcd:	49 ff c5             	inc    %r13
ffffffff812c8bd0:	e9 06 fd ff ff       	jmpq   ffffffff812c88db <vsscanf+0x26>
		}

		/* have some sort of integer conversion.
		 * first, skip white space in buffer.
		 */
		str = skip_spaces(str);
ffffffff812c8bd5:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c8bd8:	e8 32 f1 ff ff       	callq  ffffffff812c7d0f <skip_spaces>
ffffffff812c8bdd:	49 89 c5             	mov    %rax,%r13

		digit = *str;
ffffffff812c8be0:	0f b6 00             	movzbl (%rax),%eax
		if (is_sign && digit == '-')
ffffffff812c8be3:	3c 2d                	cmp    $0x2d,%al
ffffffff812c8be5:	75 0a                	jne    ffffffff812c8bf1 <vsscanf+0x33c>
ffffffff812c8be7:	45 84 f6             	test   %r14b,%r14b
ffffffff812c8bea:	74 05                	je     ffffffff812c8bf1 <vsscanf+0x33c>
			digit = *(str + 1);
ffffffff812c8bec:	41 0f b6 45 01       	movzbl 0x1(%r13),%eax

		if (!digit
ffffffff812c8bf1:	84 c0                	test   %al,%al
ffffffff812c8bf3:	0f 84 d6 01 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
		    || (base == 16 && !isxdigit(digit))
ffffffff812c8bf9:	8b 55 c4             	mov    -0x3c(%rbp),%edx
ffffffff812c8bfc:	83 fa 10             	cmp    $0x10,%edx
ffffffff812c8bff:	75 09                	jne    ffffffff812c8c0a <vsscanf+0x355>
ffffffff812c8c01:	f6 80 60 ef 63 81 44 	testb  $0x44,-0x7e9c10a0(%rax)
ffffffff812c8c08:	eb 2e                	jmp    ffffffff812c8c38 <vsscanf+0x383>
		    || (base == 10 && !isdigit(digit))
ffffffff812c8c0a:	83 fa 0a             	cmp    $0xa,%edx
ffffffff812c8c0d:	74 22                	je     ffffffff812c8c31 <vsscanf+0x37c>
		    || (base == 8 && (!isdigit(digit) || digit > '7'))
ffffffff812c8c0f:	83 fa 08             	cmp    $0x8,%edx
ffffffff812c8c12:	75 19                	jne    ffffffff812c8c2d <vsscanf+0x378>
ffffffff812c8c14:	0f b6 f0             	movzbl %al,%esi
ffffffff812c8c17:	f6 86 60 ef 63 81 04 	testb  $0x4,-0x7e9c10a0(%rsi)
ffffffff812c8c1e:	0f 84 ab 01 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
ffffffff812c8c24:	3c 37                	cmp    $0x37,%al
ffffffff812c8c26:	7e 16                	jle    ffffffff812c8c3e <vsscanf+0x389>
ffffffff812c8c28:	e9 a2 01 00 00       	jmpq   ffffffff812c8dcf <vsscanf+0x51a>
		    || (base == 0 && !isdigit(digit)))
ffffffff812c8c2d:	85 d2                	test   %edx,%edx
ffffffff812c8c2f:	75 0d                	jne    ffffffff812c8c3e <vsscanf+0x389>
ffffffff812c8c31:	f6 80 60 ef 63 81 04 	testb  $0x4,-0x7e9c10a0(%rax)
ffffffff812c8c38:	0f 84 91 01 00 00    	je     ffffffff812c8dcf <vsscanf+0x51a>
			break;

		if (is_sign)
ffffffff812c8c3e:	45 84 f6             	test   %r14b,%r14b
			val.s = qualifier != 'L' ?
				simple_strtol(str, &next, base) :
ffffffff812c8c41:	48 8d 75 c8          	lea    -0x38(%rbp),%rsi
ffffffff812c8c45:	4c 89 ef             	mov    %r13,%rdi
		    || (base == 10 && !isdigit(digit))
		    || (base == 8 && (!isdigit(digit) || digit > '7'))
		    || (base == 0 && !isdigit(digit)))
			break;

		if (is_sign)
ffffffff812c8c48:	74 1f                	je     ffffffff812c8c69 <vsscanf+0x3b4>
			val.s = qualifier != 'L' ?
				simple_strtol(str, &next, base) :
ffffffff812c8c4a:	41 80 fc 4c          	cmp    $0x4c,%r12b
ffffffff812c8c4e:	74 07                	je     ffffffff812c8c57 <vsscanf+0x3a2>
ffffffff812c8c50:	e8 43 fc ff ff       	callq  ffffffff812c8898 <simple_strtoll>
ffffffff812c8c55:	eb 17                	jmp    ffffffff812c8c6e <vsscanf+0x3b9>
ffffffff812c8c57:	e8 3c fc ff ff       	callq  ffffffff812c8898 <simple_strtoll>
		else
			val.u = qualifier != 'L' ?
				simple_strtoul(str, &next, base) :
				simple_strtoull(str, &next, base);

		if (field_width > 0 && next - str > field_width) {
ffffffff812c8c5c:	66 85 db             	test   %bx,%bx
		    || (base == 0 && !isdigit(digit)))
			break;

		if (is_sign)
			val.s = qualifier != 'L' ?
				simple_strtol(str, &next, base) :
ffffffff812c8c5f:	49 89 c0             	mov    %rax,%r8
		else
			val.u = qualifier != 'L' ?
				simple_strtoul(str, &next, base) :
				simple_strtoull(str, &next, base);

		if (field_width > 0 && next - str > field_width) {
ffffffff812c8c62:	7f 12                	jg     ffffffff812c8c76 <vsscanf+0x3c1>
ffffffff812c8c64:	e9 f7 00 00 00       	jmpq   ffffffff812c8d60 <vsscanf+0x4ab>
			val.s = qualifier != 'L' ?
				simple_strtol(str, &next, base) :
				simple_strtoll(str, &next, base);
		else
			val.u = qualifier != 'L' ?
				simple_strtoul(str, &next, base) :
ffffffff812c8c69:	e8 15 f9 ff ff       	callq  ffffffff812c8583 <simple_strtoull>
				simple_strtoull(str, &next, base);

		if (field_width > 0 && next - str > field_width) {
ffffffff812c8c6e:	66 85 db             	test   %bx,%bx
		if (is_sign)
			val.s = qualifier != 'L' ?
				simple_strtol(str, &next, base) :
				simple_strtoll(str, &next, base);
		else
			val.u = qualifier != 'L' ?
ffffffff812c8c71:	49 89 c0             	mov    %rax,%r8
				simple_strtoul(str, &next, base) :
				simple_strtoull(str, &next, base);

		if (field_width > 0 && next - str > field_width) {
ffffffff812c8c74:	7e 66                	jle    ffffffff812c8cdc <vsscanf+0x427>
ffffffff812c8c76:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812c8c7a:	4c 29 e8             	sub    %r13,%rax
ffffffff812c8c7d:	48 39 d8             	cmp    %rbx,%rax
ffffffff812c8c80:	7e 5a                	jle    ffffffff812c8cdc <vsscanf+0x427>
			if (base == 0)
ffffffff812c8c82:	83 7d c4 00          	cmpl   $0x0,-0x3c(%rbp)
ffffffff812c8c86:	75 14                	jne    ffffffff812c8c9c <vsscanf+0x3e7>
				_parse_integer_fixup_radix(str, &base);
ffffffff812c8c88:	48 8d 75 c4          	lea    -0x3c(%rbp),%rsi
ffffffff812c8c8c:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c8c8f:	4c 89 45 a0          	mov    %r8,-0x60(%rbp)
ffffffff812c8c93:	e8 65 af 00 00       	callq  ffffffff812d3bfd <_parse_integer_fixup_radix>
ffffffff812c8c98:	4c 8b 45 a0          	mov    -0x60(%rbp),%r8
			while (next - str > field_width) {
				if (is_sign)
					val.s = div_s64(val.s, base);
ffffffff812c8c9c:	4c 63 55 c4          	movslq -0x3c(%rbp),%r10
ffffffff812c8ca0:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
 * This is commonly provided by 32bit archs to provide an optimized 64bit
 * divide.
 */
static inline u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
	*remainder = dividend % divisor;
ffffffff812c8ca4:	44 89 d6             	mov    %r10d,%esi
				simple_strtoull(str, &next, base);

		if (field_width > 0 && next - str > field_width) {
			if (base == 0)
				_parse_integer_fixup_radix(str, &base);
			while (next - str > field_width) {
ffffffff812c8ca7:	48 89 f8             	mov    %rdi,%rax
ffffffff812c8caa:	4c 29 e8             	sub    %r13,%rax
ffffffff812c8cad:	48 39 c3             	cmp    %rax,%rbx
ffffffff812c8cb0:	7d 20                	jge    ffffffff812c8cd2 <vsscanf+0x41d>
				if (is_sign)
ffffffff812c8cb2:	45 84 f6             	test   %r14b,%r14b
 * div_s64_rem - signed 64bit divide with 32bit divisor with remainder
 */
static inline s64 div_s64_rem(s64 dividend, s32 divisor, s32 *remainder)
{
	*remainder = dividend % divisor;
	return dividend / divisor;
ffffffff812c8cb5:	4c 89 c0             	mov    %r8,%rax
ffffffff812c8cb8:	74 07                	je     ffffffff812c8cc1 <vsscanf+0x40c>
ffffffff812c8cba:	48 99                	cqto   
ffffffff812c8cbc:	49 f7 fa             	idiv   %r10
ffffffff812c8cbf:	eb 05                	jmp    ffffffff812c8cc6 <vsscanf+0x411>
					val.s = div_s64(val.s, base);
				else
					val.u = div_u64(val.u, base);
ffffffff812c8cc1:	31 d2                	xor    %edx,%edx
ffffffff812c8cc3:	48 f7 f6             	div    %rsi
ffffffff812c8cc6:	49 89 c0             	mov    %rax,%r8
ffffffff812c8cc9:	48 ff cf             	dec    %rdi
ffffffff812c8ccc:	c6 45 b3 01          	movb   $0x1,-0x4d(%rbp)
ffffffff812c8cd0:	eb d5                	jmp    ffffffff812c8ca7 <vsscanf+0x3f2>
ffffffff812c8cd2:	80 7d b3 00          	cmpb   $0x0,-0x4d(%rbp)
ffffffff812c8cd6:	74 04                	je     ffffffff812c8cdc <vsscanf+0x427>
ffffffff812c8cd8:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)
				--next;
			}
		}

		switch (qualifier) {
ffffffff812c8cdc:	41 80 fc 5a          	cmp    $0x5a,%r12b
ffffffff812c8ce0:	41 8b 17             	mov    (%r15),%edx
ffffffff812c8ce3:	74 7e                	je     ffffffff812c8d63 <vsscanf+0x4ae>
ffffffff812c8ce5:	77 11                	ja     ffffffff812c8cf8 <vsscanf+0x443>
ffffffff812c8ce7:	41 80 fc 48          	cmp    $0x48,%r12b
ffffffff812c8ceb:	74 1f                	je     ffffffff812c8d0c <vsscanf+0x457>
ffffffff812c8ced:	41 80 fc 4c          	cmp    $0x4c,%r12b
ffffffff812c8cf1:	74 68                	je     ffffffff812c8d5b <vsscanf+0x4a6>
ffffffff812c8cf3:	e9 92 00 00 00       	jmpq   ffffffff812c8d8a <vsscanf+0x4d5>
ffffffff812c8cf8:	41 80 fc 6c          	cmp    $0x6c,%r12b
ffffffff812c8cfc:	74 65                	je     ffffffff812c8d63 <vsscanf+0x4ae>
ffffffff812c8cfe:	41 80 fc 7a          	cmp    $0x7a,%r12b
ffffffff812c8d02:	74 5f                	je     ffffffff812c8d63 <vsscanf+0x4ae>
ffffffff812c8d04:	41 80 fc 68          	cmp    $0x68,%r12b
ffffffff812c8d08:	74 29                	je     ffffffff812c8d33 <vsscanf+0x47e>
ffffffff812c8d0a:	eb 7e                	jmp    ffffffff812c8d8a <vsscanf+0x4d5>
		case 'H':	/* that's 'hh' in format */
			if (is_sign)
				*va_arg(args, signed char *) = val.s;
			else
				*va_arg(args, unsigned char *) = val.u;
ffffffff812c8d0c:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812c8d0f:	77 0e                	ja     ffffffff812c8d1f <vsscanf+0x46a>
ffffffff812c8d11:	89 d0                	mov    %edx,%eax
ffffffff812c8d13:	83 c2 08             	add    $0x8,%edx
ffffffff812c8d16:	49 03 47 10          	add    0x10(%r15),%rax
ffffffff812c8d1a:	41 89 17             	mov    %edx,(%r15)
ffffffff812c8d1d:	eb 0c                	jmp    ffffffff812c8d2b <vsscanf+0x476>
ffffffff812c8d1f:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c8d23:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812c8d27:	49 89 57 08          	mov    %rdx,0x8(%r15)
ffffffff812c8d2b:	48 8b 00             	mov    (%rax),%rax
ffffffff812c8d2e:	44 88 00             	mov    %r8b,(%rax)
ffffffff812c8d31:	eb 7c                	jmp    ffffffff812c8daf <vsscanf+0x4fa>
			break;
		case 'h':
			if (is_sign)
				*va_arg(args, short *) = val.s;
			else
				*va_arg(args, unsigned short *) = val.u;
ffffffff812c8d33:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812c8d36:	77 0e                	ja     ffffffff812c8d46 <vsscanf+0x491>
ffffffff812c8d38:	89 d0                	mov    %edx,%eax
ffffffff812c8d3a:	83 c2 08             	add    $0x8,%edx
ffffffff812c8d3d:	49 03 47 10          	add    0x10(%r15),%rax
ffffffff812c8d41:	41 89 17             	mov    %edx,(%r15)
ffffffff812c8d44:	eb 0c                	jmp    ffffffff812c8d52 <vsscanf+0x49d>
ffffffff812c8d46:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c8d4a:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812c8d4e:	49 89 57 08          	mov    %rdx,0x8(%r15)
ffffffff812c8d52:	48 8b 00             	mov    (%rax),%rax
ffffffff812c8d55:	66 44 89 00          	mov    %r8w,(%rax)
ffffffff812c8d59:	eb 54                	jmp    ffffffff812c8daf <vsscanf+0x4fa>
				*va_arg(args, long *) = val.s;
			else
				*va_arg(args, unsigned long *) = val.u;
			break;
		case 'L':
			if (is_sign)
ffffffff812c8d5b:	45 84 f6             	test   %r14b,%r14b
ffffffff812c8d5e:	74 03                	je     ffffffff812c8d63 <vsscanf+0x4ae>
				*va_arg(args, long long *) = val.s;
ffffffff812c8d60:	41 8b 17             	mov    (%r15),%edx
			else
				*va_arg(args, unsigned long long *) = val.u;
			break;
		case 'Z':
		case 'z':
			*va_arg(args, size_t *) = val.u;
ffffffff812c8d63:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812c8d66:	77 0e                	ja     ffffffff812c8d76 <vsscanf+0x4c1>
ffffffff812c8d68:	89 d0                	mov    %edx,%eax
ffffffff812c8d6a:	83 c2 08             	add    $0x8,%edx
ffffffff812c8d6d:	49 03 47 10          	add    0x10(%r15),%rax
ffffffff812c8d71:	41 89 17             	mov    %edx,(%r15)
ffffffff812c8d74:	eb 0c                	jmp    ffffffff812c8d82 <vsscanf+0x4cd>
ffffffff812c8d76:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c8d7a:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812c8d7e:	49 89 57 08          	mov    %rdx,0x8(%r15)
ffffffff812c8d82:	48 8b 00             	mov    (%rax),%rax
ffffffff812c8d85:	4c 89 00             	mov    %r8,(%rax)
			break;
ffffffff812c8d88:	eb 25                	jmp    ffffffff812c8daf <vsscanf+0x4fa>
		default:
			if (is_sign)
				*va_arg(args, int *) = val.s;
			else
				*va_arg(args, unsigned int *) = val.u;
ffffffff812c8d8a:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812c8d8d:	77 0e                	ja     ffffffff812c8d9d <vsscanf+0x4e8>
ffffffff812c8d8f:	89 d0                	mov    %edx,%eax
ffffffff812c8d91:	83 c2 08             	add    $0x8,%edx
ffffffff812c8d94:	49 03 47 10          	add    0x10(%r15),%rax
ffffffff812c8d98:	41 89 17             	mov    %edx,(%r15)
ffffffff812c8d9b:	eb 0c                	jmp    ffffffff812c8da9 <vsscanf+0x4f4>
ffffffff812c8d9d:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812c8da1:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812c8da5:	49 89 57 08          	mov    %rdx,0x8(%r15)
ffffffff812c8da9:	48 8b 00             	mov    (%rax),%rax
ffffffff812c8dac:	44 89 00             	mov    %r8d,(%rax)
			break;
		}
		num++;

		if (!next)
ffffffff812c8daf:	4c 8b 6d c8          	mov    -0x38(%rbp),%r13
				*va_arg(args, int *) = val.s;
			else
				*va_arg(args, unsigned int *) = val.u;
			break;
		}
		num++;
ffffffff812c8db3:	ff 45 b4             	incl   -0x4c(%rbp)

		if (!next)
ffffffff812c8db6:	4d 85 ed             	test   %r13,%r13
ffffffff812c8db9:	0f 85 1c fb ff ff    	jne    ffffffff812c88db <vsscanf+0x26>
ffffffff812c8dbf:	eb 0e                	jmp    ffffffff812c8dcf <vsscanf+0x51a>
		{
			char *s = (char *)va_arg(args, char*);
			if (field_width == -1)
				field_width = 1;
			do {
				*s++ = *str++;
ffffffff812c8dc1:	41 8a 55 00          	mov    0x0(%r13),%dl
ffffffff812c8dc5:	49 ff c5             	inc    %r13
ffffffff812c8dc8:	88 10                	mov    %dl,(%rax)
ffffffff812c8dca:	e9 b7 fd ff ff       	jmpq   ffffffff812c8b86 <vsscanf+0x2d1>
			break;
		str = next;
	}

	return num;
}
ffffffff812c8dcf:	8b 45 b4             	mov    -0x4c(%rbp),%eax
ffffffff812c8dd2:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812c8dd6:	5b                   	pop    %rbx
ffffffff812c8dd7:	41 5c                	pop    %r12
ffffffff812c8dd9:	41 5d                	pop    %r13
ffffffff812c8ddb:	41 5e                	pop    %r14
ffffffff812c8ddd:	41 5f                	pop    %r15
ffffffff812c8ddf:	5d                   	pop    %rbp
ffffffff812c8de0:	c3                   	retq   

ffffffff812c8de1 <sscanf>:
 * @buf:	input buffer
 * @fmt:	formatting of buffer
 * @...:	resulting arguments
 */
int sscanf(const char *buf, const char *fmt, ...)
{
ffffffff812c8de1:	55                   	push   %rbp
ffffffff812c8de2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c8de5:	48 83 ec 50          	sub    $0x50,%rsp
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812c8de9:	48 8d 45 10          	lea    0x10(%rbp),%rax
 * @buf:	input buffer
 * @fmt:	formatting of buffer
 * @...:	resulting arguments
 */
int sscanf(const char *buf, const char *fmt, ...)
{
ffffffff812c8ded:	48 89 55 e0          	mov    %rdx,-0x20(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
	i = vsscanf(buf, fmt, args);
ffffffff812c8df1:	48 8d 55 b8          	lea    -0x48(%rbp),%rdx
 * @buf:	input buffer
 * @fmt:	formatting of buffer
 * @...:	resulting arguments
 */
int sscanf(const char *buf, const char *fmt, ...)
{
ffffffff812c8df5:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
ffffffff812c8df9:	4c 89 45 f0          	mov    %r8,-0x10(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812c8dfd:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812c8e01:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
 * @buf:	input buffer
 * @fmt:	formatting of buffer
 * @...:	resulting arguments
 */
int sscanf(const char *buf, const char *fmt, ...)
{
ffffffff812c8e05:	4c 89 4d f8          	mov    %r9,-0x8(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812c8e09:	c7 45 b8 10 00 00 00 	movl   $0x10,-0x48(%rbp)
ffffffff812c8e10:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	i = vsscanf(buf, fmt, args);
ffffffff812c8e14:	e8 9c fa ff ff       	callq  ffffffff812c88b5 <vsscanf>
	va_end(args);

	return i;
}
ffffffff812c8e19:	c9                   	leaveq 
ffffffff812c8e1a:	c3                   	retq   

ffffffff812c8e1b <simple_strtol>:
ffffffff812c8e1b:	55                   	push   %rbp
ffffffff812c8e1c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c8e1f:	e8 74 fa ff ff       	callq  ffffffff812c8898 <simple_strtoll>
ffffffff812c8e24:	5d                   	pop    %rbp
ffffffff812c8e25:	c3                   	retq   

ffffffff812c8e26 <string.isra.3>:

	return buf;
}

static noinline_for_stack
char *string(char *buf, char *end, const char *s, struct printf_spec spec)
ffffffff812c8e26:	55                   	push   %rbp
{
	int len, i;

	if ((unsigned long)s < PAGE_SIZE)
		s = "(null)";
ffffffff812c8e27:	48 c7 c0 7c 6d 7b 81 	mov    $0xffffffff817b6d7c,%rax

	return buf;
}

static noinline_for_stack
char *string(char *buf, char *end, const char *s, struct printf_spec spec)
ffffffff812c8e2e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c8e31:	41 56                	push   %r14
ffffffff812c8e33:	41 55                	push   %r13
ffffffff812c8e35:	41 54                	push   %r12
ffffffff812c8e37:	53                   	push   %rbx
ffffffff812c8e38:	49 89 d6             	mov    %rdx,%r14
ffffffff812c8e3b:	41 88 cd             	mov    %cl,%r13b
ffffffff812c8e3e:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c8e41:	49 89 f4             	mov    %rsi,%r12
ffffffff812c8e44:	48 83 ec 10          	sub    $0x10,%rsp
{
	int len, i;

	if ((unsigned long)s < PAGE_SIZE)
		s = "(null)";
ffffffff812c8e48:	48 81 fa ff 0f 00 00 	cmp    $0xfff,%rdx

	len = strnlen(s, spec.precision);
ffffffff812c8e4f:	49 0f bf f1          	movswq %r9w,%rsi
char *string(char *buf, char *end, const char *s, struct printf_spec spec)
{
	int len, i;

	if ((unsigned long)s < PAGE_SIZE)
		s = "(null)";
ffffffff812c8e53:	4c 0f 46 f0          	cmovbe %rax,%r14

	return buf;
}

static noinline_for_stack
char *string(char *buf, char *end, const char *s, struct printf_spec spec)
ffffffff812c8e57:	44 89 45 dc          	mov    %r8d,-0x24(%rbp)
	int len, i;

	if ((unsigned long)s < PAGE_SIZE)
		s = "(null)";

	len = strnlen(s, spec.precision);
ffffffff812c8e5b:	4c 89 f7             	mov    %r14,%rdi
ffffffff812c8e5e:	e8 22 ef ff ff       	callq  ffffffff812c7d85 <strnlen>

	if (!(spec.flags & LEFT)) {
ffffffff812c8e63:	41 80 e5 02          	and    $0x2,%r13b
ffffffff812c8e67:	44 8b 45 dc          	mov    -0x24(%rbp),%r8d
ffffffff812c8e6b:	75 20                	jne    ffffffff812c8e8d <string.isra.3+0x67>
ffffffff812c8e6d:	41 8d 50 ff          	lea    -0x1(%r8),%edx
		while (len < spec.field_width--) {
ffffffff812c8e71:	45 0f bf c0          	movswl %r8w,%r8d
ffffffff812c8e75:	44 39 c0             	cmp    %r8d,%eax
ffffffff812c8e78:	7d 10                	jge    ffffffff812c8e8a <string.isra.3+0x64>
			if (buf < end)
ffffffff812c8e7a:	4c 39 e3             	cmp    %r12,%rbx
ffffffff812c8e7d:	73 03                	jae    ffffffff812c8e82 <string.isra.3+0x5c>
				*buf = ' ';
ffffffff812c8e7f:	c6 03 20             	movb   $0x20,(%rbx)
			++buf;
ffffffff812c8e82:	48 ff c3             	inc    %rbx
ffffffff812c8e85:	41 89 d0             	mov    %edx,%r8d
ffffffff812c8e88:	eb e3                	jmp    ffffffff812c8e6d <string.isra.3+0x47>
		s = "(null)";

	len = strnlen(s, spec.precision);

	if (!(spec.flags & LEFT)) {
		while (len < spec.field_width--) {
ffffffff812c8e8a:	41 89 d0             	mov    %edx,%r8d
ffffffff812c8e8d:	31 d2                	xor    %edx,%edx
			if (buf < end)
				*buf = ' ';
			++buf;
		}
	}
	for (i = 0; i < len; ++i) {
ffffffff812c8e8f:	39 d0                	cmp    %edx,%eax
ffffffff812c8e91:	7e 21                	jle    ffffffff812c8eb4 <string.isra.3+0x8e>
		if (buf < end)
ffffffff812c8e93:	49 39 dc             	cmp    %rbx,%r12
ffffffff812c8e96:	76 06                	jbe    ffffffff812c8e9e <string.isra.3+0x78>
			*buf = *s;
ffffffff812c8e98:	41 8a 0c 16          	mov    (%r14,%rdx,1),%cl
ffffffff812c8e9c:	88 0b                	mov    %cl,(%rbx)
		++buf; ++s;
ffffffff812c8e9e:	48 ff c3             	inc    %rbx
ffffffff812c8ea1:	48 ff c2             	inc    %rdx
ffffffff812c8ea4:	eb e9                	jmp    ffffffff812c8e8f <string.isra.3+0x69>
	}
	while (len < spec.field_width--) {
		if (buf < end)
ffffffff812c8ea6:	49 39 dc             	cmp    %rbx,%r12
ffffffff812c8ea9:	76 03                	jbe    ffffffff812c8eae <string.isra.3+0x88>
			*buf = ' ';
ffffffff812c8eab:	c6 03 20             	movb   $0x20,(%rbx)
		++buf;
ffffffff812c8eae:	48 ff c3             	inc    %rbx
	for (i = 0; i < len; ++i) {
		if (buf < end)
			*buf = *s;
		++buf; ++s;
	}
	while (len < spec.field_width--) {
ffffffff812c8eb1:	41 89 d0             	mov    %edx,%r8d
ffffffff812c8eb4:	41 8d 50 ff          	lea    -0x1(%r8),%edx
ffffffff812c8eb8:	45 0f bf c0          	movswl %r8w,%r8d
ffffffff812c8ebc:	44 39 c0             	cmp    %r8d,%eax
ffffffff812c8ebf:	7c e5                	jl     ffffffff812c8ea6 <string.isra.3+0x80>
			*buf = ' ';
		++buf;
	}

	return buf;
}
ffffffff812c8ec1:	5a                   	pop    %rdx
ffffffff812c8ec2:	48 89 d8             	mov    %rbx,%rax
ffffffff812c8ec5:	59                   	pop    %rcx
ffffffff812c8ec6:	5b                   	pop    %rbx
ffffffff812c8ec7:	41 5c                	pop    %r12
ffffffff812c8ec9:	41 5d                	pop    %r13
ffffffff812c8ecb:	41 5e                	pop    %r14
ffffffff812c8ecd:	5d                   	pop    %rbp
ffffffff812c8ece:	c3                   	retq   

ffffffff812c8ecf <hex_string.isra.4>:
{
	int i, len = 1;		/* if we pass '%ph[CDN]', field width remains
				   negative value, fallback to the default */
	char separator;

	if (spec.field_width == 0)
ffffffff812c8ecf:	66 45 85 c0          	test   %r8w,%r8w
		/* nothing to print */
		return buf;
ffffffff812c8ed3:	48 89 f8             	mov    %rdi,%rax
{
	int i, len = 1;		/* if we pass '%ph[CDN]', field width remains
				   negative value, fallback to the default */
	char separator;

	if (spec.field_width == 0)
ffffffff812c8ed6:	0f 84 ab 00 00 00    	je     ffffffff812c8f87 <hex_string.isra.4+0xb8>

	return string(buf, end, sym, spec);
}

static noinline_for_stack
char *hex_string(char *buf, char *end, u8 *addr, struct printf_spec spec,
ffffffff812c8edc:	55                   	push   %rbp

	if (spec.field_width == 0)
		/* nothing to print */
		return buf;

	if (ZERO_OR_NULL_PTR(addr))
ffffffff812c8edd:	48 83 fa 10          	cmp    $0x10,%rdx

	return string(buf, end, sym, spec);
}

static noinline_for_stack
char *hex_string(char *buf, char *end, u8 *addr, struct printf_spec spec,
ffffffff812c8ee1:	48 89 e5             	mov    %rsp,%rbp

	if (spec.field_width == 0)
		/* nothing to print */
		return buf;

	if (ZERO_OR_NULL_PTR(addr))
ffffffff812c8ee4:	77 0c                	ja     ffffffff812c8ef2 <hex_string.isra.4+0x23>
		/* NULL pointer */
		return string(buf, end, NULL, spec);
ffffffff812c8ee6:	31 d2                	xor    %edx,%edx
ffffffff812c8ee8:	e8 39 ff ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c8eed:	e9 94 00 00 00       	jmpq   ffffffff812c8f86 <hex_string.isra.4+0xb7>
ffffffff812c8ef2:	48 8b 45 10          	mov    0x10(%rbp),%rax

	switch (fmt[1]) {
ffffffff812c8ef6:	41 b2 20             	mov    $0x20,%r10b
ffffffff812c8ef9:	8a 40 01             	mov    0x1(%rax),%al
ffffffff812c8efc:	83 e8 43             	sub    $0x43,%eax
ffffffff812c8eff:	3c 0b                	cmp    $0xb,%al
ffffffff812c8f01:	77 0a                	ja     ffffffff812c8f0d <hex_string.isra.4+0x3e>
ffffffff812c8f03:	0f b6 c0             	movzbl %al,%eax
ffffffff812c8f06:	44 8a 90 20 f4 63 81 	mov    -0x7e9c0be0(%rax),%r10b
	default:
		separator = ' ';
		break;
	}

	if (spec.field_width > 0)
ffffffff812c8f0d:	66 45 85 c0          	test   %r8w,%r8w

static noinline_for_stack
char *hex_string(char *buf, char *end, u8 *addr, struct printf_spec spec,
		 const char *fmt)
{
	int i, len = 1;		/* if we pass '%ph[CDN]', field width remains
ffffffff812c8f11:	41 b9 01 00 00 00    	mov    $0x1,%r9d
	default:
		separator = ' ';
		break;
	}

	if (spec.field_width > 0)
ffffffff812c8f17:	7e 11                	jle    ffffffff812c8f2a <hex_string.isra.4+0x5b>
		len = min_t(int, spec.field_width, 64);
ffffffff812c8f19:	45 0f bf c8          	movswl %r8w,%r9d
ffffffff812c8f1d:	b8 40 00 00 00       	mov    $0x40,%eax
ffffffff812c8f22:	41 83 f9 40          	cmp    $0x40,%r9d
ffffffff812c8f26:	44 0f 4f c8          	cmovg  %eax,%r9d
		++buf;
		if (buf < end)
			*buf = hex_asc_lo(addr[i]);
		++buf;

		if (separator && i != len - 1) {
ffffffff812c8f2a:	45 8d 41 ff          	lea    -0x1(%r9),%r8d
	}

	if (spec.field_width > 0)
		len = min_t(int, spec.field_width, 64);

	for (i = 0; i < len; ++i) {
ffffffff812c8f2e:	31 c9                	xor    %ecx,%ecx
		if (buf < end)
ffffffff812c8f30:	48 39 fe             	cmp    %rdi,%rsi
ffffffff812c8f33:	76 11                	jbe    ffffffff812c8f46 <hex_string.isra.4+0x77>
			*buf = hex_asc_hi(addr[i]);
ffffffff812c8f35:	8a 04 0a             	mov    (%rdx,%rcx,1),%al
ffffffff812c8f38:	c0 e8 04             	shr    $0x4,%al
ffffffff812c8f3b:	83 e0 0f             	and    $0xf,%eax
ffffffff812c8f3e:	8a 80 b0 32 64 81    	mov    -0x7e9bcd50(%rax),%al
ffffffff812c8f44:	88 07                	mov    %al,(%rdi)
		++buf;
		if (buf < end)
ffffffff812c8f46:	48 8d 47 01          	lea    0x1(%rdi),%rax
ffffffff812c8f4a:	48 39 c6             	cmp    %rax,%rsi
ffffffff812c8f4d:	76 0f                	jbe    ffffffff812c8f5e <hex_string.isra.4+0x8f>
			*buf = hex_asc_lo(addr[i]);
ffffffff812c8f4f:	8a 04 0a             	mov    (%rdx,%rcx,1),%al
ffffffff812c8f52:	83 e0 0f             	and    $0xf,%eax
ffffffff812c8f55:	8a 80 b0 32 64 81    	mov    -0x7e9bcd50(%rax),%al
ffffffff812c8f5b:	88 47 01             	mov    %al,0x1(%rdi)
		++buf;

		if (separator && i != len - 1) {
ffffffff812c8f5e:	45 84 d2             	test   %r10b,%r10b
		if (buf < end)
			*buf = hex_asc_hi(addr[i]);
		++buf;
		if (buf < end)
			*buf = hex_asc_lo(addr[i]);
		++buf;
ffffffff812c8f61:	48 8d 47 02          	lea    0x2(%rdi),%rax

		if (separator && i != len - 1) {
ffffffff812c8f65:	74 12                	je     ffffffff812c8f79 <hex_string.isra.4+0xaa>
ffffffff812c8f67:	41 39 c8             	cmp    %ecx,%r8d
ffffffff812c8f6a:	74 0d                	je     ffffffff812c8f79 <hex_string.isra.4+0xaa>
			if (buf < end)
ffffffff812c8f6c:	48 39 c6             	cmp    %rax,%rsi
ffffffff812c8f6f:	76 04                	jbe    ffffffff812c8f75 <hex_string.isra.4+0xa6>
				*buf = separator;
ffffffff812c8f71:	44 88 57 02          	mov    %r10b,0x2(%rdi)
			++buf;
ffffffff812c8f75:	48 8d 47 03          	lea    0x3(%rdi),%rax
ffffffff812c8f79:	48 ff c1             	inc    %rcx
	}

	if (spec.field_width > 0)
		len = min_t(int, spec.field_width, 64);

	for (i = 0; i < len; ++i) {
ffffffff812c8f7c:	41 39 c9             	cmp    %ecx,%r9d
ffffffff812c8f7f:	7e 05                	jle    ffffffff812c8f86 <hex_string.isra.4+0xb7>
ffffffff812c8f81:	48 89 c7             	mov    %rax,%rdi
ffffffff812c8f84:	eb aa                	jmp    ffffffff812c8f30 <hex_string.isra.4+0x61>
			++buf;
		}
	}

	return buf;
}
ffffffff812c8f86:	5d                   	pop    %rbp
ffffffff812c8f87:	c3                   	retq   

ffffffff812c8f88 <ip4_addr_string.isra.6>:

	return string(buf, end, ip6_addr, spec);
}

static noinline_for_stack
char *ip4_addr_string(char *buf, char *end, const u8 *addr,
ffffffff812c8f88:	55                   	push   %rbp
ffffffff812c8f89:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c8f8c:	41 54                	push   %r12
ffffffff812c8f8e:	53                   	push   %rbx
ffffffff812c8f8f:	49 89 f4             	mov    %rsi,%r12
ffffffff812c8f92:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c8f95:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c8f98:	48 83 ec 20          	sub    $0x20,%rsp
		      struct printf_spec spec, const char *fmt)
{
	char ip4_addr[sizeof("255.255.255.255")];

	ip4_string(ip4_addr, addr, fmt);
ffffffff812c8f9c:	48 8b 55 10          	mov    0x10(%rbp),%rdx
ffffffff812c8fa0:	48 8d 7d e0          	lea    -0x20(%rbp),%rdi

	return string(buf, end, ip6_addr, spec);
}

static noinline_for_stack
char *ip4_addr_string(char *buf, char *end, const u8 *addr,
ffffffff812c8fa4:	88 4d d7             	mov    %cl,-0x29(%rbp)
ffffffff812c8fa7:	44 89 45 d8          	mov    %r8d,-0x28(%rbp)
ffffffff812c8fab:	44 89 4d dc          	mov    %r9d,-0x24(%rbp)
		      struct printf_spec spec, const char *fmt)
{
	char ip4_addr[sizeof("255.255.255.255")];

	ip4_string(ip4_addr, addr, fmt);
ffffffff812c8faf:	e8 14 f5 ff ff       	callq  ffffffff812c84c8 <ip4_string>

	return string(buf, end, ip4_addr, spec);
ffffffff812c8fb4:	44 8b 4d dc          	mov    -0x24(%rbp),%r9d
ffffffff812c8fb8:	44 8b 45 d8          	mov    -0x28(%rbp),%r8d
ffffffff812c8fbc:	48 8d 55 e0          	lea    -0x20(%rbp),%rdx
ffffffff812c8fc0:	8a 4d d7             	mov    -0x29(%rbp),%cl
ffffffff812c8fc3:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c8fc6:	48 89 df             	mov    %rbx,%rdi
ffffffff812c8fc9:	e8 58 fe ff ff       	callq  ffffffff812c8e26 <string.isra.3>
}
ffffffff812c8fce:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812c8fd2:	5b                   	pop    %rbx
ffffffff812c8fd3:	41 5c                	pop    %r12
ffffffff812c8fd5:	5d                   	pop    %rbp
ffffffff812c8fd6:	c3                   	retq   

ffffffff812c8fd7 <clock.isra.8>:

	return number(buf, end, num, spec);
}

static noinline_for_stack
char *clock(char *buf, char *end, struct clk *clk, struct printf_spec spec,
ffffffff812c8fd7:	55                   	push   %rbp
	    const char *fmt)
{
	if (!IS_ENABLED(CONFIG_HAVE_CLK) || !clk)
		return string(buf, end, NULL, spec);
ffffffff812c8fd8:	45 89 c1             	mov    %r8d,%r9d
ffffffff812c8fdb:	41 89 c8             	mov    %ecx,%r8d
ffffffff812c8fde:	88 d1                	mov    %dl,%cl
ffffffff812c8fe0:	31 d2                	xor    %edx,%edx

	return number(buf, end, num, spec);
}

static noinline_for_stack
char *clock(char *buf, char *end, struct clk *clk, struct printf_spec spec,
ffffffff812c8fe2:	48 89 e5             	mov    %rsp,%rbp
	    const char *fmt)
{
	if (!IS_ENABLED(CONFIG_HAVE_CLK) || !clk)
		return string(buf, end, NULL, spec);
ffffffff812c8fe5:	e8 3c fe ff ff       	callq  ffffffff812c8e26 <string.isra.3>
		spec.field_width = sizeof(unsigned long) * 2 + 2;
		spec.flags |= SPECIAL | SMALL | ZEROPAD;
		return number(buf, end, (unsigned long)clk, spec);
#endif
	}
}
ffffffff812c8fea:	5d                   	pop    %rbp
ffffffff812c8feb:	c3                   	retq   

ffffffff812c8fec <symbol_string.isra.9>:
	}
	return buf;
}

static noinline_for_stack
char *symbol_string(char *buf, char *end, void *ptr,
ffffffff812c8fec:	55                   	push   %rbp
ffffffff812c8fed:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c8ff0:	41 57                	push   %r15
ffffffff812c8ff2:	41 56                	push   %r14
ffffffff812c8ff4:	41 55                	push   %r13
ffffffff812c8ff6:	41 54                	push   %r12
ffffffff812c8ff8:	41 88 cd             	mov    %cl,%r13b
ffffffff812c8ffb:	53                   	push   %rbx
ffffffff812c8ffc:	49 89 f4             	mov    %rsi,%r12
ffffffff812c8fff:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c9002:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c9005:	45 89 c6             	mov    %r8d,%r14d
ffffffff812c9008:	45 89 cf             	mov    %r9d,%r15d
ffffffff812c900b:	48 81 ec f8 00 00 00 	sub    $0xf8,%rsp
	if (fmt[1] == 'R')
		ptr = __builtin_extract_return_addr(ptr);
	value = (unsigned long)ptr;

#ifdef CONFIG_KALLSYMS
	if (*fmt == 'B')
ffffffff812c9012:	48 8b 45 10          	mov    0x10(%rbp),%rax
		sprint_backtrace(sym, value);
ffffffff812c9016:	48 8d bd e1 fe ff ff 	lea    -0x11f(%rbp),%rdi
	if (fmt[1] == 'R')
		ptr = __builtin_extract_return_addr(ptr);
	value = (unsigned long)ptr;

#ifdef CONFIG_KALLSYMS
	if (*fmt == 'B')
ffffffff812c901d:	8a 00                	mov    (%rax),%al
ffffffff812c901f:	3c 42                	cmp    $0x42,%al
ffffffff812c9021:	75 07                	jne    ffffffff812c902a <symbol_string.isra.9+0x3e>
		sprint_backtrace(sym, value);
ffffffff812c9023:	e8 aa 7c de ff       	callq  ffffffff810b0cd2 <sprint_backtrace>
ffffffff812c9028:	eb 14                	jmp    ffffffff812c903e <symbol_string.isra.9+0x52>
	else if (*fmt != 'f' && *fmt != 's')
ffffffff812c902a:	3c 66                	cmp    $0x66,%al
ffffffff812c902c:	74 0b                	je     ffffffff812c9039 <symbol_string.isra.9+0x4d>
ffffffff812c902e:	3c 73                	cmp    $0x73,%al
ffffffff812c9030:	74 07                	je     ffffffff812c9039 <symbol_string.isra.9+0x4d>
		sprint_symbol(sym, value);
ffffffff812c9032:	e8 7f 7b de ff       	callq  ffffffff810b0bb6 <sprint_symbol>
ffffffff812c9037:	eb 05                	jmp    ffffffff812c903e <symbol_string.isra.9+0x52>
	else
		sprint_symbol_no_offset(sym, value);
ffffffff812c9039:	e8 c0 7b de ff       	callq  ffffffff810b0bfe <sprint_symbol_no_offset>

	return string(buf, end, sym, spec);
ffffffff812c903e:	48 8d 95 e1 fe ff ff 	lea    -0x11f(%rbp),%rdx
ffffffff812c9045:	45 89 f9             	mov    %r15d,%r9d
ffffffff812c9048:	45 89 f0             	mov    %r14d,%r8d
ffffffff812c904b:	44 88 e9             	mov    %r13b,%cl
ffffffff812c904e:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c9051:	48 89 df             	mov    %rbx,%rdi
ffffffff812c9054:	e8 cd fd ff ff       	callq  ffffffff812c8e26 <string.isra.3>
	spec.flags |= SPECIAL | SMALL | ZEROPAD;
	spec.base = 16;

	return number(buf, end, value, spec);
#endif
}
ffffffff812c9059:	48 81 c4 f8 00 00 00 	add    $0xf8,%rsp
ffffffff812c9060:	5b                   	pop    %rbx
ffffffff812c9061:	41 5c                	pop    %r12
ffffffff812c9063:	41 5d                	pop    %r13
ffffffff812c9065:	41 5e                	pop    %r14
ffffffff812c9067:	41 5f                	pop    %r15
ffffffff812c9069:	5d                   	pop    %rbp
ffffffff812c906a:	c3                   	retq   

ffffffff812c906b <escaped_string.isra.11>:

	return string(buf, end, ip4_addr, spec);
}

static noinline_for_stack
char *escaped_string(char *buf, char *end, u8 *addr, struct printf_spec spec,
ffffffff812c906b:	55                   	push   %rbp
	bool found = true;
	int count = 1;
	unsigned int flags = 0;
	int len;

	if (spec.field_width == 0)
ffffffff812c906c:	66 45 85 c0          	test   %r8w,%r8w
ffffffff812c9070:	48 89 f8             	mov    %rdi,%rax

	return string(buf, end, ip4_addr, spec);
}

static noinline_for_stack
char *escaped_string(char *buf, char *end, u8 *addr, struct printf_spec spec,
ffffffff812c9073:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9076:	53                   	push   %rbx
ffffffff812c9077:	41 52                	push   %r10
ffffffff812c9079:	4c 8b 5d 10          	mov    0x10(%rbp),%r11
	bool found = true;
	int count = 1;
	unsigned int flags = 0;
	int len;

	if (spec.field_width == 0)
ffffffff812c907d:	0f 84 a3 00 00 00    	je     ffffffff812c9126 <escaped_string.isra.11+0xbb>
		return buf;				/* nothing to print */

	if (ZERO_OR_NULL_PTR(addr))
ffffffff812c9083:	48 83 fa 10          	cmp    $0x10,%rdx
ffffffff812c9087:	49 89 d2             	mov    %rdx,%r10
ffffffff812c908a:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c908d:	76 09                	jbe    ffffffff812c9098 <escaped_string.isra.11+0x2d>
ffffffff812c908f:	31 c0                	xor    %eax,%eax
ffffffff812c9091:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812c9096:	eb 12                	jmp    ffffffff812c90aa <escaped_string.isra.11+0x3f>
		return string(buf, end, NULL, spec);	/* NULL pointer */
ffffffff812c9098:	31 d2                	xor    %edx,%edx
ffffffff812c909a:	e8 87 fd ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c909f:	e9 82 00 00 00       	jmpq   ffffffff812c9126 <escaped_string.isra.11+0xbb>


	do {
		switch (fmt[count++]) {
		case 'a':
			flags |= ESCAPE_ANY;
ffffffff812c90a4:	83 c8 0f             	or     $0xf,%eax
	if (ZERO_OR_NULL_PTR(addr))
		return string(buf, end, NULL, spec);	/* NULL pointer */


	do {
		switch (fmt[count++]) {
ffffffff812c90a7:	48 63 ca             	movslq %edx,%rcx
ffffffff812c90aa:	8d 51 01             	lea    0x1(%rcx),%edx
ffffffff812c90ad:	41 8a 0c 0b          	mov    (%r11,%rcx,1),%cl
ffffffff812c90b1:	83 e9 61             	sub    $0x61,%ecx
ffffffff812c90b4:	80 f9 12             	cmp    $0x12,%cl
ffffffff812c90b7:	77 28                	ja     ffffffff812c90e1 <escaped_string.isra.11+0x76>
ffffffff812c90b9:	0f b6 c9             	movzbl %cl,%ecx
ffffffff812c90bc:	ff 24 cd 60 f1 63 81 	jmpq   *-0x7e9c0ea0(,%rcx,8)
		case 'a':
			flags |= ESCAPE_ANY;
			break;
		case 'c':
			flags |= ESCAPE_SPECIAL;
ffffffff812c90c3:	83 c8 02             	or     $0x2,%eax
ffffffff812c90c6:	eb df                	jmp    ffffffff812c90a7 <escaped_string.isra.11+0x3c>
			break;
		case 'h':
			flags |= ESCAPE_HEX;
ffffffff812c90c8:	83 c8 20             	or     $0x20,%eax
ffffffff812c90cb:	eb da                	jmp    ffffffff812c90a7 <escaped_string.isra.11+0x3c>
			break;
		case 'n':
			flags |= ESCAPE_NULL;
ffffffff812c90cd:	83 c8 04             	or     $0x4,%eax
ffffffff812c90d0:	eb d5                	jmp    ffffffff812c90a7 <escaped_string.isra.11+0x3c>
			break;
		case 'o':
			flags |= ESCAPE_OCTAL;
ffffffff812c90d2:	83 c8 08             	or     $0x8,%eax
ffffffff812c90d5:	eb d0                	jmp    ffffffff812c90a7 <escaped_string.isra.11+0x3c>
			break;
		case 'p':
			flags |= ESCAPE_NP;
ffffffff812c90d7:	83 c8 10             	or     $0x10,%eax
ffffffff812c90da:	eb cb                	jmp    ffffffff812c90a7 <escaped_string.isra.11+0x3c>
			break;
		case 's':
			flags |= ESCAPE_SPACE;
ffffffff812c90dc:	83 c8 01             	or     $0x1,%eax
ffffffff812c90df:	eb c6                	jmp    ffffffff812c90a7 <escaped_string.isra.11+0x3c>
			break;
		}
	} while (found);

	if (!flags)
		flags = ESCAPE_ANY_NP;
ffffffff812c90e1:	85 c0                	test   %eax,%eax
ffffffff812c90e3:	ba 1f 00 00 00       	mov    $0x1f,%edx

	len = spec.field_width < 0 ? 1 : spec.field_width;
ffffffff812c90e8:	41 0f bf c8          	movswl %r8w,%ecx
			break;
		}
	} while (found);

	if (!flags)
		flags = ESCAPE_ANY_NP;
ffffffff812c90ec:	0f 44 c2             	cmove  %edx,%eax

	len = spec.field_width < 0 ? 1 : spec.field_width;
ffffffff812c90ef:	66 45 85 c0          	test   %r8w,%r8w
ffffffff812c90f3:	ba 01 00 00 00       	mov    $0x1,%edx
ffffffff812c90f8:	0f 49 d1             	cmovns %ecx,%edx
	/*
	 * string_escape_mem() writes as many characters as it can to
	 * the given buffer, and returns the total size of the output
	 * had the buffer been big enough.
	 */
	buf += string_escape_mem(addr, len, buf, buf < end ? end - buf : 0, flags, NULL);
ffffffff812c90fb:	48 89 f1             	mov    %rsi,%rcx
ffffffff812c90fe:	41 89 c0             	mov    %eax,%r8d
ffffffff812c9101:	48 29 d9             	sub    %rbx,%rcx
ffffffff812c9104:	48 39 f3             	cmp    %rsi,%rbx
ffffffff812c9107:	be 00 00 00 00       	mov    $0x0,%esi
ffffffff812c910c:	48 0f 43 ce          	cmovae %rsi,%rcx
ffffffff812c9110:	4c 89 d7             	mov    %r10,%rdi
ffffffff812c9113:	48 63 f2             	movslq %edx,%rsi
ffffffff812c9116:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c9119:	48 89 da             	mov    %rbx,%rdx
ffffffff812c911c:	e8 89 a3 00 00       	callq  ffffffff812d34aa <string_escape_mem>
ffffffff812c9121:	48 98                	cltq   

	return buf;
ffffffff812c9123:	48 01 d8             	add    %rbx,%rax
}
ffffffff812c9126:	5a                   	pop    %rdx
ffffffff812c9127:	5b                   	pop    %rbx
ffffffff812c9128:	5d                   	pop    %rbp
ffffffff812c9129:	c3                   	retq   

ffffffff812c912a <dentry_name.isra.12>:
	}
	memset(buf, ' ', spaces);
}

static noinline_for_stack
char *dentry_name(char *buf, char *end, const struct dentry *d, struct printf_spec spec,
ffffffff812c912a:	55                   	push   %rbp
ffffffff812c912b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c912e:	41 55                	push   %r13
ffffffff812c9130:	41 54                	push   %r12
ffffffff812c9132:	53                   	push   %rbx
ffffffff812c9133:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c9136:	48 83 ec 28          	sub    $0x28,%rsp
	const char *array[4], *s;
	const struct dentry *p;
	int depth;
	int i, n;

	switch (fmt[1]) {
ffffffff812c913a:	48 8b 45 10          	mov    0x10(%rbp),%rax
ffffffff812c913e:	0f be 40 01          	movsbl 0x1(%rax),%eax
ffffffff812c9142:	8d 78 ce             	lea    -0x32(%rax),%edi
		case '2': case '3': case '4':
			depth = fmt[1] - '0';
ffffffff812c9145:	44 8d 50 d0          	lea    -0x30(%rax),%r10d
	const char *array[4], *s;
	const struct dentry *p;
	int depth;
	int i, n;

	switch (fmt[1]) {
ffffffff812c9149:	40 80 ff 02          	cmp    $0x2,%dil
ffffffff812c914d:	76 06                	jbe    ffffffff812c9155 <dentry_name.isra.12+0x2b>
ffffffff812c914f:	41 ba 01 00 00 00    	mov    $0x1,%r10d

#else /* #ifdef CONFIG_PREEMPT_RCU */

static inline void __rcu_read_lock(void)
{
	preempt_disable();
ffffffff812c9155:	31 ff                	xor    %edi,%edi
			depth = 1;
	}

	rcu_read_lock();
	for (i = 0; i < depth; i++, d = p) {
		p = ACCESS_ONCE(d->d_parent);
ffffffff812c9157:	4c 8b 5a 18          	mov    0x18(%rdx),%r11
		array[i] = ACCESS_ONCE(d->d_name.name);
ffffffff812c915b:	4c 8b 62 28          	mov    0x28(%rdx),%r12
ffffffff812c915f:	89 f8                	mov    %edi,%eax
		if (p == d) {
ffffffff812c9161:	49 39 d3             	cmp    %rdx,%r11
	}

	rcu_read_lock();
	for (i = 0; i < depth; i++, d = p) {
		p = ACCESS_ONCE(d->d_parent);
		array[i] = ACCESS_ONCE(d->d_name.name);
ffffffff812c9164:	4c 89 64 fd c0       	mov    %r12,-0x40(%rbp,%rdi,8)
		if (p == d) {
ffffffff812c9169:	75 12                	jne    ffffffff812c917d <dentry_name.isra.12+0x53>
			if (i)
ffffffff812c916b:	85 ff                	test   %edi,%edi
ffffffff812c916d:	74 19                	je     ffffffff812c9188 <dentry_name.isra.12+0x5e>
				array[i] = "";
ffffffff812c916f:	48 63 d7             	movslq %edi,%rdx
ffffffff812c9172:	48 c7 44 d5 c0 db e9 	movq   $0xffffffff8179e9db,-0x40(%rbp,%rdx,8)
ffffffff812c9179:	79 81 
ffffffff812c917b:	eb 0b                	jmp    ffffffff812c9188 <dentry_name.isra.12+0x5e>
ffffffff812c917d:	48 ff c7             	inc    %rdi
ffffffff812c9180:	4c 89 da             	mov    %r11,%rdx
		default:
			depth = 1;
	}

	rcu_read_lock();
	for (i = 0; i < depth; i++, d = p) {
ffffffff812c9183:	41 39 fa             	cmp    %edi,%r10d
ffffffff812c9186:	7f cf                	jg     ffffffff812c9157 <dentry_name.isra.12+0x2d>
				array[i] = "";
			i++;
			break;
		}
	}
	s = array[--i];
ffffffff812c9188:	48 63 d0             	movslq %eax,%rdx
	for (n = 0; n != spec.precision; n++, buf++) {
ffffffff812c918b:	45 0f bf c9          	movswl %r9w,%r9d
				array[i] = "";
			i++;
			break;
		}
	}
	s = array[--i];
ffffffff812c918f:	48 8b 7c d5 c0       	mov    -0x40(%rbp,%rdx,8),%rdi
	for (n = 0; n != spec.precision; n++, buf++) {
ffffffff812c9194:	31 d2                	xor    %edx,%edx
ffffffff812c9196:	44 39 ca             	cmp    %r9d,%edx
ffffffff812c9199:	74 2b                	je     ffffffff812c91c6 <dentry_name.isra.12+0x9c>
		char c = *s++;
ffffffff812c919b:	44 8a 17             	mov    (%rdi),%r10b
ffffffff812c919e:	48 ff c7             	inc    %rdi
		if (!c) {
ffffffff812c91a1:	45 84 d2             	test   %r10b,%r10b
ffffffff812c91a4:	75 11                	jne    ffffffff812c91b7 <dentry_name.isra.12+0x8d>
			if (!i)
ffffffff812c91a6:	85 c0                	test   %eax,%eax
ffffffff812c91a8:	74 1c                	je     ffffffff812c91c6 <dentry_name.isra.12+0x9c>
				break;
			c = '/';
			s = array[--i];
ffffffff812c91aa:	ff c8                	dec    %eax
	for (n = 0; n != spec.precision; n++, buf++) {
		char c = *s++;
		if (!c) {
			if (!i)
				break;
			c = '/';
ffffffff812c91ac:	41 b2 2f             	mov    $0x2f,%r10b
			s = array[--i];
ffffffff812c91af:	48 63 f8             	movslq %eax,%rdi
ffffffff812c91b2:	48 8b 7c fd c0       	mov    -0x40(%rbp,%rdi,8),%rdi
		}
		if (buf < end)
ffffffff812c91b7:	48 39 f3             	cmp    %rsi,%rbx
ffffffff812c91ba:	73 03                	jae    ffffffff812c91bf <dentry_name.isra.12+0x95>
			*buf = c;
ffffffff812c91bc:	44 88 13             	mov    %r10b,(%rbx)
			i++;
			break;
		}
	}
	s = array[--i];
	for (n = 0; n != spec.precision; n++, buf++) {
ffffffff812c91bf:	ff c2                	inc    %edx
ffffffff812c91c1:	48 ff c3             	inc    %rbx
ffffffff812c91c4:	eb d0                	jmp    ffffffff812c9196 <dentry_name.isra.12+0x6c>
		}
		if (buf < end)
			*buf = c;
	}
	rcu_read_unlock();
	if (n < spec.field_width) {
ffffffff812c91c6:	45 0f bf c0          	movswl %r8w,%r8d
ffffffff812c91ca:	48 89 d8             	mov    %rbx,%rax
ffffffff812c91cd:	41 39 d0             	cmp    %edx,%r8d
ffffffff812c91d0:	7e 72                	jle    ffffffff812c9244 <dentry_name.isra.12+0x11a>
		/* we want to pad the sucker */
		unsigned spaces = spec.field_width - n;
ffffffff812c91d2:	41 29 d0             	sub    %edx,%r8d
		if (!(spec.flags & LEFT)) {
ffffffff812c91d5:	80 e1 02             	and    $0x2,%cl
ffffffff812c91d8:	4d 63 e0             	movslq %r8d,%r12
ffffffff812c91db:	4a 8d 04 23          	lea    (%rbx,%r12,1),%rax
ffffffff812c91df:	75 5e                	jne    ffffffff812c923f <dentry_name.isra.12+0x115>
			widen(buf - n, end, n, spaces);
ffffffff812c91e1:	48 63 c2             	movslq %edx,%rax
ffffffff812c91e4:	49 89 dd             	mov    %rbx,%r13
ffffffff812c91e7:	49 29 c5             	sub    %rax,%r13
}

static void widen(char *buf, char *end, unsigned len, unsigned spaces)
{
	size_t size;
	if (buf >= end)	/* nowhere to put anything */
ffffffff812c91ea:	4c 39 ee             	cmp    %r13,%rsi
ffffffff812c91ed:	76 3f                	jbe    ffffffff812c922e <dentry_name.isra.12+0x104>
		return;
	size = end - buf;
ffffffff812c91ef:	4c 29 ee             	sub    %r13,%rsi
	if (size <= spaces) {
ffffffff812c91f2:	4c 39 e6             	cmp    %r12,%rsi
static void widen(char *buf, char *end, unsigned len, unsigned spaces)
{
	size_t size;
	if (buf >= end)	/* nowhere to put anything */
		return;
	size = end - buf;
ffffffff812c91f5:	48 89 f1             	mov    %rsi,%rcx
	if (size <= spaces) {
ffffffff812c91f8:	77 07                	ja     ffffffff812c9201 <dentry_name.isra.12+0xd7>
		memset(buf, ' ', size);
ffffffff812c91fa:	b0 20                	mov    $0x20,%al
ffffffff812c91fc:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c91ff:	eb 2b                	jmp    ffffffff812c922c <dentry_name.isra.12+0x102>
		return;
	}
	if (len) {
ffffffff812c9201:	85 d2                	test   %edx,%edx
ffffffff812c9203:	74 1f                	je     ffffffff812c9224 <dentry_name.isra.12+0xfa>
		if (len > size - spaces)
ffffffff812c9205:	48 89 c8             	mov    %rcx,%rax
ffffffff812c9208:	89 d6                	mov    %edx,%esi
			len = size - spaces;
ffffffff812c920a:	44 29 c1             	sub    %r8d,%ecx
	if (size <= spaces) {
		memset(buf, ' ', size);
		return;
	}
	if (len) {
		if (len > size - spaces)
ffffffff812c920d:	4c 29 e0             	sub    %r12,%rax
			len = size - spaces;
		memmove(buf + spaces, buf, len);
ffffffff812c9210:	4b 8d 7c 25 00       	lea    0x0(%r13,%r12,1),%rdi
		memset(buf, ' ', size);
		return;
	}
	if (len) {
		if (len > size - spaces)
			len = size - spaces;
ffffffff812c9215:	48 39 c6             	cmp    %rax,%rsi
		memmove(buf + spaces, buf, len);
ffffffff812c9218:	4c 89 ee             	mov    %r13,%rsi
		memset(buf, ' ', size);
		return;
	}
	if (len) {
		if (len > size - spaces)
			len = size - spaces;
ffffffff812c921b:	48 0f 47 d1          	cmova  %rcx,%rdx
		memmove(buf + spaces, buf, len);
ffffffff812c921f:	e8 dc 27 00 00       	callq  ffffffff812cba00 <__memmove>
	}
	memset(buf, ' ', spaces);
ffffffff812c9224:	b0 20                	mov    $0x20,%al
ffffffff812c9226:	4c 89 ef             	mov    %r13,%rdi
ffffffff812c9229:	4c 89 e1             	mov    %r12,%rcx
ffffffff812c922c:	f3 aa                	rep stos %al,%es:(%rdi)
	if (n < spec.field_width) {
		/* we want to pad the sucker */
		unsigned spaces = spec.field_width - n;
		if (!(spec.flags & LEFT)) {
			widen(buf - n, end, n, spaces);
			return buf + spaces;
ffffffff812c922e:	4a 8d 04 23          	lea    (%rbx,%r12,1),%rax
ffffffff812c9232:	eb 10                	jmp    ffffffff812c9244 <dentry_name.isra.12+0x11a>
		}
		while (spaces--) {
			if (buf < end)
ffffffff812c9234:	48 39 de             	cmp    %rbx,%rsi
ffffffff812c9237:	76 03                	jbe    ffffffff812c923c <dentry_name.isra.12+0x112>
				*buf = ' ';
ffffffff812c9239:	c6 03 20             	movb   $0x20,(%rbx)
			++buf;
ffffffff812c923c:	48 ff c3             	inc    %rbx
		unsigned spaces = spec.field_width - n;
		if (!(spec.flags & LEFT)) {
			widen(buf - n, end, n, spaces);
			return buf + spaces;
		}
		while (spaces--) {
ffffffff812c923f:	48 39 d8             	cmp    %rbx,%rax
ffffffff812c9242:	75 f0                	jne    ffffffff812c9234 <dentry_name.isra.12+0x10a>
				*buf = ' ';
			++buf;
		}
	}
	return buf;
}
ffffffff812c9244:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c9248:	5b                   	pop    %rbx
ffffffff812c9249:	41 5c                	pop    %r12
ffffffff812c924b:	41 5d                	pop    %r13
ffffffff812c924d:	5d                   	pop    %rbp
ffffffff812c924e:	c3                   	retq   

ffffffff812c924f <number.isra.13>:
	s16	field_width;	/* width of output field */
	s16	precision;	/* # of digits/chars */
};

static noinline_for_stack
char *number(char *buf, char *end, unsigned long long num,
ffffffff812c924f:	55                   	push   %rbp
ffffffff812c9250:	49 89 fb             	mov    %rdi,%r11
ffffffff812c9253:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9256:	41 57                	push   %r15
ffffffff812c9258:	41 56                	push   %r14
ffffffff812c925a:	41 55                	push   %r13
ffffffff812c925c:	41 54                	push   %r12
ffffffff812c925e:	49 89 d5             	mov    %rdx,%r13
ffffffff812c9261:	53                   	push   %rbx
{
	/* put_dec requires 2-byte alignment of the buffer. */
	char tmp[3 * sizeof(num)] __aligned(2);
	char sign;
	char locase;
	int need_pfx = ((spec.flags & SPECIAL) && spec.base != 10);
ffffffff812c9262:	88 ca                	mov    %cl,%dl
	s16	field_width;	/* width of output field */
	s16	precision;	/* # of digits/chars */
};

static noinline_for_stack
char *number(char *buf, char *end, unsigned long long num,
ffffffff812c9264:	88 cb                	mov    %cl,%bl
{
	/* put_dec requires 2-byte alignment of the buffer. */
	char tmp[3 * sizeof(num)] __aligned(2);
	char sign;
	char locase;
	int need_pfx = ((spec.flags & SPECIAL) && spec.base != 10);
ffffffff812c9266:	c0 ea 06             	shr    $0x6,%dl
	s16	field_width;	/* width of output field */
	s16	precision;	/* # of digits/chars */
};

static noinline_for_stack
char *number(char *buf, char *end, unsigned long long num,
ffffffff812c9269:	49 89 f6             	mov    %rsi,%r14
ffffffff812c926c:	48 83 ec 30          	sub    $0x30,%rsp
{
	/* put_dec requires 2-byte alignment of the buffer. */
	char tmp[3 * sizeof(num)] __aligned(2);
	char sign;
	char locase;
	int need_pfx = ((spec.flags & SPECIAL) && spec.base != 10);
ffffffff812c9270:	41 80 f8 0a          	cmp    $0xa,%r8b
	s16	field_width;	/* width of output field */
	s16	precision;	/* # of digits/chars */
};

static noinline_for_stack
char *number(char *buf, char *end, unsigned long long num,
ffffffff812c9274:	44 8b 65 10          	mov    0x10(%rbp),%r12d
{
	/* put_dec requires 2-byte alignment of the buffer. */
	char tmp[3 * sizeof(num)] __aligned(2);
	char sign;
	char locase;
	int need_pfx = ((spec.flags & SPECIAL) && spec.base != 10);
ffffffff812c9278:	0f 95 c0             	setne  %al
ffffffff812c927b:	21 c2                	and    %eax,%edx
	int i;
	bool is_zero = num == 0LL;

	/* locase = 0 or 0x20. ORing digits or letters with 'locase'
	 * produces same digits or (maybe lowercased) letters */
	locase = (spec.flags & SMALL);
ffffffff812c927d:	88 c8                	mov    %cl,%al
ffffffff812c927f:	83 e0 20             	and    $0x20,%eax
{
	/* put_dec requires 2-byte alignment of the buffer. */
	char tmp[3 * sizeof(num)] __aligned(2);
	char sign;
	char locase;
	int need_pfx = ((spec.flags & SPECIAL) && spec.base != 10);
ffffffff812c9282:	88 55 b6             	mov    %dl,-0x4a(%rbp)
	int i;
	bool is_zero = num == 0LL;

	/* locase = 0 or 0x20. ORing digits or letters with 'locase'
	 * produces same digits or (maybe lowercased) letters */
	locase = (spec.flags & SMALL);
ffffffff812c9285:	88 45 b7             	mov    %al,-0x49(%rbp)
	if (spec.flags & LEFT)
		spec.flags &= ~ZEROPAD;
ffffffff812c9288:	88 c8                	mov    %cl,%al
ffffffff812c928a:	83 e0 ef             	and    $0xffffffef,%eax
ffffffff812c928d:	80 e1 02             	and    $0x2,%cl
ffffffff812c9290:	0f 45 d8             	cmovne %eax,%ebx
	sign = 0;
	if (spec.flags & SIGN) {
ffffffff812c9293:	f6 c3 01             	test   $0x1,%bl
ffffffff812c9296:	74 33                	je     ffffffff812c92cb <number.isra.13+0x7c>
		if ((signed long long)num < 0) {
ffffffff812c9298:	4d 85 ed             	test   %r13,%r13
ffffffff812c929b:	79 0e                	jns    ffffffff812c92ab <number.isra.13+0x5c>
			sign = '-';
			num = -(signed long long)num;
ffffffff812c929d:	4c 89 ee             	mov    %r13,%rsi
			spec.field_width--;
ffffffff812c92a0:	41 ff c9             	dec    %r9d
	if (spec.flags & LEFT)
		spec.flags &= ~ZEROPAD;
	sign = 0;
	if (spec.flags & SIGN) {
		if ((signed long long)num < 0) {
			sign = '-';
ffffffff812c92a3:	41 b7 2d             	mov    $0x2d,%r15b
			num = -(signed long long)num;
ffffffff812c92a6:	48 f7 de             	neg    %rsi
ffffffff812c92a9:	eb 26                	jmp    ffffffff812c92d1 <number.isra.13+0x82>
			spec.field_width--;
		} else if (spec.flags & PLUS) {
ffffffff812c92ab:	f6 c3 04             	test   $0x4,%bl
ffffffff812c92ae:	74 0b                	je     ffffffff812c92bb <number.isra.13+0x6c>
			sign = '+';
			spec.field_width--;
ffffffff812c92b0:	41 ff c9             	dec    %r9d
ffffffff812c92b3:	4c 89 ee             	mov    %r13,%rsi
		if ((signed long long)num < 0) {
			sign = '-';
			num = -(signed long long)num;
			spec.field_width--;
		} else if (spec.flags & PLUS) {
			sign = '+';
ffffffff812c92b6:	41 b7 2b             	mov    $0x2b,%r15b
ffffffff812c92b9:	eb 16                	jmp    ffffffff812c92d1 <number.isra.13+0x82>
			spec.field_width--;
		} else if (spec.flags & SPACE) {
ffffffff812c92bb:	f6 c3 08             	test   $0x8,%bl
ffffffff812c92be:	74 0b                	je     ffffffff812c92cb <number.isra.13+0x7c>
			sign = ' ';
			spec.field_width--;
ffffffff812c92c0:	41 ff c9             	dec    %r9d
ffffffff812c92c3:	4c 89 ee             	mov    %r13,%rsi
			spec.field_width--;
		} else if (spec.flags & PLUS) {
			sign = '+';
			spec.field_width--;
		} else if (spec.flags & SPACE) {
			sign = ' ';
ffffffff812c92c6:	41 b7 20             	mov    $0x20,%r15b
ffffffff812c92c9:	eb 06                	jmp    ffffffff812c92d1 <number.isra.13+0x82>
			num = -(signed long long)num;
			spec.field_width--;
		} else if (spec.flags & PLUS) {
			sign = '+';
			spec.field_width--;
		} else if (spec.flags & SPACE) {
ffffffff812c92cb:	4c 89 ee             	mov    %r13,%rsi
	/* locase = 0 or 0x20. ORing digits or letters with 'locase'
	 * produces same digits or (maybe lowercased) letters */
	locase = (spec.flags & SMALL);
	if (spec.flags & LEFT)
		spec.flags &= ~ZEROPAD;
	sign = 0;
ffffffff812c92ce:	45 31 ff             	xor    %r15d,%r15d
		} else if (spec.flags & SPACE) {
			sign = ' ';
			spec.field_width--;
		}
	}
	if (need_pfx) {
ffffffff812c92d1:	80 7d b6 00          	cmpb   $0x0,-0x4a(%rbp)
ffffffff812c92d5:	74 15                	je     ffffffff812c92ec <number.isra.13+0x9d>
		if (spec.base == 16)
ffffffff812c92d7:	41 80 f8 10          	cmp    $0x10,%r8b
ffffffff812c92db:	75 06                	jne    ffffffff812c92e3 <number.isra.13+0x94>
			spec.field_width -= 2;
ffffffff812c92dd:	41 83 e9 02          	sub    $0x2,%r9d
ffffffff812c92e1:	eb 09                	jmp    ffffffff812c92ec <number.isra.13+0x9d>
		else if (!is_zero)
			spec.field_width--;
ffffffff812c92e3:	49 83 fd 01          	cmp    $0x1,%r13
ffffffff812c92e7:	66 41 83 d1 ff       	adc    $0xffff,%r9w
	}

	/* generate full string in tmp[], in reverse order */
	i = 0;
	if (num < spec.base)
ffffffff812c92ec:	41 0f b6 c0          	movzbl %r8b,%eax
ffffffff812c92f0:	48 39 f0             	cmp    %rsi,%rax
ffffffff812c92f3:	76 13                	jbe    ffffffff812c9308 <number.isra.13+0xb9>
		tmp[i++] = hex_asc_upper[num] | locase;
ffffffff812c92f5:	8a 45 b7             	mov    -0x49(%rbp),%al
ffffffff812c92f8:	0a 86 90 32 64 81    	or     -0x7e9bcd70(%rsi),%al
ffffffff812c92fe:	88 45 c0             	mov    %al,-0x40(%rbp)
ffffffff812c9301:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812c9306:	eb 6a                	jmp    ffffffff812c9372 <number.isra.13+0x123>
	else if (spec.base != 10) { /* 8 or 16 */
ffffffff812c9308:	41 80 f8 0a          	cmp    $0xa,%r8b
ffffffff812c930c:	4c 8d 55 c0          	lea    -0x40(%rbp),%r10
ffffffff812c9310:	74 3d                	je     ffffffff812c934f <number.isra.13+0x100>
		int mask = spec.base - 1;
ffffffff812c9312:	41 0f b6 c0          	movzbl %r8b,%eax
		int shift = 3;

		if (spec.base == 16)
			shift = 4;
ffffffff812c9316:	31 c9                	xor    %ecx,%ecx
	/* generate full string in tmp[], in reverse order */
	i = 0;
	if (num < spec.base)
		tmp[i++] = hex_asc_upper[num] | locase;
	else if (spec.base != 10) { /* 8 or 16 */
		int mask = spec.base - 1;
ffffffff812c9318:	ff c8                	dec    %eax
		int shift = 3;

		if (spec.base == 16)
			shift = 4;
ffffffff812c931a:	41 80 f8 10          	cmp    $0x10,%r8b
ffffffff812c931e:	0f 94 c1             	sete   %cl
	/* generate full string in tmp[], in reverse order */
	i = 0;
	if (num < spec.base)
		tmp[i++] = hex_asc_upper[num] | locase;
	else if (spec.base != 10) { /* 8 or 16 */
		int mask = spec.base - 1;
ffffffff812c9321:	89 45 a8             	mov    %eax,-0x58(%rbp)
		int shift = 3;

		if (spec.base == 16)
			shift = 4;
ffffffff812c9324:	31 c0                	xor    %eax,%eax
ffffffff812c9326:	83 c1 03             	add    $0x3,%ecx
		do {
			tmp[i++] = (hex_asc_upper[((unsigned char)num) & mask] | locase);
ffffffff812c9329:	40 88 f7             	mov    %sil,%dil
ffffffff812c932c:	23 7d a8             	and    -0x58(%rbp),%edi
ffffffff812c932f:	8a 55 b7             	mov    -0x49(%rbp),%dl
			num >>= shift;
ffffffff812c9332:	48 d3 ee             	shr    %cl,%rsi
		int shift = 3;

		if (spec.base == 16)
			shift = 4;
		do {
			tmp[i++] = (hex_asc_upper[((unsigned char)num) & mask] | locase);
ffffffff812c9335:	ff c0                	inc    %eax
ffffffff812c9337:	49 ff c2             	inc    %r10
ffffffff812c933a:	40 0f b6 ff          	movzbl %dil,%edi
ffffffff812c933e:	0a 97 90 32 64 81    	or     -0x7e9bcd70(%rdi),%dl
ffffffff812c9344:	41 88 52 ff          	mov    %dl,-0x1(%r10)
			num >>= shift;
		} while (num);
ffffffff812c9348:	48 85 f6             	test   %rsi,%rsi
ffffffff812c934b:	75 dc                	jne    ffffffff812c9329 <number.isra.13+0xda>
ffffffff812c934d:	eb 23                	jmp    ffffffff812c9372 <number.isra.13+0x123>
	} else { /* base 10 */
		i = put_dec(tmp, num) - tmp;
ffffffff812c934f:	4c 89 d7             	mov    %r10,%rdi
ffffffff812c9352:	44 89 4d b0          	mov    %r9d,-0x50(%rbp)
ffffffff812c9356:	44 88 45 b5          	mov    %r8b,-0x4b(%rbp)
ffffffff812c935a:	4c 89 55 a8          	mov    %r10,-0x58(%rbp)
ffffffff812c935e:	e8 fb f0 ff ff       	callq  ffffffff812c845e <put_dec>
ffffffff812c9363:	4c 8b 55 a8          	mov    -0x58(%rbp),%r10
ffffffff812c9367:	44 8b 4d b0          	mov    -0x50(%rbp),%r9d
ffffffff812c936b:	44 8a 45 b5          	mov    -0x4b(%rbp),%r8b
ffffffff812c936f:	44 29 d0             	sub    %r10d,%eax
	}

	/* printing 100 using %2d gives "100", not "00" */
	if (i > spec.precision)
ffffffff812c9372:	41 0f bf cc          	movswl %r12w,%ecx
		spec.precision = i;
ffffffff812c9376:	39 c1                	cmp    %eax,%ecx
ffffffff812c9378:	44 0f 4c e0          	cmovl  %eax,%r12d
	/* leading space padding */
	spec.field_width -= spec.precision;
ffffffff812c937c:	45 29 e1             	sub    %r12d,%r9d
	if (!(spec.flags & (ZEROPAD | LEFT))) {
ffffffff812c937f:	f6 c3 12             	test   $0x12,%bl
ffffffff812c9382:	75 14                	jne    ffffffff812c9398 <number.isra.13+0x149>
		while (--spec.field_width >= 0) {
ffffffff812c9384:	66 41 ff c9          	dec    %r9w
ffffffff812c9388:	78 0e                	js     ffffffff812c9398 <number.isra.13+0x149>
			if (buf < end)
ffffffff812c938a:	4d 39 f3             	cmp    %r14,%r11
ffffffff812c938d:	73 04                	jae    ffffffff812c9393 <number.isra.13+0x144>
				*buf = ' ';
ffffffff812c938f:	41 c6 03 20          	movb   $0x20,(%r11)
			++buf;
ffffffff812c9393:	49 ff c3             	inc    %r11
ffffffff812c9396:	eb ec                	jmp    ffffffff812c9384 <number.isra.13+0x135>
		}
	}
	/* sign */
	if (sign) {
ffffffff812c9398:	45 84 ff             	test   %r15b,%r15b
ffffffff812c939b:	74 0b                	je     ffffffff812c93a8 <number.isra.13+0x159>
		if (buf < end)
ffffffff812c939d:	4d 39 de             	cmp    %r11,%r14
ffffffff812c93a0:	76 03                	jbe    ffffffff812c93a5 <number.isra.13+0x156>
			*buf = sign;
ffffffff812c93a2:	45 88 3b             	mov    %r15b,(%r11)
		++buf;
ffffffff812c93a5:	49 ff c3             	inc    %r11
	}
	/* "0x" / "0" prefix */
	if (need_pfx) {
ffffffff812c93a8:	80 7d b6 00          	cmpb   $0x0,-0x4a(%rbp)
ffffffff812c93ac:	74 36                	je     ffffffff812c93e4 <number.isra.13+0x195>
		if (spec.base == 16 || !is_zero) {
ffffffff812c93ae:	4d 85 ed             	test   %r13,%r13
ffffffff812c93b1:	75 06                	jne    ffffffff812c93b9 <number.isra.13+0x16a>
ffffffff812c93b3:	41 80 f8 10          	cmp    $0x10,%r8b
ffffffff812c93b7:	75 2b                	jne    ffffffff812c93e4 <number.isra.13+0x195>
			if (buf < end)
ffffffff812c93b9:	4d 39 de             	cmp    %r11,%r14
ffffffff812c93bc:	76 04                	jbe    ffffffff812c93c2 <number.isra.13+0x173>
				*buf = '0';
ffffffff812c93be:	41 c6 03 30          	movb   $0x30,(%r11)
			++buf;
		}
		if (spec.base == 16) {
ffffffff812c93c2:	41 80 f8 10          	cmp    $0x10,%r8b
	/* "0x" / "0" prefix */
	if (need_pfx) {
		if (spec.base == 16 || !is_zero) {
			if (buf < end)
				*buf = '0';
			++buf;
ffffffff812c93c6:	49 8d 53 01          	lea    0x1(%r11),%rdx
		}
		if (spec.base == 16) {
ffffffff812c93ca:	75 15                	jne    ffffffff812c93e1 <number.isra.13+0x192>
			if (buf < end)
ffffffff812c93cc:	49 39 d6             	cmp    %rdx,%r14
ffffffff812c93cf:	76 0a                	jbe    ffffffff812c93db <number.isra.13+0x18c>
				*buf = ('X' | locase);
ffffffff812c93d1:	8a 55 b7             	mov    -0x49(%rbp),%dl
ffffffff812c93d4:	83 ca 58             	or     $0x58,%edx
ffffffff812c93d7:	41 88 53 01          	mov    %dl,0x1(%r11)
			++buf;
ffffffff812c93db:	49 83 c3 02          	add    $0x2,%r11
ffffffff812c93df:	eb 03                	jmp    ffffffff812c93e4 <number.isra.13+0x195>
	/* "0x" / "0" prefix */
	if (need_pfx) {
		if (spec.base == 16 || !is_zero) {
			if (buf < end)
				*buf = '0';
			++buf;
ffffffff812c93e1:	49 89 d3             	mov    %rdx,%r11
				*buf = ('X' | locase);
			++buf;
		}
	}
	/* zero or space padding */
	if (!(spec.flags & LEFT)) {
ffffffff812c93e4:	f6 c3 02             	test   $0x2,%bl
ffffffff812c93e7:	75 25                	jne    ffffffff812c940e <number.isra.13+0x1bf>
		char c = ' ' + (spec.flags & ZEROPAD);
ffffffff812c93e9:	83 e3 10             	and    $0x10,%ebx
ffffffff812c93ec:	83 c3 20             	add    $0x20,%ebx
		BUILD_BUG_ON(' ' + ZEROPAD != '0');
		while (--spec.field_width >= 0) {
ffffffff812c93ef:	66 41 ff c9          	dec    %r9w
ffffffff812c93f3:	78 19                	js     ffffffff812c940e <number.isra.13+0x1bf>
			if (buf < end)
ffffffff812c93f5:	4d 39 de             	cmp    %r11,%r14
ffffffff812c93f8:	76 03                	jbe    ffffffff812c93fd <number.isra.13+0x1ae>
				*buf = c;
ffffffff812c93fa:	41 88 1b             	mov    %bl,(%r11)
			++buf;
ffffffff812c93fd:	49 ff c3             	inc    %r11
ffffffff812c9400:	eb ed                	jmp    ffffffff812c93ef <number.isra.13+0x1a0>
		}
	}
	/* hmm even more zero padding? */
	while (i <= --spec.precision) {
		if (buf < end)
ffffffff812c9402:	4d 39 de             	cmp    %r11,%r14
ffffffff812c9405:	76 04                	jbe    ffffffff812c940b <number.isra.13+0x1bc>
			*buf = '0';
ffffffff812c9407:	41 c6 03 30          	movb   $0x30,(%r11)
		++buf;
ffffffff812c940b:	49 ff c3             	inc    %r11
ffffffff812c940e:	41 ff cc             	dec    %r12d
				*buf = c;
			++buf;
		}
	}
	/* hmm even more zero padding? */
	while (i <= --spec.precision) {
ffffffff812c9411:	41 0f bf d4          	movswl %r12w,%edx
ffffffff812c9415:	39 d0                	cmp    %edx,%eax
ffffffff812c9417:	7e e9                	jle    ffffffff812c9402 <number.isra.13+0x1b3>
ffffffff812c9419:	eb 12                	jmp    ffffffff812c942d <number.isra.13+0x1de>
			*buf = '0';
		++buf;
	}
	/* actual digits of result */
	while (--i >= 0) {
		if (buf < end)
ffffffff812c941b:	4d 39 de             	cmp    %r11,%r14
ffffffff812c941e:	76 0a                	jbe    ffffffff812c942a <number.isra.13+0x1db>
			*buf = tmp[i];
ffffffff812c9420:	48 63 d0             	movslq %eax,%rdx
ffffffff812c9423:	8a 54 15 c0          	mov    -0x40(%rbp,%rdx,1),%dl
ffffffff812c9427:	41 88 13             	mov    %dl,(%r11)
		++buf;
ffffffff812c942a:	49 ff c3             	inc    %r11
		if (buf < end)
			*buf = '0';
		++buf;
	}
	/* actual digits of result */
	while (--i >= 0) {
ffffffff812c942d:	ff c8                	dec    %eax
ffffffff812c942f:	79 ea                	jns    ffffffff812c941b <number.isra.13+0x1cc>
ffffffff812c9431:	eb 0c                	jmp    ffffffff812c943f <number.isra.13+0x1f0>
			*buf = tmp[i];
		++buf;
	}
	/* trailing space padding */
	while (--spec.field_width >= 0) {
		if (buf < end)
ffffffff812c9433:	4d 39 de             	cmp    %r11,%r14
ffffffff812c9436:	76 04                	jbe    ffffffff812c943c <number.isra.13+0x1ed>
			*buf = ' ';
ffffffff812c9438:	41 c6 03 20          	movb   $0x20,(%r11)
		++buf;
ffffffff812c943c:	49 ff c3             	inc    %r11
		if (buf < end)
			*buf = tmp[i];
		++buf;
	}
	/* trailing space padding */
	while (--spec.field_width >= 0) {
ffffffff812c943f:	66 41 ff c9          	dec    %r9w
ffffffff812c9443:	79 ee                	jns    ffffffff812c9433 <number.isra.13+0x1e4>
			*buf = ' ';
		++buf;
	}

	return buf;
}
ffffffff812c9445:	48 83 c4 30          	add    $0x30,%rsp
ffffffff812c9449:	4c 89 d8             	mov    %r11,%rax
ffffffff812c944c:	5b                   	pop    %rbx
ffffffff812c944d:	41 5c                	pop    %r12
ffffffff812c944f:	41 5d                	pop    %r13
ffffffff812c9451:	41 5e                	pop    %r14
ffffffff812c9453:	41 5f                	pop    %r15
ffffffff812c9455:	5d                   	pop    %rbp
ffffffff812c9456:	c3                   	retq   

ffffffff812c9457 <resource_string.isra.14>:
	return number(buf, end, value, spec);
#endif
}

static noinline_for_stack
char *resource_string(char *buf, char *end, struct resource *res,
ffffffff812c9457:	55                   	push   %rbp
ffffffff812c9458:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c945b:	41 57                	push   %r15
ffffffff812c945d:	41 56                	push   %r14
ffffffff812c945f:	41 55                	push   %r13
ffffffff812c9461:	41 54                	push   %r12
ffffffff812c9463:	45 89 c5             	mov    %r8d,%r13d
ffffffff812c9466:	53                   	push   %rbx
ffffffff812c9467:	45 89 ce             	mov    %r9d,%r14d
ffffffff812c946a:	48 89 d3             	mov    %rdx,%rbx
ffffffff812c946d:	48 83 ec 78          	sub    $0x78,%rsp
#define RAW_BUF_SIZE		sizeof("[mem - flags 0x]")
	char sym[max(2*RSRC_BUF_SIZE + DECODED_BUF_SIZE,
		     2*RSRC_BUF_SIZE + FLAG_BUF_SIZE + RAW_BUF_SIZE)];

	char *p = sym, *pend = sym + sizeof(sym);
	int decode = (fmt[0] == 'R') ? 1 : 0;
ffffffff812c9471:	48 8b 45 10          	mov    0x10(%rbp),%rax
	return number(buf, end, value, spec);
#endif
}

static noinline_for_stack
char *resource_string(char *buf, char *end, struct resource *res,
ffffffff812c9475:	48 89 bd 78 ff ff ff 	mov    %rdi,-0x88(%rbp)
ffffffff812c947c:	48 89 b5 70 ff ff ff 	mov    %rsi,-0x90(%rbp)
ffffffff812c9483:	88 8d 6f ff ff ff    	mov    %cl,-0x91(%rbp)

	char *p = sym, *pend = sym + sizeof(sym);
	int decode = (fmt[0] == 'R') ? 1 : 0;
	const struct printf_spec *specp;

	*p++ = '[';
ffffffff812c9489:	c6 45 85 5b          	movb   $0x5b,-0x7b(%rbp)
#define RAW_BUF_SIZE		sizeof("[mem - flags 0x]")
	char sym[max(2*RSRC_BUF_SIZE + DECODED_BUF_SIZE,
		     2*RSRC_BUF_SIZE + FLAG_BUF_SIZE + RAW_BUF_SIZE)];

	char *p = sym, *pend = sym + sizeof(sym);
	int decode = (fmt[0] == 'R') ? 1 : 0;
ffffffff812c948d:	44 8a 38             	mov    (%rax),%r15b
	const struct printf_spec *specp;

	*p++ = '[';
	if (res->flags & IORESOURCE_IO) {
ffffffff812c9490:	48 8b 42 18          	mov    0x18(%rdx),%rax
ffffffff812c9494:	f6 c4 01             	test   $0x1,%ah
ffffffff812c9497:	74 30                	je     ffffffff812c94c9 <resource_string.isra.14+0x72>
		p = string(p, pend, "io  ", str_spec);
ffffffff812c9499:	48 8d 45 85          	lea    -0x7b(%rbp),%rax
ffffffff812c949d:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c94a1:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c94a7:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c94ab:	b1 02                	mov    $0x2,%cl
ffffffff812c94ad:	48 c7 c2 a9 e2 7c 81 	mov    $0xffffffff817ce2a9,%rdx
ffffffff812c94b4:	48 8d 78 01          	lea    0x1(%rax),%rdi
		specp = &io_spec;
ffffffff812c94b8:	49 c7 c4 68 f4 63 81 	mov    $0xffffffff8163f468,%r12
	int decode = (fmt[0] == 'R') ? 1 : 0;
	const struct printf_spec *specp;

	*p++ = '[';
	if (res->flags & IORESOURCE_IO) {
		p = string(p, pend, "io  ", str_spec);
ffffffff812c94bf:	e8 62 f9 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c94c4:	e9 d6 00 00 00       	jmpq   ffffffff812c959f <resource_string.isra.14+0x148>
		specp = &io_spec;
	} else if (res->flags & IORESOURCE_MEM) {
ffffffff812c94c9:	f6 c4 02             	test   $0x2,%ah
ffffffff812c94cc:	74 30                	je     ffffffff812c94fe <resource_string.isra.14+0xa7>
		p = string(p, pend, "mem ", str_spec);
ffffffff812c94ce:	48 8d 45 85          	lea    -0x7b(%rbp),%rax
ffffffff812c94d2:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c94d6:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c94dc:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c94e0:	b1 02                	mov    $0x2,%cl
ffffffff812c94e2:	48 c7 c2 83 6d 7b 81 	mov    $0xffffffff817b6d83,%rdx
ffffffff812c94e9:	48 8d 78 01          	lea    0x1(%rax),%rdi
		specp = &mem_spec;
ffffffff812c94ed:	49 c7 c4 60 f4 63 81 	mov    $0xffffffff8163f460,%r12
	*p++ = '[';
	if (res->flags & IORESOURCE_IO) {
		p = string(p, pend, "io  ", str_spec);
		specp = &io_spec;
	} else if (res->flags & IORESOURCE_MEM) {
		p = string(p, pend, "mem ", str_spec);
ffffffff812c94f4:	e8 2d f9 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c94f9:	e9 a1 00 00 00       	jmpq   ffffffff812c959f <resource_string.isra.14+0x148>
		specp = &mem_spec;
	} else if (res->flags & IORESOURCE_IRQ) {
ffffffff812c94fe:	f6 c4 04             	test   $0x4,%ah
ffffffff812c9501:	74 1d                	je     ffffffff812c9520 <resource_string.isra.14+0xc9>
		p = string(p, pend, "irq ", str_spec);
ffffffff812c9503:	48 8d 45 85          	lea    -0x7b(%rbp),%rax
ffffffff812c9507:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c950d:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c9511:	b1 02                	mov    $0x2,%cl
ffffffff812c9513:	48 c7 c2 88 6d 7b 81 	mov    $0xffffffff817b6d88,%rdx
ffffffff812c951a:	48 8d 78 01          	lea    0x1(%rax),%rdi
ffffffff812c951e:	eb 20                	jmp    ffffffff812c9540 <resource_string.isra.14+0xe9>
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_DMA) {
ffffffff812c9520:	f6 c4 08             	test   $0x8,%ah
ffffffff812c9523:	74 2d                	je     ffffffff812c9552 <resource_string.isra.14+0xfb>
		p = string(p, pend, "dma ", str_spec);
ffffffff812c9525:	48 8d 45 85          	lea    -0x7b(%rbp),%rax
ffffffff812c9529:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c952f:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c9533:	b1 02                	mov    $0x2,%cl
ffffffff812c9535:	48 c7 c2 8d 6d 7b 81 	mov    $0xffffffff817b6d8d,%rdx
ffffffff812c953c:	48 8d 78 01          	lea    0x1(%rax),%rdi
ffffffff812c9540:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
		specp = &dec_spec;
ffffffff812c9544:	49 c7 c4 58 f4 63 81 	mov    $0xffffffff8163f458,%r12
		specp = &mem_spec;
	} else if (res->flags & IORESOURCE_IRQ) {
		p = string(p, pend, "irq ", str_spec);
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_DMA) {
		p = string(p, pend, "dma ", str_spec);
ffffffff812c954b:	e8 d6 f8 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c9550:	eb 4d                	jmp    ffffffff812c959f <resource_string.isra.14+0x148>
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_BUS) {
ffffffff812c9552:	f6 c4 10             	test   $0x10,%ah
		p = string(p, pend, "bus ", str_spec);
ffffffff812c9555:	48 8d 45 85          	lea    -0x7b(%rbp),%rax
ffffffff812c9559:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c955f:	41 b8 ff ff ff ff    	mov    $0xffffffff,%r8d
ffffffff812c9565:	b1 02                	mov    $0x2,%cl
ffffffff812c9567:	48 8d 78 01          	lea    0x1(%rax),%rdi
		p = string(p, pend, "irq ", str_spec);
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_DMA) {
		p = string(p, pend, "dma ", str_spec);
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_BUS) {
ffffffff812c956b:	74 19                	je     ffffffff812c9586 <resource_string.isra.14+0x12f>
		p = string(p, pend, "bus ", str_spec);
ffffffff812c956d:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c9571:	48 c7 c2 92 6d 7b 81 	mov    $0xffffffff817b6d92,%rdx
		specp = &bus_spec;
ffffffff812c9578:	49 c7 c4 50 f4 63 81 	mov    $0xffffffff8163f450,%r12
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_DMA) {
		p = string(p, pend, "dma ", str_spec);
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_BUS) {
		p = string(p, pend, "bus ", str_spec);
ffffffff812c957f:	e8 a2 f8 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c9584:	eb 19                	jmp    ffffffff812c959f <resource_string.isra.14+0x148>
		specp = &bus_spec;
	} else {
		p = string(p, pend, "??? ", str_spec);
ffffffff812c9586:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c958a:	48 c7 c2 97 6d 7b 81 	mov    $0xffffffff817b6d97,%rdx
		specp = &mem_spec;
ffffffff812c9591:	49 c7 c4 60 f4 63 81 	mov    $0xffffffff8163f460,%r12
		specp = &dec_spec;
	} else if (res->flags & IORESOURCE_BUS) {
		p = string(p, pend, "bus ", str_spec);
		specp = &bus_spec;
	} else {
		p = string(p, pend, "??? ", str_spec);
ffffffff812c9598:	e8 89 f8 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c959d:	eb 64                	jmp    ffffffff812c9603 <resource_string.isra.14+0x1ac>
		specp = &mem_spec;
		decode = 0;
	}
	if (decode && res->flags & IORESOURCE_UNSET) {
ffffffff812c959f:	41 80 ff 52          	cmp    $0x52,%r15b
ffffffff812c95a3:	75 5e                	jne    ffffffff812c9603 <resource_string.isra.14+0x1ac>
ffffffff812c95a5:	f6 43 1b 20          	testb  $0x20,0x1b(%rbx)
ffffffff812c95a9:	41 bf 01 00 00 00    	mov    $0x1,%r15d
ffffffff812c95af:	74 55                	je     ffffffff812c9606 <resource_string.isra.14+0x1af>
		p = string(p, pend, "size ", str_spec);
ffffffff812c95b1:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c95b5:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c95bb:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c95bf:	b1 02                	mov    $0x2,%cl
ffffffff812c95c1:	48 c7 c2 9c 6d 7b 81 	mov    $0xffffffff817b6d9c,%rdx
ffffffff812c95c8:	48 89 c7             	mov    %rax,%rdi
ffffffff812c95cb:	e8 56 f8 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
		p = number(p, pend, resource_size(res), *specp);
ffffffff812c95d0:	48 8b 73 08          	mov    0x8(%rbx),%rsi
ffffffff812c95d4:	41 8a 4c 24 01       	mov    0x1(%r12),%cl
ffffffff812c95d9:	48 89 c7             	mov    %rax,%rdi
ffffffff812c95dc:	48 8d 56 01          	lea    0x1(%rsi),%rdx
ffffffff812c95e0:	56                   	push   %rsi
ffffffff812c95e1:	66 41 8b 74 24 06    	mov    0x6(%r12),%si
ffffffff812c95e7:	48 2b 13             	sub    (%rbx),%rdx
ffffffff812c95ea:	56                   	push   %rsi
ffffffff812c95eb:	45 8a 44 24 02       	mov    0x2(%r12),%r8b
ffffffff812c95f0:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c95f4:	45 8b 4c 24 04       	mov    0x4(%r12),%r9d
ffffffff812c95f9:	e8 51 fc ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812c95fe:	5f                   	pop    %rdi
ffffffff812c95ff:	41 58                	pop    %r8
ffffffff812c9601:	eb 6e                	jmp    ffffffff812c9671 <resource_string.isra.14+0x21a>
	} else {
		p = string(p, pend, "??? ", str_spec);
		specp = &mem_spec;
		decode = 0;
	}
	if (decode && res->flags & IORESOURCE_UNSET) {
ffffffff812c9603:	45 31 ff             	xor    %r15d,%r15d
		p = string(p, pend, "size ", str_spec);
		p = number(p, pend, resource_size(res), *specp);
	} else {
		p = number(p, pend, res->start, *specp);
ffffffff812c9606:	41 8a 4c 24 01       	mov    0x1(%r12),%cl
ffffffff812c960b:	41 53                	push   %r11
ffffffff812c960d:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c9611:	66 41 8b 54 24 06    	mov    0x6(%r12),%dx
ffffffff812c9617:	48 89 c7             	mov    %rax,%rdi
ffffffff812c961a:	52                   	push   %rdx
ffffffff812c961b:	48 8b 13             	mov    (%rbx),%rdx
ffffffff812c961e:	45 8b 4c 24 04       	mov    0x4(%r12),%r9d
ffffffff812c9623:	45 8a 44 24 02       	mov    0x2(%r12),%r8b
ffffffff812c9628:	e8 22 fc ff ff       	callq  ffffffff812c924f <number.isra.13>
		if (res->start != res->end) {
ffffffff812c962d:	48 8b 73 08          	mov    0x8(%rbx),%rsi
ffffffff812c9631:	48 39 33             	cmp    %rsi,(%rbx)
ffffffff812c9634:	5a                   	pop    %rdx
ffffffff812c9635:	59                   	pop    %rcx
ffffffff812c9636:	74 30                	je     ffffffff812c9668 <resource_string.isra.14+0x211>
			*p++ = '-';
ffffffff812c9638:	c6 00 2d             	movb   $0x2d,(%rax)
ffffffff812c963b:	48 8d 78 01          	lea    0x1(%rax),%rdi
			p = number(p, pend, res->end, *specp);
ffffffff812c963f:	41 8a 4c 24 01       	mov    0x1(%r12),%cl
ffffffff812c9644:	41 50                	push   %r8
ffffffff812c9646:	66 41 8b 44 24 06    	mov    0x6(%r12),%ax
ffffffff812c964c:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c9650:	48 8b 53 08          	mov    0x8(%rbx),%rdx
ffffffff812c9654:	50                   	push   %rax
ffffffff812c9655:	45 8b 4c 24 04       	mov    0x4(%r12),%r9d
ffffffff812c965a:	45 8a 44 24 02       	mov    0x2(%r12),%r8b
ffffffff812c965f:	e8 eb fb ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812c9664:	41 59                	pop    %r9
ffffffff812c9666:	41 5a                	pop    %r10
		}
	}
	if (decode) {
ffffffff812c9668:	45 85 ff             	test   %r15d,%r15d
ffffffff812c966b:	0f 84 96 00 00 00    	je     ffffffff812c9707 <resource_string.isra.14+0x2b0>
		if (res->flags & IORESOURCE_MEM_64)
ffffffff812c9671:	f6 43 1a 10          	testb  $0x10,0x1a(%rbx)
ffffffff812c9675:	74 1f                	je     ffffffff812c9696 <resource_string.isra.14+0x23f>
			p = string(p, pend, " 64bit", str_spec);
ffffffff812c9677:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c967b:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c9681:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c9685:	b1 02                	mov    $0x2,%cl
ffffffff812c9687:	48 c7 c2 a2 6d 7b 81 	mov    $0xffffffff817b6da2,%rdx
ffffffff812c968e:	48 89 c7             	mov    %rax,%rdi
ffffffff812c9691:	e8 90 f7 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
		if (res->flags & IORESOURCE_PREFETCH)
ffffffff812c9696:	f6 43 19 20          	testb  $0x20,0x19(%rbx)
ffffffff812c969a:	74 1f                	je     ffffffff812c96bb <resource_string.isra.14+0x264>
			p = string(p, pend, " pref", str_spec);
ffffffff812c969c:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c96a0:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c96a6:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c96aa:	b1 02                	mov    $0x2,%cl
ffffffff812c96ac:	48 c7 c2 a9 6d 7b 81 	mov    $0xffffffff817b6da9,%rdx
ffffffff812c96b3:	48 89 c7             	mov    %rax,%rdi
ffffffff812c96b6:	e8 6b f7 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
		if (res->flags & IORESOURCE_WINDOW)
ffffffff812c96bb:	f6 43 1a 20          	testb  $0x20,0x1a(%rbx)
ffffffff812c96bf:	74 1f                	je     ffffffff812c96e0 <resource_string.isra.14+0x289>
			p = string(p, pend, " window", str_spec);
ffffffff812c96c1:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c96c5:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c96cb:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c96cf:	b1 02                	mov    $0x2,%cl
ffffffff812c96d1:	48 c7 c2 af 6d 7b 81 	mov    $0xffffffff817b6daf,%rdx
ffffffff812c96d8:	48 89 c7             	mov    %rax,%rdi
ffffffff812c96db:	e8 46 f7 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
		if (res->flags & IORESOURCE_DISABLED)
ffffffff812c96e0:	f6 43 1b 10          	testb  $0x10,0x1b(%rbx)
ffffffff812c96e4:	74 5d                	je     ffffffff812c9743 <resource_string.isra.14+0x2ec>
			p = string(p, pend, " disabled", str_spec);
ffffffff812c96e6:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c96ea:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c96f0:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c96f4:	b1 02                	mov    $0x2,%cl
ffffffff812c96f6:	48 c7 c2 33 2e 7b 81 	mov    $0xffffffff817b2e33,%rdx
ffffffff812c96fd:	48 89 c7             	mov    %rax,%rdi
ffffffff812c9700:	e8 21 f7 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812c9705:	eb 3c                	jmp    ffffffff812c9743 <resource_string.isra.14+0x2ec>
	} else {
		p = string(p, pend, " flags ", str_spec);
ffffffff812c9707:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c970b:	b1 02                	mov    $0x2,%cl
ffffffff812c970d:	41 b9 0a 00 00 00    	mov    $0xa,%r9d
ffffffff812c9713:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c9717:	48 c7 c2 b7 6d 7b 81 	mov    $0xffffffff817b6db7,%rdx
ffffffff812c971e:	48 89 c7             	mov    %rax,%rdi
ffffffff812c9721:	e8 00 f7 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
		p = number(p, pend, res->flags, flag_spec);
ffffffff812c9726:	48 8b 53 18          	mov    0x18(%rbx),%rdx
ffffffff812c972a:	51                   	push   %rcx
ffffffff812c972b:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c972f:	6a ff                	pushq  $0xffffffffffffffff
ffffffff812c9731:	48 89 c7             	mov    %rax,%rdi
ffffffff812c9734:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c9737:	41 b0 10             	mov    $0x10,%r8b
ffffffff812c973a:	b1 60                	mov    $0x60,%cl
ffffffff812c973c:	e8 0e fb ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812c9741:	5e                   	pop    %rsi
ffffffff812c9742:	5f                   	pop    %rdi
	}
	*p++ = ']';
	*p = '\0';

	return string(buf, end, sym, spec);
ffffffff812c9743:	8a 8d 6f ff ff ff    	mov    -0x91(%rbp),%cl
ffffffff812c9749:	48 8b b5 70 ff ff ff 	mov    -0x90(%rbp),%rsi
ffffffff812c9750:	48 8d 55 85          	lea    -0x7b(%rbp),%rdx
ffffffff812c9754:	48 8b bd 78 ff ff ff 	mov    -0x88(%rbp),%rdi
ffffffff812c975b:	45 89 f1             	mov    %r14d,%r9d
ffffffff812c975e:	45 89 e8             	mov    %r13d,%r8d
			p = string(p, pend, " disabled", str_spec);
	} else {
		p = string(p, pend, " flags ", str_spec);
		p = number(p, pend, res->flags, flag_spec);
	}
	*p++ = ']';
ffffffff812c9761:	c6 00 5d             	movb   $0x5d,(%rax)
	*p = '\0';
ffffffff812c9764:	c6 40 01 00          	movb   $0x0,0x1(%rax)

	return string(buf, end, sym, spec);
ffffffff812c9768:	e8 b9 f6 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
}
ffffffff812c976d:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812c9771:	5b                   	pop    %rbx
ffffffff812c9772:	41 5c                	pop    %r12
ffffffff812c9774:	41 5d                	pop    %r13
ffffffff812c9776:	41 5e                	pop    %r14
ffffffff812c9778:	41 5f                	pop    %r15
ffffffff812c977a:	5d                   	pop    %rbp
ffffffff812c977b:	c3                   	retq   

ffffffff812c977c <bitmap_list_string.isra.15>:
	}
	return buf;
}

static noinline_for_stack
char *bitmap_list_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c977c:	55                   	push   %rbp
			 struct printf_spec spec, const char *fmt)
{
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c977d:	b8 00 00 00 00       	mov    $0x0,%eax
	}
	return buf;
}

static noinline_for_stack
char *bitmap_list_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c9782:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9785:	41 57                	push   %r15
ffffffff812c9787:	41 56                	push   %r14
ffffffff812c9789:	41 55                	push   %r13
ffffffff812c978b:	41 54                	push   %r12
			 struct printf_spec spec, const char *fmt)
{
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c978d:	44 0f bf e1          	movswl %cx,%r12d
	}
	return buf;
}

static noinline_for_stack
char *bitmap_list_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c9791:	53                   	push   %rbx
ffffffff812c9792:	48 89 fb             	mov    %rdi,%rbx
	bool first = true;

	/* reused to print numbers */
	spec = (struct printf_spec){ .base = 10 };

	rbot = cur = find_first_bit(bitmap, nr_bits);
ffffffff812c9795:	48 89 d7             	mov    %rdx,%rdi
	}
	return buf;
}

static noinline_for_stack
char *bitmap_list_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c9798:	48 83 ec 28          	sub    $0x28,%rsp
			 struct printf_spec spec, const char *fmt)
{
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c979c:	45 85 e4             	test   %r12d,%r12d
	}
	return buf;
}

static noinline_for_stack
char *bitmap_list_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c979f:	48 89 75 c8          	mov    %rsi,-0x38(%rbp)
			 struct printf_spec spec, const char *fmt)
{
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c97a3:	44 0f 48 e0          	cmovs  %eax,%r12d
	}
	return buf;
}

static noinline_for_stack
char *bitmap_list_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c97a7:	48 89 55 c0          	mov    %rdx,-0x40(%rbp)
	bool first = true;

	/* reused to print numbers */
	spec = (struct printf_spec){ .base = 10 };

	rbot = cur = find_first_bit(bitmap, nr_bits);
ffffffff812c97ab:	49 63 c4             	movslq %r12d,%rax
ffffffff812c97ae:	48 89 c6             	mov    %rax,%rsi
ffffffff812c97b1:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812c97b5:	e8 92 74 00 00       	callq  ffffffff812d0c4c <find_first_bit>
			 struct printf_spec spec, const char *fmt)
{
	int nr_bits = max_t(int, spec.field_width, 0);
	/* current bit is 'cur', most recently seen range is [rbot, rtop] */
	int cur, rbot, rtop;
	bool first = true;
ffffffff812c97ba:	b1 01                	mov    $0x1,%cl

	/* reused to print numbers */
	spec = (struct printf_spec){ .base = 10 };

	rbot = cur = find_first_bit(bitmap, nr_bits);
ffffffff812c97bc:	41 89 c7             	mov    %eax,%r15d
ffffffff812c97bf:	41 89 c6             	mov    %eax,%r14d
	while (cur < nr_bits) {
ffffffff812c97c2:	45 39 fc             	cmp    %r15d,%r12d
ffffffff812c97c5:	0f 8e 9a 00 00 00    	jle    ffffffff812c9865 <bitmap_list_string.isra.15+0xe9>
		rtop = cur;
		cur = find_next_bit(bitmap, nr_bits, cur + 1);
ffffffff812c97cb:	45 8d 47 01          	lea    0x1(%r15),%r8d
ffffffff812c97cf:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812c97d3:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
ffffffff812c97d7:	88 4d b3             	mov    %cl,-0x4d(%rbp)
ffffffff812c97da:	49 63 d0             	movslq %r8d,%rdx
ffffffff812c97dd:	44 89 45 b4          	mov    %r8d,-0x4c(%rbp)
ffffffff812c97e1:	e8 32 75 00 00       	callq  ffffffff812d0d18 <find_next_bit>
		if (cur < nr_bits && cur <= rtop + 1)
ffffffff812c97e6:	41 39 c4             	cmp    %eax,%r12d
	spec = (struct printf_spec){ .base = 10 };

	rbot = cur = find_first_bit(bitmap, nr_bits);
	while (cur < nr_bits) {
		rtop = cur;
		cur = find_next_bit(bitmap, nr_bits, cur + 1);
ffffffff812c97e9:	49 89 c5             	mov    %rax,%r13
		if (cur < nr_bits && cur <= rtop + 1)
ffffffff812c97ec:	8a 4d b3             	mov    -0x4d(%rbp),%cl
ffffffff812c97ef:	7e 09                	jle    ffffffff812c97fa <bitmap_list_string.isra.15+0x7e>
ffffffff812c97f1:	44 8b 45 b4          	mov    -0x4c(%rbp),%r8d
ffffffff812c97f5:	41 39 c0             	cmp    %eax,%r8d
ffffffff812c97f8:	7d 63                	jge    ffffffff812c985d <bitmap_list_string.isra.15+0xe1>
			continue;

		if (!first) {
ffffffff812c97fa:	84 c9                	test   %cl,%cl
ffffffff812c97fc:	75 0c                	jne    ffffffff812c980a <bitmap_list_string.isra.15+0x8e>
			if (buf < end)
ffffffff812c97fe:	48 3b 5d c8          	cmp    -0x38(%rbp),%rbx
ffffffff812c9802:	73 03                	jae    ffffffff812c9807 <bitmap_list_string.isra.15+0x8b>
				*buf = ',';
ffffffff812c9804:	c6 03 2c             	movb   $0x2c,(%rbx)
			buf++;
ffffffff812c9807:	48 ff c3             	inc    %rbx
		}
		first = false;

		buf = number(buf, end, rbot, spec);
ffffffff812c980a:	56                   	push   %rsi
ffffffff812c980b:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
ffffffff812c980f:	48 89 df             	mov    %rbx,%rdi
ffffffff812c9812:	6a 00                	pushq  $0x0
ffffffff812c9814:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c9817:	41 b0 0a             	mov    $0xa,%r8b
ffffffff812c981a:	31 c9                	xor    %ecx,%ecx
ffffffff812c981c:	49 63 d6             	movslq %r14d,%rdx
ffffffff812c981f:	e8 2b fa ff ff       	callq  ffffffff812c924f <number.isra.13>
		if (rbot < rtop) {
ffffffff812c9824:	45 39 f7             	cmp    %r14d,%r15d
				*buf = ',';
			buf++;
		}
		first = false;

		buf = number(buf, end, rbot, spec);
ffffffff812c9827:	48 89 c3             	mov    %rax,%rbx
		if (rbot < rtop) {
ffffffff812c982a:	5f                   	pop    %rdi
ffffffff812c982b:	41 58                	pop    %r8
ffffffff812c982d:	7e 29                	jle    ffffffff812c9858 <bitmap_list_string.isra.15+0xdc>
			if (buf < end)
ffffffff812c982f:	48 39 45 c8          	cmp    %rax,-0x38(%rbp)
ffffffff812c9833:	76 03                	jbe    ffffffff812c9838 <bitmap_list_string.isra.15+0xbc>
				*buf = '-';
ffffffff812c9835:	c6 00 2d             	movb   $0x2d,(%rax)
			buf++;

			buf = number(buf, end, rtop, spec);
ffffffff812c9838:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi

		buf = number(buf, end, rbot, spec);
		if (rbot < rtop) {
			if (buf < end)
				*buf = '-';
			buf++;
ffffffff812c983c:	48 8d 7b 01          	lea    0x1(%rbx),%rdi

			buf = number(buf, end, rtop, spec);
ffffffff812c9840:	50                   	push   %rax
ffffffff812c9841:	6a 00                	pushq  $0x0
ffffffff812c9843:	49 63 d7             	movslq %r15d,%rdx
ffffffff812c9846:	31 c9                	xor    %ecx,%ecx
ffffffff812c9848:	45 31 c9             	xor    %r9d,%r9d
ffffffff812c984b:	41 b0 0a             	mov    $0xa,%r8b
ffffffff812c984e:	e8 fc f9 ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812c9853:	5a                   	pop    %rdx
ffffffff812c9854:	59                   	pop    %rcx
ffffffff812c9855:	48 89 c3             	mov    %rax,%rbx
	spec = (struct printf_spec){ .base = 10 };

	rbot = cur = find_first_bit(bitmap, nr_bits);
	while (cur < nr_bits) {
		rtop = cur;
		cur = find_next_bit(bitmap, nr_bits, cur + 1);
ffffffff812c9858:	45 89 ee             	mov    %r13d,%r14d
		if (!first) {
			if (buf < end)
				*buf = ',';
			buf++;
		}
		first = false;
ffffffff812c985b:	31 c9                	xor    %ecx,%ecx
	spec = (struct printf_spec){ .base = 10 };

	rbot = cur = find_first_bit(bitmap, nr_bits);
	while (cur < nr_bits) {
		rtop = cur;
		cur = find_next_bit(bitmap, nr_bits, cur + 1);
ffffffff812c985d:	45 89 ef             	mov    %r13d,%r15d
ffffffff812c9860:	e9 5d ff ff ff       	jmpq   ffffffff812c97c2 <bitmap_list_string.isra.15+0x46>
		}

		rbot = cur;
	}
	return buf;
}
ffffffff812c9865:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812c9869:	48 89 d8             	mov    %rbx,%rax
ffffffff812c986c:	5b                   	pop    %rbx
ffffffff812c986d:	41 5c                	pop    %r12
ffffffff812c986f:	41 5d                	pop    %r13
ffffffff812c9871:	41 5e                	pop    %r14
ffffffff812c9873:	41 5f                	pop    %r15
ffffffff812c9875:	5d                   	pop    %rbp
ffffffff812c9876:	c3                   	retq   

ffffffff812c9877 <bitmap_string.isra.16>:

	return buf;
}

static noinline_for_stack
char *bitmap_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c9877:	55                   	push   %rbp
		    struct printf_spec spec, const char *fmt)
{
	const int CHUNKSZ = 32;
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c9878:	0f bf c9             	movswl %cx,%ecx

	return buf;
}

static noinline_for_stack
char *bitmap_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c987b:	48 89 f8             	mov    %rdi,%rax
		    struct printf_spec spec, const char *fmt)
{
	const int CHUNKSZ = 32;
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c987e:	85 c9                	test   %ecx,%ecx
	int i, chunksz;
	bool first = true;
ffffffff812c9880:	40 b7 01             	mov    $0x1,%dil

	return buf;
}

static noinline_for_stack
char *bitmap_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c9883:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9886:	41 56                	push   %r14
ffffffff812c9888:	41 55                	push   %r13
ffffffff812c988a:	49 89 d5             	mov    %rdx,%r13
		    struct printf_spec spec, const char *fmt)
{
	const int CHUNKSZ = 32;
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c988d:	ba 00 00 00 00       	mov    $0x0,%edx

	return buf;
}

static noinline_for_stack
char *bitmap_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c9892:	41 54                	push   %r12
		    struct printf_spec spec, const char *fmt)
{
	const int CHUNKSZ = 32;
	int nr_bits = max_t(int, spec.field_width, 0);
ffffffff812c9894:	0f 48 ca             	cmovs  %edx,%ecx
	/* reused to print numbers */
	spec = (struct printf_spec){ .flags = SMALL | ZEROPAD, .base = 16 };

	chunksz = nr_bits & (CHUNKSZ - 1);
	if (chunksz == 0)
		chunksz = CHUNKSZ;
ffffffff812c9897:	ba 20 00 00 00       	mov    $0x20,%edx

	return buf;
}

static noinline_for_stack
char *bitmap_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c989c:	53                   	push   %rbx
	/* reused to print numbers */
	spec = (struct printf_spec){ .flags = SMALL | ZEROPAD, .base = 16 };

	chunksz = nr_bits & (CHUNKSZ - 1);
	if (chunksz == 0)
		chunksz = CHUNKSZ;
ffffffff812c989d:	41 89 c9             	mov    %ecx,%r9d

	return buf;
}

static noinline_for_stack
char *bitmap_string(char *buf, char *end, unsigned long *bitmap,
ffffffff812c98a0:	49 89 f4             	mov    %rsi,%r12
		int word, bit;

		chunkmask = ((1ULL << chunksz) - 1);
		word = i / BITS_PER_LONG;
		bit = i % BITS_PER_LONG;
		val = (bitmap[word] >> bit) & chunkmask;
ffffffff812c98a3:	41 be 01 00 00 00    	mov    $0x1,%r14d
	/* reused to print numbers */
	spec = (struct printf_spec){ .flags = SMALL | ZEROPAD, .base = 16 };

	chunksz = nr_bits & (CHUNKSZ - 1);
	if (chunksz == 0)
		chunksz = CHUNKSZ;
ffffffff812c98a9:	41 83 e1 1f          	and    $0x1f,%r9d
ffffffff812c98ad:	44 0f 44 ca          	cmove  %edx,%r9d

	i = ALIGN(nr_bits, CHUNKSZ) - CHUNKSZ;
ffffffff812c98b1:	83 c1 1f             	add    $0x1f,%ecx
ffffffff812c98b4:	83 e1 e0             	and    $0xffffffe0,%ecx
ffffffff812c98b7:	8d 59 e0             	lea    -0x20(%rcx),%ebx
	for (; i >= 0; i -= CHUNKSZ) {
ffffffff812c98ba:	85 db                	test   %ebx,%ebx
ffffffff812c98bc:	78 59                	js     ffffffff812c9917 <bitmap_string.isra.16+0xa0>
		int word, bit;

		chunkmask = ((1ULL << chunksz) - 1);
		word = i / BITS_PER_LONG;
		bit = i % BITS_PER_LONG;
		val = (bitmap[word] >> bit) & chunkmask;
ffffffff812c98be:	89 da                	mov    %ebx,%edx
ffffffff812c98c0:	89 d9                	mov    %ebx,%ecx
ffffffff812c98c2:	c1 fa 06             	sar    $0x6,%edx
ffffffff812c98c5:	48 63 d2             	movslq %edx,%rdx
ffffffff812c98c8:	49 8b 74 d5 00       	mov    0x0(%r13,%rdx,8),%rsi
ffffffff812c98cd:	4c 89 f2             	mov    %r14,%rdx
ffffffff812c98d0:	48 d3 ee             	shr    %cl,%rsi
ffffffff812c98d3:	44 88 c9             	mov    %r9b,%cl
ffffffff812c98d6:	48 d3 e2             	shl    %cl,%rdx
ffffffff812c98d9:	ff ca                	dec    %edx
ffffffff812c98db:	21 d6                	and    %edx,%esi

		if (!first) {
ffffffff812c98dd:	40 84 ff             	test   %dil,%dil
ffffffff812c98e0:	75 0b                	jne    ffffffff812c98ed <bitmap_string.isra.16+0x76>
			if (buf < end)
ffffffff812c98e2:	4c 39 e0             	cmp    %r12,%rax
ffffffff812c98e5:	73 03                	jae    ffffffff812c98ea <bitmap_string.isra.16+0x73>
				*buf = ',';
ffffffff812c98e7:	c6 00 2c             	movb   $0x2c,(%rax)
			buf++;
ffffffff812c98ea:	48 ff c0             	inc    %rax
		}
		first = false;

		spec.field_width = DIV_ROUND_UP(chunksz, 4);
		buf = number(buf, end, val, spec);
ffffffff812c98ed:	6a 00                	pushq  $0x0
ffffffff812c98ef:	41 83 c1 03          	add    $0x3,%r9d
ffffffff812c98f3:	89 f2                	mov    %esi,%edx
ffffffff812c98f5:	41 c1 e9 02          	shr    $0x2,%r9d
ffffffff812c98f9:	48 89 c7             	mov    %rax,%rdi
ffffffff812c98fc:	41 b0 10             	mov    $0x10,%r8b
ffffffff812c98ff:	b1 30                	mov    $0x30,%cl
ffffffff812c9901:	4c 89 e6             	mov    %r12,%rsi
	chunksz = nr_bits & (CHUNKSZ - 1);
	if (chunksz == 0)
		chunksz = CHUNKSZ;

	i = ALIGN(nr_bits, CHUNKSZ) - CHUNKSZ;
	for (; i >= 0; i -= CHUNKSZ) {
ffffffff812c9904:	83 eb 20             	sub    $0x20,%ebx
			buf++;
		}
		first = false;

		spec.field_width = DIV_ROUND_UP(chunksz, 4);
		buf = number(buf, end, val, spec);
ffffffff812c9907:	e8 43 f9 ff ff       	callq  ffffffff812c924f <number.isra.13>

		chunksz = CHUNKSZ;
ffffffff812c990c:	41 b9 20 00 00 00    	mov    $0x20,%r9d
		if (!first) {
			if (buf < end)
				*buf = ',';
			buf++;
		}
		first = false;
ffffffff812c9912:	31 ff                	xor    %edi,%edi
ffffffff812c9914:	5a                   	pop    %rdx
ffffffff812c9915:	eb a3                	jmp    ffffffff812c98ba <bitmap_string.isra.16+0x43>
		buf = number(buf, end, val, spec);

		chunksz = CHUNKSZ;
	}
	return buf;
}
ffffffff812c9917:	48 8d 65 e0          	lea    -0x20(%rbp),%rsp
ffffffff812c991b:	5b                   	pop    %rbx
ffffffff812c991c:	41 5c                	pop    %r12
ffffffff812c991e:	41 5d                	pop    %r13
ffffffff812c9920:	41 5e                	pop    %r14
ffffffff812c9922:	5d                   	pop    %rbp
ffffffff812c9923:	c3                   	retq   

ffffffff812c9924 <ip4_addr_string_sa.isra.17>:

	return string(buf, end, ip6_addr, spec);
}

static noinline_for_stack
char *ip4_addr_string_sa(char *buf, char *end, const struct sockaddr_in *sa,
ffffffff812c9924:	55                   	push   %rbp
	char *p, ip4_addr[sizeof("255.255.255.255") + sizeof(":12345")];
	char *pend = ip4_addr + sizeof(ip4_addr);
	const u8 *addr = (const u8 *) &sa->sin_addr.s_addr;
	char fmt4[3] = { fmt[0], '4', 0 };

	fmt++;
ffffffff812c9925:	45 31 d2             	xor    %r10d,%r10d
ffffffff812c9928:	41 bb 01 00 00 00    	mov    $0x1,%r11d

	return string(buf, end, ip6_addr, spec);
}

static noinline_for_stack
char *ip4_addr_string_sa(char *buf, char *end, const struct sockaddr_in *sa,
ffffffff812c992e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9931:	41 57                	push   %r15
ffffffff812c9933:	41 56                	push   %r14
ffffffff812c9935:	41 55                	push   %r13
ffffffff812c9937:	41 54                	push   %r12
ffffffff812c9939:	49 89 f6             	mov    %rsi,%r14
ffffffff812c993c:	53                   	push   %rbx
ffffffff812c993d:	49 89 d7             	mov    %rdx,%r15
			 struct printf_spec spec, const char *fmt)
{
	bool have_p = false;
	char *p, ip4_addr[sizeof("255.255.255.255") + sizeof(":12345")];
	char *pend = ip4_addr + sizeof(ip4_addr);
	const u8 *addr = (const u8 *) &sa->sin_addr.s_addr;
ffffffff812c9940:	48 8d 72 04          	lea    0x4(%rdx),%rsi

	return string(buf, end, ip6_addr, spec);
}

static noinline_for_stack
char *ip4_addr_string_sa(char *buf, char *end, const struct sockaddr_in *sa,
ffffffff812c9944:	88 cb                	mov    %cl,%bl
ffffffff812c9946:	45 89 cc             	mov    %r9d,%r12d
			 struct printf_spec spec, const char *fmt)
{
	bool have_p = false;
ffffffff812c9949:	45 31 ed             	xor    %r13d,%r13d

	return string(buf, end, ip6_addr, spec);
}

static noinline_for_stack
char *ip4_addr_string_sa(char *buf, char *end, const struct sockaddr_in *sa,
ffffffff812c994c:	48 83 ec 38          	sub    $0x38,%rsp
ffffffff812c9950:	48 8b 45 18          	mov    0x18(%rbp),%rax
ffffffff812c9954:	48 89 7d a8          	mov    %rdi,-0x58(%rbp)
{
	bool have_p = false;
	char *p, ip4_addr[sizeof("255.255.255.255") + sizeof(":12345")];
	char *pend = ip4_addr + sizeof(ip4_addr);
	const u8 *addr = (const u8 *) &sa->sin_addr.s_addr;
	char fmt4[3] = { fmt[0], '4', 0 };
ffffffff812c9958:	c6 45 b7 34          	movb   $0x34,-0x49(%rbp)
ffffffff812c995c:	c6 45 b8 00          	movb   $0x0,-0x48(%rbp)

	fmt++;
ffffffff812c9960:	31 ff                	xor    %edi,%edi
{
	bool have_p = false;
	char *p, ip4_addr[sizeof("255.255.255.255") + sizeof(":12345")];
	char *pend = ip4_addr + sizeof(ip4_addr);
	const u8 *addr = (const u8 *) &sa->sin_addr.s_addr;
	char fmt4[3] = { fmt[0], '4', 0 };
ffffffff812c9962:	8a 10                	mov    (%rax),%dl

	fmt++;
ffffffff812c9964:	48 ff c0             	inc    %rax
{
	bool have_p = false;
	char *p, ip4_addr[sizeof("255.255.255.255") + sizeof(":12345")];
	char *pend = ip4_addr + sizeof(ip4_addr);
	const u8 *addr = (const u8 *) &sa->sin_addr.s_addr;
	char fmt4[3] = { fmt[0], '4', 0 };
ffffffff812c9967:	88 55 b6             	mov    %dl,-0x4a(%rbp)

	fmt++;
	while (isalpha(*++fmt)) {
ffffffff812c996a:	48 ff c0             	inc    %rax
ffffffff812c996d:	0f b6 08             	movzbl (%rax),%ecx
ffffffff812c9970:	f6 81 60 ef 63 81 03 	testb  $0x3,-0x7e9c10a0(%rcx)
ffffffff812c9977:	48 89 ca             	mov    %rcx,%rdx
ffffffff812c997a:	74 2d                	je     ffffffff812c99a9 <ip4_addr_string_sa.isra.17+0x85>
ffffffff812c997c:	8d 4a 9e             	lea    -0x62(%rdx),%ecx
ffffffff812c997f:	80 f9 0e             	cmp    $0xe,%cl
ffffffff812c9982:	77 e6                	ja     ffffffff812c996a <ip4_addr_string_sa.isra.17+0x46>
ffffffff812c9984:	4d 89 d9             	mov    %r11,%r9
ffffffff812c9987:	49 d3 e1             	shl    %cl,%r9
ffffffff812c998a:	41 f7 c1 41 14 00 00 	test   $0x1441,%r9d
ffffffff812c9991:	4c 89 c9             	mov    %r9,%rcx
ffffffff812c9994:	75 0b                	jne    ffffffff812c99a1 <ip4_addr_string_sa.isra.17+0x7d>
		switch (*fmt) {
		case 'p':
			have_p = true;
ffffffff812c9996:	80 e5 40             	and    $0x40,%ch
ffffffff812c9999:	b2 01                	mov    $0x1,%dl
ffffffff812c999b:	44 0f 45 ea          	cmovne %edx,%r13d
ffffffff812c999f:	eb c9                	jmp    ffffffff812c996a <ip4_addr_string_sa.isra.17+0x46>
	char *pend = ip4_addr + sizeof(ip4_addr);
	const u8 *addr = (const u8 *) &sa->sin_addr.s_addr;
	char fmt4[3] = { fmt[0], '4', 0 };

	fmt++;
	while (isalpha(*++fmt)) {
ffffffff812c99a1:	40 88 d7             	mov    %dl,%dil
ffffffff812c99a4:	41 b2 01             	mov    $0x1,%r10b
ffffffff812c99a7:	eb c1                	jmp    ffffffff812c996a <ip4_addr_string_sa.isra.17+0x46>
ffffffff812c99a9:	45 84 d2             	test   %r10b,%r10b
ffffffff812c99ac:	74 04                	je     ffffffff812c99b2 <ip4_addr_string_sa.isra.17+0x8e>
ffffffff812c99ae:	40 88 7d b8          	mov    %dil,-0x48(%rbp)
			fmt4[2] = *fmt;
			break;
		}
	}

	p = ip4_string(ip4_addr, addr, fmt4);
ffffffff812c99b2:	48 8d 55 b6          	lea    -0x4a(%rbp),%rdx
ffffffff812c99b6:	48 8d 7d b9          	lea    -0x47(%rbp),%rdi
ffffffff812c99ba:	44 88 45 a7          	mov    %r8b,-0x59(%rbp)
ffffffff812c99be:	e8 05 eb ff ff       	callq  ffffffff812c84c8 <ip4_string>
	if (have_p) {
ffffffff812c99c3:	45 84 ed             	test   %r13b,%r13b
ffffffff812c99c6:	74 28                	je     ffffffff812c99f0 <ip4_addr_string_sa.isra.17+0xcc>
		*p++ = ':';
ffffffff812c99c8:	c6 00 3a             	movb   $0x3a,(%rax)
 */

static inline __attribute_const__ __u16 __fswab16(__u16 val)
{
#ifdef __HAVE_BUILTIN_BSWAP16__
	return __builtin_bswap16(val);
ffffffff812c99cb:	41 0f b7 57 02       	movzwl 0x2(%r15),%edx
ffffffff812c99d0:	48 8d 78 01          	lea    0x1(%rax),%rdi
		p = number(p, pend, ntohs(sa->sin_port), spec);
ffffffff812c99d4:	8b 45 10             	mov    0x10(%rbp),%eax
ffffffff812c99d7:	44 8a 45 a7          	mov    -0x59(%rbp),%r8b
ffffffff812c99db:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c99df:	45 89 e1             	mov    %r12d,%r9d
ffffffff812c99e2:	88 d9                	mov    %bl,%cl
ffffffff812c99e4:	86 f2                	xchg   %dh,%dl
ffffffff812c99e6:	50                   	push   %rax
ffffffff812c99e7:	0f b7 d2             	movzwl %dx,%edx
ffffffff812c99ea:	e8 60 f8 ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812c99ef:	5a                   	pop    %rdx
	}
	*p = '\0';

	return string(buf, end, ip4_addr, spec);
ffffffff812c99f0:	44 8b 4d 10          	mov    0x10(%rbp),%r9d
ffffffff812c99f4:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
ffffffff812c99f8:	48 8d 55 b9          	lea    -0x47(%rbp),%rdx
ffffffff812c99fc:	45 89 e0             	mov    %r12d,%r8d
ffffffff812c99ff:	88 d9                	mov    %bl,%cl
ffffffff812c9a01:	4c 89 f6             	mov    %r14,%rsi
	p = ip4_string(ip4_addr, addr, fmt4);
	if (have_p) {
		*p++ = ':';
		p = number(p, pend, ntohs(sa->sin_port), spec);
	}
	*p = '\0';
ffffffff812c9a04:	c6 00 00             	movb   $0x0,(%rax)

	return string(buf, end, ip4_addr, spec);
ffffffff812c9a07:	e8 1a f4 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
}
ffffffff812c9a0c:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812c9a10:	5b                   	pop    %rbx
ffffffff812c9a11:	41 5c                	pop    %r12
ffffffff812c9a13:	41 5d                	pop    %r13
ffffffff812c9a15:	41 5e                	pop    %r14
ffffffff812c9a17:	41 5f                	pop    %r15
ffffffff812c9a19:	5d                   	pop    %rbp
ffffffff812c9a1a:	c3                   	retq   

ffffffff812c9a1b <address_val.isra.20>:

	return number(buf, end, *(const netdev_features_t *)addr, spec);
}

static noinline_for_stack
char *address_val(char *buf, char *end, const void *addr,
ffffffff812c9a1b:	55                   	push   %rbp
ffffffff812c9a1c:	48 8b 12             	mov    (%rdx),%rdx
		  struct printf_spec spec, const char *fmt)
{
	unsigned long long num;

	spec.flags |= SPECIAL | SMALL | ZEROPAD;
ffffffff812c9a1f:	83 c9 70             	or     $0x70,%ecx
		num = *(const phys_addr_t *)addr;
		spec.field_width = sizeof(phys_addr_t) * 2 + 2;
		break;
	}

	return number(buf, end, num, spec);
ffffffff812c9a22:	41 b9 12 00 00 00    	mov    $0x12,%r9d

	return number(buf, end, *(const netdev_features_t *)addr, spec);
}

static noinline_for_stack
char *address_val(char *buf, char *end, const void *addr,
ffffffff812c9a28:	48 89 e5             	mov    %rsp,%rbp
		num = *(const phys_addr_t *)addr;
		spec.field_width = sizeof(phys_addr_t) * 2 + 2;
		break;
	}

	return number(buf, end, num, spec);
ffffffff812c9a2b:	41 50                	push   %r8
ffffffff812c9a2d:	41 b0 10             	mov    $0x10,%r8b
ffffffff812c9a30:	e8 1a f8 ff ff       	callq  ffffffff812c924f <number.isra.13>
}
ffffffff812c9a35:	c9                   	leaveq 
ffffffff812c9a36:	c3                   	retq   

ffffffff812c9a37 <ip6_string>:
	return p;
}

static noinline_for_stack
char *ip6_string(char *p, const char *addr, const char *fmt)
{
ffffffff812c9a37:	55                   	push   %rbp
ffffffff812c9a38:	48 89 f8             	mov    %rdi,%rax
ffffffff812c9a3b:	31 c9                	xor    %ecx,%ecx
ffffffff812c9a3d:	48 89 e5             	mov    %rsp,%rbp
	int i;

	for (i = 0; i < 8; i++) {
		p = hex_byte_pack(p, *addr++);
ffffffff812c9a40:	40 8a 3c 4e          	mov    (%rsi,%rcx,2),%dil
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9a44:	41 88 f8             	mov    %dil,%r8b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9a47:	83 e7 0f             	and    $0xf,%edi
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9a4a:	41 c0 e8 04          	shr    $0x4,%r8b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9a4e:	40 8a bf b0 32 64 81 	mov    -0x7e9bcd50(%rdi),%dil
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9a55:	41 83 e0 0f          	and    $0xf,%r8d
ffffffff812c9a59:	45 8a 80 b0 32 64 81 	mov    -0x7e9bcd50(%r8),%r8b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9a60:	40 88 78 01          	mov    %dil,0x1(%rax)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9a64:	44 88 00             	mov    %r8b,(%rax)
		p = hex_byte_pack(p, *addr++);
ffffffff812c9a67:	40 8a 7c 4e 01       	mov    0x1(%rsi,%rcx,2),%dil
ffffffff812c9a6c:	41 88 f8             	mov    %dil,%r8b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9a6f:	83 e7 0f             	and    $0xf,%edi
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9a72:	41 c0 e8 04          	shr    $0x4,%r8b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9a76:	40 8a bf b0 32 64 81 	mov    -0x7e9bcd50(%rdi),%dil
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9a7d:	41 83 e0 0f          	and    $0xf,%r8d
ffffffff812c9a81:	45 8a 80 b0 32 64 81 	mov    -0x7e9bcd50(%r8),%r8b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9a88:	40 88 78 03          	mov    %dil,0x3(%rax)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9a8c:	44 88 40 02          	mov    %r8b,0x2(%rax)
		if (fmt[0] == 'I' && i != 7)
ffffffff812c9a90:	80 3a 49             	cmpb   $0x49,(%rdx)
ffffffff812c9a93:	75 05                	jne    ffffffff812c9a9a <ip6_string+0x63>
ffffffff812c9a95:	83 f9 07             	cmp    $0x7,%ecx
ffffffff812c9a98:	75 06                	jne    ffffffff812c9aa0 <ip6_string+0x69>
	*buf++ = hex_asc_lo(byte);
ffffffff812c9a9a:	48 83 c0 04          	add    $0x4,%rax
ffffffff812c9a9e:	eb 08                	jmp    ffffffff812c9aa8 <ip6_string+0x71>
			*p++ = ':';
ffffffff812c9aa0:	c6 40 04 3a          	movb   $0x3a,0x4(%rax)
ffffffff812c9aa4:	48 83 c0 05          	add    $0x5,%rax
ffffffff812c9aa8:	48 ff c1             	inc    %rcx
static noinline_for_stack
char *ip6_string(char *p, const char *addr, const char *fmt)
{
	int i;

	for (i = 0; i < 8; i++) {
ffffffff812c9aab:	48 83 f9 08          	cmp    $0x8,%rcx
ffffffff812c9aaf:	75 8f                	jne    ffffffff812c9a40 <ip6_string+0x9>
		p = hex_byte_pack(p, *addr++);
		p = hex_byte_pack(p, *addr++);
		if (fmt[0] == 'I' && i != 7)
			*p++ = ':';
	}
	*p = '\0';
ffffffff812c9ab1:	c6 00 00             	movb   $0x0,(%rax)

	return p;
}
ffffffff812c9ab4:	5d                   	pop    %rbp
ffffffff812c9ab5:	c3                   	retq   

ffffffff812c9ab6 <uuid_string.isra.7>:

	return buf;
}

static noinline_for_stack
char *uuid_string(char *buf, char *end, const u8 *addr,
ffffffff812c9ab6:	55                   	push   %rbp
	const u8 *index = be;
	bool uc = false;

	switch (*(++fmt)) {
	case 'L':
		uc = true;		/* fall-through */
ffffffff812c9ab7:	41 b2 01             	mov    $0x1,%r10b

	return buf;
}

static noinline_for_stack
char *uuid_string(char *buf, char *end, const u8 *addr,
ffffffff812c9aba:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9abd:	41 57                	push   %r15
ffffffff812c9abf:	41 56                	push   %r14
ffffffff812c9ac1:	41 55                	push   %r13
ffffffff812c9ac3:	41 54                	push   %r12

	switch (*(++fmt)) {
	case 'L':
		uc = true;		/* fall-through */
	case 'l':
		index = le;
ffffffff812c9ac5:	49 c7 c5 30 f4 63 81 	mov    $0xffffffff8163f430,%r13

	return buf;
}

static noinline_for_stack
char *uuid_string(char *buf, char *end, const u8 *addr,
ffffffff812c9acc:	53                   	push   %rbx
ffffffff812c9acd:	41 88 cc             	mov    %cl,%r12b
ffffffff812c9ad0:	48 83 ec 38          	sub    $0x38,%rsp
	static const u8 be[16] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15};
	static const u8 le[16] = {3,2,1,0,5,4,7,6,8,9,10,11,12,13,14,15};
	const u8 *index = be;
	bool uc = false;

	switch (*(++fmt)) {
ffffffff812c9ad4:	48 8b 45 10          	mov    0x10(%rbp),%rax
ffffffff812c9ad8:	8a 40 01             	mov    0x1(%rax),%al
ffffffff812c9adb:	3c 4c                	cmp    $0x4c,%al
ffffffff812c9add:	74 14                	je     ffffffff812c9af3 <uuid_string.isra.7+0x3d>
	char *p = uuid;
	int i;
	static const u8 be[16] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15};
	static const u8 le[16] = {3,2,1,0,5,4,7,6,8,9,10,11,12,13,14,15};
	const u8 *index = be;
	bool uc = false;
ffffffff812c9adf:	45 31 d2             	xor    %r10d,%r10d

	switch (*(++fmt)) {
ffffffff812c9ae2:	3c 6c                	cmp    $0x6c,%al
ffffffff812c9ae4:	74 0d                	je     ffffffff812c9af3 <uuid_string.isra.7+0x3d>
	char *p = uuid;
	int i;
	static const u8 be[16] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15};
	static const u8 le[16] = {3,2,1,0,5,4,7,6,8,9,10,11,12,13,14,15};
	const u8 *index = be;
	bool uc = false;
ffffffff812c9ae6:	3c 42                	cmp    $0x42,%al
ffffffff812c9ae8:	49 c7 c5 40 f4 63 81 	mov    $0xffffffff8163f440,%r13
ffffffff812c9aef:	41 0f 94 c2          	sete   %r10b
static noinline_for_stack
char *uuid_string(char *buf, char *end, const u8 *addr,
		  struct printf_spec spec, const char *fmt)
{
	char uuid[sizeof("xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx")];
	char *p = uuid;
ffffffff812c9af3:	48 8d 45 ab          	lea    -0x55(%rbp),%rax
ffffffff812c9af7:	31 c9                	xor    %ecx,%ecx
ffffffff812c9af9:	41 be 01 00 00 00    	mov    $0x1,%r14d
ffffffff812c9aff:	48 89 c3             	mov    %rax,%rbx
		uc = true;
		break;
	}

	for (i = 0; i < 16; i++) {
		p = hex_byte_pack(p, addr[index[i]]);
ffffffff812c9b02:	45 0f b6 5c 0d 00    	movzbl 0x0(%r13,%rcx,1),%r11d
ffffffff812c9b08:	46 8a 3c 1a          	mov    (%rdx,%r11,1),%r15b
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9b0c:	45 88 fb             	mov    %r15b,%r11b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9b0f:	41 83 e7 0f          	and    $0xf,%r15d
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9b13:	41 c0 eb 04          	shr    $0x4,%r11b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9b17:	45 8a bf b0 32 64 81 	mov    -0x7e9bcd50(%r15),%r15b
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9b1e:	41 83 e3 0f          	and    $0xf,%r11d
ffffffff812c9b22:	48 83 f9 09          	cmp    $0x9,%rcx
ffffffff812c9b26:	45 8a 9b b0 32 64 81 	mov    -0x7e9bcd50(%r11),%r11b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9b2d:	44 88 78 01          	mov    %r15b,0x1(%rax)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9b31:	44 88 18             	mov    %r11b,(%rax)
	*buf++ = hex_asc_lo(byte);
ffffffff812c9b34:	4c 8d 58 02          	lea    0x2(%rax),%r11
ffffffff812c9b38:	77 17                	ja     ffffffff812c9b51 <uuid_string.isra.7+0x9b>
ffffffff812c9b3a:	4d 89 f7             	mov    %r14,%r15
ffffffff812c9b3d:	49 d3 e7             	shl    %cl,%r15
ffffffff812c9b40:	41 f7 c7 a8 02 00 00 	test   $0x2a8,%r15d
ffffffff812c9b47:	74 08                	je     ffffffff812c9b51 <uuid_string.isra.7+0x9b>
		switch (i) {
		case 3:
		case 5:
		case 7:
		case 9:
			*p++ = '-';
ffffffff812c9b49:	4c 8d 58 03          	lea    0x3(%rax),%r11
ffffffff812c9b4d:	c6 40 02 2d          	movb   $0x2d,0x2(%rax)
ffffffff812c9b51:	48 ff c1             	inc    %rcx
	case 'B':
		uc = true;
		break;
	}

	for (i = 0; i < 16; i++) {
ffffffff812c9b54:	48 83 f9 10          	cmp    $0x10,%rcx
ffffffff812c9b58:	74 05                	je     ffffffff812c9b5f <uuid_string.isra.7+0xa9>
ffffffff812c9b5a:	4c 89 d8             	mov    %r11,%rax
ffffffff812c9b5d:	eb a3                	jmp    ffffffff812c9b02 <uuid_string.isra.7+0x4c>
			*p++ = '-';
			break;
		}
	}

	*p = 0;
ffffffff812c9b5f:	41 c6 03 00          	movb   $0x0,(%r11)

	if (uc) {
ffffffff812c9b63:	48 89 d8             	mov    %rbx,%rax
ffffffff812c9b66:	45 84 d2             	test   %r10b,%r10b
ffffffff812c9b69:	74 21                	je     ffffffff812c9b8c <uuid_string.isra.7+0xd6>
		p = uuid;
		do {
			*p = toupper(*p);
ffffffff812c9b6b:	44 0f b6 10          	movzbl (%rax),%r10d
}

static inline unsigned char __toupper(unsigned char c)
{
	if (islower(c))
		c -= 'a'-'A';
ffffffff812c9b6f:	41 f6 82 60 ef 63 81 	testb  $0x2,-0x7e9c10a0(%r10)
ffffffff812c9b76:	02 
ffffffff812c9b77:	41 8d 52 e0          	lea    -0x20(%r10),%edx
ffffffff812c9b7b:	4c 89 d1             	mov    %r10,%rcx
ffffffff812c9b7e:	0f 45 ca             	cmovne %edx,%ecx
		} while (*(++p));
ffffffff812c9b81:	48 ff c0             	inc    %rax
	*p = 0;

	if (uc) {
		p = uuid;
		do {
			*p = toupper(*p);
ffffffff812c9b84:	88 48 ff             	mov    %cl,-0x1(%rax)
		} while (*(++p));
ffffffff812c9b87:	80 38 00             	cmpb   $0x0,(%rax)
ffffffff812c9b8a:	eb dd                	jmp    ffffffff812c9b69 <uuid_string.isra.7+0xb3>
	}

	return string(buf, end, uuid, spec);
ffffffff812c9b8c:	44 88 e1             	mov    %r12b,%cl
ffffffff812c9b8f:	48 89 da             	mov    %rbx,%rdx
ffffffff812c9b92:	e8 8f f2 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
}
ffffffff812c9b97:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812c9b9b:	5b                   	pop    %rbx
ffffffff812c9b9c:	41 5c                	pop    %r12
ffffffff812c9b9e:	41 5d                	pop    %r13
ffffffff812c9ba0:	41 5e                	pop    %r14
ffffffff812c9ba2:	41 5f                	pop    %r15
ffffffff812c9ba4:	5d                   	pop    %rbp
ffffffff812c9ba5:	c3                   	retq   

ffffffff812c9ba6 <mac_address_string.isra.5>:
	}
	return buf;
}

static noinline_for_stack
char *mac_address_string(char *buf, char *end, u8 *addr,
ffffffff812c9ba6:	55                   	push   %rbp
ffffffff812c9ba7:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9baa:	41 57                	push   %r15
ffffffff812c9bac:	41 56                	push   %r14
ffffffff812c9bae:	41 55                	push   %r13
ffffffff812c9bb0:	41 54                	push   %r12
ffffffff812c9bb2:	49 89 d5             	mov    %rdx,%r13
ffffffff812c9bb5:	53                   	push   %rbx
ffffffff812c9bb6:	48 83 ec 28          	sub    $0x28,%rsp
ffffffff812c9bba:	48 8b 45 10          	mov    0x10(%rbp),%rax
	char *p = mac_addr;
	int i;
	char separator;
	bool reversed = false;

	switch (fmt[1]) {
ffffffff812c9bbe:	8a 50 01             	mov    0x1(%rax),%dl
ffffffff812c9bc1:	80 fa 46             	cmp    $0x46,%dl
ffffffff812c9bc4:	74 0b                	je     ffffffff812c9bd1 <mac_address_string.isra.5+0x2b>
{
	char mac_addr[sizeof("xx:xx:xx:xx:xx:xx")];
	char *p = mac_addr;
	int i;
	char separator;
	bool reversed = false;
ffffffff812c9bc6:	80 fa 52             	cmp    $0x52,%dl
ffffffff812c9bc9:	41 b4 3a             	mov    $0x3a,%r12b
ffffffff812c9bcc:	0f 94 c3             	sete   %bl
ffffffff812c9bcf:	eb 05                	jmp    ffffffff812c9bd6 <mac_address_string.isra.5+0x30>
ffffffff812c9bd1:	31 db                	xor    %ebx,%ebx

	switch (fmt[1]) {
	case 'F':
		separator = '-';
ffffffff812c9bd3:	41 b4 2d             	mov    $0x2d,%r12b
		if (reversed)
			p = hex_byte_pack(p, addr[5 - i]);
		else
			p = hex_byte_pack(p, addr[i]);

		if (fmt[0] == 'M' && i != 5)
ffffffff812c9bd6:	80 38 4d             	cmpb   $0x4d,(%rax)
static noinline_for_stack
char *mac_address_string(char *buf, char *end, u8 *addr,
			 struct printf_spec spec, const char *fmt)
{
	char mac_addr[sizeof("xx:xx:xx:xx:xx:xx")];
	char *p = mac_addr;
ffffffff812c9bd9:	48 8d 45 be          	lea    -0x42(%rbp),%rax
ffffffff812c9bdd:	48 89 c2             	mov    %rax,%rdx
		if (reversed)
			p = hex_byte_pack(p, addr[5 - i]);
		else
			p = hex_byte_pack(p, addr[i]);

		if (fmt[0] == 'M' && i != 5)
ffffffff812c9be0:	41 0f 94 c6          	sete   %r14b
ffffffff812c9be4:	45 31 d2             	xor    %r10d,%r10d
		separator = ':';
		break;
	}

	for (i = 0; i < 6; i++) {
		if (reversed)
ffffffff812c9be7:	84 db                	test   %bl,%bl
ffffffff812c9be9:	74 0d                	je     ffffffff812c9bf8 <mac_address_string.isra.5+0x52>
			p = hex_byte_pack(p, addr[5 - i]);
ffffffff812c9beb:	4d 89 d3             	mov    %r10,%r11
ffffffff812c9bee:	49 f7 db             	neg    %r11
ffffffff812c9bf1:	47 8a 5c 1d 05       	mov    0x5(%r13,%r11,1),%r11b
ffffffff812c9bf6:	eb 05                	jmp    ffffffff812c9bfd <mac_address_string.isra.5+0x57>
		else
			p = hex_byte_pack(p, addr[i]);
ffffffff812c9bf8:	47 8a 5c 15 00       	mov    0x0(%r13,%r10,1),%r11b
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9bfd:	45 88 df             	mov    %r11b,%r15b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9c00:	41 83 e3 0f          	and    $0xf,%r11d
ffffffff812c9c04:	48 83 c0 02          	add    $0x2,%rax
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9c08:	41 c0 ef 04          	shr    $0x4,%r15b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9c0c:	45 8a 9b b0 32 64 81 	mov    -0x7e9bcd50(%r11),%r11b
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9c13:	41 83 e7 0f          	and    $0xf,%r15d
ffffffff812c9c17:	45 8a bf b0 32 64 81 	mov    -0x7e9bcd50(%r15),%r15b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9c1e:	44 88 58 ff          	mov    %r11b,-0x1(%rax)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9c22:	44 88 78 fe          	mov    %r15b,-0x2(%rax)

		if (fmt[0] == 'M' && i != 5)
ffffffff812c9c26:	41 83 fa 05          	cmp    $0x5,%r10d
ffffffff812c9c2a:	74 0b                	je     ffffffff812c9c37 <mac_address_string.isra.5+0x91>
ffffffff812c9c2c:	45 84 f6             	test   %r14b,%r14b
ffffffff812c9c2f:	74 06                	je     ffffffff812c9c37 <mac_address_string.isra.5+0x91>
			*p++ = separator;
ffffffff812c9c31:	44 88 20             	mov    %r12b,(%rax)
ffffffff812c9c34:	48 ff c0             	inc    %rax
ffffffff812c9c37:	49 ff c2             	inc    %r10
	default:
		separator = ':';
		break;
	}

	for (i = 0; i < 6; i++) {
ffffffff812c9c3a:	49 83 fa 06          	cmp    $0x6,%r10
ffffffff812c9c3e:	75 a7                	jne    ffffffff812c9be7 <mac_address_string.isra.5+0x41>
			p = hex_byte_pack(p, addr[i]);

		if (fmt[0] == 'M' && i != 5)
			*p++ = separator;
	}
	*p = '\0';
ffffffff812c9c40:	c6 00 00             	movb   $0x0,(%rax)

	return string(buf, end, mac_addr, spec);
ffffffff812c9c43:	e8 de f1 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
}
ffffffff812c9c48:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c9c4c:	5b                   	pop    %rbx
ffffffff812c9c4d:	41 5c                	pop    %r12
ffffffff812c9c4f:	41 5d                	pop    %r13
ffffffff812c9c51:	41 5e                	pop    %r14
ffffffff812c9c53:	41 5f                	pop    %r15
ffffffff812c9c55:	5d                   	pop    %rbp
ffffffff812c9c56:	c3                   	retq   

ffffffff812c9c57 <ip6_compressed_string>:
	return p;
}

static noinline_for_stack
char *ip6_compressed_string(char *p, const char *addr)
{
ffffffff812c9c57:	55                   	push   %rbp
ffffffff812c9c58:	48 89 f8             	mov    %rdi,%rax
ffffffff812c9c5b:	41 ba 01 00 00 00    	mov    $0x1,%r10d
ffffffff812c9c61:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9c64:	53                   	push   %rbx
ffffffff812c9c65:	48 83 ec 28          	sub    $0x28,%rsp
	u8 hi, lo;
	bool needcolon = false;
	bool useIPv4;
	struct in6_addr in6;

	memcpy(&in6, addr, sizeof(struct in6_addr));
ffffffff812c9c69:	48 8b 5e 08          	mov    0x8(%rsi),%rbx
ffffffff812c9c6d:	48 8b 0e             	mov    (%rsi),%rcx
ffffffff812c9c70:	48 89 5d e8          	mov    %rbx,-0x18(%rbp)
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS) && BITS_PER_LONG == 64
		*(unsigned long *)a |
#else
		(__force unsigned long)(a->s6_addr32[0] | a->s6_addr32[1]) |
#endif
		(__force unsigned long)(a->s6_addr32[2] ^
ffffffff812c9c74:	8b 55 e8             	mov    -0x18(%rbp),%edx
ffffffff812c9c77:	48 89 4d e0          	mov    %rcx,-0x20(%rbp)

	useIPv4 = ipv6_addr_v4mapped(&in6) || ipv6_addr_is_isatap(&in6);
ffffffff812c9c7b:	89 d1                	mov    %edx,%ecx
ffffffff812c9c7d:	81 f1 00 00 ff ff    	xor    $0xffff0000,%ecx
ffffffff812c9c83:	48 0b 4d e0          	or     -0x20(%rbp),%rcx
ffffffff812c9c87:	74 10                	je     ffffffff812c9c99 <ip6_compressed_string+0x42>
ffffffff812c9c89:	83 ca 02             	or     $0x2,%edx
ffffffff812c9c8c:	45 31 d2             	xor    %r10d,%r10d
ffffffff812c9c8f:	81 fa 02 00 5e fe    	cmp    $0xfe5e0002,%edx
ffffffff812c9c95:	41 0f 94 c2          	sete   %r10b
	memset(zerolength, 0, sizeof(zerolength));

	if (useIPv4)
		range = 6;
	else
		range = 8;
ffffffff812c9c99:	41 83 fa 01          	cmp    $0x1,%r10d

	memcpy(&in6, addr, sizeof(struct in6_addr));

	useIPv4 = ipv6_addr_v4mapped(&in6) || ipv6_addr_is_isatap(&in6);

	memset(zerolength, 0, sizeof(zerolength));
ffffffff812c9c9d:	48 c7 45 d8 00 00 00 	movq   $0x0,-0x28(%rbp)
ffffffff812c9ca4:	00 

	if (useIPv4)
		range = 6;
	else
		range = 8;
ffffffff812c9ca5:	19 c9                	sbb    %ecx,%ecx
ffffffff812c9ca7:	31 d2                	xor    %edx,%edx
ffffffff812c9ca9:	83 e1 02             	and    $0x2,%ecx
ffffffff812c9cac:	83 c1 06             	add    $0x6,%ecx
ffffffff812c9caf:	89 d6                	mov    %edx,%esi

	/* find position of longest 0 run */
	for (i = 0; i < range; i++) {
		for (j = i; j < range; j++) {
ffffffff812c9cb1:	39 ce                	cmp    %ecx,%esi
ffffffff812c9cb3:	7d 13                	jge    ffffffff812c9cc8 <ip6_compressed_string+0x71>
			if (in6.s6_addr16[j] != 0)
ffffffff812c9cb5:	48 63 fe             	movslq %esi,%rdi
ffffffff812c9cb8:	66 83 7c 7d e0 00    	cmpw   $0x0,-0x20(%rbp,%rdi,2)
ffffffff812c9cbe:	75 08                	jne    ffffffff812c9cc8 <ip6_compressed_string+0x71>
				break;
			zerolength[i]++;
ffffffff812c9cc0:	fe 44 15 d8          	incb   -0x28(%rbp,%rdx,1)
	else
		range = 8;

	/* find position of longest 0 run */
	for (i = 0; i < range; i++) {
		for (j = i; j < range; j++) {
ffffffff812c9cc4:	ff c6                	inc    %esi
ffffffff812c9cc6:	eb e9                	jmp    ffffffff812c9cb1 <ip6_compressed_string+0x5a>
ffffffff812c9cc8:	48 ff c2             	inc    %rdx
		range = 6;
	else
		range = 8;

	/* find position of longest 0 run */
	for (i = 0; i < range; i++) {
ffffffff812c9ccb:	39 d1                	cmp    %edx,%ecx
ffffffff812c9ccd:	7f e0                	jg     ffffffff812c9caf <ip6_compressed_string+0x58>
ffffffff812c9ccf:	31 d2                	xor    %edx,%edx
ffffffff812c9cd1:	41 83 c8 ff          	or     $0xffffffff,%r8d
ffffffff812c9cd5:	bf 01 00 00 00       	mov    $0x1,%edi
				break;
			zerolength[i]++;
		}
	}
	for (i = 0; i < range; i++) {
		if (zerolength[i] > longest) {
ffffffff812c9cda:	0f b6 74 15 d8       	movzbl -0x28(%rbp,%rdx,1),%esi
ffffffff812c9cdf:	39 f7                	cmp    %esi,%edi
ffffffff812c9ce1:	7d 05                	jge    ffffffff812c9ce8 <ip6_compressed_string+0x91>
ffffffff812c9ce3:	41 89 d0             	mov    %edx,%r8d
ffffffff812c9ce6:	89 f7                	mov    %esi,%edi
ffffffff812c9ce8:	48 ff c2             	inc    %rdx
			if (in6.s6_addr16[j] != 0)
				break;
			zerolength[i]++;
		}
	}
	for (i = 0; i < range; i++) {
ffffffff812c9ceb:	39 d1                	cmp    %edx,%ecx
ffffffff812c9ced:	7f eb                	jg     ffffffff812c9cda <ip6_compressed_string+0x83>
			longest = zerolength[i];
			colonpos = i;
		}
	}
	if (longest == 1)		/* don't compress a single 0 */
		colonpos = -1;
ffffffff812c9cef:	83 ff 01             	cmp    $0x1,%edi
ffffffff812c9cf2:	ba ff ff ff ff       	mov    $0xffffffff,%edx
		if (i == colonpos) {
			if (needcolon || i == 0)
				*p++ = ':';
			*p++ = ':';
			needcolon = false;
			i += longest - 1;
ffffffff812c9cf7:	44 8d 5f ff          	lea    -0x1(%rdi),%r11d
			longest = zerolength[i];
			colonpos = i;
		}
	}
	if (longest == 1)		/* don't compress a single 0 */
		colonpos = -1;
ffffffff812c9cfb:	44 0f 44 c2          	cmove  %edx,%r8d
	unsigned char zerolength[8];
	int longest = 1;
	int colonpos = -1;
	u16 word;
	u8 hi, lo;
	bool needcolon = false;
ffffffff812c9cff:	31 d2                	xor    %edx,%edx
	}
	if (longest == 1)		/* don't compress a single 0 */
		colonpos = -1;

	/* emit address */
	for (i = 0; i < range; i++) {
ffffffff812c9d01:	31 f6                	xor    %esi,%esi
		if (i == colonpos) {
ffffffff812c9d03:	41 39 f0             	cmp    %esi,%r8d
ffffffff812c9d06:	75 1f                	jne    ffffffff812c9d27 <ip6_compressed_string+0xd0>
			if (needcolon || i == 0)
ffffffff812c9d08:	45 85 c0             	test   %r8d,%r8d
ffffffff812c9d0b:	74 04                	je     ffffffff812c9d11 <ip6_compressed_string+0xba>
ffffffff812c9d0d:	84 d2                	test   %dl,%dl
ffffffff812c9d0f:	74 06                	je     ffffffff812c9d17 <ip6_compressed_string+0xc0>
				*p++ = ':';
ffffffff812c9d11:	c6 00 3a             	movb   $0x3a,(%rax)
ffffffff812c9d14:	48 ff c0             	inc    %rax
			*p++ = ':';
ffffffff812c9d17:	c6 00 3a             	movb   $0x3a,(%rax)
			needcolon = false;
			i += longest - 1;
ffffffff812c9d1a:	44 01 de             	add    %r11d,%esi
	/* emit address */
	for (i = 0; i < range; i++) {
		if (i == colonpos) {
			if (needcolon || i == 0)
				*p++ = ':';
			*p++ = ':';
ffffffff812c9d1d:	48 ff c0             	inc    %rax
			needcolon = false;
ffffffff812c9d20:	31 d2                	xor    %edx,%edx
			i += longest - 1;
			continue;
ffffffff812c9d22:	e9 c1 00 00 00       	jmpq   ffffffff812c9de8 <ip6_compressed_string+0x191>
		}
		if (needcolon) {
ffffffff812c9d27:	84 d2                	test   %dl,%dl
ffffffff812c9d29:	74 06                	je     ffffffff812c9d31 <ip6_compressed_string+0xda>
			*p++ = ':';
ffffffff812c9d2b:	c6 00 3a             	movb   $0x3a,(%rax)
ffffffff812c9d2e:	48 ff c0             	inc    %rax
			needcolon = false;
		}
		/* hex u16 without leading 0s */
		word = ntohs(in6.s6_addr16[i]);
ffffffff812c9d31:	48 63 d6             	movslq %esi,%rdx
ffffffff812c9d34:	0f b7 54 55 e0       	movzwl -0x20(%rbp,%rdx,2),%edx
ffffffff812c9d39:	86 f2                	xchg   %dh,%dl
		hi = word >> 8;
		lo = word & 0xff;
		if (hi) {
ffffffff812c9d3b:	89 d7                	mov    %edx,%edi
			needcolon = false;
		}
		/* hex u16 without leading 0s */
		word = ntohs(in6.s6_addr16[i]);
		hi = word >> 8;
		lo = word & 0xff;
ffffffff812c9d3d:	44 0f b6 ca          	movzbl %dl,%r9d
		if (hi) {
ffffffff812c9d41:	66 c1 ef 08          	shr    $0x8,%di
ffffffff812c9d45:	74 65                	je     ffffffff812c9dac <ip6_compressed_string+0x155>
			if (hi > 0x0f)
ffffffff812c9d47:	66 83 ff 0f          	cmp    $0xf,%di
ffffffff812c9d4b:	76 29                	jbe    ffffffff812c9d76 <ip6_compressed_string+0x11f>
ffffffff812c9d4d:	41 88 f9             	mov    %dil,%r9b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9d50:	83 e7 0f             	and    $0xf,%edi
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9d53:	41 c0 e9 04          	shr    $0x4,%r9b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9d57:	40 8a bf b0 32 64 81 	mov    -0x7e9bcd50(%rdi),%dil
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9d5e:	41 83 e1 0f          	and    $0xf,%r9d
ffffffff812c9d62:	45 8a 89 b0 32 64 81 	mov    -0x7e9bcd50(%r9),%r9b
	*buf++ = hex_asc_lo(byte);
ffffffff812c9d69:	40 88 78 01          	mov    %dil,0x1(%rax)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9d6d:	44 88 08             	mov    %r9b,(%rax)
	*buf++ = hex_asc_lo(byte);
ffffffff812c9d70:	4c 8d 48 02          	lea    0x2(%rax),%r9
ffffffff812c9d74:	eb 12                	jmp    ffffffff812c9d88 <ip6_compressed_string+0x131>
				p = hex_byte_pack(p, hi);
			else
				*p++ = hex_asc_lo(hi);
ffffffff812c9d76:	40 0f b6 ff          	movzbl %dil,%edi
ffffffff812c9d7a:	4c 8d 48 01          	lea    0x1(%rax),%r9
ffffffff812c9d7e:	40 8a bf b0 32 64 81 	mov    -0x7e9bcd50(%rdi),%dil
ffffffff812c9d85:	40 88 38             	mov    %dil,(%rax)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9d88:	88 d0                	mov    %dl,%al
	*buf++ = hex_asc_lo(byte);
ffffffff812c9d8a:	83 e2 0f             	and    $0xf,%edx
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9d8d:	c0 e8 04             	shr    $0x4,%al
	*buf++ = hex_asc_lo(byte);
ffffffff812c9d90:	8a 92 b0 32 64 81    	mov    -0x7e9bcd50(%rdx),%dl
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9d96:	83 e0 0f             	and    $0xf,%eax
ffffffff812c9d99:	8a 80 b0 32 64 81    	mov    -0x7e9bcd50(%rax),%al
	*buf++ = hex_asc_lo(byte);
ffffffff812c9d9f:	41 88 51 01          	mov    %dl,0x1(%r9)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9da3:	41 88 01             	mov    %al,(%r9)
	*buf++ = hex_asc_lo(byte);
ffffffff812c9da6:	49 8d 41 02          	lea    0x2(%r9),%rax
ffffffff812c9daa:	eb 3a                	jmp    ffffffff812c9de6 <ip6_compressed_string+0x18f>
			p = hex_byte_pack(p, lo);
		}
		else if (lo > 0x0f)
ffffffff812c9dac:	41 80 f9 0f          	cmp    $0xf,%r9b
ffffffff812c9db0:	76 27                	jbe    ffffffff812c9dd9 <ip6_compressed_string+0x182>
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9db2:	40 88 d7             	mov    %dl,%dil
	*buf++ = hex_asc_lo(byte);
ffffffff812c9db5:	83 e2 0f             	and    $0xf,%edx
			p = hex_byte_pack(p, lo);
ffffffff812c9db8:	48 83 c0 02          	add    $0x2,%rax
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9dbc:	40 c0 ef 04          	shr    $0x4,%dil
	*buf++ = hex_asc_lo(byte);
ffffffff812c9dc0:	8a 92 b0 32 64 81    	mov    -0x7e9bcd50(%rdx),%dl
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9dc6:	83 e7 0f             	and    $0xf,%edi
ffffffff812c9dc9:	40 8a bf b0 32 64 81 	mov    -0x7e9bcd50(%rdi),%dil
	*buf++ = hex_asc_lo(byte);
ffffffff812c9dd0:	88 50 ff             	mov    %dl,-0x1(%rax)
#define hex_asc_lo(x)	hex_asc[((x) & 0x0f)]
#define hex_asc_hi(x)	hex_asc[((x) & 0xf0) >> 4]

static inline char *hex_byte_pack(char *buf, u8 byte)
{
	*buf++ = hex_asc_hi(byte);
ffffffff812c9dd3:	40 88 78 fe          	mov    %dil,-0x2(%rax)
ffffffff812c9dd7:	eb 0d                	jmp    ffffffff812c9de6 <ip6_compressed_string+0x18f>
		else
			*p++ = hex_asc_lo(lo);
ffffffff812c9dd9:	41 8a 91 b0 32 64 81 	mov    -0x7e9bcd50(%r9),%dl
ffffffff812c9de0:	48 ff c0             	inc    %rax
ffffffff812c9de3:	88 50 ff             	mov    %dl,-0x1(%rax)
		needcolon = true;
ffffffff812c9de6:	b2 01                	mov    $0x1,%dl
	}
	if (longest == 1)		/* don't compress a single 0 */
		colonpos = -1;

	/* emit address */
	for (i = 0; i < range; i++) {
ffffffff812c9de8:	ff c6                	inc    %esi
ffffffff812c9dea:	39 f1                	cmp    %esi,%ecx
ffffffff812c9dec:	0f 8f 11 ff ff ff    	jg     ffffffff812c9d03 <ip6_compressed_string+0xac>
		else
			*p++ = hex_asc_lo(lo);
		needcolon = true;
	}

	if (useIPv4) {
ffffffff812c9df2:	45 85 d2             	test   %r10d,%r10d
ffffffff812c9df5:	74 1d                	je     ffffffff812c9e14 <ip6_compressed_string+0x1bd>
		if (needcolon)
ffffffff812c9df7:	84 d2                	test   %dl,%dl
ffffffff812c9df9:	74 06                	je     ffffffff812c9e01 <ip6_compressed_string+0x1aa>
			*p++ = ':';
ffffffff812c9dfb:	c6 00 3a             	movb   $0x3a,(%rax)
ffffffff812c9dfe:	48 ff c0             	inc    %rax
		p = ip4_string(p, &in6.s6_addr[12], "I4");
ffffffff812c9e01:	48 8d 75 ec          	lea    -0x14(%rbp),%rsi
ffffffff812c9e05:	48 c7 c2 9d 22 7d 81 	mov    $0xffffffff817d229d,%rdx
ffffffff812c9e0c:	48 89 c7             	mov    %rax,%rdi
ffffffff812c9e0f:	e8 b4 e6 ff ff       	callq  ffffffff812c84c8 <ip4_string>
	}
	*p = '\0';
ffffffff812c9e14:	c6 00 00             	movb   $0x0,(%rax)

	return p;
}
ffffffff812c9e17:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812c9e1b:	5b                   	pop    %rbx
ffffffff812c9e1c:	5d                   	pop    %rbp
ffffffff812c9e1d:	c3                   	retq   

ffffffff812c9e1e <ip6_addr_string.isra.10>:

	return p;
}

static noinline_for_stack
char *ip6_addr_string(char *buf, char *end, const u8 *addr,
ffffffff812c9e1e:	55                   	push   %rbp
ffffffff812c9e1f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9e22:	41 55                	push   %r13
ffffffff812c9e24:	41 54                	push   %r12
ffffffff812c9e26:	53                   	push   %rbx
ffffffff812c9e27:	49 89 f4             	mov    %rsi,%r12
ffffffff812c9e2a:	48 89 d6             	mov    %rdx,%rsi
ffffffff812c9e2d:	48 89 fb             	mov    %rdi,%rbx
ffffffff812c9e30:	41 88 cd             	mov    %cl,%r13b
ffffffff812c9e33:	48 83 ec 48          	sub    $0x48,%rsp
ffffffff812c9e37:	48 8b 55 10          	mov    0x10(%rbp),%rdx
		      struct printf_spec spec, const char *fmt)
{
	char ip6_addr[sizeof("xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:255.255.255.255")];

	if (fmt[0] == 'I' && fmt[2] == 'c')
ffffffff812c9e3b:	80 3a 49             	cmpb   $0x49,(%rdx)
ffffffff812c9e3e:	75 21                	jne    ffffffff812c9e61 <ip6_addr_string.isra.10+0x43>
ffffffff812c9e40:	80 7a 02 63          	cmpb   $0x63,0x2(%rdx)
ffffffff812c9e44:	75 1b                	jne    ffffffff812c9e61 <ip6_addr_string.isra.10+0x43>
		ip6_compressed_string(ip6_addr, addr);
ffffffff812c9e46:	48 8d 7d b2          	lea    -0x4e(%rbp),%rdi
ffffffff812c9e4a:	44 89 4d a8          	mov    %r9d,-0x58(%rbp)
ffffffff812c9e4e:	44 89 45 ac          	mov    %r8d,-0x54(%rbp)
ffffffff812c9e52:	e8 00 fe ff ff       	callq  ffffffff812c9c57 <ip6_compressed_string>
ffffffff812c9e57:	44 8b 45 ac          	mov    -0x54(%rbp),%r8d
ffffffff812c9e5b:	44 8b 4d a8          	mov    -0x58(%rbp),%r9d
ffffffff812c9e5f:	eb 19                	jmp    ffffffff812c9e7a <ip6_addr_string.isra.10+0x5c>
	else
		ip6_string(ip6_addr, addr, fmt);
ffffffff812c9e61:	48 8d 7d b2          	lea    -0x4e(%rbp),%rdi
ffffffff812c9e65:	44 89 4d a8          	mov    %r9d,-0x58(%rbp)
ffffffff812c9e69:	44 89 45 ac          	mov    %r8d,-0x54(%rbp)
ffffffff812c9e6d:	e8 c5 fb ff ff       	callq  ffffffff812c9a37 <ip6_string>
ffffffff812c9e72:	44 8b 4d a8          	mov    -0x58(%rbp),%r9d
ffffffff812c9e76:	44 8b 45 ac          	mov    -0x54(%rbp),%r8d

	return string(buf, end, ip6_addr, spec);
ffffffff812c9e7a:	48 8d 55 b2          	lea    -0x4e(%rbp),%rdx
ffffffff812c9e7e:	44 88 e9             	mov    %r13b,%cl
ffffffff812c9e81:	4c 89 e6             	mov    %r12,%rsi
ffffffff812c9e84:	48 89 df             	mov    %rbx,%rdi
ffffffff812c9e87:	e8 9a ef ff ff       	callq  ffffffff812c8e26 <string.isra.3>
}
ffffffff812c9e8c:	48 83 c4 48          	add    $0x48,%rsp
ffffffff812c9e90:	5b                   	pop    %rbx
ffffffff812c9e91:	41 5c                	pop    %r12
ffffffff812c9e93:	41 5d                	pop    %r13
ffffffff812c9e95:	5d                   	pop    %rbp
ffffffff812c9e96:	c3                   	retq   

ffffffff812c9e97 <ip6_addr_string_sa.isra.18>:

	return string(buf, end, ip4_addr, spec);
}

static noinline_for_stack
char *ip6_addr_string_sa(char *buf, char *end, const struct sockaddr_in6 *sa,
ffffffff812c9e97:	55                   	push   %rbp
ffffffff812c9e98:	48 89 e5             	mov    %rsp,%rbp
ffffffff812c9e9b:	41 57                	push   %r15
ffffffff812c9e9d:	41 56                	push   %r14
ffffffff812c9e9f:	41 55                	push   %r13
ffffffff812c9ea1:	41 54                	push   %r12
ffffffff812c9ea3:	49 89 d7             	mov    %rdx,%r15
ffffffff812c9ea6:	53                   	push   %rbx
			 struct printf_spec spec, const char *fmt)
{
	bool have_p = false, have_s = false, have_f = false, have_c = false;
ffffffff812c9ea7:	45 31 ed             	xor    %r13d,%r13d
ffffffff812c9eaa:	45 31 e4             	xor    %r12d,%r12d
ffffffff812c9ead:	31 db                	xor    %ebx,%ebx

	return string(buf, end, ip4_addr, spec);
}

static noinline_for_stack
char *ip6_addr_string_sa(char *buf, char *end, const struct sockaddr_in6 *sa,
ffffffff812c9eaf:	48 83 ec 78          	sub    $0x78,%rsp
ffffffff812c9eb3:	48 8b 45 18          	mov    0x18(%rbp),%rax
ffffffff812c9eb7:	44 88 85 7e ff ff ff 	mov    %r8b,-0x82(%rbp)
ffffffff812c9ebe:	48 89 b5 68 ff ff ff 	mov    %rsi,-0x98(%rbp)
	bool have_p = false, have_s = false, have_f = false, have_c = false;
	char ip6_addr[sizeof("[xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:255.255.255.255]") +
		      sizeof(":12345") + sizeof("/123456789") +
		      sizeof("%1234567890")];
	char *p = ip6_addr, *pend = ip6_addr + sizeof(ip6_addr);
	const u8 *addr = (const u8 *) &sa->sin6_addr;
ffffffff812c9ec5:	48 8d 72 08          	lea    0x8(%rdx),%rsi

	return string(buf, end, ip4_addr, spec);
}

static noinline_for_stack
char *ip6_addr_string_sa(char *buf, char *end, const struct sockaddr_in6 *sa,
ffffffff812c9ec9:	88 8d 7f ff ff ff    	mov    %cl,-0x81(%rbp)
ffffffff812c9ecf:	48 89 bd 70 ff ff ff 	mov    %rdi,-0x90(%rbp)
ffffffff812c9ed6:	66 44 89 8d 7c ff ff 	mov    %r9w,-0x84(%rbp)
ffffffff812c9edd:	ff 
			 struct printf_spec spec, const char *fmt)
{
	bool have_p = false, have_s = false, have_f = false, have_c = false;
ffffffff812c9ede:	31 c9                	xor    %ecx,%ecx
	char ip6_addr[sizeof("[xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:255.255.255.255]") +
		      sizeof(":12345") + sizeof("/123456789") +
		      sizeof("%1234567890")];
	char *p = ip6_addr, *pend = ip6_addr + sizeof(ip6_addr);
	const u8 *addr = (const u8 *) &sa->sin6_addr;
	char fmt6[2] = { fmt[0], '6' };
ffffffff812c9ee0:	44 8a 00             	mov    (%rax),%r8b
ffffffff812c9ee3:	c6 45 81 36          	movb   $0x36,-0x7f(%rbp)
	u8 off = 0;

	fmt++;
ffffffff812c9ee7:	48 ff c0             	inc    %rax
	char ip6_addr[sizeof("[xxxx:xxxx:xxxx:xxxx:xxxx:xxxx:255.255.255.255]") +
		      sizeof(":12345") + sizeof("/123456789") +
		      sizeof("%1234567890")];
	char *p = ip6_addr, *pend = ip6_addr + sizeof(ip6_addr);
	const u8 *addr = (const u8 *) &sa->sin6_addr;
	char fmt6[2] = { fmt[0], '6' };
ffffffff812c9eea:	44 88 45 80          	mov    %r8b,-0x80(%rbp)
	u8 off = 0;

	fmt++;
	while (isalpha(*++fmt)) {
ffffffff812c9eee:	48 ff c0             	inc    %rax
ffffffff812c9ef1:	0f b6 38             	movzbl (%rax),%edi
ffffffff812c9ef4:	f6 87 60 ef 63 81 03 	testb  $0x3,-0x7e9c10a0(%rdi)
ffffffff812c9efb:	48 89 fa             	mov    %rdi,%rdx
ffffffff812c9efe:	74 2a                	je     ffffffff812c9f2a <ip6_addr_string_sa.isra.18+0x93>
		switch (*fmt) {
ffffffff812c9f00:	80 fa 66             	cmp    $0x66,%dl
ffffffff812c9f03:	74 20                	je     ffffffff812c9f25 <ip6_addr_string_sa.isra.18+0x8e>
ffffffff812c9f05:	7f 0a                	jg     ffffffff812c9f11 <ip6_addr_string_sa.isra.18+0x7a>
			break;
		case 's':
			have_s = true;
			break;
		case 'c':
			have_c = true;
ffffffff812c9f07:	80 fa 63             	cmp    $0x63,%dl
ffffffff812c9f0a:	b2 01                	mov    $0x1,%dl
ffffffff812c9f0c:	0f 44 ca             	cmove  %edx,%ecx
ffffffff812c9f0f:	eb dd                	jmp    ffffffff812c9eee <ip6_addr_string_sa.isra.18+0x57>
	char fmt6[2] = { fmt[0], '6' };
	u8 off = 0;

	fmt++;
	while (isalpha(*++fmt)) {
		switch (*fmt) {
ffffffff812c9f11:	80 fa 70             	cmp    $0x70,%dl
ffffffff812c9f14:	74 0b                	je     ffffffff812c9f21 <ip6_addr_string_sa.isra.18+0x8a>
			break;
		case 'f':
			have_f = true;
			break;
		case 's':
			have_s = true;
ffffffff812c9f16:	80 fa 73             	cmp    $0x73,%dl
ffffffff812c9f19:	b2 01                	mov    $0x1,%dl
ffffffff812c9f1b:	44 0f 44 e2          	cmove  %edx,%r12d
ffffffff812c9f1f:	eb cd                	jmp    ffffffff812c9eee <ip6_addr_string_sa.isra.18+0x57>

	fmt++;
	while (isalpha(*++fmt)) {
		switch (*fmt) {
		case 'p':
			have_p = true;
ffffffff812c9f21:	b3 01                	mov    $0x1,%bl
ffffffff812c9f23:	eb c9                	jmp    ffffffff812c9eee <ip6_addr_string_sa.isra.18+0x57>
			break;
		case 'f':
			have_f = true;
ffffffff812c9f25:	41 b5 01             	mov    $0x1,%r13b
ffffffff812c9f28:	eb c4                	jmp    ffffffff812c9eee <ip6_addr_string_sa.isra.18+0x57>
ffffffff812c9f2a:	45 88 e6             	mov    %r12b,%r14b
		      sizeof(":12345") + sizeof("/123456789") +
		      sizeof("%1234567890")];
	char *p = ip6_addr, *pend = ip6_addr + sizeof(ip6_addr);
	const u8 *addr = (const u8 *) &sa->sin6_addr;
	char fmt6[2] = { fmt[0], '6' };
	u8 off = 0;
ffffffff812c9f2d:	31 ff                	xor    %edi,%edi
ffffffff812c9f2f:	45 09 ee             	or     %r13d,%r14d
			have_c = true;
			break;
		}
	}

	if (have_p || have_s || have_f) {
ffffffff812c9f32:	41 08 de             	or     %bl,%r14b
ffffffff812c9f35:	74 09                	je     ffffffff812c9f40 <ip6_addr_string_sa.isra.18+0xa9>
		*p = '[';
ffffffff812c9f37:	c6 45 82 5b          	movb   $0x5b,-0x7e(%rbp)
		off = 1;
ffffffff812c9f3b:	bf 01 00 00 00       	mov    $0x1,%edi
	}

	if (fmt6[0] == 'I' && have_c)
		p = ip6_compressed_string(ip6_addr + off, addr);
ffffffff812c9f40:	48 8d 45 82          	lea    -0x7e(%rbp),%rax
ffffffff812c9f44:	48 01 c7             	add    %rax,%rdi
	if (have_p || have_s || have_f) {
		*p = '[';
		off = 1;
	}

	if (fmt6[0] == 'I' && have_c)
ffffffff812c9f47:	41 80 f8 49          	cmp    $0x49,%r8b
ffffffff812c9f4b:	75 0b                	jne    ffffffff812c9f58 <ip6_addr_string_sa.isra.18+0xc1>
ffffffff812c9f4d:	84 c9                	test   %cl,%cl
ffffffff812c9f4f:	74 07                	je     ffffffff812c9f58 <ip6_addr_string_sa.isra.18+0xc1>
		p = ip6_compressed_string(ip6_addr + off, addr);
ffffffff812c9f51:	e8 01 fd ff ff       	callq  ffffffff812c9c57 <ip6_compressed_string>
ffffffff812c9f56:	eb 09                	jmp    ffffffff812c9f61 <ip6_addr_string_sa.isra.18+0xca>
	else
		p = ip6_string(ip6_addr + off, addr, fmt6);
ffffffff812c9f58:	48 8d 55 80          	lea    -0x80(%rbp),%rdx
ffffffff812c9f5c:	e8 d6 fa ff ff       	callq  ffffffff812c9a37 <ip6_string>

	if (have_p || have_s || have_f)
ffffffff812c9f61:	45 84 f6             	test   %r14b,%r14b
ffffffff812c9f64:	74 45                	je     ffffffff812c9fab <ip6_addr_string_sa.isra.18+0x114>
		*p++ = ']';

	if (have_p) {
ffffffff812c9f66:	84 db                	test   %bl,%bl
		p = ip6_compressed_string(ip6_addr + off, addr);
	else
		p = ip6_string(ip6_addr + off, addr, fmt6);

	if (have_p || have_s || have_f)
		*p++ = ']';
ffffffff812c9f68:	c6 00 5d             	movb   $0x5d,(%rax)

	if (have_p) {
ffffffff812c9f6b:	75 05                	jne    ffffffff812c9f72 <ip6_addr_string_sa.isra.18+0xdb>
		p = ip6_compressed_string(ip6_addr + off, addr);
	else
		p = ip6_string(ip6_addr + off, addr, fmt6);

	if (have_p || have_s || have_f)
		*p++ = ']';
ffffffff812c9f6d:	48 ff c0             	inc    %rax
ffffffff812c9f70:	eb 39                	jmp    ffffffff812c9fab <ip6_addr_string_sa.isra.18+0x114>

	if (have_p) {
		*p++ = ':';
ffffffff812c9f72:	c6 40 01 3a          	movb   $0x3a,0x1(%rax)
ffffffff812c9f76:	41 0f b7 57 02       	movzwl 0x2(%r15),%edx
ffffffff812c9f7b:	48 8d 78 02          	lea    0x2(%rax),%rdi
		p = number(p, pend, ntohs(sa->sin6_port), spec);
ffffffff812c9f7f:	8b 45 10             	mov    0x10(%rbp),%eax
ffffffff812c9f82:	44 8a 85 7e ff ff ff 	mov    -0x82(%rbp),%r8b
ffffffff812c9f89:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c9f8d:	41 51                	push   %r9
ffffffff812c9f8f:	8a 8d 7f ff ff ff    	mov    -0x81(%rbp),%cl
ffffffff812c9f95:	44 8b 8d 7c ff ff ff 	mov    -0x84(%rbp),%r9d
ffffffff812c9f9c:	86 f2                	xchg   %dh,%dl
ffffffff812c9f9e:	50                   	push   %rax
ffffffff812c9f9f:	0f b7 d2             	movzwl %dx,%edx
ffffffff812c9fa2:	e8 a8 f2 ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812c9fa7:	41 5a                	pop    %r10
ffffffff812c9fa9:	41 5b                	pop    %r11
	}
	if (have_f) {
ffffffff812c9fab:	45 84 ed             	test   %r13b,%r13b
ffffffff812c9fae:	74 37                	je     ffffffff812c9fe7 <ip6_addr_string_sa.isra.18+0x150>
		*p++ = '/';
ffffffff812c9fb0:	c6 00 2f             	movb   $0x2f,(%rax)
		p = number(p, pend, ntohl(sa->sin6_flowinfo &
ffffffff812c9fb3:	41 8b 57 04          	mov    0x4(%r15),%edx
	if (have_p) {
		*p++ = ':';
		p = number(p, pend, ntohs(sa->sin6_port), spec);
	}
	if (have_f) {
		*p++ = '/';
ffffffff812c9fb7:	48 8d 78 01          	lea    0x1(%rax),%rdi
		p = number(p, pend, ntohl(sa->sin6_flowinfo &
ffffffff812c9fbb:	8b 45 10             	mov    0x10(%rbp),%eax
ffffffff812c9fbe:	44 8a 85 7e ff ff ff 	mov    -0x82(%rbp),%r8b
ffffffff812c9fc5:	44 8b 8d 7c ff ff ff 	mov    -0x84(%rbp),%r9d
ffffffff812c9fcc:	8a 8d 7f ff ff ff    	mov    -0x81(%rbp),%cl
ffffffff812c9fd2:	80 e2 0f             	and    $0xf,%dl
ffffffff812c9fd5:	56                   	push   %rsi
ffffffff812c9fd6:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c9fda:	50                   	push   %rax
}

static inline __attribute_const__ __u32 __fswab32(__u32 val)
{
#ifdef __HAVE_BUILTIN_BSWAP32__
	return __builtin_bswap32(val);
ffffffff812c9fdb:	0f ca                	bswap  %edx
ffffffff812c9fdd:	89 d2                	mov    %edx,%edx
ffffffff812c9fdf:	e8 6b f2 ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812c9fe4:	5f                   	pop    %rdi
ffffffff812c9fe5:	41 58                	pop    %r8
					  IPV6_FLOWINFO_MASK), spec);
	}
	if (have_s) {
ffffffff812c9fe7:	45 84 e4             	test   %r12b,%r12b
ffffffff812c9fea:	74 2f                	je     ffffffff812ca01b <ip6_addr_string_sa.isra.18+0x184>
		*p++ = '%';
ffffffff812c9fec:	c6 00 25             	movb   $0x25,(%rax)
ffffffff812c9fef:	48 8d 78 01          	lea    0x1(%rax),%rdi
		p = number(p, pend, sa->sin6_scope_id, spec);
ffffffff812c9ff3:	50                   	push   %rax
ffffffff812c9ff4:	8b 45 10             	mov    0x10(%rbp),%eax
ffffffff812c9ff7:	41 8b 57 18          	mov    0x18(%r15),%edx
ffffffff812c9ffb:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
ffffffff812c9fff:	8a 8d 7f ff ff ff    	mov    -0x81(%rbp),%cl
ffffffff812ca005:	44 8b 8d 7c ff ff ff 	mov    -0x84(%rbp),%r9d
ffffffff812ca00c:	44 8a 85 7e ff ff ff 	mov    -0x82(%rbp),%r8b
ffffffff812ca013:	50                   	push   %rax
ffffffff812ca014:	e8 36 f2 ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812ca019:	5a                   	pop    %rdx
ffffffff812ca01a:	59                   	pop    %rcx
	}
	*p = '\0';

	return string(buf, end, ip6_addr, spec);
ffffffff812ca01b:	44 8b 4d 10          	mov    0x10(%rbp),%r9d
ffffffff812ca01f:	44 8b 85 7c ff ff ff 	mov    -0x84(%rbp),%r8d
ffffffff812ca026:	48 8d 55 82          	lea    -0x7e(%rbp),%rdx
ffffffff812ca02a:	8a 8d 7f ff ff ff    	mov    -0x81(%rbp),%cl
ffffffff812ca030:	48 8b b5 68 ff ff ff 	mov    -0x98(%rbp),%rsi
ffffffff812ca037:	48 8b bd 70 ff ff ff 	mov    -0x90(%rbp),%rdi
	}
	if (have_s) {
		*p++ = '%';
		p = number(p, pend, sa->sin6_scope_id, spec);
	}
	*p = '\0';
ffffffff812ca03e:	c6 00 00             	movb   $0x0,(%rax)

	return string(buf, end, ip6_addr, spec);
ffffffff812ca041:	e8 e0 ed ff ff       	callq  ffffffff812c8e26 <string.isra.3>
}
ffffffff812ca046:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812ca04a:	5b                   	pop    %rbx
ffffffff812ca04b:	41 5c                	pop    %r12
ffffffff812ca04d:	41 5d                	pop    %r13
ffffffff812ca04f:	41 5e                	pop    %r14
ffffffff812ca051:	41 5f                	pop    %r15
ffffffff812ca053:	5d                   	pop    %rbp
ffffffff812ca054:	c3                   	retq   

ffffffff812ca055 <pointer.isra.21>:
 * Note: The difference between 'S' and 'F' is that on ia64 and ppc64
 * function pointers are really function descriptors, which contain a
 * pointer to the real address.
 */
static noinline_for_stack
char *pointer(const char *fmt, char *buf, char *end, void *ptr,
ffffffff812ca055:	55                   	push   %rbp
ffffffff812ca056:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca059:	41 57                	push   %r15
ffffffff812ca05b:	41 56                	push   %r14
ffffffff812ca05d:	41 55                	push   %r13
ffffffff812ca05f:	41 54                	push   %r12
ffffffff812ca061:	45 88 c4             	mov    %r8b,%r12b
ffffffff812ca064:	53                   	push   %rbx
	      struct printf_spec spec)
{
	int default_width = 2 * sizeof(void *) + (spec.flags & SPECIAL ? 2 : 0);
ffffffff812ca065:	44 88 e0             	mov    %r12b,%al
 * Note: The difference between 'S' and 'F' is that on ia64 and ppc64
 * function pointers are really function descriptors, which contain a
 * pointer to the real address.
 */
static noinline_for_stack
char *pointer(const char *fmt, char *buf, char *end, void *ptr,
ffffffff812ca068:	49 89 d6             	mov    %rdx,%r14
	      struct printf_spec spec)
{
	int default_width = 2 * sizeof(void *) + (spec.flags & SPECIAL ? 2 : 0);
ffffffff812ca06b:	83 e0 40             	and    $0x40,%eax
 * Note: The difference between 'S' and 'F' is that on ia64 and ppc64
 * function pointers are really function descriptors, which contain a
 * pointer to the real address.
 */
static noinline_for_stack
char *pointer(const char *fmt, char *buf, char *end, void *ptr,
ffffffff812ca06e:	48 89 ca             	mov    %rcx,%rdx
ffffffff812ca071:	49 89 f5             	mov    %rsi,%r13
ffffffff812ca074:	48 83 ec 48          	sub    $0x48,%rsp
	      struct printf_spec spec)
{
	int default_width = 2 * sizeof(void *) + (spec.flags & SPECIAL ? 2 : 0);
ffffffff812ca078:	3c 01                	cmp    $0x1,%al
 * Note: The difference between 'S' and 'F' is that on ia64 and ppc64
 * function pointers are really function descriptors, which contain a
 * pointer to the real address.
 */
static noinline_for_stack
char *pointer(const char *fmt, char *buf, char *end, void *ptr,
ffffffff812ca07a:	45 88 c8             	mov    %r9b,%r8b
	      struct printf_spec spec)
{
	int default_width = 2 * sizeof(void *) + (spec.flags & SPECIAL ? 2 : 0);
ffffffff812ca07d:	19 c9                	sbb    %ecx,%ecx
 * Note: The difference between 'S' and 'F' is that on ia64 and ppc64
 * function pointers are really function descriptors, which contain a
 * pointer to the real address.
 */
static noinline_for_stack
char *pointer(const char *fmt, char *buf, char *end, void *ptr,
ffffffff812ca07f:	8b 5d 10             	mov    0x10(%rbp),%ebx
ffffffff812ca082:	44 8b 7d 18          	mov    0x18(%rbp),%r15d
	      struct printf_spec spec)
{
	int default_width = 2 * sizeof(void *) + (spec.flags & SPECIAL ? 2 : 0);
ffffffff812ca086:	83 e1 fe             	and    $0xfffffffe,%ecx
ffffffff812ca089:	83 c1 12             	add    $0x12,%ecx

	if (!ptr && *fmt != 'K') {
ffffffff812ca08c:	48 85 d2             	test   %rdx,%rdx
ffffffff812ca08f:	75 21                	jne    ffffffff812ca0b2 <pointer.isra.21+0x5d>
ffffffff812ca091:	80 3f 4b             	cmpb   $0x4b,(%rdi)
ffffffff812ca094:	74 1c                	je     ffffffff812ca0b2 <pointer.isra.21+0x5d>
		/*
		 * Print (null) with the same width as a pointer so it makes
		 * tabular output look nice.
		 */
		if (spec.field_width == -1)
			spec.field_width = default_width;
ffffffff812ca096:	66 83 fb ff          	cmp    $0xffff,%bx
		return string(buf, end, "(null)", spec);
ffffffff812ca09a:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca09d:	48 c7 c2 7c 6d 7b 81 	mov    $0xffffffff817b6d7c,%rdx
ffffffff812ca0a4:	0f 44 d9             	cmove  %ecx,%ebx
ffffffff812ca0a7:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca0aa:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca0ad:	e9 3e 01 00 00       	jmpq   ffffffff812ca1f0 <pointer.isra.21+0x19b>
	}

	switch (*fmt) {
ffffffff812ca0b2:	8a 07                	mov    (%rdi),%al
ffffffff812ca0b4:	83 e8 42             	sub    $0x42,%eax
ffffffff812ca0b7:	3c 31                	cmp    $0x31,%al
ffffffff812ca0b9:	0f 87 c6 02 00 00    	ja     ffffffff812ca385 <pointer.isra.21+0x330>
ffffffff812ca0bf:	0f b6 c0             	movzbl %al,%eax
ffffffff812ca0c2:	ff 24 c5 f8 f1 63 81 	jmpq   *-0x7e9c0e08(,%rax,8)
		ptr = dereference_function_descriptor(ptr);
		/* Fallthrough */
	case 'S':
	case 's':
	case 'B':
		return symbol_string(buf, end, ptr, spec, fmt);
ffffffff812ca0c9:	50                   	push   %rax
ffffffff812ca0ca:	57                   	push   %rdi
ffffffff812ca0cb:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca0ce:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca0d1:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca0d4:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca0d7:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca0da:	e8 0d ef ff ff       	callq  ffffffff812c8fec <symbol_string.isra.9>
ffffffff812ca0df:	e9 cb 02 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
	case 'R':
	case 'r':
		return resource_string(buf, end, ptr, spec, fmt);
ffffffff812ca0e4:	50                   	push   %rax
ffffffff812ca0e5:	57                   	push   %rdi
ffffffff812ca0e6:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca0e9:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca0ec:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca0ef:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca0f2:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca0f5:	e8 5d f3 ff ff       	callq  ffffffff812c9457 <resource_string.isra.14>
ffffffff812ca0fa:	e9 b0 02 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
	case 'h':
		return hex_string(buf, end, ptr, spec, fmt);
ffffffff812ca0ff:	50                   	push   %rax
ffffffff812ca100:	57                   	push   %rdi
ffffffff812ca101:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca104:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca107:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca10a:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca10d:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca110:	e8 ba ed ff ff       	callq  ffffffff812c8ecf <hex_string.isra.4>
ffffffff812ca115:	e9 95 02 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
	case 'b':
		switch (fmt[1]) {
ffffffff812ca11a:	80 7f 01 6c          	cmpb   $0x6c,0x1(%rdi)
		case 'l':
			return bitmap_list_string(buf, end, ptr, spec, fmt);
ffffffff812ca11e:	89 d9                	mov    %ebx,%ecx
ffffffff812ca120:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca123:	4c 89 ef             	mov    %r13,%rdi
	case 'r':
		return resource_string(buf, end, ptr, spec, fmt);
	case 'h':
		return hex_string(buf, end, ptr, spec, fmt);
	case 'b':
		switch (fmt[1]) {
ffffffff812ca126:	75 0a                	jne    ffffffff812ca132 <pointer.isra.21+0xdd>
		case 'l':
			return bitmap_list_string(buf, end, ptr, spec, fmt);
ffffffff812ca128:	e8 4f f6 ff ff       	callq  ffffffff812c977c <bitmap_list_string.isra.15>
ffffffff812ca12d:	e9 7f 02 00 00       	jmpq   ffffffff812ca3b1 <pointer.isra.21+0x35c>
		default:
			return bitmap_string(buf, end, ptr, spec, fmt);
ffffffff812ca132:	e8 40 f7 ff ff       	callq  ffffffff812c9877 <bitmap_string.isra.16>
ffffffff812ca137:	e9 75 02 00 00       	jmpq   ffffffff812ca3b1 <pointer.isra.21+0x35c>
		}
	case 'M':			/* Colon separated: 00:01:02:03:04:05 */
	case 'm':			/* Contiguous: 000102030405 */
					/* [mM]F (FDDI) */
					/* [mM]R (Reverse order; Bluetooth) */
		return mac_address_string(buf, end, ptr, spec, fmt);
ffffffff812ca13c:	50                   	push   %rax
ffffffff812ca13d:	57                   	push   %rdi
ffffffff812ca13e:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca141:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca144:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca147:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca14a:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca14d:	e8 54 fa ff ff       	callq  ffffffff812c9ba6 <mac_address_string.isra.5>
ffffffff812ca152:	e9 58 02 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
					 */
	case 'i':			/* Contiguous:
					 * 4:	001.002.003.004
					 * 6:   000102...0f
					 */
		switch (fmt[1]) {
ffffffff812ca157:	8a 47 01             	mov    0x1(%rdi),%al
ffffffff812ca15a:	3c 36                	cmp    $0x36,%al
ffffffff812ca15c:	74 27                	je     ffffffff812ca185 <pointer.isra.21+0x130>
ffffffff812ca15e:	3c 53                	cmp    $0x53,%al
ffffffff812ca160:	74 3e                	je     ffffffff812ca1a0 <pointer.isra.21+0x14b>
ffffffff812ca162:	3c 34                	cmp    $0x34,%al
ffffffff812ca164:	0f 85 1b 02 00 00    	jne    ffffffff812ca385 <pointer.isra.21+0x330>
		case '6':
			return ip6_addr_string(buf, end, ptr, spec, fmt);
		case '4':
			return ip4_addr_string(buf, end, ptr, spec, fmt);
ffffffff812ca16a:	50                   	push   %rax
ffffffff812ca16b:	57                   	push   %rdi
ffffffff812ca16c:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca16f:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca172:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca175:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca178:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca17b:	e8 08 ee ff ff       	callq  ffffffff812c8f88 <ip4_addr_string.isra.6>
ffffffff812ca180:	e9 2a 02 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
					 * 4:	001.002.003.004
					 * 6:   000102...0f
					 */
		switch (fmt[1]) {
		case '6':
			return ip6_addr_string(buf, end, ptr, spec, fmt);
ffffffff812ca185:	50                   	push   %rax
ffffffff812ca186:	57                   	push   %rdi
ffffffff812ca187:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca18a:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca18d:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca190:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca193:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca196:	e8 83 fc ff ff       	callq  ffffffff812c9e1e <ip6_addr_string.isra.10>
ffffffff812ca19b:	e9 0f 02 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
				struct sockaddr		raw;
				struct sockaddr_in	v4;
				struct sockaddr_in6	v6;
			} *sa = ptr;

			switch (sa->raw.sa_family) {
ffffffff812ca1a0:	8b 02                	mov    (%rdx),%eax
ffffffff812ca1a2:	66 83 f8 02          	cmp    $0x2,%ax
ffffffff812ca1a6:	74 1f                	je     ffffffff812ca1c7 <pointer.isra.21+0x172>
ffffffff812ca1a8:	66 83 f8 0a          	cmp    $0xa,%ax
ffffffff812ca1ac:	75 32                	jne    ffffffff812ca1e0 <pointer.isra.21+0x18b>
			case AF_INET:
				return ip4_addr_string_sa(buf, end, &sa->v4, spec, fmt);
			case AF_INET6:
				return ip6_addr_string_sa(buf, end, &sa->v6, spec, fmt);
ffffffff812ca1ae:	57                   	push   %rdi
ffffffff812ca1af:	41 57                	push   %r15
ffffffff812ca1b1:	41 89 d9             	mov    %ebx,%r9d
ffffffff812ca1b4:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca1b7:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca1ba:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca1bd:	e8 d5 fc ff ff       	callq  ffffffff812c9e97 <ip6_addr_string_sa.isra.18>
ffffffff812ca1c2:	e9 e8 01 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
				struct sockaddr_in6	v6;
			} *sa = ptr;

			switch (sa->raw.sa_family) {
			case AF_INET:
				return ip4_addr_string_sa(buf, end, &sa->v4, spec, fmt);
ffffffff812ca1c7:	57                   	push   %rdi
ffffffff812ca1c8:	41 57                	push   %r15
ffffffff812ca1ca:	41 89 d9             	mov    %ebx,%r9d
ffffffff812ca1cd:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca1d0:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca1d3:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca1d6:	e8 49 f7 ff ff       	callq  ffffffff812c9924 <ip4_addr_string_sa.isra.17>
ffffffff812ca1db:	e9 cf 01 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
			case AF_INET6:
				return ip6_addr_string_sa(buf, end, &sa->v6, spec, fmt);
			default:
				return string(buf, end, "(invalid address)", spec);
ffffffff812ca1e0:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca1e3:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca1e6:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca1e9:	48 c7 c2 bf 6d 7b 81 	mov    $0xffffffff817b6dbf,%rdx
ffffffff812ca1f0:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca1f3:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca1f6:	e8 2b ec ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812ca1fb:	e9 b1 01 00 00       	jmpq   ffffffff812ca3b1 <pointer.isra.21+0x35c>
			}}
		}
		break;
	case 'E':
		return escaped_string(buf, end, ptr, spec, fmt);
ffffffff812ca200:	41 53                	push   %r11
ffffffff812ca202:	57                   	push   %rdi
ffffffff812ca203:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca206:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca209:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca20c:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca20f:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca212:	e8 54 ee ff ff       	callq  ffffffff812c906b <escaped_string.isra.11>
ffffffff812ca217:	e9 93 01 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
	case 'U':
		return uuid_string(buf, end, ptr, spec, fmt);
ffffffff812ca21c:	41 52                	push   %r10
ffffffff812ca21e:	57                   	push   %rdi
ffffffff812ca21f:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca222:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca225:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca228:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca22b:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca22e:	e8 83 f8 ff ff       	callq  ffffffff812c9ab6 <uuid_string.isra.7>
ffffffff812ca233:	e9 77 01 00 00       	jmpq   ffffffff812ca3af <pointer.isra.21+0x35a>
	case 'V':
		{
			va_list va;

			va_copy(va, *((struct va_format *)ptr)->va);
ffffffff812ca238:	48 8d 45 b8          	lea    -0x48(%rbp),%rax
ffffffff812ca23c:	48 8b 72 08          	mov    0x8(%rdx),%rsi
ffffffff812ca240:	b9 06 00 00 00       	mov    $0x6,%ecx
ffffffff812ca245:	48 89 c7             	mov    %rax,%rdi
ffffffff812ca248:	f3 a5                	rep movsl %ds:(%rsi),%es:(%rdi)
			buf += vsnprintf(buf, end > buf ? end - buf : 0,
ffffffff812ca24a:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca24d:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca250:	4c 29 ee             	sub    %r13,%rsi
ffffffff812ca253:	4d 39 f5             	cmp    %r14,%r13
ffffffff812ca256:	48 8b 12             	mov    (%rdx),%rdx
ffffffff812ca259:	48 0f 43 f1          	cmovae %rcx,%rsi
ffffffff812ca25d:	48 89 c1             	mov    %rax,%rcx
ffffffff812ca260:	e8 5b 01 00 00       	callq  ffffffff812ca3c0 <vsnprintf>
ffffffff812ca265:	48 98                	cltq   
					 ((struct va_format *)ptr)->fmt, va);
			va_end(va);
			return buf;
ffffffff812ca267:	4c 01 e8             	add    %r13,%rax
ffffffff812ca26a:	e9 42 01 00 00       	jmpq   ffffffff812ca3b1 <pointer.isra.21+0x35c>
	case 'K':
		/*
		 * %pK cannot be used in IRQ context because its test
		 * for CAP_SYSLOG would be meaningless.
		 */
		if (kptr_restrict && (in_irq() || in_serving_softirq() ||
ffffffff812ca26f:	8b 35 8b f5 78 00    	mov    0x78f58b(%rip),%esi        # ffffffff81a59800 <kptr_restrict>
ffffffff812ca275:	85 f6                	test   %esi,%esi
ffffffff812ca277:	0f 84 08 01 00 00    	je     ffffffff812ca385 <pointer.isra.21+0x330>
ffffffff812ca27d:	65 8b 05 14 07 d4 7e 	mov    %gs:0x7ed40714(%rip),%eax        # a998 <__preempt_count>
ffffffff812ca284:	a9 00 00 0f 00       	test   $0xf0000,%eax
ffffffff812ca289:	75 0c                	jne    ffffffff812ca297 <pointer.isra.21+0x242>
ffffffff812ca28b:	f6 c4 01             	test   $0x1,%ah
ffffffff812ca28e:	75 07                	jne    ffffffff812ca297 <pointer.isra.21+0x242>
ffffffff812ca290:	a9 00 00 10 00       	test   $0x100000,%eax
ffffffff812ca295:	74 1c                	je     ffffffff812ca2b3 <pointer.isra.21+0x25e>
				      in_nmi())) {
			if (spec.field_width == -1)
				spec.field_width = default_width;
ffffffff812ca297:	66 83 fb ff          	cmp    $0xffff,%bx
			return string(buf, end, "pK-error", spec);
ffffffff812ca29b:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca29e:	48 c7 c2 d1 6d 7b 81 	mov    $0xffffffff817b6dd1,%rdx
ffffffff812ca2a5:	0f 44 d9             	cmove  %ecx,%ebx
ffffffff812ca2a8:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca2ab:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca2ae:	e9 3d ff ff ff       	jmpq   ffffffff812ca1f0 <pointer.isra.21+0x19b>
		}

		switch (kptr_restrict) {
ffffffff812ca2b3:	ff ce                	dec    %esi
ffffffff812ca2b5:	48 89 55 98          	mov    %rdx,-0x68(%rbp)
ffffffff812ca2b9:	0f 85 c4 00 00 00    	jne    ffffffff812ca383 <pointer.isra.21+0x32e>
ffffffff812ca2bf:	65 48 8b 3c 25 00 aa 	mov    %gs:0xaa00,%rdi
ffffffff812ca2c6:	00 00 
			 * access to files is checked at open() time, but %pK
			 * checks permission at read() time. We don't want to
			 * leak pointer values if a binary opens a file using
			 * %pK and then elevates privileges before reading it.
			 */
			const struct cred *cred = current_cred();
ffffffff812ca2c8:	4c 8b 87 b8 04 00 00 	mov    0x4b8(%rdi),%r8

			if (!has_capability_noaudit(current, CAP_SYSLOG) ||
ffffffff812ca2cf:	be 22 00 00 00       	mov    $0x22,%esi
ffffffff812ca2d4:	89 4d a4             	mov    %ecx,-0x5c(%rbp)
			 * access to files is checked at open() time, but %pK
			 * checks permission at read() time. We don't want to
			 * leak pointer values if a binary opens a file using
			 * %pK and then elevates privileges before reading it.
			 */
			const struct cred *cred = current_cred();
ffffffff812ca2d7:	4c 89 45 a8          	mov    %r8,-0x58(%rbp)

			if (!has_capability_noaudit(current, CAP_SYSLOG) ||
ffffffff812ca2db:	e8 ee 21 da ff       	callq  ffffffff8106c4ce <has_capability_noaudit>
ffffffff812ca2e0:	84 c0                	test   %al,%al
ffffffff812ca2e2:	8b 4d a4             	mov    -0x5c(%rbp),%ecx
ffffffff812ca2e5:	0f 84 98 00 00 00    	je     ffffffff812ca383 <pointer.isra.21+0x32e>
ffffffff812ca2eb:	4c 8b 45 a8          	mov    -0x58(%rbp),%r8
ffffffff812ca2ef:	48 8b 55 98          	mov    -0x68(%rbp),%rdx
ffffffff812ca2f3:	41 8b 40 04          	mov    0x4(%r8),%eax
ffffffff812ca2f7:	41 39 40 14          	cmp    %eax,0x14(%r8)
ffffffff812ca2fb:	0f 85 82 00 00 00    	jne    ffffffff812ca383 <pointer.isra.21+0x32e>
			    !uid_eq(cred->euid, cred->uid) ||
			    !gid_eq(cred->egid, cred->gid))
				ptr = NULL;
ffffffff812ca301:	41 8b 40 08          	mov    0x8(%r8),%eax
ffffffff812ca305:	41 39 40 18          	cmp    %eax,0x18(%r8)
ffffffff812ca309:	b8 00 00 00 00       	mov    $0x0,%eax
ffffffff812ca30e:	48 0f 45 d0          	cmovne %rax,%rdx
ffffffff812ca312:	eb 71                	jmp    ffffffff812ca385 <pointer.isra.21+0x330>
			break;
		}
		break;

	case 'N':
		switch (fmt[1]) {
ffffffff812ca314:	80 7f 01 46          	cmpb   $0x46,0x1(%rdi)
ffffffff812ca318:	75 6b                	jne    ffffffff812ca385 <pointer.isra.21+0x330>

static
char *netdev_feature_string(char *buf, char *end, const u8 *addr,
		      struct printf_spec spec)
{
	spec.flags |= SPECIAL | SMALL | ZEROPAD;
ffffffff812ca31a:	44 88 e1             	mov    %r12b,%cl
	if (spec.field_width == -1)
		spec.field_width = 2 + 2 * sizeof(netdev_features_t);
ffffffff812ca31d:	b8 12 00 00 00       	mov    $0x12,%eax
	spec.base = 16;

	return number(buf, end, *(const netdev_features_t *)addr, spec);
ffffffff812ca322:	41 b0 10             	mov    $0x10,%r8b

static
char *netdev_feature_string(char *buf, char *end, const u8 *addr,
		      struct printf_spec spec)
{
	spec.flags |= SPECIAL | SMALL | ZEROPAD;
ffffffff812ca325:	83 c9 70             	or     $0x70,%ecx
	if (spec.field_width == -1)
		spec.field_width = 2 + 2 * sizeof(netdev_features_t);
ffffffff812ca328:	66 83 fb ff          	cmp    $0xffff,%bx
	spec.base = 16;

	return number(buf, end, *(const netdev_features_t *)addr, spec);
ffffffff812ca32c:	41 51                	push   %r9
char *netdev_feature_string(char *buf, char *end, const u8 *addr,
		      struct printf_spec spec)
{
	spec.flags |= SPECIAL | SMALL | ZEROPAD;
	if (spec.field_width == -1)
		spec.field_width = 2 + 2 * sizeof(netdev_features_t);
ffffffff812ca32e:	0f 44 d8             	cmove  %eax,%ebx
	spec.base = 16;

	return number(buf, end, *(const netdev_features_t *)addr, spec);
ffffffff812ca331:	41 57                	push   %r15
ffffffff812ca333:	48 8b 12             	mov    (%rdx),%rdx
ffffffff812ca336:	41 89 d9             	mov    %ebx,%r9d
ffffffff812ca339:	eb 69                	jmp    ffffffff812ca3a4 <pointer.isra.21+0x34f>
		case 'F':
			return netdev_feature_string(buf, end, ptr, spec);
		}
		break;
	case 'a':
		return address_val(buf, end, ptr, spec, fmt);
ffffffff812ca33b:	49 89 f9             	mov    %rdi,%r9
ffffffff812ca33e:	45 89 f8             	mov    %r15d,%r8d
ffffffff812ca341:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca344:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca347:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca34a:	e8 cc f6 ff ff       	callq  ffffffff812c9a1b <address_val.isra.20>
ffffffff812ca34f:	eb 60                	jmp    ffffffff812ca3b1 <pointer.isra.21+0x35c>
	case 'd':
		return dentry_name(buf, end, ptr, spec, fmt);
	case 'C':
		return clock(buf, end, ptr, spec, fmt);
ffffffff812ca351:	45 89 f8             	mov    %r15d,%r8d
ffffffff812ca354:	89 d9                	mov    %ebx,%ecx
ffffffff812ca356:	44 88 e2             	mov    %r12b,%dl
ffffffff812ca359:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca35c:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca35f:	e8 73 ec ff ff       	callq  ffffffff812c8fd7 <clock.isra.8>
ffffffff812ca364:	eb 4b                	jmp    ffffffff812ca3b1 <pointer.isra.21+0x35c>
	case 'D':
		return dentry_name(buf, end,
ffffffff812ca366:	48 8b 52 18          	mov    0x18(%rdx),%rdx
ffffffff812ca36a:	41 50                	push   %r8
ffffffff812ca36c:	57                   	push   %rdi
ffffffff812ca36d:	45 89 f9             	mov    %r15d,%r9d
ffffffff812ca370:	41 89 d8             	mov    %ebx,%r8d
ffffffff812ca373:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca376:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca379:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca37c:	e8 a9 ed ff ff       	callq  ffffffff812c912a <dentry_name.isra.12>
ffffffff812ca381:	eb 2c                	jmp    ffffffff812ca3af <pointer.isra.21+0x35a>
			const struct cred *cred = current_cred();

			if (!has_capability_noaudit(current, CAP_SYSLOG) ||
			    !uid_eq(cred->euid, cred->uid) ||
			    !gid_eq(cred->egid, cred->gid))
				ptr = NULL;
ffffffff812ca383:	31 d2                	xor    %edx,%edx
		return dentry_name(buf, end,
				   ((const struct file *)ptr)->f_path.dentry,
				   spec, fmt);
	}
	spec.flags |= SMALL;
	if (spec.field_width == -1) {
ffffffff812ca385:	66 83 fb ff          	cmp    $0xffff,%bx
ffffffff812ca389:	74 08                	je     ffffffff812ca393 <pointer.isra.21+0x33e>
	case 'D':
		return dentry_name(buf, end,
				   ((const struct file *)ptr)->f_path.dentry,
				   spec, fmt);
	}
	spec.flags |= SMALL;
ffffffff812ca38b:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca38e:	83 c9 20             	or     $0x20,%ecx
ffffffff812ca391:	eb 08                	jmp    ffffffff812ca39b <pointer.isra.21+0x346>
	if (spec.field_width == -1) {
		spec.field_width = default_width;
ffffffff812ca393:	89 cb                	mov    %ecx,%ebx
		spec.flags |= ZEROPAD;
ffffffff812ca395:	44 88 e1             	mov    %r12b,%cl
ffffffff812ca398:	83 c9 30             	or     $0x30,%ecx
	}
	spec.base = 16;

	return number(buf, end, (unsigned long) ptr, spec);
ffffffff812ca39b:	56                   	push   %rsi
ffffffff812ca39c:	41 57                	push   %r15
ffffffff812ca39e:	41 89 d9             	mov    %ebx,%r9d
ffffffff812ca3a1:	41 b0 10             	mov    $0x10,%r8b
ffffffff812ca3a4:	4c 89 f6             	mov    %r14,%rsi
ffffffff812ca3a7:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ca3aa:	e8 a0 ee ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812ca3af:	5a                   	pop    %rdx
ffffffff812ca3b0:	59                   	pop    %rcx
}
ffffffff812ca3b1:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812ca3b5:	5b                   	pop    %rbx
ffffffff812ca3b6:	41 5c                	pop    %r12
ffffffff812ca3b8:	41 5d                	pop    %r13
ffffffff812ca3ba:	41 5e                	pop    %r14
ffffffff812ca3bc:	41 5f                	pop    %r15
ffffffff812ca3be:	5d                   	pop    %rbp
ffffffff812ca3bf:	c3                   	retq   

ffffffff812ca3c0 <vsnprintf>:
 * string is truncated.
 *
 * If you're not already dealing with a va_list consider using snprintf().
 */
int vsnprintf(char *buf, size_t size, const char *fmt, va_list args)
{
ffffffff812ca3c0:	55                   	push   %rbp
ffffffff812ca3c1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca3c4:	41 57                	push   %r15
ffffffff812ca3c6:	41 56                	push   %r14
ffffffff812ca3c8:	41 55                	push   %r13
ffffffff812ca3ca:	41 54                	push   %r12
ffffffff812ca3cc:	53                   	push   %rbx
ffffffff812ca3cd:	48 83 ec 28          	sub    $0x28,%rsp
	char *str, *end;
	struct printf_spec spec = {0};

	/* Reject out-of-range values early.  Large positive sizes are
	   used for unknown buffer sizes. */
	if (WARN_ON_ONCE(size > INT_MAX))
ffffffff812ca3d1:	48 81 fe ff ff ff 7f 	cmp    $0x7fffffff,%rsi
 * string is truncated.
 *
 * If you're not already dealing with a va_list consider using snprintf().
 */
int vsnprintf(char *buf, size_t size, const char *fmt, va_list args)
{
ffffffff812ca3d8:	48 89 7d b8          	mov    %rdi,-0x48(%rbp)
	unsigned long long num;
	char *str, *end;
	struct printf_spec spec = {0};
ffffffff812ca3dc:	48 c7 45 c8 00 00 00 	movq   $0x0,-0x38(%rbp)
ffffffff812ca3e3:	00 

	/* Reject out-of-range values early.  Large positive sizes are
	   used for unknown buffer sizes. */
	if (WARN_ON_ONCE(size > INT_MAX))
ffffffff812ca3e4:	76 2d                	jbe    ffffffff812ca413 <vsnprintf+0x53>
		return 0;
ffffffff812ca3e6:	45 31 ff             	xor    %r15d,%r15d
	char *str, *end;
	struct printf_spec spec = {0};

	/* Reject out-of-range values early.  Large positive sizes are
	   used for unknown buffer sizes. */
	if (WARN_ON_ONCE(size > INT_MAX))
ffffffff812ca3e9:	80 3d fd ce 78 00 00 	cmpb   $0x0,0x78cefd(%rip)        # ffffffff81a572ed <__warned.46031>
ffffffff812ca3f0:	0f 85 65 03 00 00    	jne    ffffffff812ca75b <vsnprintf+0x39b>
ffffffff812ca3f6:	be 4b 07 00 00       	mov    $0x74b,%esi
ffffffff812ca3fb:	48 c7 c7 6d 6d 7b 81 	mov    $0xffffffff817b6d6d,%rdi
ffffffff812ca402:	e8 ae bf d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
ffffffff812ca407:	c6 05 df ce 78 00 01 	movb   $0x1,0x78cedf(%rip)        # ffffffff81a572ed <__warned.46031>
ffffffff812ca40e:	e9 48 03 00 00       	jmpq   ffffffff812ca75b <vsnprintf+0x39b>

	str = buf;
	end = buf + size;

	/* Make sure end is always >= buf */
	if (end < buf) {
ffffffff812ca413:	4c 8b 65 b8          	mov    -0x48(%rbp),%r12
ffffffff812ca417:	49 89 f5             	mov    %rsi,%r13
ffffffff812ca41a:	48 89 cb             	mov    %rcx,%rbx
ffffffff812ca41d:	49 01 f4             	add    %rsi,%r12
ffffffff812ca420:	73 0b                	jae    ffffffff812ca42d <vsnprintf+0x6d>
		end = ((void *)-1);
		size = end - buf;
ffffffff812ca422:	4c 8b 6d b8          	mov    -0x48(%rbp),%r13
	str = buf;
	end = buf + size;

	/* Make sure end is always >= buf */
	if (end < buf) {
		end = ((void *)-1);
ffffffff812ca426:	49 83 cc ff          	or     $0xffffffffffffffff,%r12
		size = end - buf;
ffffffff812ca42a:	49 f7 d5             	not    %r13
	str = buf;
	end = buf + size;

	/* Make sure end is always >= buf */
	if (end < buf) {
		end = ((void *)-1);
ffffffff812ca42d:	4c 8b 7d b8          	mov    -0x48(%rbp),%r15
		size = end - buf;
	}

	while (*fmt) {
ffffffff812ca431:	80 3a 00             	cmpb   $0x0,(%rdx)
ffffffff812ca434:	0f 84 07 03 00 00    	je     ffffffff812ca741 <vsnprintf+0x381>
		const char *old_fmt = fmt;
		int read = format_decode(fmt, &spec);
ffffffff812ca43a:	48 8d 75 c8          	lea    -0x38(%rbp),%rsi
ffffffff812ca43e:	48 89 d7             	mov    %rdx,%rdi
ffffffff812ca441:	48 89 55 b0          	mov    %rdx,-0x50(%rbp)
ffffffff812ca445:	e8 8d e1 ff ff       	callq  ffffffff812c85d7 <format_decode>

		fmt += read;

		switch (spec.type) {
ffffffff812ca44a:	0f b6 7d c8          	movzbl -0x38(%rbp),%edi

	while (*fmt) {
		const char *old_fmt = fmt;
		int read = format_decode(fmt, &spec);

		fmt += read;
ffffffff812ca44e:	48 8b 55 b0          	mov    -0x50(%rbp),%rdx
ffffffff812ca452:	4c 63 c0             	movslq %eax,%r8
ffffffff812ca455:	4e 8d 34 02          	lea    (%rdx,%r8,1),%r14

		switch (spec.type) {
ffffffff812ca459:	40 80 ff 07          	cmp    $0x7,%dil
ffffffff812ca45d:	0f 87 9e 01 00 00    	ja     ffffffff812ca601 <vsnprintf+0x241>
ffffffff812ca463:	ff 24 fd 88 f3 63 81 	jmpq   *-0x7e9c0c78(,%rdi,8)
		case FORMAT_TYPE_NONE: {
			int copy = read;
			if (str < end) {
ffffffff812ca46a:	4d 39 e7             	cmp    %r12,%r15
ffffffff812ca46d:	73 17                	jae    ffffffff812ca486 <vsnprintf+0xc6>
				if (copy > end - str)
ffffffff812ca46f:	4c 89 e1             	mov    %r12,%rcx
					copy = end - str;
				memcpy(str, old_fmt, copy);
ffffffff812ca472:	4c 89 ff             	mov    %r15,%rdi
ffffffff812ca475:	48 89 d6             	mov    %rdx,%rsi

		switch (spec.type) {
		case FORMAT_TYPE_NONE: {
			int copy = read;
			if (str < end) {
				if (copy > end - str)
ffffffff812ca478:	4c 29 f9             	sub    %r15,%rcx
					copy = end - str;
ffffffff812ca47b:	49 39 c8             	cmp    %rcx,%r8
ffffffff812ca47e:	0f 4f c1             	cmovg  %ecx,%eax
				memcpy(str, old_fmt, copy);
ffffffff812ca481:	48 63 c8             	movslq %eax,%rcx
ffffffff812ca484:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
			}
			str += read;
ffffffff812ca486:	4d 01 c7             	add    %r8,%r15
			break;
ffffffff812ca489:	e9 ab 02 00 00       	jmpq   ffffffff812ca739 <vsnprintf+0x379>
		}

		case FORMAT_TYPE_WIDTH:
			spec.field_width = va_arg(args, int);
ffffffff812ca48e:	8b 13                	mov    (%rbx),%edx
ffffffff812ca490:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca493:	77 0d                	ja     ffffffff812ca4a2 <vsnprintf+0xe2>
ffffffff812ca495:	89 d0                	mov    %edx,%eax
ffffffff812ca497:	83 c2 08             	add    $0x8,%edx
ffffffff812ca49a:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca49e:	89 13                	mov    %edx,(%rbx)
ffffffff812ca4a0:	eb 0c                	jmp    ffffffff812ca4ae <vsnprintf+0xee>
ffffffff812ca4a2:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca4a6:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca4aa:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca4ae:	8b 00                	mov    (%rax),%eax
ffffffff812ca4b0:	66 89 45 cc          	mov    %ax,-0x34(%rbp)
			break;
ffffffff812ca4b4:	e9 80 02 00 00       	jmpq   ffffffff812ca739 <vsnprintf+0x379>

		case FORMAT_TYPE_PRECISION:
			spec.precision = va_arg(args, int);
ffffffff812ca4b9:	8b 13                	mov    (%rbx),%edx
ffffffff812ca4bb:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca4be:	77 0d                	ja     ffffffff812ca4cd <vsnprintf+0x10d>
ffffffff812ca4c0:	89 d0                	mov    %edx,%eax
ffffffff812ca4c2:	83 c2 08             	add    $0x8,%edx
ffffffff812ca4c5:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca4c9:	89 13                	mov    %edx,(%rbx)
ffffffff812ca4cb:	eb 0c                	jmp    ffffffff812ca4d9 <vsnprintf+0x119>
ffffffff812ca4cd:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca4d1:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca4d5:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca4d9:	8b 00                	mov    (%rax),%eax
ffffffff812ca4db:	66 89 45 ce          	mov    %ax,-0x32(%rbp)
			break;
ffffffff812ca4df:	e9 55 02 00 00       	jmpq   ffffffff812ca739 <vsnprintf+0x379>

		case FORMAT_TYPE_CHAR: {
			char c;

			if (!(spec.flags & LEFT)) {
ffffffff812ca4e4:	f6 45 c9 02          	testb  $0x2,-0x37(%rbp)
ffffffff812ca4e8:	75 1c                	jne    ffffffff812ca506 <vsnprintf+0x146>
				while (--spec.field_width > 0) {
ffffffff812ca4ea:	8b 45 cc             	mov    -0x34(%rbp),%eax
ffffffff812ca4ed:	ff c8                	dec    %eax
ffffffff812ca4ef:	66 85 c0             	test   %ax,%ax
ffffffff812ca4f2:	66 89 45 cc          	mov    %ax,-0x34(%rbp)
ffffffff812ca4f6:	7e 0e                	jle    ffffffff812ca506 <vsnprintf+0x146>
					if (str < end)
ffffffff812ca4f8:	4d 39 e7             	cmp    %r12,%r15
ffffffff812ca4fb:	73 04                	jae    ffffffff812ca501 <vsnprintf+0x141>
						*str = ' ';
ffffffff812ca4fd:	41 c6 07 20          	movb   $0x20,(%r15)
					++str;
ffffffff812ca501:	49 ff c7             	inc    %r15
ffffffff812ca504:	eb e4                	jmp    ffffffff812ca4ea <vsnprintf+0x12a>

				}
			}
			c = (unsigned char) va_arg(args, int);
ffffffff812ca506:	8b 13                	mov    (%rbx),%edx
ffffffff812ca508:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca50b:	77 0d                	ja     ffffffff812ca51a <vsnprintf+0x15a>
ffffffff812ca50d:	89 d0                	mov    %edx,%eax
ffffffff812ca50f:	83 c2 08             	add    $0x8,%edx
ffffffff812ca512:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca516:	89 13                	mov    %edx,(%rbx)
ffffffff812ca518:	eb 0c                	jmp    ffffffff812ca526 <vsnprintf+0x166>
ffffffff812ca51a:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca51e:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca522:	48 89 53 08          	mov    %rdx,0x8(%rbx)
			if (str < end)
ffffffff812ca526:	4d 39 e7             	cmp    %r12,%r15
ffffffff812ca529:	73 10                	jae    ffffffff812ca53b <vsnprintf+0x17b>
				*str = c;
ffffffff812ca52b:	8b 00                	mov    (%rax),%eax
ffffffff812ca52d:	41 88 07             	mov    %al,(%r15)
ffffffff812ca530:	eb 09                	jmp    ffffffff812ca53b <vsnprintf+0x17b>
			++str;
			while (--spec.field_width > 0) {
				if (str < end)
ffffffff812ca532:	4d 39 e7             	cmp    %r12,%r15
ffffffff812ca535:	73 04                	jae    ffffffff812ca53b <vsnprintf+0x17b>
					*str = ' ';
ffffffff812ca537:	41 c6 07 20          	movb   $0x20,(%r15)
			}
			c = (unsigned char) va_arg(args, int);
			if (str < end)
				*str = c;
			++str;
			while (--spec.field_width > 0) {
ffffffff812ca53b:	8b 45 cc             	mov    -0x34(%rbp),%eax
				if (str < end)
					*str = ' ';
				++str;
ffffffff812ca53e:	49 ff c7             	inc    %r15
			}
			c = (unsigned char) va_arg(args, int);
			if (str < end)
				*str = c;
			++str;
			while (--spec.field_width > 0) {
ffffffff812ca541:	ff c8                	dec    %eax
ffffffff812ca543:	66 85 c0             	test   %ax,%ax
ffffffff812ca546:	66 89 45 cc          	mov    %ax,-0x34(%rbp)
ffffffff812ca54a:	7f e6                	jg     ffffffff812ca532 <vsnprintf+0x172>
ffffffff812ca54c:	e9 e8 01 00 00       	jmpq   ffffffff812ca739 <vsnprintf+0x379>
			}
			break;
		}

		case FORMAT_TYPE_STR:
			str = string(str, end, va_arg(args, char *), spec);
ffffffff812ca551:	8b 13                	mov    (%rbx),%edx
ffffffff812ca553:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca556:	77 0d                	ja     ffffffff812ca565 <vsnprintf+0x1a5>
ffffffff812ca558:	89 d0                	mov    %edx,%eax
ffffffff812ca55a:	83 c2 08             	add    $0x8,%edx
ffffffff812ca55d:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca561:	89 13                	mov    %edx,(%rbx)
ffffffff812ca563:	eb 0c                	jmp    ffffffff812ca571 <vsnprintf+0x1b1>
ffffffff812ca565:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca569:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca56d:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca571:	66 44 8b 4d ce       	mov    -0x32(%rbp),%r9w
ffffffff812ca576:	44 8b 45 cc          	mov    -0x34(%rbp),%r8d
ffffffff812ca57a:	4c 89 ff             	mov    %r15,%rdi
ffffffff812ca57d:	8a 4d c9             	mov    -0x37(%rbp),%cl
ffffffff812ca580:	48 8b 10             	mov    (%rax),%rdx
ffffffff812ca583:	4c 89 e6             	mov    %r12,%rsi
ffffffff812ca586:	e8 9b e8 ff ff       	callq  ffffffff812c8e26 <string.isra.3>
ffffffff812ca58b:	49 89 c7             	mov    %rax,%r15
			break;
ffffffff812ca58e:	e9 a6 01 00 00       	jmpq   ffffffff812ca739 <vsnprintf+0x379>

		case FORMAT_TYPE_PTR:
			str = pointer(fmt, str, end, va_arg(args, void *),
ffffffff812ca593:	8b 13                	mov    (%rbx),%edx
ffffffff812ca595:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca598:	77 0d                	ja     ffffffff812ca5a7 <vsnprintf+0x1e7>
ffffffff812ca59a:	89 d0                	mov    %edx,%eax
ffffffff812ca59c:	83 c2 08             	add    $0x8,%edx
ffffffff812ca59f:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca5a3:	89 13                	mov    %edx,(%rbx)
ffffffff812ca5a5:	eb 0c                	jmp    ffffffff812ca5b3 <vsnprintf+0x1f3>
ffffffff812ca5a7:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca5ab:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca5af:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca5b3:	66 8b 55 ce          	mov    -0x32(%rbp),%dx
ffffffff812ca5b7:	44 8a 4d ca          	mov    -0x36(%rbp),%r9b
ffffffff812ca5bb:	4c 89 fe             	mov    %r15,%rsi
ffffffff812ca5be:	44 8a 45 c9          	mov    -0x37(%rbp),%r8b
ffffffff812ca5c2:	4c 89 f7             	mov    %r14,%rdi
ffffffff812ca5c5:	52                   	push   %rdx
ffffffff812ca5c6:	8b 55 cc             	mov    -0x34(%rbp),%edx
ffffffff812ca5c9:	52                   	push   %rdx
ffffffff812ca5ca:	48 8b 08             	mov    (%rax),%rcx
ffffffff812ca5cd:	4c 89 e2             	mov    %r12,%rdx
ffffffff812ca5d0:	e8 80 fa ff ff       	callq  ffffffff812ca055 <pointer.isra.21>
				      spec);
			while (isalnum(*fmt))
ffffffff812ca5d5:	5e                   	pop    %rsi
ffffffff812ca5d6:	5f                   	pop    %rdi
		case FORMAT_TYPE_STR:
			str = string(str, end, va_arg(args, char *), spec);
			break;

		case FORMAT_TYPE_PTR:
			str = pointer(fmt, str, end, va_arg(args, void *),
ffffffff812ca5d7:	49 89 c7             	mov    %rax,%r15
				      spec);
			while (isalnum(*fmt))
ffffffff812ca5da:	41 0f b6 06          	movzbl (%r14),%eax
ffffffff812ca5de:	f6 80 60 ef 63 81 07 	testb  $0x7,-0x7e9c10a0(%rax)
ffffffff812ca5e5:	0f 84 4e 01 00 00    	je     ffffffff812ca739 <vsnprintf+0x379>
				fmt++;
ffffffff812ca5eb:	49 ff c6             	inc    %r14
ffffffff812ca5ee:	eb ea                	jmp    ffffffff812ca5da <vsnprintf+0x21a>
				*str = '%';
			++str;
			break;

		case FORMAT_TYPE_INVALID:
			if (str < end)
ffffffff812ca5f0:	4d 39 e7             	cmp    %r12,%r15
ffffffff812ca5f3:	73 04                	jae    ffffffff812ca5f9 <vsnprintf+0x239>
				*str = '%';
ffffffff812ca5f5:	41 c6 07 25          	movb   $0x25,(%r15)
			++str;
ffffffff812ca5f9:	49 ff c7             	inc    %r15
			break;
ffffffff812ca5fc:	e9 38 01 00 00       	jmpq   ffffffff812ca739 <vsnprintf+0x379>

		default:
			switch (spec.type) {
ffffffff812ca601:	83 ef 08             	sub    $0x8,%edi
ffffffff812ca604:	8b 13                	mov    (%rbx),%edx
ffffffff812ca606:	40 80 ff 0a          	cmp    $0xa,%dil
ffffffff812ca60a:	0f 87 e8 00 00 00    	ja     ffffffff812ca6f8 <vsnprintf+0x338>
ffffffff812ca610:	40 0f b6 ff          	movzbl %dil,%edi
ffffffff812ca614:	ff 24 fd c8 f3 63 81 	jmpq   *-0x7e9c0c38(,%rdi,8)
					num = va_arg(args, ssize_t);
				else
					num = va_arg(args, size_t);
				break;
			case FORMAT_TYPE_PTRDIFF:
				num = va_arg(args, ptrdiff_t);
ffffffff812ca61b:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca61e:	77 0d                	ja     ffffffff812ca62d <vsnprintf+0x26d>
ffffffff812ca620:	89 d0                	mov    %edx,%eax
ffffffff812ca622:	83 c2 08             	add    $0x8,%edx
ffffffff812ca625:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca629:	89 13                	mov    %edx,(%rbx)
ffffffff812ca62b:	eb 0c                	jmp    ffffffff812ca639 <vsnprintf+0x279>
ffffffff812ca62d:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca631:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca635:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca639:	48 8b 10             	mov    (%rax),%rdx
				break;
ffffffff812ca63c:	e9 d7 00 00 00       	jmpq   ffffffff812ca718 <vsnprintf+0x358>
			case FORMAT_TYPE_UBYTE:
				num = (unsigned char) va_arg(args, int);
ffffffff812ca641:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca644:	77 0d                	ja     ffffffff812ca653 <vsnprintf+0x293>
ffffffff812ca646:	89 d0                	mov    %edx,%eax
ffffffff812ca648:	83 c2 08             	add    $0x8,%edx
ffffffff812ca64b:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca64f:	89 13                	mov    %edx,(%rbx)
ffffffff812ca651:	eb 0c                	jmp    ffffffff812ca65f <vsnprintf+0x29f>
ffffffff812ca653:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca657:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca65b:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca65f:	0f b6 10             	movzbl (%rax),%edx
				break;
ffffffff812ca662:	e9 b1 00 00 00       	jmpq   ffffffff812ca718 <vsnprintf+0x358>
			case FORMAT_TYPE_BYTE:
				num = (signed char) va_arg(args, int);
ffffffff812ca667:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca66a:	77 0d                	ja     ffffffff812ca679 <vsnprintf+0x2b9>
ffffffff812ca66c:	89 d0                	mov    %edx,%eax
ffffffff812ca66e:	83 c2 08             	add    $0x8,%edx
ffffffff812ca671:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca675:	89 13                	mov    %edx,(%rbx)
ffffffff812ca677:	eb 0c                	jmp    ffffffff812ca685 <vsnprintf+0x2c5>
ffffffff812ca679:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca67d:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca681:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca685:	48 0f be 10          	movsbq (%rax),%rdx
				break;
ffffffff812ca689:	e9 8a 00 00 00       	jmpq   ffffffff812ca718 <vsnprintf+0x358>
			case FORMAT_TYPE_USHORT:
				num = (unsigned short) va_arg(args, int);
ffffffff812ca68e:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca691:	77 0d                	ja     ffffffff812ca6a0 <vsnprintf+0x2e0>
ffffffff812ca693:	89 d0                	mov    %edx,%eax
ffffffff812ca695:	83 c2 08             	add    $0x8,%edx
ffffffff812ca698:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca69c:	89 13                	mov    %edx,(%rbx)
ffffffff812ca69e:	eb 0c                	jmp    ffffffff812ca6ac <vsnprintf+0x2ec>
ffffffff812ca6a0:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca6a4:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca6a8:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca6ac:	0f b7 10             	movzwl (%rax),%edx
				break;
ffffffff812ca6af:	eb 67                	jmp    ffffffff812ca718 <vsnprintf+0x358>
			case FORMAT_TYPE_SHORT:
				num = (short) va_arg(args, int);
ffffffff812ca6b1:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca6b4:	77 0d                	ja     ffffffff812ca6c3 <vsnprintf+0x303>
ffffffff812ca6b6:	89 d0                	mov    %edx,%eax
ffffffff812ca6b8:	83 c2 08             	add    $0x8,%edx
ffffffff812ca6bb:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca6bf:	89 13                	mov    %edx,(%rbx)
ffffffff812ca6c1:	eb 0c                	jmp    ffffffff812ca6cf <vsnprintf+0x30f>
ffffffff812ca6c3:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca6c7:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca6cb:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca6cf:	48 0f bf 10          	movswq (%rax),%rdx
				break;
ffffffff812ca6d3:	eb 43                	jmp    ffffffff812ca718 <vsnprintf+0x358>
			case FORMAT_TYPE_INT:
				num = (int) va_arg(args, int);
ffffffff812ca6d5:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca6d8:	77 0d                	ja     ffffffff812ca6e7 <vsnprintf+0x327>
ffffffff812ca6da:	89 d0                	mov    %edx,%eax
ffffffff812ca6dc:	83 c2 08             	add    $0x8,%edx
ffffffff812ca6df:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca6e3:	89 13                	mov    %edx,(%rbx)
ffffffff812ca6e5:	eb 0c                	jmp    ffffffff812ca6f3 <vsnprintf+0x333>
ffffffff812ca6e7:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca6eb:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca6ef:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca6f3:	48 63 10             	movslq (%rax),%rdx
				break;
ffffffff812ca6f6:	eb 20                	jmp    ffffffff812ca718 <vsnprintf+0x358>
			default:
				num = va_arg(args, unsigned int);
ffffffff812ca6f8:	83 fa 2f             	cmp    $0x2f,%edx
ffffffff812ca6fb:	77 0d                	ja     ffffffff812ca70a <vsnprintf+0x34a>
ffffffff812ca6fd:	89 d0                	mov    %edx,%eax
ffffffff812ca6ff:	83 c2 08             	add    $0x8,%edx
ffffffff812ca702:	48 03 43 10          	add    0x10(%rbx),%rax
ffffffff812ca706:	89 13                	mov    %edx,(%rbx)
ffffffff812ca708:	eb 0c                	jmp    ffffffff812ca716 <vsnprintf+0x356>
ffffffff812ca70a:	48 8b 43 08          	mov    0x8(%rbx),%rax
ffffffff812ca70e:	48 8d 50 08          	lea    0x8(%rax),%rdx
ffffffff812ca712:	48 89 53 08          	mov    %rdx,0x8(%rbx)
ffffffff812ca716:	8b 10                	mov    (%rax),%edx
			}

			str = number(str, end, num, spec);
ffffffff812ca718:	50                   	push   %rax
ffffffff812ca719:	66 8b 45 ce          	mov    -0x32(%rbp),%ax
ffffffff812ca71d:	4c 89 ff             	mov    %r15,%rdi
ffffffff812ca720:	8a 4d c9             	mov    -0x37(%rbp),%cl
ffffffff812ca723:	44 8b 4d cc          	mov    -0x34(%rbp),%r9d
ffffffff812ca727:	4c 89 e6             	mov    %r12,%rsi
ffffffff812ca72a:	44 8a 45 ca          	mov    -0x36(%rbp),%r8b
ffffffff812ca72e:	50                   	push   %rax
ffffffff812ca72f:	e8 1b eb ff ff       	callq  ffffffff812c924f <number.isra.13>
ffffffff812ca734:	5a                   	pop    %rdx
ffffffff812ca735:	59                   	pop    %rcx
ffffffff812ca736:	49 89 c7             	mov    %rax,%r15
	str = buf;
	end = buf + size;

	/* Make sure end is always >= buf */
	if (end < buf) {
		end = ((void *)-1);
ffffffff812ca739:	4c 89 f2             	mov    %r14,%rdx
ffffffff812ca73c:	e9 f0 fc ff ff       	jmpq   ffffffff812ca431 <vsnprintf+0x71>

			str = number(str, end, num, spec);
		}
	}

	if (size > 0) {
ffffffff812ca741:	4d 85 ed             	test   %r13,%r13
ffffffff812ca744:	74 11                	je     ffffffff812ca757 <vsnprintf+0x397>
		if (str < end)
ffffffff812ca746:	4d 39 e7             	cmp    %r12,%r15
ffffffff812ca749:	73 06                	jae    ffffffff812ca751 <vsnprintf+0x391>
			*str = '\0';
ffffffff812ca74b:	41 c6 07 00          	movb   $0x0,(%r15)
ffffffff812ca74f:	eb 06                	jmp    ffffffff812ca757 <vsnprintf+0x397>
		else
			end[-1] = '\0';
ffffffff812ca751:	41 c6 44 24 ff 00    	movb   $0x0,-0x1(%r12)
	}

	/* the trailing null byte doesn't count towards the total */
	return str-buf;
ffffffff812ca757:	44 2b 7d b8          	sub    -0x48(%rbp),%r15d

}
ffffffff812ca75b:	48 8d 65 d8          	lea    -0x28(%rbp),%rsp
ffffffff812ca75f:	44 89 f8             	mov    %r15d,%eax
ffffffff812ca762:	5b                   	pop    %rbx
ffffffff812ca763:	41 5c                	pop    %r12
ffffffff812ca765:	41 5d                	pop    %r13
ffffffff812ca767:	41 5e                	pop    %r14
ffffffff812ca769:	41 5f                	pop    %r15
ffffffff812ca76b:	5d                   	pop    %rbp
ffffffff812ca76c:	c3                   	retq   

ffffffff812ca76d <vscnprintf>:
 * If you're not already dealing with a va_list consider using scnprintf().
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int vscnprintf(char *buf, size_t size, const char *fmt, va_list args)
{
ffffffff812ca76d:	55                   	push   %rbp
ffffffff812ca76e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca771:	53                   	push   %rbx
ffffffff812ca772:	41 50                	push   %r8
ffffffff812ca774:	48 89 f3             	mov    %rsi,%rbx
	int i;

	i = vsnprintf(buf, size, fmt, args);
ffffffff812ca777:	e8 44 fc ff ff       	callq  ffffffff812ca3c0 <vsnprintf>

	if (likely(i < size))
ffffffff812ca77c:	48 63 d0             	movslq %eax,%rdx
ffffffff812ca77f:	48 39 d3             	cmp    %rdx,%rbx
ffffffff812ca782:	77 0b                	ja     ffffffff812ca78f <vscnprintf+0x22>
		return i;
	if (size != 0)
		return size - 1;
ffffffff812ca784:	8d 53 ff             	lea    -0x1(%rbx),%edx
	return 0;
ffffffff812ca787:	31 c0                	xor    %eax,%eax
	i = vsnprintf(buf, size, fmt, args);

	if (likely(i < size))
		return i;
	if (size != 0)
		return size - 1;
ffffffff812ca789:	48 85 db             	test   %rbx,%rbx
ffffffff812ca78c:	0f 45 c2             	cmovne %edx,%eax
	return 0;
}
ffffffff812ca78f:	5a                   	pop    %rdx
ffffffff812ca790:	5b                   	pop    %rbx
ffffffff812ca791:	5d                   	pop    %rbp
ffffffff812ca792:	c3                   	retq   

ffffffff812ca793 <scnprintf>:
 * The return value is the number of characters written into @buf not including
 * the trailing '\0'. If @size is == 0 the function returns 0.
 */

int scnprintf(char *buf, size_t size, const char *fmt, ...)
{
ffffffff812ca793:	55                   	push   %rbp
ffffffff812ca794:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca797:	48 83 ec 50          	sub    $0x50,%rsp
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812ca79b:	48 8d 45 10          	lea    0x10(%rbp),%rax
 * The return value is the number of characters written into @buf not including
 * the trailing '\0'. If @size is == 0 the function returns 0.
 */

int scnprintf(char *buf, size_t size, const char *fmt, ...)
{
ffffffff812ca79f:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
	i = vscnprintf(buf, size, fmt, args);
ffffffff812ca7a3:	48 8d 4d b8          	lea    -0x48(%rbp),%rcx
 * The return value is the number of characters written into @buf not including
 * the trailing '\0'. If @size is == 0 the function returns 0.
 */

int scnprintf(char *buf, size_t size, const char *fmt, ...)
{
ffffffff812ca7a7:	4c 89 45 f0          	mov    %r8,-0x10(%rbp)
ffffffff812ca7ab:	4c 89 4d f8          	mov    %r9,-0x8(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812ca7af:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812ca7b3:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
ffffffff812ca7b7:	c7 45 b8 18 00 00 00 	movl   $0x18,-0x48(%rbp)
ffffffff812ca7be:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	i = vscnprintf(buf, size, fmt, args);
ffffffff812ca7c2:	e8 a6 ff ff ff       	callq  ffffffff812ca76d <vscnprintf>
	va_end(args);

	return i;
}
ffffffff812ca7c7:	c9                   	leaveq 
ffffffff812ca7c8:	c3                   	retq   

ffffffff812ca7c9 <vsprintf>:
 * If you're not already dealing with a va_list consider using sprintf().
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int vsprintf(char *buf, const char *fmt, va_list args)
{
ffffffff812ca7c9:	55                   	push   %rbp
	return vsnprintf(buf, INT_MAX, fmt, args);
ffffffff812ca7ca:	48 89 d1             	mov    %rdx,%rcx
ffffffff812ca7cd:	48 89 f2             	mov    %rsi,%rdx
ffffffff812ca7d0:	be ff ff ff 7f       	mov    $0x7fffffff,%esi
 * If you're not already dealing with a va_list consider using sprintf().
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int vsprintf(char *buf, const char *fmt, va_list args)
{
ffffffff812ca7d5:	48 89 e5             	mov    %rsp,%rbp
	return vsnprintf(buf, INT_MAX, fmt, args);
ffffffff812ca7d8:	e8 e3 fb ff ff       	callq  ffffffff812ca3c0 <vsnprintf>
}
ffffffff812ca7dd:	5d                   	pop    %rbp
ffffffff812ca7de:	c3                   	retq   

ffffffff812ca7df <snprintf>:
 * @size, the resulting string is truncated.
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int snprintf(char *buf, size_t size, const char *fmt, ...)
{
ffffffff812ca7df:	55                   	push   %rbp
ffffffff812ca7e0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca7e3:	48 83 ec 50          	sub    $0x50,%rsp
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812ca7e7:	48 8d 45 10          	lea    0x10(%rbp),%rax
 * @size, the resulting string is truncated.
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int snprintf(char *buf, size_t size, const char *fmt, ...)
{
ffffffff812ca7eb:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
	i = vsnprintf(buf, size, fmt, args);
ffffffff812ca7ef:	48 8d 4d b8          	lea    -0x48(%rbp),%rcx
 * @size, the resulting string is truncated.
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int snprintf(char *buf, size_t size, const char *fmt, ...)
{
ffffffff812ca7f3:	4c 89 45 f0          	mov    %r8,-0x10(%rbp)
ffffffff812ca7f7:	4c 89 4d f8          	mov    %r9,-0x8(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812ca7fb:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812ca7ff:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
ffffffff812ca803:	c7 45 b8 18 00 00 00 	movl   $0x18,-0x48(%rbp)
ffffffff812ca80a:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	i = vsnprintf(buf, size, fmt, args);
ffffffff812ca80e:	e8 ad fb ff ff       	callq  ffffffff812ca3c0 <vsnprintf>
	va_end(args);

	return i;
}
ffffffff812ca813:	c9                   	leaveq 
ffffffff812ca814:	c3                   	retq   

ffffffff812ca815 <sprintf>:
 * buffer overflows.
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int sprintf(char *buf, const char *fmt, ...)
{
ffffffff812ca815:	55                   	push   %rbp
ffffffff812ca816:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca819:	48 83 ec 50          	sub    $0x50,%rsp
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812ca81d:	48 8d 45 10          	lea    0x10(%rbp),%rax
 * buffer overflows.
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int sprintf(char *buf, const char *fmt, ...)
{
ffffffff812ca821:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
	i = vsnprintf(buf, INT_MAX, fmt, args);
ffffffff812ca825:	48 8d 4d b8          	lea    -0x48(%rbp),%rcx
 * buffer overflows.
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int sprintf(char *buf, const char *fmt, ...)
{
ffffffff812ca829:	48 89 55 e0          	mov    %rdx,-0x20(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
	i = vsnprintf(buf, INT_MAX, fmt, args);
ffffffff812ca82d:	48 89 f2             	mov    %rsi,%rdx
ffffffff812ca830:	be ff ff ff 7f       	mov    $0x7fffffff,%esi
int sprintf(char *buf, const char *fmt, ...)
{
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812ca835:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812ca839:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
 * buffer overflows.
 *
 * See the vsnprintf() documentation for format string extensions over C99.
 */
int sprintf(char *buf, const char *fmt, ...)
{
ffffffff812ca83d:	4c 89 45 f0          	mov    %r8,-0x10(%rbp)
ffffffff812ca841:	4c 89 4d f8          	mov    %r9,-0x8(%rbp)
	va_list args;
	int i;

	va_start(args, fmt);
ffffffff812ca845:	c7 45 b8 10 00 00 00 	movl   $0x10,-0x48(%rbp)
ffffffff812ca84c:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	i = vsnprintf(buf, INT_MAX, fmt, args);
ffffffff812ca850:	e8 6b fb ff ff       	callq  ffffffff812ca3c0 <vsnprintf>
	va_end(args);

	return i;
}
ffffffff812ca855:	c9                   	leaveq 
ffffffff812ca856:	c3                   	retq   

ffffffff812ca857 <num_to_str>:
 * Returns the length of string.  On buffer overflow, returns 0.
 *
 * If speed is not important, use snprintf(). It's easy to read the code.
 */
int num_to_str(char *buf, int size, unsigned long long num)
{
ffffffff812ca857:	55                   	push   %rbp
ffffffff812ca858:	41 89 f3             	mov    %esi,%r11d
ffffffff812ca85b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca85e:	41 54                	push   %r12
ffffffff812ca860:	53                   	push   %rbx
ffffffff812ca861:	49 89 fc             	mov    %rdi,%r12
ffffffff812ca864:	48 83 ec 20          	sub    $0x20,%rsp
	/* put_dec requires 2-byte alignment of the buffer. */
	char tmp[sizeof(num) * 3] __aligned(2);
	int idx, len;

	/* put_dec() may work incorrectly for num = 0 (generate "", not "0") */
	if (num <= 9) {
ffffffff812ca868:	48 83 fa 09          	cmp    $0x9,%rdx
ffffffff812ca86c:	77 0d                	ja     ffffffff812ca87b <num_to_str+0x24>
		tmp[0] = '0' + num;
ffffffff812ca86e:	83 c2 30             	add    $0x30,%edx
ffffffff812ca871:	88 55 d8             	mov    %dl,-0x28(%rbp)
		len = 1;
ffffffff812ca874:	ba 01 00 00 00       	mov    $0x1,%edx
ffffffff812ca879:	eb 13                	jmp    ffffffff812ca88e <num_to_str+0x37>
	} else {
		len = put_dec(tmp, num) - tmp;
ffffffff812ca87b:	48 8d 5d d8          	lea    -0x28(%rbp),%rbx
ffffffff812ca87f:	48 89 d6             	mov    %rdx,%rsi
ffffffff812ca882:	48 89 df             	mov    %rbx,%rdi
ffffffff812ca885:	e8 d4 db ff ff       	callq  ffffffff812c845e <put_dec>
ffffffff812ca88a:	89 c2                	mov    %eax,%edx
ffffffff812ca88c:	29 da                	sub    %ebx,%edx
	}

	if (len > size)
		return 0;
ffffffff812ca88e:	31 c0                	xor    %eax,%eax
		len = 1;
	} else {
		len = put_dec(tmp, num) - tmp;
	}

	if (len > size)
ffffffff812ca890:	44 39 da             	cmp    %r11d,%edx
ffffffff812ca893:	7f 1e                	jg     ffffffff812ca8b3 <num_to_str+0x5c>
ffffffff812ca895:	8d 4a ff             	lea    -0x1(%rdx),%ecx
ffffffff812ca898:	31 c0                	xor    %eax,%eax
		return 0;
	for (idx = 0; idx < len; ++idx)
ffffffff812ca89a:	39 c2                	cmp    %eax,%edx
ffffffff812ca89c:	7e 13                	jle    ffffffff812ca8b1 <num_to_str+0x5a>
		buf[idx] = tmp[len - idx - 1];
ffffffff812ca89e:	48 63 f1             	movslq %ecx,%rsi
ffffffff812ca8a1:	ff c9                	dec    %ecx
ffffffff812ca8a3:	40 8a 74 35 d8       	mov    -0x28(%rbp,%rsi,1),%sil
ffffffff812ca8a8:	41 88 34 04          	mov    %sil,(%r12,%rax,1)
ffffffff812ca8ac:	48 ff c0             	inc    %rax
ffffffff812ca8af:	eb e9                	jmp    ffffffff812ca89a <num_to_str+0x43>
ffffffff812ca8b1:	89 d0                	mov    %edx,%eax
	return len;
}
ffffffff812ca8b3:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812ca8b7:	5b                   	pop    %rbx
ffffffff812ca8b8:	41 5c                	pop    %r12
ffffffff812ca8ba:	5d                   	pop    %rbp
ffffffff812ca8bb:	c3                   	retq   
ffffffff812ca8bc:	90                   	nop
ffffffff812ca8bd:	90                   	nop
ffffffff812ca8be:	90                   	nop
ffffffff812ca8bf:	90                   	nop

ffffffff812ca8c0 <clear_page>:
 * %rdi	- page
 */
ENTRY(clear_page)
	CFI_STARTPROC

	ALTERNATIVE_2 "jmp clear_page_orig", "", X86_FEATURE_REP_GOOD, \
ffffffff812ca8c0:	e9 0b 00 00 00       	jmpq   ffffffff812ca8d0 <clear_page_orig>
		      "jmp clear_page_c_e", X86_FEATURE_ERMS

	movl $4096/8,%ecx
ffffffff812ca8c5:	b9 00 02 00 00       	mov    $0x200,%ecx
	xorl %eax,%eax
ffffffff812ca8ca:	31 c0                	xor    %eax,%eax
	rep stosq
ffffffff812ca8cc:	f3 48 ab             	rep stos %rax,%es:(%rdi)
	ret
ffffffff812ca8cf:	c3                   	retq   

ffffffff812ca8d0 <clear_page_orig>:
ENDPROC(clear_page)

ENTRY(clear_page_orig)
	CFI_STARTPROC

	xorl   %eax,%eax
ffffffff812ca8d0:	31 c0                	xor    %eax,%eax
	movl   $4096/64,%ecx
ffffffff812ca8d2:	b9 40 00 00 00       	mov    $0x40,%ecx
ffffffff812ca8d7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
ffffffff812ca8de:	00 00 
	.p2align 4
.Lloop:
	decl	%ecx
ffffffff812ca8e0:	ff c9                	dec    %ecx
#define PUT(x) movq %rax,x*8(%rdi)
	movq %rax,(%rdi)
ffffffff812ca8e2:	48 89 07             	mov    %rax,(%rdi)
	PUT(1)
ffffffff812ca8e5:	48 89 47 08          	mov    %rax,0x8(%rdi)
	PUT(2)
ffffffff812ca8e9:	48 89 47 10          	mov    %rax,0x10(%rdi)
	PUT(3)
ffffffff812ca8ed:	48 89 47 18          	mov    %rax,0x18(%rdi)
	PUT(4)
ffffffff812ca8f1:	48 89 47 20          	mov    %rax,0x20(%rdi)
	PUT(5)
ffffffff812ca8f5:	48 89 47 28          	mov    %rax,0x28(%rdi)
	PUT(6)
ffffffff812ca8f9:	48 89 47 30          	mov    %rax,0x30(%rdi)
	PUT(7)
ffffffff812ca8fd:	48 89 47 38          	mov    %rax,0x38(%rdi)
	leaq	64(%rdi),%rdi
ffffffff812ca901:	48 8d 7f 40          	lea    0x40(%rdi),%rdi
	jnz	.Lloop
ffffffff812ca905:	75 d9                	jne    ffffffff812ca8e0 <clear_page_orig+0x10>
	nop
ffffffff812ca907:	90                   	nop
	ret
ffffffff812ca908:	c3                   	retq   
ffffffff812ca909:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

ffffffff812ca910 <clear_page_c_e>:
	CFI_ENDPROC
ENDPROC(clear_page_orig)

ENTRY(clear_page_c_e)
	CFI_STARTPROC
	movl $4096,%ecx
ffffffff812ca910:	b9 00 10 00 00       	mov    $0x1000,%ecx
	xorl %eax,%eax
ffffffff812ca915:	31 c0                	xor    %eax,%eax
	rep stosb
ffffffff812ca917:	f3 aa                	rep stos %al,%es:(%rdi)
	ret
ffffffff812ca919:	c3                   	retq   

ffffffff812ca91a <cmdline_find_option_bool>:
		st_wordcmp,	/* Comparing this word */
		st_wordskip,	/* Miscompare, skip */
	} state = st_wordstart;

	if (!cmdline)
		return -1;      /* No command line */
ffffffff812ca91a:	83 c8 ff             	or     $0xffffffff,%eax
		st_wordstart = 0,	/* Start of word/after whitespace */
		st_wordcmp,	/* Comparing this word */
		st_wordskip,	/* Miscompare, skip */
	} state = st_wordstart;

	if (!cmdline)
ffffffff812ca91d:	48 85 ff             	test   %rdi,%rdi
ffffffff812ca920:	0f 84 ab 00 00 00    	je     ffffffff812ca9d1 <cmdline_find_option_bool+0xb7>
		return -1;      /* No command line */

	len = min_t(int, strlen(cmdline), COMMAND_LINE_SIZE);
ffffffff812ca926:	31 c0                	xor    %eax,%eax
ffffffff812ca928:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812ca92c:	48 89 fa             	mov    %rdi,%rdx
ffffffff812ca92f:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812ca931:	b8 00 08 00 00       	mov    $0x800,%eax
ffffffff812ca936:	48 f7 d1             	not    %rcx
ffffffff812ca939:	48 ff c9             	dec    %rcx
ffffffff812ca93c:	81 f9 00 08 00 00    	cmp    $0x800,%ecx
ffffffff812ca942:	0f 4f c8             	cmovg  %eax,%ecx
	if (!len)
		return 0;
ffffffff812ca945:	31 c0                	xor    %eax,%eax

	if (!cmdline)
		return -1;      /* No command line */

	len = min_t(int, strlen(cmdline), COMMAND_LINE_SIZE);
	if (!len)
ffffffff812ca947:	85 c9                	test   %ecx,%ecx
ffffffff812ca949:	0f 84 82 00 00 00    	je     ffffffff812ca9d1 <cmdline_find_option_bool+0xb7>
 *
 * Returns the position of that @option (starts counting with 1)
 * or 0 on not found.
 */
int cmdline_find_option_bool(const char *cmdline, const char *option)
{
ffffffff812ca94f:	55                   	push   %rbp
ffffffff812ca950:	48 89 d7             	mov    %rdx,%rdi

	if (!cmdline)
		return -1;      /* No command line */

	len = min_t(int, strlen(cmdline), COMMAND_LINE_SIZE);
	if (!len)
ffffffff812ca953:	45 31 d2             	xor    %r10d,%r10d
ffffffff812ca956:	31 d2                	xor    %edx,%edx
ffffffff812ca958:	45 31 c9             	xor    %r9d,%r9d
 *
 * Returns the position of that @option (starts counting with 1)
 * or 0 on not found.
 */
int cmdline_find_option_bool(const char *cmdline, const char *option)
{
ffffffff812ca95b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ca95e:	53                   	push   %rbx

		case st_wordskip:
			if (!c)
				return 0;
			else if (myisspace(c))
				state = st_wordstart;
ffffffff812ca95f:	31 db                	xor    %ebx,%ebx
	if (!len)
		return 0;

	while (len--) {
		c = *(char *)cmdline++;
		pos++;
ffffffff812ca961:	41 ff c1             	inc    %r9d

		switch (state) {
ffffffff812ca964:	83 fa 01             	cmp    $0x1,%edx
	len = min_t(int, strlen(cmdline), COMMAND_LINE_SIZE);
	if (!len)
		return 0;

	while (len--) {
		c = *(char *)cmdline++;
ffffffff812ca967:	44 8a 07             	mov    (%rdi),%r8b
		pos++;

		switch (state) {
ffffffff812ca96a:	74 16                	je     ffffffff812ca982 <cmdline_find_option_bool+0x68>
ffffffff812ca96c:	83 fa 02             	cmp    $0x2,%edx
ffffffff812ca96f:	74 44                	je     ffffffff812ca9b5 <cmdline_find_option_bool+0x9b>
		case st_wordstart:
			if (!c)
ffffffff812ca971:	45 84 c0             	test   %r8b,%r8b
ffffffff812ca974:	74 57                	je     ffffffff812ca9cd <cmdline_find_option_bool+0xb3>
				return 0;
			else if (myisspace(c))
ffffffff812ca976:	41 80 f8 20          	cmp    $0x20,%r8b
ffffffff812ca97a:	76 47                	jbe    ffffffff812ca9c3 <cmdline_find_option_bool+0xa9>
				break;

			state = st_wordcmp;
			opptr = option;
ffffffff812ca97c:	49 89 f2             	mov    %rsi,%r10
			wstart = pos;
ffffffff812ca97f:	44 89 c8             	mov    %r9d,%eax
			/* fall through */

		case st_wordcmp:
			if (!*opptr)
ffffffff812ca982:	45 8a 1a             	mov    (%r10),%r11b
ffffffff812ca985:	45 84 db             	test   %r11b,%r11b
ffffffff812ca988:	75 0d                	jne    ffffffff812ca997 <cmdline_find_option_bool+0x7d>
				if (!c || myisspace(c))
ffffffff812ca98a:	41 80 f8 20          	cmp    $0x20,%r8b
ffffffff812ca98e:	76 3f                	jbe    ffffffff812ca9cf <cmdline_find_option_bool+0xb5>
					return wstart;
				else
					state = st_wordskip;
ffffffff812ca990:	ba 02 00 00 00       	mov    $0x2,%edx
ffffffff812ca995:	eb 2e                	jmp    ffffffff812ca9c5 <cmdline_find_option_bool+0xab>
			else if (!c)
ffffffff812ca997:	45 84 c0             	test   %r8b,%r8b
ffffffff812ca99a:	74 31                	je     ffffffff812ca9cd <cmdline_find_option_bool+0xb3>
				return 0;
			else if (c != *opptr++)
ffffffff812ca99c:	49 ff c2             	inc    %r10
ffffffff812ca99f:	45 38 d8             	cmp    %r11b,%r8b
				state = st_wordskip;
ffffffff812ca9a2:	ba 02 00 00 00       	mov    $0x2,%edx
					return wstart;
				else
					state = st_wordskip;
			else if (!c)
				return 0;
			else if (c != *opptr++)
ffffffff812ca9a7:	75 1c                	jne    ffffffff812ca9c5 <cmdline_find_option_bool+0xab>
				state = st_wordskip;
			else if (!len)		/* last word and is matching */
ffffffff812ca9a9:	44 39 c9             	cmp    %r9d,%ecx
ffffffff812ca9ac:	74 21                	je     ffffffff812ca9cf <cmdline_find_option_bool+0xb5>
ffffffff812ca9ae:	ba 01 00 00 00       	mov    $0x1,%edx
ffffffff812ca9b3:	eb 10                	jmp    ffffffff812ca9c5 <cmdline_find_option_bool+0xab>
				return wstart;
			break;

		case st_wordskip:
			if (!c)
ffffffff812ca9b5:	45 84 c0             	test   %r8b,%r8b
ffffffff812ca9b8:	74 13                	je     ffffffff812ca9cd <cmdline_find_option_bool+0xb3>
				return 0;
			else if (myisspace(c))
				state = st_wordstart;
ffffffff812ca9ba:	41 80 f8 20          	cmp    $0x20,%r8b
ffffffff812ca9be:	0f 46 d3             	cmovbe %ebx,%edx
ffffffff812ca9c1:	eb 02                	jmp    ffffffff812ca9c5 <cmdline_find_option_bool+0xab>
ffffffff812ca9c3:	31 d2                	xor    %edx,%edx
ffffffff812ca9c5:	48 ff c7             	inc    %rdi

	len = min_t(int, strlen(cmdline), COMMAND_LINE_SIZE);
	if (!len)
		return 0;

	while (len--) {
ffffffff812ca9c8:	44 39 c9             	cmp    %r9d,%ecx
ffffffff812ca9cb:	75 94                	jne    ffffffff812ca961 <cmdline_find_option_bool+0x47>
	if (!cmdline)
		return -1;      /* No command line */

	len = min_t(int, strlen(cmdline), COMMAND_LINE_SIZE);
	if (!len)
		return 0;
ffffffff812ca9cd:	31 c0                	xor    %eax,%eax
			break;
		}
	}

	return 0;	/* Buffer overrun */
}
ffffffff812ca9cf:	5b                   	pop    %rbx
ffffffff812ca9d0:	5d                   	pop    %rbp
ffffffff812ca9d1:	c3                   	retq   
ffffffff812ca9d2:	90                   	nop
ffffffff812ca9d3:	90                   	nop
ffffffff812ca9d4:	90                   	nop
ffffffff812ca9d5:	90                   	nop
ffffffff812ca9d6:	90                   	nop
ffffffff812ca9d7:	90                   	nop
ffffffff812ca9d8:	90                   	nop
ffffffff812ca9d9:	90                   	nop
ffffffff812ca9da:	90                   	nop
ffffffff812ca9db:	90                   	nop
ffffffff812ca9dc:	90                   	nop
ffffffff812ca9dd:	90                   	nop
ffffffff812ca9de:	90                   	nop
ffffffff812ca9df:	90                   	nop

ffffffff812ca9e0 <copy_page>:
 * prefetch distance based on SMP/UP.
 */
	ALIGN
ENTRY(copy_page)
	CFI_STARTPROC
	ALTERNATIVE "jmp copy_page_regs", "", X86_FEATURE_REP_GOOD
ffffffff812ca9e0:	e9 0b 00 00 00       	jmpq   ffffffff812ca9f0 <copy_page_regs>
	movl	$4096/8, %ecx
ffffffff812ca9e5:	b9 00 02 00 00       	mov    $0x200,%ecx
	rep	movsq
ffffffff812ca9ea:	f3 48 a5             	rep movsq %ds:(%rsi),%es:(%rdi)
	ret
ffffffff812ca9ed:	c3                   	retq   
ffffffff812ca9ee:	66 90                	xchg   %ax,%ax

ffffffff812ca9f0 <copy_page_regs>:
	CFI_ENDPROC
ENDPROC(copy_page)

ENTRY(copy_page_regs)
	CFI_STARTPROC
	subq	$2*8,	%rsp
ffffffff812ca9f0:	48 83 ec 10          	sub    $0x10,%rsp
	CFI_ADJUST_CFA_OFFSET 2*8
	movq	%rbx,	(%rsp)
ffffffff812ca9f4:	48 89 1c 24          	mov    %rbx,(%rsp)
	CFI_REL_OFFSET rbx, 0
	movq	%r12,	1*8(%rsp)
ffffffff812ca9f8:	4c 89 64 24 08       	mov    %r12,0x8(%rsp)
	CFI_REL_OFFSET r12, 1*8

	movl	$(4096/64)-5,	%ecx
ffffffff812ca9fd:	b9 3b 00 00 00       	mov    $0x3b,%ecx
ffffffff812caa02:	0f 1f 40 00          	nopl   0x0(%rax)
ffffffff812caa06:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812caa0d:	00 00 00 
	.p2align 4
.Loop64:
	dec	%rcx
ffffffff812caa10:	48 ff c9             	dec    %rcx
	movq	0x8*0(%rsi), %rax
ffffffff812caa13:	48 8b 06             	mov    (%rsi),%rax
	movq	0x8*1(%rsi), %rbx
ffffffff812caa16:	48 8b 5e 08          	mov    0x8(%rsi),%rbx
	movq	0x8*2(%rsi), %rdx
ffffffff812caa1a:	48 8b 56 10          	mov    0x10(%rsi),%rdx
	movq	0x8*3(%rsi), %r8
ffffffff812caa1e:	4c 8b 46 18          	mov    0x18(%rsi),%r8
	movq	0x8*4(%rsi), %r9
ffffffff812caa22:	4c 8b 4e 20          	mov    0x20(%rsi),%r9
	movq	0x8*5(%rsi), %r10
ffffffff812caa26:	4c 8b 56 28          	mov    0x28(%rsi),%r10
	movq	0x8*6(%rsi), %r11
ffffffff812caa2a:	4c 8b 5e 30          	mov    0x30(%rsi),%r11
	movq	0x8*7(%rsi), %r12
ffffffff812caa2e:	4c 8b 66 38          	mov    0x38(%rsi),%r12

	prefetcht0 5*64(%rsi)
ffffffff812caa32:	0f 18 8e 40 01 00 00 	prefetcht0 0x140(%rsi)

	movq	%rax, 0x8*0(%rdi)
ffffffff812caa39:	48 89 07             	mov    %rax,(%rdi)
	movq	%rbx, 0x8*1(%rdi)
ffffffff812caa3c:	48 89 5f 08          	mov    %rbx,0x8(%rdi)
	movq	%rdx, 0x8*2(%rdi)
ffffffff812caa40:	48 89 57 10          	mov    %rdx,0x10(%rdi)
	movq	%r8,  0x8*3(%rdi)
ffffffff812caa44:	4c 89 47 18          	mov    %r8,0x18(%rdi)
	movq	%r9,  0x8*4(%rdi)
ffffffff812caa48:	4c 89 4f 20          	mov    %r9,0x20(%rdi)
	movq	%r10, 0x8*5(%rdi)
ffffffff812caa4c:	4c 89 57 28          	mov    %r10,0x28(%rdi)
	movq	%r11, 0x8*6(%rdi)
ffffffff812caa50:	4c 89 5f 30          	mov    %r11,0x30(%rdi)
	movq	%r12, 0x8*7(%rdi)
ffffffff812caa54:	4c 89 67 38          	mov    %r12,0x38(%rdi)

	leaq	64 (%rsi), %rsi
ffffffff812caa58:	48 8d 76 40          	lea    0x40(%rsi),%rsi
	leaq	64 (%rdi), %rdi
ffffffff812caa5c:	48 8d 7f 40          	lea    0x40(%rdi),%rdi

	jnz	.Loop64
ffffffff812caa60:	75 ae                	jne    ffffffff812caa10 <copy_page_regs+0x20>

	movl	$5, %ecx
ffffffff812caa62:	b9 05 00 00 00       	mov    $0x5,%ecx
ffffffff812caa67:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
ffffffff812caa6e:	00 00 
	.p2align 4
.Loop2:
	decl	%ecx
ffffffff812caa70:	ff c9                	dec    %ecx

	movq	0x8*0(%rsi), %rax
ffffffff812caa72:	48 8b 06             	mov    (%rsi),%rax
	movq	0x8*1(%rsi), %rbx
ffffffff812caa75:	48 8b 5e 08          	mov    0x8(%rsi),%rbx
	movq	0x8*2(%rsi), %rdx
ffffffff812caa79:	48 8b 56 10          	mov    0x10(%rsi),%rdx
	movq	0x8*3(%rsi), %r8
ffffffff812caa7d:	4c 8b 46 18          	mov    0x18(%rsi),%r8
	movq	0x8*4(%rsi), %r9
ffffffff812caa81:	4c 8b 4e 20          	mov    0x20(%rsi),%r9
	movq	0x8*5(%rsi), %r10
ffffffff812caa85:	4c 8b 56 28          	mov    0x28(%rsi),%r10
	movq	0x8*6(%rsi), %r11
ffffffff812caa89:	4c 8b 5e 30          	mov    0x30(%rsi),%r11
	movq	0x8*7(%rsi), %r12
ffffffff812caa8d:	4c 8b 66 38          	mov    0x38(%rsi),%r12

	movq	%rax, 0x8*0(%rdi)
ffffffff812caa91:	48 89 07             	mov    %rax,(%rdi)
	movq	%rbx, 0x8*1(%rdi)
ffffffff812caa94:	48 89 5f 08          	mov    %rbx,0x8(%rdi)
	movq	%rdx, 0x8*2(%rdi)
ffffffff812caa98:	48 89 57 10          	mov    %rdx,0x10(%rdi)
	movq	%r8,  0x8*3(%rdi)
ffffffff812caa9c:	4c 89 47 18          	mov    %r8,0x18(%rdi)
	movq	%r9,  0x8*4(%rdi)
ffffffff812caaa0:	4c 89 4f 20          	mov    %r9,0x20(%rdi)
	movq	%r10, 0x8*5(%rdi)
ffffffff812caaa4:	4c 89 57 28          	mov    %r10,0x28(%rdi)
	movq	%r11, 0x8*6(%rdi)
ffffffff812caaa8:	4c 89 5f 30          	mov    %r11,0x30(%rdi)
	movq	%r12, 0x8*7(%rdi)
ffffffff812caaac:	4c 89 67 38          	mov    %r12,0x38(%rdi)

	leaq	64(%rdi), %rdi
ffffffff812caab0:	48 8d 7f 40          	lea    0x40(%rdi),%rdi
	leaq	64(%rsi), %rsi
ffffffff812caab4:	48 8d 76 40          	lea    0x40(%rsi),%rsi
	jnz	.Loop2
ffffffff812caab8:	75 b6                	jne    ffffffff812caa70 <copy_page_regs+0x80>

	movq	(%rsp), %rbx
ffffffff812caaba:	48 8b 1c 24          	mov    (%rsp),%rbx
	CFI_RESTORE rbx
	movq	1*8(%rsp), %r12
ffffffff812caabe:	4c 8b 64 24 08       	mov    0x8(%rsp),%r12
	CFI_RESTORE r12
	addq	$2*8, %rsp
ffffffff812caac3:	48 83 c4 10          	add    $0x10,%rsp
	CFI_ADJUST_CFA_OFFSET -2*8
	ret
ffffffff812caac7:	c3                   	retq   
ffffffff812caac8:	90                   	nop
ffffffff812caac9:	90                   	nop
ffffffff812caaca:	90                   	nop
ffffffff812caacb:	90                   	nop
ffffffff812caacc:	90                   	nop
ffffffff812caacd:	90                   	nop
ffffffff812caace:	90                   	nop
ffffffff812caacf:	90                   	nop

ffffffff812caad0 <_copy_to_user>:
	.endm

/* Standard copy_to_user with segment limit checking */
ENTRY(_copy_to_user)
	CFI_STARTPROC
	GET_THREAD_INFO(%rax)
ffffffff812caad0:	65 48 8b 04 25 a0 a9 	mov    %gs:0xa9a0,%rax
ffffffff812caad7:	00 00 
ffffffff812caad9:	48 2d 00 40 00 00    	sub    $0x4000,%rax
	movq %rdi,%rcx
ffffffff812caadf:	48 89 f9             	mov    %rdi,%rcx
	addq %rdx,%rcx
ffffffff812caae2:	48 01 d1             	add    %rdx,%rcx
	jc bad_to_user
ffffffff812caae5:	0f 82 5b 16 17 00    	jb     ffffffff8143c146 <bad_to_user>
	cmpq TI_addr_limit(%rax),%rcx
ffffffff812caaeb:	48 3b 48 18          	cmp    0x18(%rax),%rcx
	ja bad_to_user
ffffffff812caaef:	0f 87 51 16 17 00    	ja     ffffffff8143c146 <bad_to_user>
	ALTERNATIVE_2 "jmp copy_user_generic_unrolled",		\
ffffffff812caaf5:	e9 36 00 00 00       	jmpq   ffffffff812cab30 <copy_user_generic_unrolled>
ffffffff812caafa:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

ffffffff812cab00 <_copy_from_user>:
ENDPROC(_copy_to_user)

/* Standard copy_from_user with segment limit checking */
ENTRY(_copy_from_user)
	CFI_STARTPROC
	GET_THREAD_INFO(%rax)
ffffffff812cab00:	65 48 8b 04 25 a0 a9 	mov    %gs:0xa9a0,%rax
ffffffff812cab07:	00 00 
ffffffff812cab09:	48 2d 00 40 00 00    	sub    $0x4000,%rax
	movq %rsi,%rcx
ffffffff812cab0f:	48 89 f1             	mov    %rsi,%rcx
	addq %rdx,%rcx
ffffffff812cab12:	48 01 d1             	add    %rdx,%rcx
	jc bad_from_user
ffffffff812cab15:	0f 82 25 16 17 00    	jb     ffffffff8143c140 <bad_from_user>
	cmpq TI_addr_limit(%rax),%rcx
ffffffff812cab1b:	48 3b 48 18          	cmp    0x18(%rax),%rcx
	ja bad_from_user
ffffffff812cab1f:	0f 87 1b 16 17 00    	ja     ffffffff8143c140 <bad_from_user>
	ALTERNATIVE_2 "jmp copy_user_generic_unrolled",		\
ffffffff812cab25:	e9 06 00 00 00       	jmpq   ffffffff812cab30 <copy_user_generic_unrolled>
ffffffff812cab2a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

ffffffff812cab30 <copy_user_generic_unrolled>:
ffffffff812cab30:	90                   	nop
ffffffff812cab31:	90                   	nop
ffffffff812cab32:	90                   	nop
 * eax uncopied bytes or 0 if successful.
 */
ENTRY(copy_user_generic_unrolled)
	CFI_STARTPROC
	ASM_STAC
	cmpl $8,%edx
ffffffff812cab33:	83 fa 08             	cmp    $0x8,%edx
	jb 20f		/* less then 8 bytes, go to byte copy loop */
ffffffff812cab36:	0f 82 8c 00 00 00    	jb     ffffffff812cabc8 <copy_user_generic_unrolled+0x98>
	ALIGN_DESTINATION
ffffffff812cab3c:	89 f9                	mov    %edi,%ecx
ffffffff812cab3e:	83 e1 07             	and    $0x7,%ecx
ffffffff812cab41:	74 15                	je     ffffffff812cab58 <copy_user_generic_unrolled+0x28>
ffffffff812cab43:	83 e9 08             	sub    $0x8,%ecx
ffffffff812cab46:	f7 d9                	neg    %ecx
ffffffff812cab48:	29 ca                	sub    %ecx,%edx
ffffffff812cab4a:	8a 06                	mov    (%rsi),%al
ffffffff812cab4c:	88 07                	mov    %al,(%rdi)
ffffffff812cab4e:	48 ff c6             	inc    %rsi
ffffffff812cab51:	48 ff c7             	inc    %rdi
ffffffff812cab54:	ff c9                	dec    %ecx
ffffffff812cab56:	75 f2                	jne    ffffffff812cab4a <copy_user_generic_unrolled+0x1a>
	movl %edx,%ecx
ffffffff812cab58:	89 d1                	mov    %edx,%ecx
	andl $63,%edx
ffffffff812cab5a:	83 e2 3f             	and    $0x3f,%edx
	shrl $6,%ecx
ffffffff812cab5d:	c1 e9 06             	shr    $0x6,%ecx
	jz 17f
ffffffff812cab60:	74 4a                	je     ffffffff812cabac <copy_user_generic_unrolled+0x7c>
1:	movq (%rsi),%r8
ffffffff812cab62:	4c 8b 06             	mov    (%rsi),%r8
2:	movq 1*8(%rsi),%r9
ffffffff812cab65:	4c 8b 4e 08          	mov    0x8(%rsi),%r9
3:	movq 2*8(%rsi),%r10
ffffffff812cab69:	4c 8b 56 10          	mov    0x10(%rsi),%r10
4:	movq 3*8(%rsi),%r11
ffffffff812cab6d:	4c 8b 5e 18          	mov    0x18(%rsi),%r11
5:	movq %r8,(%rdi)
ffffffff812cab71:	4c 89 07             	mov    %r8,(%rdi)
6:	movq %r9,1*8(%rdi)
ffffffff812cab74:	4c 89 4f 08          	mov    %r9,0x8(%rdi)
7:	movq %r10,2*8(%rdi)
ffffffff812cab78:	4c 89 57 10          	mov    %r10,0x10(%rdi)
8:	movq %r11,3*8(%rdi)
ffffffff812cab7c:	4c 89 5f 18          	mov    %r11,0x18(%rdi)
9:	movq 4*8(%rsi),%r8
ffffffff812cab80:	4c 8b 46 20          	mov    0x20(%rsi),%r8
10:	movq 5*8(%rsi),%r9
ffffffff812cab84:	4c 8b 4e 28          	mov    0x28(%rsi),%r9
11:	movq 6*8(%rsi),%r10
ffffffff812cab88:	4c 8b 56 30          	mov    0x30(%rsi),%r10
12:	movq 7*8(%rsi),%r11
ffffffff812cab8c:	4c 8b 5e 38          	mov    0x38(%rsi),%r11
13:	movq %r8,4*8(%rdi)
ffffffff812cab90:	4c 89 47 20          	mov    %r8,0x20(%rdi)
14:	movq %r9,5*8(%rdi)
ffffffff812cab94:	4c 89 4f 28          	mov    %r9,0x28(%rdi)
15:	movq %r10,6*8(%rdi)
ffffffff812cab98:	4c 89 57 30          	mov    %r10,0x30(%rdi)
16:	movq %r11,7*8(%rdi)
ffffffff812cab9c:	4c 89 5f 38          	mov    %r11,0x38(%rdi)
	leaq 64(%rsi),%rsi
ffffffff812caba0:	48 8d 76 40          	lea    0x40(%rsi),%rsi
	leaq 64(%rdi),%rdi
ffffffff812caba4:	48 8d 7f 40          	lea    0x40(%rdi),%rdi
	decl %ecx
ffffffff812caba8:	ff c9                	dec    %ecx
	jnz 1b
ffffffff812cabaa:	75 b6                	jne    ffffffff812cab62 <copy_user_generic_unrolled+0x32>
17:	movl %edx,%ecx
ffffffff812cabac:	89 d1                	mov    %edx,%ecx
	andl $7,%edx
ffffffff812cabae:	83 e2 07             	and    $0x7,%edx
	shrl $3,%ecx
ffffffff812cabb1:	c1 e9 03             	shr    $0x3,%ecx
	jz 20f
ffffffff812cabb4:	74 12                	je     ffffffff812cabc8 <copy_user_generic_unrolled+0x98>
18:	movq (%rsi),%r8
ffffffff812cabb6:	4c 8b 06             	mov    (%rsi),%r8
19:	movq %r8,(%rdi)
ffffffff812cabb9:	4c 89 07             	mov    %r8,(%rdi)
	leaq 8(%rsi),%rsi
ffffffff812cabbc:	48 8d 76 08          	lea    0x8(%rsi),%rsi
	leaq 8(%rdi),%rdi
ffffffff812cabc0:	48 8d 7f 08          	lea    0x8(%rdi),%rdi
	decl %ecx
ffffffff812cabc4:	ff c9                	dec    %ecx
	jnz 18b
ffffffff812cabc6:	75 ee                	jne    ffffffff812cabb6 <copy_user_generic_unrolled+0x86>
20:	andl %edx,%edx
ffffffff812cabc8:	21 d2                	and    %edx,%edx
	jz 23f
ffffffff812cabca:	74 10                	je     ffffffff812cabdc <copy_user_generic_unrolled+0xac>
	movl %edx,%ecx
ffffffff812cabcc:	89 d1                	mov    %edx,%ecx
21:	movb (%rsi),%al
ffffffff812cabce:	8a 06                	mov    (%rsi),%al
22:	movb %al,(%rdi)
ffffffff812cabd0:	88 07                	mov    %al,(%rdi)
	incq %rsi
ffffffff812cabd2:	48 ff c6             	inc    %rsi
	incq %rdi
ffffffff812cabd5:	48 ff c7             	inc    %rdi
	decl %ecx
ffffffff812cabd8:	ff c9                	dec    %ecx
	jnz 21b
ffffffff812cabda:	75 f2                	jne    ffffffff812cabce <copy_user_generic_unrolled+0x9e>
23:	xor %eax,%eax
ffffffff812cabdc:	31 c0                	xor    %eax,%eax
ffffffff812cabde:	90                   	nop
ffffffff812cabdf:	90                   	nop
ffffffff812cabe0:	90                   	nop
	ASM_CLAC
	ret
ffffffff812cabe1:	c3                   	retq   
ffffffff812cabe2:	0f 1f 40 00          	nopl   0x0(%rax)
ffffffff812cabe6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cabed:	00 00 00 

ffffffff812cabf0 <copy_user_generic_string>:
ffffffff812cabf0:	90                   	nop
ffffffff812cabf1:	90                   	nop
ffffffff812cabf2:	90                   	nop
 * eax uncopied bytes or 0 if successful.
 */
ENTRY(copy_user_generic_string)
	CFI_STARTPROC
	ASM_STAC
	cmpl $8,%edx
ffffffff812cabf3:	83 fa 08             	cmp    $0x8,%edx
	jb 2f		/* less than 8 bytes, go to byte copy loop */
ffffffff812cabf6:	72 27                	jb     ffffffff812cac1f <copy_user_generic_string+0x2f>
	ALIGN_DESTINATION
ffffffff812cabf8:	89 f9                	mov    %edi,%ecx
ffffffff812cabfa:	83 e1 07             	and    $0x7,%ecx
ffffffff812cabfd:	74 15                	je     ffffffff812cac14 <copy_user_generic_string+0x24>
ffffffff812cabff:	83 e9 08             	sub    $0x8,%ecx
ffffffff812cac02:	f7 d9                	neg    %ecx
ffffffff812cac04:	29 ca                	sub    %ecx,%edx
ffffffff812cac06:	8a 06                	mov    (%rsi),%al
ffffffff812cac08:	88 07                	mov    %al,(%rdi)
ffffffff812cac0a:	48 ff c6             	inc    %rsi
ffffffff812cac0d:	48 ff c7             	inc    %rdi
ffffffff812cac10:	ff c9                	dec    %ecx
ffffffff812cac12:	75 f2                	jne    ffffffff812cac06 <copy_user_generic_string+0x16>
	movl %edx,%ecx
ffffffff812cac14:	89 d1                	mov    %edx,%ecx
	shrl $3,%ecx
ffffffff812cac16:	c1 e9 03             	shr    $0x3,%ecx
	andl $7,%edx
ffffffff812cac19:	83 e2 07             	and    $0x7,%edx
1:	rep
ffffffff812cac1c:	f3 48 a5             	rep movsq %ds:(%rsi),%es:(%rdi)
	movsq
2:	movl %edx,%ecx
ffffffff812cac1f:	89 d1                	mov    %edx,%ecx
3:	rep
ffffffff812cac21:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
	movsb
	xorl %eax,%eax
ffffffff812cac23:	31 c0                	xor    %eax,%eax
ffffffff812cac25:	90                   	nop
ffffffff812cac26:	90                   	nop
ffffffff812cac27:	90                   	nop
	ASM_CLAC
	ret
ffffffff812cac28:	c3                   	retq   
ffffffff812cac29:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

ffffffff812cac30 <copy_user_enhanced_fast_string>:
ffffffff812cac30:	90                   	nop
ffffffff812cac31:	90                   	nop
ffffffff812cac32:	90                   	nop
 * eax uncopied bytes or 0 if successful.
 */
ENTRY(copy_user_enhanced_fast_string)
	CFI_STARTPROC
	ASM_STAC
	movl %edx,%ecx
ffffffff812cac33:	89 d1                	mov    %edx,%ecx
1:	rep
ffffffff812cac35:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
	movsb
	xorl %eax,%eax
ffffffff812cac37:	31 c0                	xor    %eax,%eax
ffffffff812cac39:	90                   	nop
ffffffff812cac3a:	90                   	nop
ffffffff812cac3b:	90                   	nop
	ASM_CLAC
	ret
ffffffff812cac3c:	c3                   	retq   
ffffffff812cac3d:	90                   	nop
ffffffff812cac3e:	90                   	nop
ffffffff812cac3f:	90                   	nop

ffffffff812cac40 <__copy_user_nocache>:
ffffffff812cac40:	90                   	nop
ffffffff812cac41:	90                   	nop
ffffffff812cac42:	90                   	nop
 * This will force destination/source out of cache for more performance.
 */
ENTRY(__copy_user_nocache)
	CFI_STARTPROC
	ASM_STAC
	cmpl $8,%edx
ffffffff812cac43:	83 fa 08             	cmp    $0x8,%edx
	jb 20f		/* less then 8 bytes, go to byte copy loop */
ffffffff812cac46:	0f 82 95 00 00 00    	jb     ffffffff812cace1 <__copy_user_nocache+0xa1>
	ALIGN_DESTINATION
ffffffff812cac4c:	89 f9                	mov    %edi,%ecx
ffffffff812cac4e:	83 e1 07             	and    $0x7,%ecx
ffffffff812cac51:	74 15                	je     ffffffff812cac68 <__copy_user_nocache+0x28>
ffffffff812cac53:	83 e9 08             	sub    $0x8,%ecx
ffffffff812cac56:	f7 d9                	neg    %ecx
ffffffff812cac58:	29 ca                	sub    %ecx,%edx
ffffffff812cac5a:	8a 06                	mov    (%rsi),%al
ffffffff812cac5c:	88 07                	mov    %al,(%rdi)
ffffffff812cac5e:	48 ff c6             	inc    %rsi
ffffffff812cac61:	48 ff c7             	inc    %rdi
ffffffff812cac64:	ff c9                	dec    %ecx
ffffffff812cac66:	75 f2                	jne    ffffffff812cac5a <__copy_user_nocache+0x1a>
	movl %edx,%ecx
ffffffff812cac68:	89 d1                	mov    %edx,%ecx
	andl $63,%edx
ffffffff812cac6a:	83 e2 3f             	and    $0x3f,%edx
	shrl $6,%ecx
ffffffff812cac6d:	c1 e9 06             	shr    $0x6,%ecx
	jz 17f
ffffffff812cac70:	74 52                	je     ffffffff812cacc4 <__copy_user_nocache+0x84>
1:	movq (%rsi),%r8
ffffffff812cac72:	4c 8b 06             	mov    (%rsi),%r8
2:	movq 1*8(%rsi),%r9
ffffffff812cac75:	4c 8b 4e 08          	mov    0x8(%rsi),%r9
3:	movq 2*8(%rsi),%r10
ffffffff812cac79:	4c 8b 56 10          	mov    0x10(%rsi),%r10
4:	movq 3*8(%rsi),%r11
ffffffff812cac7d:	4c 8b 5e 18          	mov    0x18(%rsi),%r11
5:	movnti %r8,(%rdi)
ffffffff812cac81:	4c 0f c3 07          	movnti %r8,(%rdi)
6:	movnti %r9,1*8(%rdi)
ffffffff812cac85:	4c 0f c3 4f 08       	movnti %r9,0x8(%rdi)
7:	movnti %r10,2*8(%rdi)
ffffffff812cac8a:	4c 0f c3 57 10       	movnti %r10,0x10(%rdi)
8:	movnti %r11,3*8(%rdi)
ffffffff812cac8f:	4c 0f c3 5f 18       	movnti %r11,0x18(%rdi)
9:	movq 4*8(%rsi),%r8
ffffffff812cac94:	4c 8b 46 20          	mov    0x20(%rsi),%r8
10:	movq 5*8(%rsi),%r9
ffffffff812cac98:	4c 8b 4e 28          	mov    0x28(%rsi),%r9
11:	movq 6*8(%rsi),%r10
ffffffff812cac9c:	4c 8b 56 30          	mov    0x30(%rsi),%r10
12:	movq 7*8(%rsi),%r11
ffffffff812caca0:	4c 8b 5e 38          	mov    0x38(%rsi),%r11
13:	movnti %r8,4*8(%rdi)
ffffffff812caca4:	4c 0f c3 47 20       	movnti %r8,0x20(%rdi)
14:	movnti %r9,5*8(%rdi)
ffffffff812caca9:	4c 0f c3 4f 28       	movnti %r9,0x28(%rdi)
15:	movnti %r10,6*8(%rdi)
ffffffff812cacae:	4c 0f c3 57 30       	movnti %r10,0x30(%rdi)
16:	movnti %r11,7*8(%rdi)
ffffffff812cacb3:	4c 0f c3 5f 38       	movnti %r11,0x38(%rdi)
	leaq 64(%rsi),%rsi
ffffffff812cacb8:	48 8d 76 40          	lea    0x40(%rsi),%rsi
	leaq 64(%rdi),%rdi
ffffffff812cacbc:	48 8d 7f 40          	lea    0x40(%rdi),%rdi
	decl %ecx
ffffffff812cacc0:	ff c9                	dec    %ecx
	jnz 1b
ffffffff812cacc2:	75 ae                	jne    ffffffff812cac72 <__copy_user_nocache+0x32>
17:	movl %edx,%ecx
ffffffff812cacc4:	89 d1                	mov    %edx,%ecx
	andl $7,%edx
ffffffff812cacc6:	83 e2 07             	and    $0x7,%edx
	shrl $3,%ecx
ffffffff812cacc9:	c1 e9 03             	shr    $0x3,%ecx
	jz 20f
ffffffff812caccc:	74 13                	je     ffffffff812cace1 <__copy_user_nocache+0xa1>
18:	movq (%rsi),%r8
ffffffff812cacce:	4c 8b 06             	mov    (%rsi),%r8
19:	movnti %r8,(%rdi)
ffffffff812cacd1:	4c 0f c3 07          	movnti %r8,(%rdi)
	leaq 8(%rsi),%rsi
ffffffff812cacd5:	48 8d 76 08          	lea    0x8(%rsi),%rsi
	leaq 8(%rdi),%rdi
ffffffff812cacd9:	48 8d 7f 08          	lea    0x8(%rdi),%rdi
	decl %ecx
ffffffff812cacdd:	ff c9                	dec    %ecx
	jnz 18b
ffffffff812cacdf:	75 ed                	jne    ffffffff812cacce <__copy_user_nocache+0x8e>
20:	andl %edx,%edx
ffffffff812cace1:	21 d2                	and    %edx,%edx
	jz 23f
ffffffff812cace3:	74 10                	je     ffffffff812cacf5 <__copy_user_nocache+0xb5>
	movl %edx,%ecx
ffffffff812cace5:	89 d1                	mov    %edx,%ecx
21:	movb (%rsi),%al
ffffffff812cace7:	8a 06                	mov    (%rsi),%al
22:	movb %al,(%rdi)
ffffffff812cace9:	88 07                	mov    %al,(%rdi)
	incq %rsi
ffffffff812caceb:	48 ff c6             	inc    %rsi
	incq %rdi
ffffffff812cacee:	48 ff c7             	inc    %rdi
	decl %ecx
ffffffff812cacf1:	ff c9                	dec    %ecx
	jnz 21b
ffffffff812cacf3:	75 f2                	jne    ffffffff812cace7 <__copy_user_nocache+0xa7>
23:	xorl %eax,%eax
ffffffff812cacf5:	31 c0                	xor    %eax,%eax
ffffffff812cacf7:	90                   	nop
ffffffff812cacf8:	90                   	nop
ffffffff812cacf9:	90                   	nop
	ASM_CLAC
	sfence
ffffffff812cacfa:	0f ae f8             	sfence 
	ret
ffffffff812cacfd:	c3                   	retq   

ffffffff812cacfe <csum_partial>:
 * for the last fragment, which may be odd
 *
 * it's best to have buff aligned on a 64-bit boundary
 */
__wsum csum_partial(const void *buff, int len, __wsum sum)
{
ffffffff812cacfe:	55                   	push   %rbp
{
	unsigned odd, count;
	unsigned long result = 0;

	if (unlikely(len == 0))
		return result; 
ffffffff812cacff:	31 c0                	xor    %eax,%eax
static unsigned do_csum(const unsigned char *buff, unsigned len)
{
	unsigned odd, count;
	unsigned long result = 0;

	if (unlikely(len == 0))
ffffffff812cad01:	85 f6                	test   %esi,%esi
 * for the last fragment, which may be odd
 *
 * it's best to have buff aligned on a 64-bit boundary
 */
__wsum csum_partial(const void *buff, int len, __wsum sum)
{
ffffffff812cad03:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cad06:	53                   	push   %rbx
static unsigned do_csum(const unsigned char *buff, unsigned len)
{
	unsigned odd, count;
	unsigned long result = 0;

	if (unlikely(len == 0))
ffffffff812cad07:	0f 84 30 01 00 00    	je     ffffffff812cae3d <csum_partial+0x13f>
		return result; 
	odd = 1 & (unsigned long) buff;
	if (unlikely(odd)) {
ffffffff812cad0d:	41 89 f8             	mov    %edi,%r8d
 * Using interleaving with more registers to break the carry chains.
 */
static unsigned do_csum(const unsigned char *buff, unsigned len)
{
	unsigned odd, count;
	unsigned long result = 0;
ffffffff812cad10:	31 c0                	xor    %eax,%eax

	if (unlikely(len == 0))
		return result; 
	odd = 1 & (unsigned long) buff;
	if (unlikely(odd)) {
ffffffff812cad12:	41 83 e0 01          	and    $0x1,%r8d
ffffffff812cad16:	74 0d                	je     ffffffff812cad25 <csum_partial+0x27>
		result = *buff << 8;
ffffffff812cad18:	0f b6 07             	movzbl (%rdi),%eax
		len--;
ffffffff812cad1b:	ff ce                	dec    %esi
		buff++;
ffffffff812cad1d:	48 ff c7             	inc    %rdi

	if (unlikely(len == 0))
		return result; 
	odd = 1 & (unsigned long) buff;
	if (unlikely(odd)) {
		result = *buff << 8;
ffffffff812cad20:	c1 e0 08             	shl    $0x8,%eax
ffffffff812cad23:	48 98                	cltq   
		len--;
		buff++;
	}
	count = len >> 1;		/* nr of 16-bit words.. */
	if (count) {
ffffffff812cad25:	89 f1                	mov    %esi,%ecx
ffffffff812cad27:	d1 e9                	shr    %ecx
ffffffff812cad29:	0f 84 cf 00 00 00    	je     ffffffff812cadfe <csum_partial+0x100>
		if (2 & (unsigned long) buff) {
ffffffff812cad2f:	40 f6 c7 02          	test   $0x2,%dil
ffffffff812cad33:	74 10                	je     ffffffff812cad45 <csum_partial+0x47>
			result += *(unsigned short *)buff;
ffffffff812cad35:	44 0f b7 0f          	movzwl (%rdi),%r9d
			count--;
ffffffff812cad39:	ff c9                	dec    %ecx
			len -= 2;
ffffffff812cad3b:	83 ee 02             	sub    $0x2,%esi
			buff += 2;
ffffffff812cad3e:	48 83 c7 02          	add    $0x2,%rdi
		buff++;
	}
	count = len >> 1;		/* nr of 16-bit words.. */
	if (count) {
		if (2 & (unsigned long) buff) {
			result += *(unsigned short *)buff;
ffffffff812cad42:	4c 01 c8             	add    %r9,%rax
			count--;
			len -= 2;
			buff += 2;
		}
		count >>= 1;		/* nr of 32-bit words.. */
		if (count) {
ffffffff812cad45:	d1 e9                	shr    %ecx
ffffffff812cad47:	0f 84 a1 00 00 00    	je     ffffffff812cadee <csum_partial+0xf0>
			unsigned long zero;
			unsigned count64;
			if (4 & (unsigned long) buff) {
ffffffff812cad4d:	40 f6 c7 04          	test   $0x4,%dil
ffffffff812cad51:	74 0f                	je     ffffffff812cad62 <csum_partial+0x64>
				result += *(unsigned int *) buff;
ffffffff812cad53:	44 8b 0f             	mov    (%rdi),%r9d
				count--;
ffffffff812cad56:	ff c9                	dec    %ecx
				len -= 4;
ffffffff812cad58:	83 ee 04             	sub    $0x4,%esi
				buff += 4;
ffffffff812cad5b:	48 83 c7 04          	add    $0x4,%rdi
		count >>= 1;		/* nr of 32-bit words.. */
		if (count) {
			unsigned long zero;
			unsigned count64;
			if (4 & (unsigned long) buff) {
				result += *(unsigned int *) buff;
ffffffff812cad5f:	4c 01 c8             	add    %r9,%rax
				count--;
				len -= 4;
				buff += 4;
			}
			count >>= 1;	/* nr of 64-bit words.. */
ffffffff812cad62:	41 89 c9             	mov    %ecx,%r9d

			/* main loop using 64byte blocks */
			zero = 0;
			count64 = count >> 3;
ffffffff812cad65:	c1 e9 04             	shr    $0x4,%ecx
ffffffff812cad68:	49 89 fb             	mov    %rdi,%r11
				result += *(unsigned int *) buff;
				count--;
				len -= 4;
				buff += 4;
			}
			count >>= 1;	/* nr of 64-bit words.. */
ffffffff812cad6b:	41 d1 e9             	shr    %r9d

			/* main loop using 64byte blocks */
			zero = 0;
			count64 = count >> 3;
ffffffff812cad6e:	41 89 ca             	mov    %ecx,%r10d
			while (count64) { 
				asm("addq 0*8(%[src]),%[res]\n\t"
ffffffff812cad71:	31 db                	xor    %ebx,%ebx
			count >>= 1;	/* nr of 64-bit words.. */

			/* main loop using 64byte blocks */
			zero = 0;
			count64 = count >> 3;
			while (count64) { 
ffffffff812cad73:	45 85 d2             	test   %r10d,%r10d
ffffffff812cad76:	74 2b                	je     ffffffff812cada3 <csum_partial+0xa5>
				asm("addq 0*8(%[src]),%[res]\n\t"
ffffffff812cad78:	49 03 03             	add    (%r11),%rax
ffffffff812cad7b:	49 13 43 08          	adc    0x8(%r11),%rax
ffffffff812cad7f:	49 13 43 10          	adc    0x10(%r11),%rax
ffffffff812cad83:	49 13 43 18          	adc    0x18(%r11),%rax
ffffffff812cad87:	49 13 43 20          	adc    0x20(%r11),%rax
ffffffff812cad8b:	49 13 43 28          	adc    0x28(%r11),%rax
ffffffff812cad8f:	49 13 43 30          	adc    0x30(%r11),%rax
ffffffff812cad93:	49 13 43 38          	adc    0x38(%r11),%rax
ffffffff812cad97:	48 11 d8             	adc    %rbx,%rax
				    "adcq %[zero],%[res]"
				    : [res] "=r" (result)
				    : [src] "r" (buff), [zero] "r" (zero),
				    "[res]" (result));
				buff += 64;
				count64--;
ffffffff812cad9a:	41 ff ca             	dec    %r10d
				    "adcq 7*8(%[src]),%[res]\n\t"
				    "adcq %[zero],%[res]"
				    : [res] "=r" (result)
				    : [src] "r" (buff), [zero] "r" (zero),
				    "[res]" (result));
				buff += 64;
ffffffff812cad9d:	49 83 c3 40          	add    $0x40,%r11
ffffffff812cada1:	eb d0                	jmp    ffffffff812cad73 <csum_partial+0x75>
ffffffff812cada3:	48 c1 e1 06          	shl    $0x6,%rcx
			}

			/* last up to 7 8byte blocks */
			count %= 8; 
			while (count) { 
				asm("addq %1,%0\n\t"
ffffffff812cada7:	45 31 db             	xor    %r11d,%r11d
ffffffff812cadaa:	48 01 cf             	add    %rcx,%rdi
				buff += 64;
				count64--;
			}

			/* last up to 7 8byte blocks */
			count %= 8; 
ffffffff812cadad:	44 89 c9             	mov    %r9d,%ecx
ffffffff812cadb0:	83 e1 07             	and    $0x7,%ecx
ffffffff812cadb3:	49 89 fa             	mov    %rdi,%r10
ffffffff812cadb6:	41 89 c9             	mov    %ecx,%r9d
			while (count) { 
ffffffff812cadb9:	45 85 c9             	test   %r9d,%r9d
ffffffff812cadbc:	74 0f                	je     ffffffff812cadcd <csum_partial+0xcf>
				asm("addq %1,%0\n\t"
ffffffff812cadbe:	49 03 02             	add    (%r10),%rax
ffffffff812cadc1:	4c 11 d8             	adc    %r11,%rax
				    "adcq %2,%0\n" 
					    : "=r" (result)
				    : "m" (*(unsigned long *)buff), 
				    "r" (zero),  "0" (result));
				--count; 
ffffffff812cadc4:	41 ff c9             	dec    %r9d
					buff += 8;
ffffffff812cadc7:	49 83 c2 08          	add    $0x8,%r10
ffffffff812cadcb:	eb ec                	jmp    ffffffff812cadb9 <csum_partial+0xbb>
ffffffff812cadcd:	48 8d 3c cf          	lea    (%rdi,%rcx,8),%rdi
csum_ipv6_magic(const struct in6_addr *saddr, const struct in6_addr *daddr,
		__u32 len, unsigned short proto, __wsum sum);

static inline unsigned add32_with_carry(unsigned a, unsigned b)
{
	asm("addl %2,%0\n\t"
ffffffff812cadd1:	48 89 c1             	mov    %rax,%rcx
ffffffff812cadd4:	48 c1 e9 20          	shr    $0x20,%rcx
ffffffff812cadd8:	01 c1                	add    %eax,%ecx
ffffffff812cadda:	83 d1 00             	adc    $0x0,%ecx
			}
			result = add32_with_carry(result>>32,
						  result&0xffffffff); 

			if (len & 4) {
ffffffff812caddd:	40 f6 c6 04          	test   $0x4,%sil
				    : "m" (*(unsigned long *)buff), 
				    "r" (zero),  "0" (result));
				--count; 
					buff += 8;
			}
			result = add32_with_carry(result>>32,
ffffffff812cade1:	89 c8                	mov    %ecx,%eax
						  result&0xffffffff); 

			if (len & 4) {
ffffffff812cade3:	74 09                	je     ffffffff812cadee <csum_partial+0xf0>
				result += *(unsigned int *) buff;
ffffffff812cade5:	8b 0f                	mov    (%rdi),%ecx
				buff += 4;
ffffffff812cade7:	48 83 c7 04          	add    $0x4,%rdi
			}
			result = add32_with_carry(result>>32,
						  result&0xffffffff); 

			if (len & 4) {
				result += *(unsigned int *) buff;
ffffffff812cadeb:	48 01 c8             	add    %rcx,%rax
				buff += 4;
			}
		}
		if (len & 2) {
ffffffff812cadee:	40 f6 c6 02          	test   $0x2,%sil
ffffffff812cadf2:	74 0a                	je     ffffffff812cadfe <csum_partial+0x100>
			result += *(unsigned short *) buff;
ffffffff812cadf4:	0f b7 0f             	movzwl (%rdi),%ecx
			buff += 2;
ffffffff812cadf7:	48 83 c7 02          	add    $0x2,%rdi
				result += *(unsigned int *) buff;
				buff += 4;
			}
		}
		if (len & 2) {
			result += *(unsigned short *) buff;
ffffffff812cadfb:	48 01 c8             	add    %rcx,%rax
			buff += 2;
		}
	}
	if (len & 1)
ffffffff812cadfe:	40 80 e6 01          	and    $0x1,%sil
ffffffff812cae02:	74 06                	je     ffffffff812cae0a <csum_partial+0x10c>
		result += *buff;
ffffffff812cae04:	0f b6 0f             	movzbl (%rdi),%ecx
ffffffff812cae07:	48 01 c8             	add    %rcx,%rax
ffffffff812cae0a:	48 89 c1             	mov    %rax,%rcx
ffffffff812cae0d:	48 c1 e9 20          	shr    $0x20,%rcx
ffffffff812cae11:	01 c1                	add    %eax,%ecx
ffffffff812cae13:	83 d1 00             	adc    $0x0,%ecx
	result = add32_with_carry(result>>32, result & 0xffffffff); 
	if (unlikely(odd)) { 
ffffffff812cae16:	45 85 c0             	test   %r8d,%r8d
			buff += 2;
		}
	}
	if (len & 1)
		result += *buff;
	result = add32_with_carry(result>>32, result & 0xffffffff); 
ffffffff812cae19:	89 c8                	mov    %ecx,%eax
	if (unlikely(odd)) { 
ffffffff812cae1b:	74 20                	je     ffffffff812cae3d <csum_partial+0x13f>
#include <linux/module.h>
#include <asm/checksum.h>

static inline unsigned short from32to16(unsigned a) 
{
	unsigned short b = a >> 16; 
ffffffff812cae1d:	89 c8                	mov    %ecx,%eax
ffffffff812cae1f:	c1 e8 10             	shr    $0x10,%eax
	asm("addw %w2,%w0\n\t"
ffffffff812cae22:	66 01 c8             	add    %cx,%ax
ffffffff812cae25:	66 83 d0 00          	adc    $0x0,%ax
	}
	if (len & 1)
		result += *buff;
	result = add32_with_carry(result>>32, result & 0xffffffff); 
	if (unlikely(odd)) { 
		result = from32to16(result);
ffffffff812cae29:	0f b7 c8             	movzwl %ax,%ecx
		result = ((result >> 8) & 0xff) | ((result & 0xff) << 8);
ffffffff812cae2c:	48 89 c8             	mov    %rcx,%rax
ffffffff812cae2f:	48 c1 e1 08          	shl    $0x8,%rcx
ffffffff812cae33:	48 c1 e8 08          	shr    $0x8,%rax
ffffffff812cae37:	0f b7 c9             	movzwl %cx,%ecx
ffffffff812cae3a:	48 09 c8             	or     %rcx,%rax
ffffffff812cae3d:	01 d0                	add    %edx,%eax
ffffffff812cae3f:	83 d0 00             	adc    $0x0,%eax
 */
__wsum csum_partial(const void *buff, int len, __wsum sum)
{
	return (__force __wsum)add32_with_carry(do_csum(buff, len),
						(__force u32)sum);
}
ffffffff812cae42:	5b                   	pop    %rbx
ffffffff812cae43:	5d                   	pop    %rbp
ffffffff812cae44:	c3                   	retq   

ffffffff812cae45 <ip_compute_csum>:
/*
 * this routine is used for miscellaneous IP-like checksums, mainly
 * in icmp.c
 */
__sum16 ip_compute_csum(const void *buff, int len)
{
ffffffff812cae45:	55                   	push   %rbp
	return csum_fold(csum_partial(buff,len,0));
ffffffff812cae46:	31 d2                	xor    %edx,%edx
/*
 * this routine is used for miscellaneous IP-like checksums, mainly
 * in icmp.c
 */
__sum16 ip_compute_csum(const void *buff, int len)
{
ffffffff812cae48:	48 89 e5             	mov    %rsp,%rbp
	return csum_fold(csum_partial(buff,len,0));
ffffffff812cae4b:	e8 ae fe ff ff       	callq  ffffffff812cacfe <csum_partial>
 * the last step before putting a checksum into a packet.
 * Make sure not to mix with 64bit checksums.
 */
static inline __sum16 csum_fold(__wsum sum)
{
	asm("  addl %1,%0\n"
ffffffff812cae50:	89 c2                	mov    %eax,%edx
ffffffff812cae52:	66 31 c0             	xor    %ax,%ax
ffffffff812cae55:	c1 e2 10             	shl    $0x10,%edx
ffffffff812cae58:	01 d0                	add    %edx,%eax
ffffffff812cae5a:	15 ff ff 00 00       	adc    $0xffff,%eax
ffffffff812cae5f:	f7 d0                	not    %eax
ffffffff812cae61:	c1 e8 10             	shr    $0x10,%eax
}
ffffffff812cae64:	5d                   	pop    %rbp
ffffffff812cae65:	c3                   	retq   
ffffffff812cae66:	90                   	nop
ffffffff812cae67:	90                   	nop
ffffffff812cae68:	90                   	nop
ffffffff812cae69:	90                   	nop
ffffffff812cae6a:	90                   	nop
ffffffff812cae6b:	90                   	nop
ffffffff812cae6c:	90                   	nop
ffffffff812cae6d:	90                   	nop
ffffffff812cae6e:	90                   	nop
ffffffff812cae6f:	90                   	nop

ffffffff812cae70 <delay_loop>:
# include <asm/smp.h>
#endif

/* simple loop based delay: */
static void delay_loop(unsigned long loops)
{
ffffffff812cae70:	55                   	push   %rbp
	asm volatile(
ffffffff812cae71:	48 89 f8             	mov    %rdi,%rax
# include <asm/smp.h>
#endif

/* simple loop based delay: */
static void delay_loop(unsigned long loops)
{
ffffffff812cae74:	48 89 e5             	mov    %rsp,%rbp
	asm volatile(
ffffffff812cae77:	48 85 c0             	test   %rax,%rax
ffffffff812cae7a:	74 19                	je     ffffffff812cae95 <delay_loop+0x25>
ffffffff812cae7c:	eb 02                	jmp    ffffffff812cae80 <delay_loop+0x10>
ffffffff812cae7e:	66 90                	xchg   %ax,%ax
ffffffff812cae80:	eb 0e                	jmp    ffffffff812cae90 <delay_loop+0x20>
ffffffff812cae82:	0f 1f 40 00          	nopl   0x0(%rax)
ffffffff812cae86:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cae8d:	00 00 00 
ffffffff812cae90:	48 ff c8             	dec    %rax
ffffffff812cae93:	75 fb                	jne    ffffffff812cae90 <delay_loop+0x20>
ffffffff812cae95:	48 ff c8             	dec    %rax
		"3:	dec %0		\n"

		: /* we don't need output */
		:"a" (loops)
	);
}
ffffffff812cae98:	5d                   	pop    %rbp
ffffffff812cae99:	c3                   	retq   

ffffffff812cae9a <__delay>:
	}
	return -1;
}

void __delay(unsigned long loops)
{
ffffffff812cae9a:	55                   	push   %rbp
ffffffff812cae9b:	48 89 e5             	mov    %rsp,%rbp
	delay_fn(loops);
ffffffff812cae9e:	ff 15 c4 b1 76 00    	callq  *0x76b1c4(%rip)        # ffffffff81a36068 <delay_fn>
}
ffffffff812caea4:	5d                   	pop    %rbp
ffffffff812caea5:	c3                   	retq   

ffffffff812caea6 <__const_udelay>:
EXPORT_SYMBOL(__delay);

inline void __const_udelay(unsigned long xloops)
{
ffffffff812caea6:	55                   	push   %rbp
	int d0;

	xloops *= 4;
ffffffff812caea7:	48 8d 04 bd 00 00 00 	lea    0x0(,%rdi,4),%rax
ffffffff812caeae:	00 
	asm("mull %%edx"
		:"=d" (xloops), "=&a" (d0)
		:"1" (xloops), "0"
		(this_cpu_read(cpu_info.loops_per_jiffy) * (HZ/4)));
ffffffff812caeaf:	65 48 8b 15 39 f2 d3 	mov    %gs:0x7ed3f239(%rip),%rdx        # a0f0 <cpu_info+0xb0>
ffffffff812caeb6:	7e 
inline void __const_udelay(unsigned long xloops)
{
	int d0;

	xloops *= 4;
	asm("mull %%edx"
ffffffff812caeb7:	48 69 d2 fa 00 00 00 	imul   $0xfa,%rdx,%rdx
	delay_fn(loops);
}
EXPORT_SYMBOL(__delay);

inline void __const_udelay(unsigned long xloops)
{
ffffffff812caebe:	48 89 e5             	mov    %rsp,%rbp
	int d0;

	xloops *= 4;
	asm("mull %%edx"
ffffffff812caec1:	f7 e2                	mul    %edx
		:"=d" (xloops), "=&a" (d0)
		:"1" (xloops), "0"
		(this_cpu_read(cpu_info.loops_per_jiffy) * (HZ/4)));

	__delay(++xloops);
ffffffff812caec3:	48 8d 7a 01          	lea    0x1(%rdx),%rdi
ffffffff812caec7:	e8 ce ff ff ff       	callq  ffffffff812cae9a <__delay>
}
ffffffff812caecc:	5d                   	pop    %rbp
ffffffff812caecd:	c3                   	retq   

ffffffff812caece <__udelay>:
EXPORT_SYMBOL(__const_udelay);

void __udelay(unsigned long usecs)
{
	__const_udelay(usecs * 0x000010c7); /* 2**32 / 1000000 (rounded up) */
ffffffff812caece:	48 69 ff c7 10 00 00 	imul   $0x10c7,%rdi,%rdi
	__delay(++xloops);
}
EXPORT_SYMBOL(__const_udelay);

void __udelay(unsigned long usecs)
{
ffffffff812caed5:	55                   	push   %rbp
ffffffff812caed6:	48 89 e5             	mov    %rsp,%rbp
	__const_udelay(usecs * 0x000010c7); /* 2**32 / 1000000 (rounded up) */
ffffffff812caed9:	e8 c8 ff ff ff       	callq  ffffffff812caea6 <__const_udelay>
}
ffffffff812caede:	5d                   	pop    %rbp
ffffffff812caedf:	c3                   	retq   

ffffffff812caee0 <__ndelay>:
EXPORT_SYMBOL(__udelay);

void __ndelay(unsigned long nsecs)
{
ffffffff812caee0:	55                   	push   %rbp
	__const_udelay(nsecs * 0x00005); /* 2**32 / 1000000000 (rounded up) */
ffffffff812caee1:	48 8d 3c bf          	lea    (%rdi,%rdi,4),%rdi
	__const_udelay(usecs * 0x000010c7); /* 2**32 / 1000000 (rounded up) */
}
EXPORT_SYMBOL(__udelay);

void __ndelay(unsigned long nsecs)
{
ffffffff812caee5:	48 89 e5             	mov    %rsp,%rbp
	__const_udelay(nsecs * 0x00005); /* 2**32 / 1000000000 (rounded up) */
ffffffff812caee8:	e8 b9 ff ff ff       	callq  ffffffff812caea6 <__const_udelay>
}
ffffffff812caeed:	5d                   	pop    %rbp
ffffffff812caeee:	c3                   	retq   

ffffffff812caeef <delay_tsc>:
	);
}

/* TSC based delay: */
static void delay_tsc(unsigned long __loops)
{
ffffffff812caeef:	55                   	push   %rbp
ffffffff812caef0:	48 89 e5             	mov    %rsp,%rbp
	u32 bclock, now, loops = __loops;
	int cpu;

	preempt_disable();
	cpu = smp_processor_id();
ffffffff812caef3:	65 8b 35 16 f2 d3 7e 	mov    %gs:0x7ed3f216(%rip),%esi        # a110 <cpu_number>
ffffffff812caefa:	90                   	nop
ffffffff812caefb:	90                   	nop
ffffffff812caefc:	90                   	nop

static __always_inline unsigned long long __native_read_tsc(void)
{
	DECLARE_ARGS(val, low, high);

	asm volatile("rdtsc" : EAX_EDX_RET(val, low, high));
ffffffff812caefd:	0f 31                	rdtsc  
	rdtsc_barrier();
	rdtscl(bclock);
ffffffff812caeff:	89 c1                	mov    %eax,%ecx
ffffffff812caf01:	90                   	nop
ffffffff812caf02:	90                   	nop
ffffffff812caf03:	90                   	nop
ffffffff812caf04:	0f 31                	rdtsc  
	for (;;) {
		rdtsc_barrier();
		rdtscl(now);
ffffffff812caf06:	48 c1 e2 20          	shl    $0x20,%rdx
ffffffff812caf0a:	89 c0                	mov    %eax,%eax
ffffffff812caf0c:	48 09 c2             	or     %rax,%rdx
ffffffff812caf0f:	41 89 d0             	mov    %edx,%r8d
		if ((now - bclock) >= loops)
ffffffff812caf12:	29 ca                	sub    %ecx,%edx
ffffffff812caf14:	39 d7                	cmp    %edx,%edi
ffffffff812caf16:	76 16                	jbe    ffffffff812caf2e <delay_tsc+0x3f>
ffffffff812caf18:	f3 90                	pause  
		 * least" the amount of time. Being moved to another
		 * CPU could make the wait longer but we just need to
		 * make sure we waited long enough. Rebalance the
		 * counter for this CPU.
		 */
		if (unlikely(cpu != smp_processor_id())) {
ffffffff812caf1a:	65 8b 05 ef f1 d3 7e 	mov    %gs:0x7ed3f1ef(%rip),%eax        # a110 <cpu_number>
ffffffff812caf21:	39 c6                	cmp    %eax,%esi
ffffffff812caf23:	74 dc                	je     ffffffff812caf01 <delay_tsc+0x12>
			loops -= (now - bclock);
ffffffff812caf25:	44 29 c1             	sub    %r8d,%ecx
			cpu = smp_processor_id();
ffffffff812caf28:	89 c6                	mov    %eax,%esi
		 * CPU could make the wait longer but we just need to
		 * make sure we waited long enough. Rebalance the
		 * counter for this CPU.
		 */
		if (unlikely(cpu != smp_processor_id())) {
			loops -= (now - bclock);
ffffffff812caf2a:	01 cf                	add    %ecx,%edi
ffffffff812caf2c:	eb cc                	jmp    ffffffff812caefa <delay_tsc+0xb>
			rdtsc_barrier();
			rdtscl(bclock);
		}
	}
	preempt_enable();
}
ffffffff812caf2e:	5d                   	pop    %rbp
ffffffff812caf2f:	c3                   	retq   

ffffffff812caf30 <use_tsc_delay>:
 * function should be set once at boot and not changed
 */
static void (*delay_fn)(unsigned long) = delay_loop;

void use_tsc_delay(void)
{
ffffffff812caf30:	55                   	push   %rbp
	delay_fn = delay_tsc;
ffffffff812caf31:	48 c7 05 2c b1 76 00 	movq   $0xffffffff812caeef,0x76b12c(%rip)        # ffffffff81a36068 <delay_fn>
ffffffff812caf38:	ef ae 2c 81 
 * function should be set once at boot and not changed
 */
static void (*delay_fn)(unsigned long) = delay_loop;

void use_tsc_delay(void)
{
ffffffff812caf3c:	48 89 e5             	mov    %rsp,%rbp
	delay_fn = delay_tsc;
}
ffffffff812caf3f:	5d                   	pop    %rbp
ffffffff812caf40:	c3                   	retq   

ffffffff812caf41 <read_current_timer>:

int read_current_timer(unsigned long *timer_val)
{
ffffffff812caf41:	83 c8 ff             	or     $0xffffffff,%eax
	if (delay_fn == delay_tsc) {
ffffffff812caf44:	48 81 3d 19 b1 76 00 	cmpq   $0xffffffff812caeef,0x76b119(%rip)        # ffffffff81a36068 <delay_fn>
ffffffff812caf4b:	ef ae 2c 81 
{
	delay_fn = delay_tsc;
}

int read_current_timer(unsigned long *timer_val)
{
ffffffff812caf4f:	55                   	push   %rbp
ffffffff812caf50:	48 89 e5             	mov    %rsp,%rbp
	if (delay_fn == delay_tsc) {
ffffffff812caf53:	75 10                	jne    ffffffff812caf65 <read_current_timer+0x24>
ffffffff812caf55:	0f 31                	rdtsc  
		rdtscll(*timer_val);
ffffffff812caf57:	48 c1 e2 20          	shl    $0x20,%rdx
ffffffff812caf5b:	89 c0                	mov    %eax,%eax
ffffffff812caf5d:	48 09 d0             	or     %rdx,%rax
ffffffff812caf60:	48 89 07             	mov    %rax,(%rdi)
		return 0;
ffffffff812caf63:	31 c0                	xor    %eax,%eax
	}
	return -1;
}
ffffffff812caf65:	5d                   	pop    %rbp
ffffffff812caf66:	c3                   	retq   
ffffffff812caf67:	90                   	nop
ffffffff812caf68:	90                   	nop
ffffffff812caf69:	90                   	nop
ffffffff812caf6a:	90                   	nop
ffffffff812caf6b:	90                   	nop
ffffffff812caf6c:	90                   	nop
ffffffff812caf6d:	90                   	nop
ffffffff812caf6e:	90                   	nop
ffffffff812caf6f:	90                   	nop

ffffffff812caf70 <__get_user_1>:
#include <asm/smap.h>

	.text
ENTRY(__get_user_1)
	CFI_STARTPROC
	GET_THREAD_INFO(%_ASM_DX)
ffffffff812caf70:	65 48 8b 14 25 a0 a9 	mov    %gs:0xa9a0,%rdx
ffffffff812caf77:	00 00 
ffffffff812caf79:	48 81 ea 00 40 00 00 	sub    $0x4000,%rdx
	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
ffffffff812caf80:	48 3b 42 18          	cmp    0x18(%rdx),%rax
	jae bad_get_user
ffffffff812caf84:	0f 83 9f 00 00 00    	jae    ffffffff812cb029 <bad_get_user>
ffffffff812caf8a:	90                   	nop
ffffffff812caf8b:	90                   	nop
ffffffff812caf8c:	90                   	nop
	ASM_STAC
1:	movzbl (%_ASM_AX),%edx
ffffffff812caf8d:	0f b6 10             	movzbl (%rax),%edx
	xor %eax,%eax
ffffffff812caf90:	31 c0                	xor    %eax,%eax
ffffffff812caf92:	90                   	nop
ffffffff812caf93:	90                   	nop
ffffffff812caf94:	90                   	nop
	ASM_CLAC
	ret
ffffffff812caf95:	c3                   	retq   
ffffffff812caf96:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812caf9d:	00 00 00 

ffffffff812cafa0 <__get_user_2>:
	CFI_ENDPROC
ENDPROC(__get_user_1)

ENTRY(__get_user_2)
	CFI_STARTPROC
	add $1,%_ASM_AX
ffffffff812cafa0:	48 83 c0 01          	add    $0x1,%rax
	jc bad_get_user
ffffffff812cafa4:	0f 82 7f 00 00 00    	jb     ffffffff812cb029 <bad_get_user>
	GET_THREAD_INFO(%_ASM_DX)
ffffffff812cafaa:	65 48 8b 14 25 a0 a9 	mov    %gs:0xa9a0,%rdx
ffffffff812cafb1:	00 00 
ffffffff812cafb3:	48 81 ea 00 40 00 00 	sub    $0x4000,%rdx
	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
ffffffff812cafba:	48 3b 42 18          	cmp    0x18(%rdx),%rax
	jae bad_get_user
ffffffff812cafbe:	73 69                	jae    ffffffff812cb029 <bad_get_user>
ffffffff812cafc0:	90                   	nop
ffffffff812cafc1:	90                   	nop
ffffffff812cafc2:	90                   	nop
	ASM_STAC
2:	movzwl -1(%_ASM_AX),%edx
ffffffff812cafc3:	0f b7 50 ff          	movzwl -0x1(%rax),%edx
	xor %eax,%eax
ffffffff812cafc7:	31 c0                	xor    %eax,%eax
ffffffff812cafc9:	90                   	nop
ffffffff812cafca:	90                   	nop
ffffffff812cafcb:	90                   	nop
	ASM_CLAC
	ret
ffffffff812cafcc:	c3                   	retq   
ffffffff812cafcd:	0f 1f 00             	nopl   (%rax)

ffffffff812cafd0 <__get_user_4>:
	CFI_ENDPROC
ENDPROC(__get_user_2)

ENTRY(__get_user_4)
	CFI_STARTPROC
	add $3,%_ASM_AX
ffffffff812cafd0:	48 83 c0 03          	add    $0x3,%rax
	jc bad_get_user
ffffffff812cafd4:	72 53                	jb     ffffffff812cb029 <bad_get_user>
	GET_THREAD_INFO(%_ASM_DX)
ffffffff812cafd6:	65 48 8b 14 25 a0 a9 	mov    %gs:0xa9a0,%rdx
ffffffff812cafdd:	00 00 
ffffffff812cafdf:	48 81 ea 00 40 00 00 	sub    $0x4000,%rdx
	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
ffffffff812cafe6:	48 3b 42 18          	cmp    0x18(%rdx),%rax
	jae bad_get_user
ffffffff812cafea:	73 3d                	jae    ffffffff812cb029 <bad_get_user>
ffffffff812cafec:	90                   	nop
ffffffff812cafed:	90                   	nop
ffffffff812cafee:	90                   	nop
	ASM_STAC
3:	movl -3(%_ASM_AX),%edx
ffffffff812cafef:	8b 50 fd             	mov    -0x3(%rax),%edx
	xor %eax,%eax
ffffffff812caff2:	31 c0                	xor    %eax,%eax
ffffffff812caff4:	90                   	nop
ffffffff812caff5:	90                   	nop
ffffffff812caff6:	90                   	nop
	ASM_CLAC
	ret
ffffffff812caff7:	c3                   	retq   
ffffffff812caff8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
ffffffff812cafff:	00 

ffffffff812cb000 <__get_user_8>:
ENDPROC(__get_user_4)

ENTRY(__get_user_8)
	CFI_STARTPROC
#ifdef CONFIG_X86_64
	add $7,%_ASM_AX
ffffffff812cb000:	48 83 c0 07          	add    $0x7,%rax
	jc bad_get_user
ffffffff812cb004:	72 23                	jb     ffffffff812cb029 <bad_get_user>
	GET_THREAD_INFO(%_ASM_DX)
ffffffff812cb006:	65 48 8b 14 25 a0 a9 	mov    %gs:0xa9a0,%rdx
ffffffff812cb00d:	00 00 
ffffffff812cb00f:	48 81 ea 00 40 00 00 	sub    $0x4000,%rdx
	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX
ffffffff812cb016:	48 3b 42 18          	cmp    0x18(%rdx),%rax
	jae bad_get_user
ffffffff812cb01a:	73 0d                	jae    ffffffff812cb029 <bad_get_user>
ffffffff812cb01c:	90                   	nop
ffffffff812cb01d:	90                   	nop
ffffffff812cb01e:	90                   	nop
	ASM_STAC
4:	movq -7(%_ASM_AX),%rdx
ffffffff812cb01f:	48 8b 50 f9          	mov    -0x7(%rax),%rdx
	xor %eax,%eax
ffffffff812cb023:	31 c0                	xor    %eax,%eax
ffffffff812cb025:	90                   	nop
ffffffff812cb026:	90                   	nop
ffffffff812cb027:	90                   	nop
	ASM_CLAC
	ret
ffffffff812cb028:	c3                   	retq   

ffffffff812cb029 <bad_get_user>:
ENDPROC(__get_user_8)


bad_get_user:
	CFI_STARTPROC
	xor %edx,%edx
ffffffff812cb029:	31 d2                	xor    %edx,%edx
	mov $(-EFAULT),%_ASM_AX
ffffffff812cb02b:	48 c7 c0 f2 ff ff ff 	mov    $0xfffffffffffffff2,%rax
ffffffff812cb032:	90                   	nop
ffffffff812cb033:	90                   	nop
ffffffff812cb034:	90                   	nop
	ASM_CLAC
	ret
ffffffff812cb035:	c3                   	retq   

ffffffff812cb036 <insn_init>:
 * @insn:	&struct insn to be initialized
 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)
 * @x86_64:	!0 for 64-bit kernel or 64-bit app
 */
void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
{
ffffffff812cb036:	41 89 c9             	mov    %ecx,%r9d
	 * even if the input buffer is long enough to hold them.
	 */
	if (buf_len > MAX_INSN_SIZE)
		buf_len = MAX_INSN_SIZE;

	memset(insn, 0, sizeof(*insn));
ffffffff812cb039:	31 c0                	xor    %eax,%eax
ffffffff812cb03b:	b9 1a 00 00 00       	mov    $0x1a,%ecx
 * @insn:	&struct insn to be initialized
 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)
 * @x86_64:	!0 for 64-bit kernel or 64-bit app
 */
void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
{
ffffffff812cb040:	49 89 f8             	mov    %rdi,%r8
	if (buf_len > MAX_INSN_SIZE)
		buf_len = MAX_INSN_SIZE;

	memset(insn, 0, sizeof(*insn));
	insn->kaddr = kaddr;
	insn->end_kaddr = kaddr + buf_len;
ffffffff812cb043:	83 fa 0f             	cmp    $0xf,%edx
 * @insn:	&struct insn to be initialized
 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)
 * @x86_64:	!0 for 64-bit kernel or 64-bit app
 */
void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
{
ffffffff812cb046:	55                   	push   %rbp
	 * even if the input buffer is long enough to hold them.
	 */
	if (buf_len > MAX_INSN_SIZE)
		buf_len = MAX_INSN_SIZE;

	memset(insn, 0, sizeof(*insn));
ffffffff812cb047:	f3 ab                	rep stos %eax,%es:(%rdi)
	insn->kaddr = kaddr;
	insn->end_kaddr = kaddr + buf_len;
ffffffff812cb049:	b8 0f 00 00 00       	mov    $0xf,%eax
 * @insn:	&struct insn to be initialized
 * @kaddr:	address (in kernel memory) of instruction (or copy thereof)
 * @x86_64:	!0 for 64-bit kernel or 64-bit app
 */
void insn_init(struct insn *insn, const void *kaddr, int buf_len, int x86_64)
{
ffffffff812cb04e:	48 89 e5             	mov    %rsp,%rbp
	 */
	if (buf_len > MAX_INSN_SIZE)
		buf_len = MAX_INSN_SIZE;

	memset(insn, 0, sizeof(*insn));
	insn->kaddr = kaddr;
ffffffff812cb051:	49 89 70 50          	mov    %rsi,0x50(%r8)
	insn->end_kaddr = kaddr + buf_len;
ffffffff812cb055:	0f 4f d0             	cmovg  %eax,%edx
	insn->next_byte = kaddr;
ffffffff812cb058:	49 89 70 60          	mov    %rsi,0x60(%r8)
	insn->x86_64 = x86_64 ? 1 : 0;
	insn->opnd_bytes = 4;
ffffffff812cb05c:	41 c6 40 4c 04       	movb   $0x4,0x4c(%r8)
	if (buf_len > MAX_INSN_SIZE)
		buf_len = MAX_INSN_SIZE;

	memset(insn, 0, sizeof(*insn));
	insn->kaddr = kaddr;
	insn->end_kaddr = kaddr + buf_len;
ffffffff812cb061:	48 63 d2             	movslq %edx,%rdx
ffffffff812cb064:	48 01 f2             	add    %rsi,%rdx
	insn->next_byte = kaddr;
	insn->x86_64 = x86_64 ? 1 : 0;
ffffffff812cb067:	45 85 c9             	test   %r9d,%r9d
ffffffff812cb06a:	41 0f 95 40 4f       	setne  0x4f(%r8)
	insn->opnd_bytes = 4;
	if (x86_64)
		insn->addr_bytes = 8;
ffffffff812cb06f:	41 83 f9 01          	cmp    $0x1,%r9d
	if (buf_len > MAX_INSN_SIZE)
		buf_len = MAX_INSN_SIZE;

	memset(insn, 0, sizeof(*insn));
	insn->kaddr = kaddr;
	insn->end_kaddr = kaddr + buf_len;
ffffffff812cb073:	49 89 50 58          	mov    %rdx,0x58(%r8)
	insn->next_byte = kaddr;
	insn->x86_64 = x86_64 ? 1 : 0;
	insn->opnd_bytes = 4;
	if (x86_64)
		insn->addr_bytes = 8;
ffffffff812cb077:	19 c0                	sbb    %eax,%eax
ffffffff812cb079:	83 e0 fc             	and    $0xfffffffc,%eax
ffffffff812cb07c:	83 c0 08             	add    $0x8,%eax
ffffffff812cb07f:	41 88 40 4d          	mov    %al,0x4d(%r8)
	else
		insn->addr_bytes = 4;
}
ffffffff812cb083:	5d                   	pop    %rbp
ffffffff812cb084:	c3                   	retq   

ffffffff812cb085 <insn_get_prefixes>:
	struct insn_field *prefixes = &insn->prefixes;
	insn_attr_t attr;
	insn_byte_t b, lb;
	int i, nb;

	if (prefixes->got)
ffffffff812cb085:	80 7f 04 00          	cmpb   $0x0,0x4(%rdi)
ffffffff812cb089:	0f 85 c5 01 00 00    	jne    ffffffff812cb254 <insn_get_prefixes+0x1cf>
		return;

	nb = 0;
	lb = 0;
	b = peek_next(insn_byte_t, insn);
ffffffff812cb08f:	48 8b 47 60          	mov    0x60(%rdi),%rax
ffffffff812cb093:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812cb097:	48 39 57 58          	cmp    %rdx,0x58(%rdi)
ffffffff812cb09b:	0f 82 b3 01 00 00    	jb     ffffffff812cb254 <insn_get_prefixes+0x1cf>
 * Populates the @insn->prefixes bitmap, and updates @insn->next_byte
 * to point to the (first) opcode.  No effect if @insn->prefixes.got
 * is already set.
 */
void insn_get_prefixes(struct insn *insn)
{
ffffffff812cb0a1:	55                   	push   %rbp
ffffffff812cb0a2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb0a5:	41 56                	push   %r14
ffffffff812cb0a7:	41 55                	push   %r13
ffffffff812cb0a9:	41 54                	push   %r12
ffffffff812cb0ab:	53                   	push   %rbx
ffffffff812cb0ac:	48 89 fb             	mov    %rdi,%rbx
	if (prefixes->got)
		return;

	nb = 0;
	lb = 0;
	b = peek_next(insn_byte_t, insn);
ffffffff812cb0af:	0f b6 38             	movzbl (%rax),%edi
ffffffff812cb0b2:	41 89 fc             	mov    %edi,%r12d
	attr = inat_get_opcode_attribute(b);
ffffffff812cb0b5:	e8 e5 0e 00 00       	callq  ffffffff812cbf9f <inat_get_opcode_attribute>
	int i, nb;

	if (prefixes->got)
		return;

	nb = 0;
ffffffff812cb0ba:	31 d2                	xor    %edx,%edx
	lb = 0;
ffffffff812cb0bc:	31 c9                	xor    %ecx,%ecx
					  insn_byte_t vex_pp);

/* Attribute checking functions */
static inline int inat_is_legacy_prefix(insn_attr_t attr)
{
	attr &= INAT_PFX_MASK;
ffffffff812cb0be:	83 e0 0f             	and    $0xf,%eax
	b = peek_next(insn_byte_t, insn);
	attr = inat_get_opcode_attribute(b);
	while (inat_is_legacy_prefix(attr)) {
ffffffff812cb0c1:	8d 70 ff             	lea    -0x1(%rax),%esi
ffffffff812cb0c4:	83 fe 0a             	cmp    $0xa,%esi
ffffffff812cb0c7:	77 75                	ja     ffffffff812cb13e <insn_get_prefixes+0xb9>
ffffffff812cb0c9:	31 f6                	xor    %esi,%esi
		/* Skip if same prefix */
		for (i = 0; i < nb; i++)
ffffffff812cb0cb:	39 f2                	cmp    %esi,%edx
ffffffff812cb0cd:	7e 0f                	jle    ffffffff812cb0de <insn_get_prefixes+0x59>
ffffffff812cb0cf:	48 ff c6             	inc    %rsi
			if (prefixes->bytes[i] == b)
ffffffff812cb0d2:	44 38 64 33 ff       	cmp    %r12b,-0x1(%rbx,%rsi,1)
ffffffff812cb0d7:	75 f2                	jne    ffffffff812cb0cb <insn_get_prefixes+0x46>
ffffffff812cb0d9:	41 89 d6             	mov    %edx,%r14d
ffffffff812cb0dc:	eb 2c                	jmp    ffffffff812cb10a <insn_get_prefixes+0x85>
				goto found;
		if (nb == 4)
ffffffff812cb0de:	83 fa 04             	cmp    $0x4,%edx
ffffffff812cb0e1:	74 5b                	je     ffffffff812cb13e <insn_get_prefixes+0xb9>
			/* Invalid instruction */
			break;
		prefixes->bytes[nb++] = b;
		if (inat_is_address_size_prefix(attr)) {
ffffffff812cb0e3:	83 f8 0b             	cmp    $0xb,%eax
			if (prefixes->bytes[i] == b)
				goto found;
		if (nb == 4)
			/* Invalid instruction */
			break;
		prefixes->bytes[nb++] = b;
ffffffff812cb0e6:	44 8d 72 01          	lea    0x1(%rdx),%r14d
ffffffff812cb0ea:	44 88 24 13          	mov    %r12b,(%rbx,%rdx,1)
		if (inat_is_address_size_prefix(attr)) {
ffffffff812cb0ee:	75 12                	jne    ffffffff812cb102 <insn_get_prefixes+0x7d>
			/* address size switches 2/4 or 4/8 */
			if (insn->x86_64)
ffffffff812cb0f0:	80 7b 4f 00          	cmpb   $0x0,0x4f(%rbx)
ffffffff812cb0f4:	74 06                	je     ffffffff812cb0fc <insn_get_prefixes+0x77>
				insn->addr_bytes ^= 12;
ffffffff812cb0f6:	80 73 4d 0c          	xorb   $0xc,0x4d(%rbx)
ffffffff812cb0fa:	eb 0e                	jmp    ffffffff812cb10a <insn_get_prefixes+0x85>
			else
				insn->addr_bytes ^= 6;
ffffffff812cb0fc:	80 73 4d 06          	xorb   $0x6,0x4d(%rbx)
ffffffff812cb100:	eb 08                	jmp    ffffffff812cb10a <insn_get_prefixes+0x85>
		} else if (inat_is_operand_size_prefix(attr)) {
ffffffff812cb102:	ff c8                	dec    %eax
ffffffff812cb104:	75 04                	jne    ffffffff812cb10a <insn_get_prefixes+0x85>
			/* oprand size switches 2/4 */
			insn->opnd_bytes ^= 6;
ffffffff812cb106:	80 73 4c 06          	xorb   $0x6,0x4c(%rbx)
		}
found:
		prefixes->nbytes++;
		insn->next_byte++;
ffffffff812cb10a:	48 8b 43 60          	mov    0x60(%rbx),%rax
		} else if (inat_is_operand_size_prefix(attr)) {
			/* oprand size switches 2/4 */
			insn->opnd_bytes ^= 6;
		}
found:
		prefixes->nbytes++;
ffffffff812cb10e:	fe 43 05             	incb   0x5(%rbx)
		insn->next_byte++;
ffffffff812cb111:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812cb115:	48 89 53 60          	mov    %rdx,0x60(%rbx)
		lb = b;
		b = peek_next(insn_byte_t, insn);
ffffffff812cb119:	48 8d 50 02          	lea    0x2(%rax),%rdx
ffffffff812cb11d:	48 39 53 58          	cmp    %rdx,0x58(%rbx)
ffffffff812cb121:	0f 82 25 01 00 00    	jb     ffffffff812cb24c <insn_get_prefixes+0x1c7>
ffffffff812cb127:	0f b6 78 01          	movzbl 0x1(%rax),%edi
ffffffff812cb12b:	41 89 fd             	mov    %edi,%r13d
		attr = inat_get_opcode_attribute(b);
ffffffff812cb12e:	e8 6c 0e 00 00       	callq  ffffffff812cbf9f <inat_get_opcode_attribute>
ffffffff812cb133:	44 88 e1             	mov    %r12b,%cl
ffffffff812cb136:	49 63 d6             	movslq %r14d,%rdx
		}
found:
		prefixes->nbytes++;
		insn->next_byte++;
		lb = b;
		b = peek_next(insn_byte_t, insn);
ffffffff812cb139:	45 88 ec             	mov    %r13b,%r12b
ffffffff812cb13c:	eb 80                	jmp    ffffffff812cb0be <insn_get_prefixes+0x39>
		attr = inat_get_opcode_attribute(b);
	}
	/* Set the last prefix */
	if (lb && lb != insn->prefixes.bytes[3]) {
ffffffff812cb13e:	84 c9                	test   %cl,%cl
ffffffff812cb140:	74 27                	je     ffffffff812cb169 <insn_get_prefixes+0xe4>
ffffffff812cb142:	40 8a 73 03          	mov    0x3(%rbx),%sil
ffffffff812cb146:	40 38 f1             	cmp    %sil,%cl
ffffffff812cb149:	74 1e                	je     ffffffff812cb169 <insn_get_prefixes+0xe4>
		if (unlikely(insn->prefixes.bytes[3])) {
ffffffff812cb14b:	31 c0                	xor    %eax,%eax
ffffffff812cb14d:	40 84 f6             	test   %sil,%sil
ffffffff812cb150:	75 11                	jne    ffffffff812cb163 <insn_get_prefixes+0xde>
			b = insn->prefixes.bytes[3];
			for (i = 0; i < nb; i++)
				if (prefixes->bytes[i] == lb)
					prefixes->bytes[i] = b;
		}
		insn->prefixes.bytes[3] = lb;
ffffffff812cb152:	88 4b 03             	mov    %cl,0x3(%rbx)
ffffffff812cb155:	eb 12                	jmp    ffffffff812cb169 <insn_get_prefixes+0xe4>
	if (lb && lb != insn->prefixes.bytes[3]) {
		if (unlikely(insn->prefixes.bytes[3])) {
			/* Swap the last prefix */
			b = insn->prefixes.bytes[3];
			for (i = 0; i < nb; i++)
				if (prefixes->bytes[i] == lb)
ffffffff812cb157:	3a 0c 03             	cmp    (%rbx,%rax,1),%cl
ffffffff812cb15a:	75 04                	jne    ffffffff812cb160 <insn_get_prefixes+0xdb>
					prefixes->bytes[i] = b;
ffffffff812cb15c:	40 88 34 03          	mov    %sil,(%rbx,%rax,1)
ffffffff812cb160:	48 ff c0             	inc    %rax
	/* Set the last prefix */
	if (lb && lb != insn->prefixes.bytes[3]) {
		if (unlikely(insn->prefixes.bytes[3])) {
			/* Swap the last prefix */
			b = insn->prefixes.bytes[3];
			for (i = 0; i < nb; i++)
ffffffff812cb163:	39 c2                	cmp    %eax,%edx
ffffffff812cb165:	7f f0                	jg     ffffffff812cb157 <insn_get_prefixes+0xd2>
ffffffff812cb167:	eb e9                	jmp    ffffffff812cb152 <insn_get_prefixes+0xcd>
		}
		insn->prefixes.bytes[3] = lb;
	}

	/* Decode REX prefix */
	if (insn->x86_64) {
ffffffff812cb169:	80 7b 4f 00          	cmpb   $0x0,0x4f(%rbx)
ffffffff812cb16d:	74 3f                	je     ffffffff812cb1ae <insn_get_prefixes+0x129>
		b = peek_next(insn_byte_t, insn);
ffffffff812cb16f:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb173:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812cb177:	48 39 53 58          	cmp    %rdx,0x58(%rbx)
ffffffff812cb17b:	0f 82 cb 00 00 00    	jb     ffffffff812cb24c <insn_get_prefixes+0x1c7>
ffffffff812cb181:	44 0f b6 28          	movzbl (%rax),%r13d
		attr = inat_get_opcode_attribute(b);
ffffffff812cb185:	44 89 ef             	mov    %r13d,%edi
		insn->prefixes.bytes[3] = lb;
	}

	/* Decode REX prefix */
	if (insn->x86_64) {
		b = peek_next(insn_byte_t, insn);
ffffffff812cb188:	45 89 ec             	mov    %r13d,%r12d
		attr = inat_get_opcode_attribute(b);
ffffffff812cb18b:	e8 0f 0e 00 00       	callq  ffffffff812cbf9f <inat_get_opcode_attribute>
		if (inat_is_rex_prefix(attr)) {
ffffffff812cb190:	83 e0 0f             	and    $0xf,%eax
ffffffff812cb193:	83 f8 0c             	cmp    $0xc,%eax
ffffffff812cb196:	75 16                	jne    ffffffff812cb1ae <insn_get_prefixes+0x129>
			insn->rex_prefix.value = b;
			insn->rex_prefix.nbytes = 1;
			insn->next_byte++;
ffffffff812cb198:	48 ff 43 60          	incq   0x60(%rbx)
			if (X86_REX_W(b))
ffffffff812cb19c:	41 80 e4 08          	and    $0x8,%r12b
	/* Decode REX prefix */
	if (insn->x86_64) {
		b = peek_next(insn_byte_t, insn);
		attr = inat_get_opcode_attribute(b);
		if (inat_is_rex_prefix(attr)) {
			insn->rex_prefix.value = b;
ffffffff812cb1a0:	44 89 6b 08          	mov    %r13d,0x8(%rbx)
			insn->rex_prefix.nbytes = 1;
ffffffff812cb1a4:	c6 43 0d 01          	movb   $0x1,0xd(%rbx)
			insn->next_byte++;
			if (X86_REX_W(b))
ffffffff812cb1a8:	74 04                	je     ffffffff812cb1ae <insn_get_prefixes+0x129>
				/* REX.W overrides opnd_size */
				insn->opnd_bytes = 8;
ffffffff812cb1aa:	c6 43 4c 08          	movb   $0x8,0x4c(%rbx)
		}
	}
	insn->rex_prefix.got = 1;

	/* Decode VEX prefix */
	b = peek_next(insn_byte_t, insn);
ffffffff812cb1ae:	48 8b 43 60          	mov    0x60(%rbx),%rax
			if (X86_REX_W(b))
				/* REX.W overrides opnd_size */
				insn->opnd_bytes = 8;
		}
	}
	insn->rex_prefix.got = 1;
ffffffff812cb1b2:	c6 43 0c 01          	movb   $0x1,0xc(%rbx)

	/* Decode VEX prefix */
	b = peek_next(insn_byte_t, insn);
ffffffff812cb1b6:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812cb1ba:	48 39 53 58          	cmp    %rdx,0x58(%rbx)
ffffffff812cb1be:	0f 82 88 00 00 00    	jb     ffffffff812cb24c <insn_get_prefixes+0x1c7>
ffffffff812cb1c4:	0f b6 38             	movzbl (%rax),%edi
ffffffff812cb1c7:	41 89 fc             	mov    %edi,%r12d
	attr = inat_get_opcode_attribute(b);
ffffffff812cb1ca:	e8 d0 0d 00 00       	callq  ffffffff812cbf9f <inat_get_opcode_attribute>
		return attr & INAT_PFX_MASK;
}

static inline int inat_is_vex_prefix(insn_attr_t attr)
{
	attr &= INAT_PFX_MASK;
ffffffff812cb1cf:	83 e0 0f             	and    $0xf,%eax
	if (inat_is_vex_prefix(attr)) {
ffffffff812cb1d2:	8d 50 f3             	lea    -0xd(%rax),%edx
ffffffff812cb1d5:	83 fa 01             	cmp    $0x1,%edx
ffffffff812cb1d8:	77 6a                	ja     ffffffff812cb244 <insn_get_prefixes+0x1bf>
		insn_byte_t b2 = peek_nbyte_next(insn_byte_t, insn, 1);
ffffffff812cb1da:	48 8b 4b 60          	mov    0x60(%rbx),%rcx
ffffffff812cb1de:	4c 8b 43 58          	mov    0x58(%rbx),%r8
ffffffff812cb1e2:	48 8d 71 02          	lea    0x2(%rcx),%rsi
ffffffff812cb1e6:	4c 39 c6             	cmp    %r8,%rsi
ffffffff812cb1e9:	77 61                	ja     ffffffff812cb24c <insn_get_prefixes+0x1c7>
		if (!insn->x86_64) {
ffffffff812cb1eb:	40 8a 7b 4f          	mov    0x4f(%rbx),%dil

	/* Decode VEX prefix */
	b = peek_next(insn_byte_t, insn);
	attr = inat_get_opcode_attribute(b);
	if (inat_is_vex_prefix(attr)) {
		insn_byte_t b2 = peek_nbyte_next(insn_byte_t, insn, 1);
ffffffff812cb1ef:	8a 51 01             	mov    0x1(%rcx),%dl
		if (!insn->x86_64) {
ffffffff812cb1f2:	40 84 ff             	test   %dil,%dil
ffffffff812cb1f5:	75 0d                	jne    ffffffff812cb204 <insn_get_prefixes+0x17f>
			/*
			 * In 32-bits mode, if the [7:6] bits (mod bits of
			 * ModRM) on the second byte are not 11b, it is
			 * LDS or LES.
			 */
			if (X86_MODRM_MOD(b2) != 3)
ffffffff812cb1f7:	41 88 d1             	mov    %dl,%r9b
ffffffff812cb1fa:	41 c0 e9 06          	shr    $0x6,%r9b
ffffffff812cb1fe:	41 80 f9 03          	cmp    $0x3,%r9b
ffffffff812cb202:	75 40                	jne    ffffffff812cb244 <insn_get_prefixes+0x1bf>
				goto vex_end;
		}
		insn->vex_prefix.bytes[0] = b;
		insn->vex_prefix.bytes[1] = b2;
		if (inat_is_vex3_prefix(attr)) {
ffffffff812cb204:	83 f8 0e             	cmp    $0xe,%eax
			 * LDS or LES.
			 */
			if (X86_MODRM_MOD(b2) != 3)
				goto vex_end;
		}
		insn->vex_prefix.bytes[0] = b;
ffffffff812cb207:	44 88 63 10          	mov    %r12b,0x10(%rbx)
		insn->vex_prefix.bytes[1] = b2;
ffffffff812cb20b:	88 53 11             	mov    %dl,0x11(%rbx)
		if (inat_is_vex3_prefix(attr)) {
ffffffff812cb20e:	75 26                	jne    ffffffff812cb236 <insn_get_prefixes+0x1b1>
			b2 = peek_nbyte_next(insn_byte_t, insn, 2);
ffffffff812cb210:	48 8d 51 03          	lea    0x3(%rcx),%rdx
ffffffff812cb214:	49 39 d0             	cmp    %rdx,%r8
ffffffff812cb217:	72 33                	jb     ffffffff812cb24c <insn_get_prefixes+0x1c7>
ffffffff812cb219:	8a 41 02             	mov    0x2(%rcx),%al
			insn->vex_prefix.bytes[2] = b2;
			insn->vex_prefix.nbytes = 3;
			insn->next_byte += 3;
			if (insn->x86_64 && X86_VEX_W(b2))
ffffffff812cb21c:	40 84 ff             	test   %dil,%dil
		insn->vex_prefix.bytes[0] = b;
		insn->vex_prefix.bytes[1] = b2;
		if (inat_is_vex3_prefix(attr)) {
			b2 = peek_nbyte_next(insn_byte_t, insn, 2);
			insn->vex_prefix.bytes[2] = b2;
			insn->vex_prefix.nbytes = 3;
ffffffff812cb21f:	c6 43 15 03          	movb   $0x3,0x15(%rbx)
			insn->next_byte += 3;
ffffffff812cb223:	48 89 53 60          	mov    %rdx,0x60(%rbx)
		}
		insn->vex_prefix.bytes[0] = b;
		insn->vex_prefix.bytes[1] = b2;
		if (inat_is_vex3_prefix(attr)) {
			b2 = peek_nbyte_next(insn_byte_t, insn, 2);
			insn->vex_prefix.bytes[2] = b2;
ffffffff812cb227:	88 43 12             	mov    %al,0x12(%rbx)
			insn->vex_prefix.nbytes = 3;
			insn->next_byte += 3;
			if (insn->x86_64 && X86_VEX_W(b2))
ffffffff812cb22a:	74 18                	je     ffffffff812cb244 <insn_get_prefixes+0x1bf>
ffffffff812cb22c:	84 c0                	test   %al,%al
ffffffff812cb22e:	79 14                	jns    ffffffff812cb244 <insn_get_prefixes+0x1bf>
				/* VEX.W overrides opnd_size */
				insn->opnd_bytes = 8;
ffffffff812cb230:	c6 43 4c 08          	movb   $0x8,0x4c(%rbx)
ffffffff812cb234:	eb 0e                	jmp    ffffffff812cb244 <insn_get_prefixes+0x1bf>
			/*
			 * For VEX2, fake VEX3-like byte#2.
			 * Makes it easier to decode vex.W, vex.vvvv,
			 * vex.L and vex.pp. Masking with 0x7f sets vex.W == 0.
			 */
			insn->vex_prefix.bytes[2] = b2 & 0x7f;
ffffffff812cb236:	83 e2 7f             	and    $0x7f,%edx
			insn->vex_prefix.nbytes = 2;
ffffffff812cb239:	c6 43 15 02          	movb   $0x2,0x15(%rbx)
			insn->next_byte += 2;
ffffffff812cb23d:	48 89 73 60          	mov    %rsi,0x60(%rbx)
			/*
			 * For VEX2, fake VEX3-like byte#2.
			 * Makes it easier to decode vex.W, vex.vvvv,
			 * vex.L and vex.pp. Masking with 0x7f sets vex.W == 0.
			 */
			insn->vex_prefix.bytes[2] = b2 & 0x7f;
ffffffff812cb241:	88 53 12             	mov    %dl,0x12(%rbx)
			insn->vex_prefix.nbytes = 2;
			insn->next_byte += 2;
		}
	}
vex_end:
	insn->vex_prefix.got = 1;
ffffffff812cb244:	c6 43 14 01          	movb   $0x1,0x14(%rbx)

	prefixes->got = 1;
ffffffff812cb248:	c6 43 04 01          	movb   $0x1,0x4(%rbx)

err_out:
	return;
}
ffffffff812cb24c:	5b                   	pop    %rbx
ffffffff812cb24d:	41 5c                	pop    %r12
ffffffff812cb24f:	41 5d                	pop    %r13
ffffffff812cb251:	41 5e                	pop    %r14
ffffffff812cb253:	5d                   	pop    %rbp
ffffffff812cb254:	c3                   	retq   

ffffffff812cb255 <insn_last_prefix_id>:
		return X86_VEX_P(insn->vex_prefix.bytes[2]);
}

/* Get the last prefix id from last prefix or VEX prefix */
static inline int insn_last_prefix_id(struct insn *insn)
{
ffffffff812cb255:	55                   	push   %rbp
ffffffff812cb256:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb259:	53                   	push   %rbx
ffffffff812cb25a:	51                   	push   %rcx
#endif
}

static inline int insn_is_avx(struct insn *insn)
{
	if (!insn->prefixes.got)
ffffffff812cb25b:	80 7f 04 00          	cmpb   $0x0,0x4(%rdi)
		return X86_VEX_P(insn->vex_prefix.bytes[2]);
}

/* Get the last prefix id from last prefix or VEX prefix */
static inline int insn_last_prefix_id(struct insn *insn)
{
ffffffff812cb25f:	48 89 fb             	mov    %rdi,%rbx
#endif
}

static inline int insn_is_avx(struct insn *insn)
{
	if (!insn->prefixes.got)
ffffffff812cb262:	75 05                	jne    ffffffff812cb269 <insn_last_prefix_id+0x14>
		insn_get_prefixes(insn);
ffffffff812cb264:	e8 1c fe ff ff       	callq  ffffffff812cb085 <insn_get_prefixes>
	return (insn->vex_prefix.value != 0);
ffffffff812cb269:	8b 43 10             	mov    0x10(%rbx),%eax
}

/* Get the last prefix id from last prefix or VEX prefix */
static inline int insn_last_prefix_id(struct insn *insn)
{
	if (insn_is_avx(insn))
ffffffff812cb26c:	85 c0                	test   %eax,%eax
ffffffff812cb26e:	74 18                	je     ffffffff812cb288 <insn_last_prefix_id+0x33>
}

static inline insn_byte_t insn_vex_p_bits(struct insn *insn)
{
	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
		return X86_VEX_P(insn->vex_prefix.bytes[1]);
ffffffff812cb270:	8a 53 11             	mov    0x11(%rbx),%dl
ffffffff812cb273:	8a 43 12             	mov    0x12(%rbx),%al
ffffffff812cb276:	83 e2 03             	and    $0x3,%edx
ffffffff812cb279:	83 e0 03             	and    $0x3,%eax
ffffffff812cb27c:	80 7b 15 02          	cmpb   $0x2,0x15(%rbx)
ffffffff812cb280:	0f 44 c2             	cmove  %edx,%eax

/* Get the last prefix id from last prefix or VEX prefix */
static inline int insn_last_prefix_id(struct insn *insn)
{
	if (insn_is_avx(insn))
		return insn_vex_p_bits(insn);	/* VEX_p is a SIMD prefix id */
ffffffff812cb283:	0f b6 c0             	movzbl %al,%eax
ffffffff812cb286:	eb 0e                	jmp    ffffffff812cb296 <insn_last_prefix_id+0x41>

	if (insn->prefixes.bytes[3])
ffffffff812cb288:	0f b6 7b 03          	movzbl 0x3(%rbx),%edi
ffffffff812cb28c:	40 84 ff             	test   %dil,%dil
ffffffff812cb28f:	74 05                	je     ffffffff812cb296 <insn_last_prefix_id+0x41>
		return inat_get_last_prefix_id(insn->prefixes.bytes[3]);
ffffffff812cb291:	e8 1a 0d 00 00       	callq  ffffffff812cbfb0 <inat_get_last_prefix_id>

	return 0;
}
ffffffff812cb296:	5a                   	pop    %rdx
ffffffff812cb297:	5b                   	pop    %rbx
ffffffff812cb298:	5d                   	pop    %rbp
ffffffff812cb299:	c3                   	retq   

ffffffff812cb29a <insn_get_opcode>:
void insn_get_opcode(struct insn *insn)
{
	struct insn_field *opcode = &insn->opcode;
	insn_byte_t op;
	int pfx_id;
	if (opcode->got)
ffffffff812cb29a:	80 7f 1c 00          	cmpb   $0x0,0x1c(%rdi)
ffffffff812cb29e:	0f 85 ea 00 00 00    	jne    ffffffff812cb38e <insn_get_opcode+0xf4>
 * If necessary, first collects any preceding (prefix) bytes.
 * Sets @insn->opcode.value = opcode1.  No effect if @insn->opcode.got
 * is already 1.
 */
void insn_get_opcode(struct insn *insn)
{
ffffffff812cb2a4:	55                   	push   %rbp
ffffffff812cb2a5:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb2a8:	41 54                	push   %r12
ffffffff812cb2aa:	53                   	push   %rbx
	struct insn_field *opcode = &insn->opcode;
	insn_byte_t op;
	int pfx_id;
	if (opcode->got)
		return;
	if (!insn->prefixes.got)
ffffffff812cb2ab:	80 7f 04 00          	cmpb   $0x0,0x4(%rdi)
ffffffff812cb2af:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cb2b2:	75 05                	jne    ffffffff812cb2b9 <insn_get_opcode+0x1f>
		insn_get_prefixes(insn);
ffffffff812cb2b4:	e8 cc fd ff ff       	callq  ffffffff812cb085 <insn_get_prefixes>

	/* Get first opcode */
	op = get_next(insn_byte_t, insn);
ffffffff812cb2b9:	48 8b 53 60          	mov    0x60(%rbx),%rdx
ffffffff812cb2bd:	48 8d 42 01          	lea    0x1(%rdx),%rax
ffffffff812cb2c1:	48 3b 43 58          	cmp    0x58(%rbx),%rax
ffffffff812cb2c5:	0f 87 bf 00 00 00    	ja     ffffffff812cb38a <insn_get_opcode+0xf0>
#endif
}

static inline int insn_is_avx(struct insn *insn)
{
	if (!insn->prefixes.got)
ffffffff812cb2cb:	80 7b 04 00          	cmpb   $0x0,0x4(%rbx)
ffffffff812cb2cf:	44 8a 22             	mov    (%rdx),%r12b
ffffffff812cb2d2:	48 89 43 60          	mov    %rax,0x60(%rbx)
	opcode->bytes[0] = op;
	opcode->nbytes = 1;
ffffffff812cb2d6:	c6 43 1d 01          	movb   $0x1,0x1d(%rbx)
	if (!insn->prefixes.got)
		insn_get_prefixes(insn);

	/* Get first opcode */
	op = get_next(insn_byte_t, insn);
	opcode->bytes[0] = op;
ffffffff812cb2da:	44 88 63 18          	mov    %r12b,0x18(%rbx)
ffffffff812cb2de:	75 08                	jne    ffffffff812cb2e8 <insn_get_opcode+0x4e>
		insn_get_prefixes(insn);
ffffffff812cb2e0:	48 89 df             	mov    %rbx,%rdi
ffffffff812cb2e3:	e8 9d fd ff ff       	callq  ffffffff812cb085 <insn_get_prefixes>
	opcode->nbytes = 1;

	/* Check if there is VEX prefix or not */
	if (insn_is_avx(insn)) {
ffffffff812cb2e8:	83 7b 10 00          	cmpl   $0x0,0x10(%rbx)
ffffffff812cb2ec:	41 0f b6 fc          	movzbl %r12b,%edi
ffffffff812cb2f0:	74 42                	je     ffffffff812cb334 <insn_get_opcode+0x9a>
ffffffff812cb2f2:	8a 53 11             	mov    0x11(%rbx),%dl
ffffffff812cb2f5:	8a 4b 15             	mov    0x15(%rbx),%cl
static inline insn_byte_t insn_vex_m_bits(struct insn *insn)
{
	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
		return X86_VEX2_M;
	else
		return X86_VEX3_M(insn->vex_prefix.bytes[1]);
ffffffff812cb2f8:	40 b6 01             	mov    $0x1,%sil
ffffffff812cb2fb:	88 d0                	mov    %dl,%al
ffffffff812cb2fd:	83 e0 1f             	and    $0x1f,%eax
ffffffff812cb300:	80 f9 02             	cmp    $0x2,%cl
ffffffff812cb303:	0f 45 f0             	cmovne %eax,%esi
}

static inline insn_byte_t insn_vex_p_bits(struct insn *insn)
{
	if (insn->vex_prefix.nbytes == 2)	/* 2 bytes VEX */
		return X86_VEX_P(insn->vex_prefix.bytes[1]);
ffffffff812cb306:	8a 43 12             	mov    0x12(%rbx),%al
ffffffff812cb309:	83 e2 03             	and    $0x3,%edx
		insn_byte_t m, p;
		m = insn_vex_m_bits(insn);
		p = insn_vex_p_bits(insn);
		insn->attr = inat_get_avx_attribute(op, m, p);
ffffffff812cb30c:	40 0f b6 f6          	movzbl %sil,%esi
ffffffff812cb310:	83 e0 03             	and    $0x3,%eax
ffffffff812cb313:	80 f9 02             	cmp    $0x2,%cl
ffffffff812cb316:	0f 45 d0             	cmovne %eax,%edx
ffffffff812cb319:	0f b6 d2             	movzbl %dl,%edx
ffffffff812cb31c:	e8 55 0d 00 00       	callq  ffffffff812cc076 <inat_get_avx_attribute>
		if (!inat_accept_vex(insn->attr) && !inat_is_group(insn->attr))
ffffffff812cb321:	a9 c0 07 08 00       	test   $0x807c0,%eax
	/* Check if there is VEX prefix or not */
	if (insn_is_avx(insn)) {
		insn_byte_t m, p;
		m = insn_vex_m_bits(insn);
		p = insn_vex_p_bits(insn);
		insn->attr = inat_get_avx_attribute(op, m, p);
ffffffff812cb326:	89 43 48             	mov    %eax,0x48(%rbx)
		if (!inat_accept_vex(insn->attr) && !inat_is_group(insn->attr))
ffffffff812cb329:	75 5b                	jne    ffffffff812cb386 <insn_get_opcode+0xec>
			insn->attr = 0;	/* This instruction is bad */
ffffffff812cb32b:	c7 43 48 00 00 00 00 	movl   $0x0,0x48(%rbx)
ffffffff812cb332:	eb 52                	jmp    ffffffff812cb386 <insn_get_opcode+0xec>
		goto end;	/* VEX has only 1 byte for opcode */
	}

	insn->attr = inat_get_opcode_attribute(op);
ffffffff812cb334:	e8 66 0c 00 00       	callq  ffffffff812cbf9f <inat_get_opcode_attribute>
	while (inat_is_escape(insn->attr)) {
		/* Get escaped opcode */
		op = get_next(insn_byte_t, insn);
		opcode->bytes[opcode->nbytes++] = op;
		pfx_id = insn_last_prefix_id(insn);
		insn->attr = inat_get_escape_attribute(op, pfx_id, insn->attr);
ffffffff812cb339:	89 43 48             	mov    %eax,0x48(%rbx)
	return (attr & INAT_PFX_MASK) == INAT_PFX_VEX3;
}

static inline int inat_is_escape(insn_attr_t attr)
{
	return attr & INAT_ESC_MASK;
ffffffff812cb33c:	8b 43 48             	mov    0x48(%rbx),%eax
			insn->attr = 0;	/* This instruction is bad */
		goto end;	/* VEX has only 1 byte for opcode */
	}

	insn->attr = inat_get_opcode_attribute(op);
	while (inat_is_escape(insn->attr)) {
ffffffff812cb33f:	a8 30                	test   $0x30,%al
ffffffff812cb341:	74 3c                	je     ffffffff812cb37f <insn_get_opcode+0xe5>
		/* Get escaped opcode */
		op = get_next(insn_byte_t, insn);
ffffffff812cb343:	48 8b 53 60          	mov    0x60(%rbx),%rdx
ffffffff812cb347:	48 8d 42 01          	lea    0x1(%rdx),%rax
ffffffff812cb34b:	48 3b 43 58          	cmp    0x58(%rbx),%rax
ffffffff812cb34f:	77 39                	ja     ffffffff812cb38a <insn_get_opcode+0xf0>
ffffffff812cb351:	44 8a 22             	mov    (%rdx),%r12b
ffffffff812cb354:	48 89 43 60          	mov    %rax,0x60(%rbx)
		opcode->bytes[opcode->nbytes++] = op;
		pfx_id = insn_last_prefix_id(insn);
ffffffff812cb358:	48 89 df             	mov    %rbx,%rdi

	insn->attr = inat_get_opcode_attribute(op);
	while (inat_is_escape(insn->attr)) {
		/* Get escaped opcode */
		op = get_next(insn_byte_t, insn);
		opcode->bytes[opcode->nbytes++] = op;
ffffffff812cb35b:	0f b6 43 1d          	movzbl 0x1d(%rbx),%eax
ffffffff812cb35f:	8d 50 01             	lea    0x1(%rax),%edx
ffffffff812cb362:	88 53 1d             	mov    %dl,0x1d(%rbx)
ffffffff812cb365:	44 88 64 03 18       	mov    %r12b,0x18(%rbx,%rax,1)
		pfx_id = insn_last_prefix_id(insn);
ffffffff812cb36a:	e8 e6 fe ff ff       	callq  ffffffff812cb255 <insn_last_prefix_id>
		insn->attr = inat_get_escape_attribute(op, pfx_id, insn->attr);
ffffffff812cb36f:	8b 53 48             	mov    0x48(%rbx),%edx
ffffffff812cb372:	41 0f b6 fc          	movzbl %r12b,%edi
ffffffff812cb376:	89 c6                	mov    %eax,%esi
ffffffff812cb378:	e8 52 0c 00 00       	callq  ffffffff812cbfcf <inat_get_escape_attribute>
ffffffff812cb37d:	eb ba                	jmp    ffffffff812cb339 <insn_get_opcode+0x9f>
	}
	if (inat_must_vex(insn->attr))
ffffffff812cb37f:	a9 00 00 10 00       	test   $0x100000,%eax
ffffffff812cb384:	75 a5                	jne    ffffffff812cb32b <insn_get_opcode+0x91>
		insn->attr = 0;	/* This instruction is bad */
end:
	opcode->got = 1;
ffffffff812cb386:	c6 43 1c 01          	movb   $0x1,0x1c(%rbx)

err_out:
	return;
}
ffffffff812cb38a:	5b                   	pop    %rbx
ffffffff812cb38b:	41 5c                	pop    %r12
ffffffff812cb38d:	5d                   	pop    %rbp
ffffffff812cb38e:	c3                   	retq   

ffffffff812cb38f <insn_get_modrm>:
 */
void insn_get_modrm(struct insn *insn)
{
	struct insn_field *modrm = &insn->modrm;
	insn_byte_t pfx_id, mod;
	if (modrm->got)
ffffffff812cb38f:	80 7f 24 00          	cmpb   $0x0,0x24(%rdi)
ffffffff812cb393:	0f 85 94 00 00 00    	jne    ffffffff812cb42d <insn_get_modrm+0x9e>
 * Populates @insn->modrm and updates @insn->next_byte to point past the
 * ModRM byte, if any.  If necessary, first collects the preceding bytes
 * (prefixes and opcode(s)).  No effect if @insn->modrm.got is already 1.
 */
void insn_get_modrm(struct insn *insn)
{
ffffffff812cb399:	55                   	push   %rbp
ffffffff812cb39a:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb39d:	41 54                	push   %r12
ffffffff812cb39f:	53                   	push   %rbx
	struct insn_field *modrm = &insn->modrm;
	insn_byte_t pfx_id, mod;
	if (modrm->got)
		return;
	if (!insn->opcode.got)
ffffffff812cb3a0:	80 7f 1c 00          	cmpb   $0x0,0x1c(%rdi)
ffffffff812cb3a4:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cb3a7:	75 05                	jne    ffffffff812cb3ae <insn_get_modrm+0x1f>
		insn_get_opcode(insn);
ffffffff812cb3a9:	e8 ec fe ff ff       	callq  ffffffff812cb29a <insn_get_opcode>
	return (attr & INAT_IMM_MASK) >> INAT_IMM_OFFS;
}

static inline int inat_has_modrm(insn_attr_t attr)
{
	return attr & INAT_MODRM;
ffffffff812cb3ae:	8b 43 48             	mov    0x48(%rbx),%eax

	if (inat_has_modrm(insn->attr)) {
ffffffff812cb3b1:	f6 c4 40             	test   $0x40,%ah
ffffffff812cb3b4:	74 5f                	je     ffffffff812cb415 <insn_get_modrm+0x86>
		mod = get_next(insn_byte_t, insn);
ffffffff812cb3b6:	48 8b 4b 60          	mov    0x60(%rbx),%rcx
ffffffff812cb3ba:	48 8d 51 01          	lea    0x1(%rcx),%rdx
ffffffff812cb3be:	48 3b 53 58          	cmp    0x58(%rbx),%rdx
ffffffff812cb3c2:	77 65                	ja     ffffffff812cb429 <insn_get_modrm+0x9a>
ffffffff812cb3c4:	44 0f b6 21          	movzbl (%rcx),%r12d
		modrm->value = mod;
		modrm->nbytes = 1;
		if (inat_is_group(insn->attr)) {
ffffffff812cb3c8:	a9 c0 07 00 00       	test   $0x7c0,%eax
		return;
	if (!insn->opcode.got)
		insn_get_opcode(insn);

	if (inat_has_modrm(insn->attr)) {
		mod = get_next(insn_byte_t, insn);
ffffffff812cb3cd:	48 89 53 60          	mov    %rdx,0x60(%rbx)
		modrm->value = mod;
		modrm->nbytes = 1;
ffffffff812cb3d1:	c6 43 25 01          	movb   $0x1,0x25(%rbx)
	if (!insn->opcode.got)
		insn_get_opcode(insn);

	if (inat_has_modrm(insn->attr)) {
		mod = get_next(insn_byte_t, insn);
		modrm->value = mod;
ffffffff812cb3d5:	44 89 63 20          	mov    %r12d,0x20(%rbx)
		modrm->nbytes = 1;
		if (inat_is_group(insn->attr)) {
ffffffff812cb3d9:	74 3a                	je     ffffffff812cb415 <insn_get_modrm+0x86>
			pfx_id = insn_last_prefix_id(insn);
ffffffff812cb3db:	48 89 df             	mov    %rbx,%rdi
ffffffff812cb3de:	e8 72 fe ff ff       	callq  ffffffff812cb255 <insn_last_prefix_id>
			insn->attr = inat_get_group_attribute(mod, pfx_id,
ffffffff812cb3e3:	8b 53 48             	mov    0x48(%rbx),%edx
ffffffff812cb3e6:	0f b6 f0             	movzbl %al,%esi
ffffffff812cb3e9:	44 89 e7             	mov    %r12d,%edi
ffffffff812cb3ec:	e8 28 0c 00 00       	callq  ffffffff812cc019 <inat_get_group_attribute>
#endif
}

static inline int insn_is_avx(struct insn *insn)
{
	if (!insn->prefixes.got)
ffffffff812cb3f1:	80 7b 04 00          	cmpb   $0x0,0x4(%rbx)
ffffffff812cb3f5:	89 43 48             	mov    %eax,0x48(%rbx)
ffffffff812cb3f8:	75 08                	jne    ffffffff812cb402 <insn_get_modrm+0x73>
		insn_get_prefixes(insn);
ffffffff812cb3fa:	48 89 df             	mov    %rbx,%rdi
ffffffff812cb3fd:	e8 83 fc ff ff       	callq  ffffffff812cb085 <insn_get_prefixes>
							      insn->attr);
			if (insn_is_avx(insn) && !inat_accept_vex(insn->attr))
ffffffff812cb402:	83 7b 10 00          	cmpl   $0x0,0x10(%rbx)
ffffffff812cb406:	74 0d                	je     ffffffff812cb415 <insn_get_modrm+0x86>
ffffffff812cb408:	f6 43 4a 08          	testb  $0x8,0x4a(%rbx)
ffffffff812cb40c:	75 07                	jne    ffffffff812cb415 <insn_get_modrm+0x86>
				insn->attr = 0;	/* This is bad */
ffffffff812cb40e:	c7 43 48 00 00 00 00 	movl   $0x0,0x48(%rbx)
		}
	}

	if (insn->x86_64 && inat_is_force64(insn->attr))
ffffffff812cb415:	80 7b 4f 00          	cmpb   $0x0,0x4f(%rbx)
ffffffff812cb419:	74 0a                	je     ffffffff812cb425 <insn_get_modrm+0x96>
ffffffff812cb41b:	f6 43 49 80          	testb  $0x80,0x49(%rbx)
ffffffff812cb41f:	74 04                	je     ffffffff812cb425 <insn_get_modrm+0x96>
		insn->opnd_bytes = 8;
ffffffff812cb421:	c6 43 4c 08          	movb   $0x8,0x4c(%rbx)
	modrm->got = 1;
ffffffff812cb425:	c6 43 24 01          	movb   $0x1,0x24(%rbx)

err_out:
	return;
}
ffffffff812cb429:	5b                   	pop    %rbx
ffffffff812cb42a:	41 5c                	pop    %r12
ffffffff812cb42c:	5d                   	pop    %rbp
ffffffff812cb42d:	c3                   	retq   

ffffffff812cb42e <insn_rip_relative>:
 */
int insn_rip_relative(struct insn *insn)
{
	struct insn_field *modrm = &insn->modrm;

	if (!insn->x86_64)
ffffffff812cb42e:	80 7f 4f 00          	cmpb   $0x0,0x4f(%rdi)
ffffffff812cb432:	74 33                	je     ffffffff812cb467 <insn_rip_relative+0x39>
 *
 * If necessary, first collects the instruction up to and including the
 * ModRM byte.  No effect if @insn->x86_64 is 0.
 */
int insn_rip_relative(struct insn *insn)
{
ffffffff812cb434:	55                   	push   %rbp
ffffffff812cb435:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb438:	53                   	push   %rbx
ffffffff812cb439:	51                   	push   %rcx
	struct insn_field *modrm = &insn->modrm;

	if (!insn->x86_64)
		return 0;
	if (!modrm->got)
ffffffff812cb43a:	80 7f 24 00          	cmpb   $0x0,0x24(%rdi)
ffffffff812cb43e:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cb441:	75 05                	jne    ffffffff812cb448 <insn_rip_relative+0x1a>
		insn_get_modrm(insn);
ffffffff812cb443:	e8 47 ff ff ff       	callq  ffffffff812cb38f <insn_get_modrm>
	/*
	 * For rip-relative instructions, the mod field (top 2 bits)
	 * is zero and the r/m field (bottom 3 bits) is 0x5.
	 */
	return (modrm->nbytes && (modrm->value & 0xc7) == 0x5);
ffffffff812cb448:	80 7b 25 00          	cmpb   $0x0,0x25(%rbx)
ffffffff812cb44c:	74 13                	je     ffffffff812cb461 <insn_rip_relative+0x33>
ffffffff812cb44e:	8b 53 20             	mov    0x20(%rbx),%edx
ffffffff812cb451:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812cb456:	81 e2 c7 00 00 00    	and    $0xc7,%edx
ffffffff812cb45c:	83 fa 05             	cmp    $0x5,%edx
ffffffff812cb45f:	74 02                	je     ffffffff812cb463 <insn_rip_relative+0x35>
ffffffff812cb461:	31 c0                	xor    %eax,%eax
}
ffffffff812cb463:	5a                   	pop    %rdx
ffffffff812cb464:	5b                   	pop    %rbx
ffffffff812cb465:	5d                   	pop    %rbp
ffffffff812cb466:	c3                   	retq   
		insn_get_modrm(insn);
	/*
	 * For rip-relative instructions, the mod field (top 2 bits)
	 * is zero and the r/m field (bottom 3 bits) is 0x5.
	 */
	return (modrm->nbytes && (modrm->value & 0xc7) == 0x5);
ffffffff812cb467:	31 c0                	xor    %eax,%eax
}
ffffffff812cb469:	c3                   	retq   

ffffffff812cb46a <insn_get_sib>:
 */
void insn_get_sib(struct insn *insn)
{
	insn_byte_t modrm;

	if (insn->sib.got)
ffffffff812cb46a:	80 7f 2c 00          	cmpb   $0x0,0x2c(%rdi)
ffffffff812cb46e:	75 58                	jne    ffffffff812cb4c8 <insn_get_sib+0x5e>
 *
 * If necessary, first collects the instruction up to and including the
 * ModRM byte.
 */
void insn_get_sib(struct insn *insn)
{
ffffffff812cb470:	55                   	push   %rbp
ffffffff812cb471:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb474:	53                   	push   %rbx
ffffffff812cb475:	52                   	push   %rdx
	insn_byte_t modrm;

	if (insn->sib.got)
		return;
	if (!insn->modrm.got)
ffffffff812cb476:	80 7f 24 00          	cmpb   $0x0,0x24(%rdi)
ffffffff812cb47a:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cb47d:	75 05                	jne    ffffffff812cb484 <insn_get_sib+0x1a>
		insn_get_modrm(insn);
ffffffff812cb47f:	e8 0b ff ff ff       	callq  ffffffff812cb38f <insn_get_modrm>
	if (insn->modrm.nbytes) {
ffffffff812cb484:	80 7b 25 00          	cmpb   $0x0,0x25(%rbx)
ffffffff812cb488:	74 37                	je     ffffffff812cb4c1 <insn_get_sib+0x57>
		modrm = (insn_byte_t)insn->modrm.value;
		if (insn->addr_bytes != 2 &&
ffffffff812cb48a:	80 7b 4d 02          	cmpb   $0x2,0x4d(%rbx)
ffffffff812cb48e:	74 31                	je     ffffffff812cb4c1 <insn_get_sib+0x57>
	if (insn->sib.got)
		return;
	if (!insn->modrm.got)
		insn_get_modrm(insn);
	if (insn->modrm.nbytes) {
		modrm = (insn_byte_t)insn->modrm.value;
ffffffff812cb490:	8b 43 20             	mov    0x20(%rbx),%eax
		if (insn->addr_bytes != 2 &&
ffffffff812cb493:	88 c2                	mov    %al,%dl
ffffffff812cb495:	c0 ea 06             	shr    $0x6,%dl
ffffffff812cb498:	80 fa 03             	cmp    $0x3,%dl
ffffffff812cb49b:	74 24                	je     ffffffff812cb4c1 <insn_get_sib+0x57>
		    X86_MODRM_MOD(modrm) != 3 && X86_MODRM_RM(modrm) == 4) {
ffffffff812cb49d:	83 e0 07             	and    $0x7,%eax
ffffffff812cb4a0:	83 f8 04             	cmp    $0x4,%eax
ffffffff812cb4a3:	75 1c                	jne    ffffffff812cb4c1 <insn_get_sib+0x57>
			insn->sib.value = get_next(insn_byte_t, insn);
ffffffff812cb4a5:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb4a9:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812cb4ad:	48 3b 53 58          	cmp    0x58(%rbx),%rdx
ffffffff812cb4b1:	77 12                	ja     ffffffff812cb4c5 <insn_get_sib+0x5b>
ffffffff812cb4b3:	0f b6 00             	movzbl (%rax),%eax
ffffffff812cb4b6:	48 89 53 60          	mov    %rdx,0x60(%rbx)
			insn->sib.nbytes = 1;
ffffffff812cb4ba:	c6 43 2d 01          	movb   $0x1,0x2d(%rbx)
		insn_get_modrm(insn);
	if (insn->modrm.nbytes) {
		modrm = (insn_byte_t)insn->modrm.value;
		if (insn->addr_bytes != 2 &&
		    X86_MODRM_MOD(modrm) != 3 && X86_MODRM_RM(modrm) == 4) {
			insn->sib.value = get_next(insn_byte_t, insn);
ffffffff812cb4be:	89 43 28             	mov    %eax,0x28(%rbx)
			insn->sib.nbytes = 1;
		}
	}
	insn->sib.got = 1;
ffffffff812cb4c1:	c6 43 2c 01          	movb   $0x1,0x2c(%rbx)

err_out:
	return;
}
ffffffff812cb4c5:	58                   	pop    %rax
ffffffff812cb4c6:	5b                   	pop    %rbx
ffffffff812cb4c7:	5d                   	pop    %rbp
ffffffff812cb4c8:	c3                   	retq   

ffffffff812cb4c9 <insn_get_displacement>:
 */
void insn_get_displacement(struct insn *insn)
{
	insn_byte_t mod, rm, base;

	if (insn->displacement.got)
ffffffff812cb4c9:	80 7f 34 00          	cmpb   $0x0,0x34(%rdi)
ffffffff812cb4cd:	0f 85 df 00 00 00    	jne    ffffffff812cb5b2 <insn_get_displacement+0xe9>
 * If necessary, first collects the instruction up to and including the
 * SIB byte.
 * Displacement value is sign-expanded.
 */
void insn_get_displacement(struct insn *insn)
{
ffffffff812cb4d3:	55                   	push   %rbp
ffffffff812cb4d4:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb4d7:	53                   	push   %rbx
ffffffff812cb4d8:	52                   	push   %rdx
	insn_byte_t mod, rm, base;

	if (insn->displacement.got)
		return;
	if (!insn->sib.got)
ffffffff812cb4d9:	80 7f 2c 00          	cmpb   $0x0,0x2c(%rdi)
ffffffff812cb4dd:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cb4e0:	75 05                	jne    ffffffff812cb4e7 <insn_get_displacement+0x1e>
		insn_get_sib(insn);
ffffffff812cb4e2:	e8 83 ff ff ff       	callq  ffffffff812cb46a <insn_get_sib>
	if (insn->modrm.nbytes) {
ffffffff812cb4e7:	80 7b 25 00          	cmpb   $0x0,0x25(%rbx)
ffffffff812cb4eb:	0f 84 ba 00 00 00    	je     ffffffff812cb5ab <insn_get_displacement+0xe2>
		 * mod != 11, r/m = 100 - SIB byte exists
		 * mod = 00, SIB base = 101 - displacement field is 4 bytes
		 * mod = 00, r/m = 101 - rip-relative addressing, displacement
		 * 	field is 4 bytes
		 */
		mod = X86_MODRM_MOD(insn->modrm.value);
ffffffff812cb4f1:	8b 53 20             	mov    0x20(%rbx),%edx
ffffffff812cb4f4:	89 d0                	mov    %edx,%eax
ffffffff812cb4f6:	25 c0 00 00 00       	and    $0xc0,%eax
ffffffff812cb4fb:	c1 f8 06             	sar    $0x6,%eax
		rm = X86_MODRM_RM(insn->modrm.value);
		base = X86_SIB_BASE(insn->sib.value);
		if (mod == 3)
ffffffff812cb4fe:	83 f8 03             	cmp    $0x3,%eax
ffffffff812cb501:	0f 84 a4 00 00 00    	je     ffffffff812cb5ab <insn_get_displacement+0xe2>
			goto out;
		if (mod == 1) {
ffffffff812cb507:	83 f8 01             	cmp    $0x1,%eax
ffffffff812cb50a:	75 22                	jne    ffffffff812cb52e <insn_get_displacement+0x65>
			insn->displacement.value = get_next(char, insn);
ffffffff812cb50c:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb510:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812cb514:	48 3b 53 58          	cmp    0x58(%rbx),%rdx
ffffffff812cb518:	0f 87 91 00 00 00    	ja     ffffffff812cb5af <insn_get_displacement+0xe6>
ffffffff812cb51e:	0f be 00             	movsbl (%rax),%eax
ffffffff812cb521:	48 89 53 60          	mov    %rdx,0x60(%rbx)
			insn->displacement.nbytes = 1;
ffffffff812cb525:	c6 43 35 01          	movb   $0x1,0x35(%rbx)
		rm = X86_MODRM_RM(insn->modrm.value);
		base = X86_SIB_BASE(insn->sib.value);
		if (mod == 3)
			goto out;
		if (mod == 1) {
			insn->displacement.value = get_next(char, insn);
ffffffff812cb529:	89 43 30             	mov    %eax,0x30(%rbx)
ffffffff812cb52c:	eb 7d                	jmp    ffffffff812cb5ab <insn_get_displacement+0xe2>
		 * mod = 00, SIB base = 101 - displacement field is 4 bytes
		 * mod = 00, r/m = 101 - rip-relative addressing, displacement
		 * 	field is 4 bytes
		 */
		mod = X86_MODRM_MOD(insn->modrm.value);
		rm = X86_MODRM_RM(insn->modrm.value);
ffffffff812cb52e:	83 e2 07             	and    $0x7,%edx
		if (mod == 3)
			goto out;
		if (mod == 1) {
			insn->displacement.value = get_next(char, insn);
			insn->displacement.nbytes = 1;
		} else if (insn->addr_bytes == 2) {
ffffffff812cb531:	80 7b 4d 02          	cmpb   $0x2,0x4d(%rbx)
ffffffff812cb535:	75 32                	jne    ffffffff812cb569 <insn_get_displacement+0xa0>
			if ((mod == 0 && rm == 6) || mod == 2) {
ffffffff812cb537:	84 c0                	test   %al,%al
ffffffff812cb539:	0f 94 c1             	sete   %cl
ffffffff812cb53c:	80 fa 06             	cmp    $0x6,%dl
ffffffff812cb53f:	0f 94 c2             	sete   %dl
ffffffff812cb542:	84 d1                	test   %dl,%cl
ffffffff812cb544:	75 05                	jne    ffffffff812cb54b <insn_get_displacement+0x82>
ffffffff812cb546:	83 f8 02             	cmp    $0x2,%eax
ffffffff812cb549:	75 60                	jne    ffffffff812cb5ab <insn_get_displacement+0xe2>
				insn->displacement.value =
					 get_next(short, insn);
ffffffff812cb54b:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb54f:	48 8d 50 02          	lea    0x2(%rax),%rdx
ffffffff812cb553:	48 3b 53 58          	cmp    0x58(%rbx),%rdx
ffffffff812cb557:	77 56                	ja     ffffffff812cb5af <insn_get_displacement+0xe6>
ffffffff812cb559:	0f bf 00             	movswl (%rax),%eax
ffffffff812cb55c:	48 89 53 60          	mov    %rdx,0x60(%rbx)
				insn->displacement.nbytes = 2;
ffffffff812cb560:	c6 43 35 02          	movb   $0x2,0x35(%rbx)
		if (mod == 1) {
			insn->displacement.value = get_next(char, insn);
			insn->displacement.nbytes = 1;
		} else if (insn->addr_bytes == 2) {
			if ((mod == 0 && rm == 6) || mod == 2) {
				insn->displacement.value =
ffffffff812cb564:	89 43 30             	mov    %eax,0x30(%rbx)
ffffffff812cb567:	eb 42                	jmp    ffffffff812cb5ab <insn_get_displacement+0xe2>
					 get_next(short, insn);
				insn->displacement.nbytes = 2;
			}
		} else {
			if ((mod == 0 && rm == 5) || mod == 2 ||
ffffffff812cb569:	84 c0                	test   %al,%al
		 * mod = 00, r/m = 101 - rip-relative addressing, displacement
		 * 	field is 4 bytes
		 */
		mod = X86_MODRM_MOD(insn->modrm.value);
		rm = X86_MODRM_RM(insn->modrm.value);
		base = X86_SIB_BASE(insn->sib.value);
ffffffff812cb56b:	8b 4b 28             	mov    0x28(%rbx),%ecx
				insn->displacement.value =
					 get_next(short, insn);
				insn->displacement.nbytes = 2;
			}
		} else {
			if ((mod == 0 && rm == 5) || mod == 2 ||
ffffffff812cb56e:	40 0f 94 c6          	sete   %sil
ffffffff812cb572:	80 fa 05             	cmp    $0x5,%dl
ffffffff812cb575:	0f 94 c2             	sete   %dl
ffffffff812cb578:	40 84 f2             	test   %sil,%dl
ffffffff812cb57b:	75 13                	jne    ffffffff812cb590 <insn_get_displacement+0xc7>
ffffffff812cb57d:	83 f8 02             	cmp    $0x2,%eax
ffffffff812cb580:	74 0e                	je     ffffffff812cb590 <insn_get_displacement+0xc7>
ffffffff812cb582:	88 c8                	mov    %cl,%al
ffffffff812cb584:	83 e0 07             	and    $0x7,%eax
ffffffff812cb587:	3c 05                	cmp    $0x5,%al
ffffffff812cb589:	75 20                	jne    ffffffff812cb5ab <insn_get_displacement+0xe2>
ffffffff812cb58b:	40 84 f6             	test   %sil,%sil
ffffffff812cb58e:	74 1b                	je     ffffffff812cb5ab <insn_get_displacement+0xe2>
			    (mod == 0 && base == 5)) {
				insn->displacement.value = get_next(int, insn);
ffffffff812cb590:	48 8b 53 60          	mov    0x60(%rbx),%rdx
ffffffff812cb594:	48 8d 42 04          	lea    0x4(%rdx),%rax
ffffffff812cb598:	48 3b 43 58          	cmp    0x58(%rbx),%rax
ffffffff812cb59c:	77 11                	ja     ffffffff812cb5af <insn_get_displacement+0xe6>
ffffffff812cb59e:	8b 12                	mov    (%rdx),%edx
ffffffff812cb5a0:	48 89 43 60          	mov    %rax,0x60(%rbx)
				insn->displacement.nbytes = 4;
ffffffff812cb5a4:	c6 43 35 04          	movb   $0x4,0x35(%rbx)
				insn->displacement.nbytes = 2;
			}
		} else {
			if ((mod == 0 && rm == 5) || mod == 2 ||
			    (mod == 0 && base == 5)) {
				insn->displacement.value = get_next(int, insn);
ffffffff812cb5a8:	89 53 30             	mov    %edx,0x30(%rbx)
				insn->displacement.nbytes = 4;
			}
		}
	}
out:
	insn->displacement.got = 1;
ffffffff812cb5ab:	c6 43 34 01          	movb   $0x1,0x34(%rbx)

err_out:
	return;
}
ffffffff812cb5af:	58                   	pop    %rax
ffffffff812cb5b0:	5b                   	pop    %rbx
ffffffff812cb5b1:	5d                   	pop    %rbp
ffffffff812cb5b2:	c3                   	retq   

ffffffff812cb5b3 <insn_get_immediate>:
 * Basically, most of immediates are sign-expanded. Unsigned-value can be
 * get by bit masking with ((1 << (nbytes * 8)) - 1)
 */
void insn_get_immediate(struct insn *insn)
{
	if (insn->immediate.got)
ffffffff812cb5b3:	80 7f 3c 00          	cmpb   $0x0,0x3c(%rdi)
ffffffff812cb5b7:	0f 85 d4 02 00 00    	jne    ffffffff812cb891 <insn_get_immediate+0x2de>
 * displacement bytes.
 * Basically, most of immediates are sign-expanded. Unsigned-value can be
 * get by bit masking with ((1 << (nbytes * 8)) - 1)
 */
void insn_get_immediate(struct insn *insn)
{
ffffffff812cb5bd:	55                   	push   %rbp
ffffffff812cb5be:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb5c1:	53                   	push   %rbx
ffffffff812cb5c2:	52                   	push   %rdx
	if (insn->immediate.got)
		return;
	if (!insn->displacement.got)
ffffffff812cb5c3:	80 7f 34 00          	cmpb   $0x0,0x34(%rdi)
ffffffff812cb5c7:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cb5ca:	75 05                	jne    ffffffff812cb5d1 <insn_get_immediate+0x1e>
		insn_get_displacement(insn);
ffffffff812cb5cc:	e8 f8 fe ff ff       	callq  ffffffff812cb4c9 <insn_get_displacement>

	if (inat_has_moffset(insn->attr)) {
ffffffff812cb5d1:	8b 53 48             	mov    0x48(%rbx),%edx
ffffffff812cb5d4:	f7 c2 00 00 02 00    	test   $0x20000,%edx
ffffffff812cb5da:	0f 84 9c 00 00 00    	je     ffffffff812cb67c <insn_get_immediate+0xc9>
}

/* Decode moffset16/32/64. Return 0 if failed */
static int __get_moffset(struct insn *insn)
{
	switch (insn->addr_bytes) {
ffffffff812cb5e0:	8a 43 4d             	mov    0x4d(%rbx),%al
ffffffff812cb5e3:	3c 04                	cmp    $0x4,%al
ffffffff812cb5e5:	74 2e                	je     ffffffff812cb615 <insn_get_immediate+0x62>
ffffffff812cb5e7:	3c 08                	cmp    $0x8,%al
ffffffff812cb5e9:	74 4b                	je     ffffffff812cb636 <insn_get_immediate+0x83>
ffffffff812cb5eb:	3c 02                	cmp    $0x2,%al
ffffffff812cb5ed:	0f 85 9b 02 00 00    	jne    ffffffff812cb88e <insn_get_immediate+0x2db>
	case 2:
		insn->moffset1.value = get_next(short, insn);
ffffffff812cb5f3:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb5f7:	48 8d 50 02          	lea    0x2(%rax),%rdx
ffffffff812cb5fb:	48 3b 53 58          	cmp    0x58(%rbx),%rdx
ffffffff812cb5ff:	0f 87 89 02 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb605:	0f bf 00             	movswl (%rax),%eax
ffffffff812cb608:	48 89 53 60          	mov    %rdx,0x60(%rbx)
		insn->moffset1.nbytes = 2;
ffffffff812cb60c:	c6 43 3d 02          	movb   $0x2,0x3d(%rbx)
/* Decode moffset16/32/64. Return 0 if failed */
static int __get_moffset(struct insn *insn)
{
	switch (insn->addr_bytes) {
	case 2:
		insn->moffset1.value = get_next(short, insn);
ffffffff812cb610:	89 43 38             	mov    %eax,0x38(%rbx)
ffffffff812cb613:	eb 5e                	jmp    ffffffff812cb673 <insn_get_immediate+0xc0>
		insn->moffset1.nbytes = 2;
		break;
	case 4:
		insn->moffset1.value = get_next(int, insn);
ffffffff812cb615:	48 8b 53 60          	mov    0x60(%rbx),%rdx
ffffffff812cb619:	48 8d 42 04          	lea    0x4(%rdx),%rax
ffffffff812cb61d:	48 3b 43 58          	cmp    0x58(%rbx),%rax
ffffffff812cb621:	0f 87 67 02 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb627:	8b 12                	mov    (%rdx),%edx
ffffffff812cb629:	48 89 43 60          	mov    %rax,0x60(%rbx)
		insn->moffset1.nbytes = 4;
ffffffff812cb62d:	c6 43 3d 04          	movb   $0x4,0x3d(%rbx)
	case 2:
		insn->moffset1.value = get_next(short, insn);
		insn->moffset1.nbytes = 2;
		break;
	case 4:
		insn->moffset1.value = get_next(int, insn);
ffffffff812cb631:	89 53 38             	mov    %edx,0x38(%rbx)
ffffffff812cb634:	eb 3d                	jmp    ffffffff812cb673 <insn_get_immediate+0xc0>
		insn->moffset1.nbytes = 4;
		break;
	case 8:
		insn->moffset1.value = get_next(int, insn);
ffffffff812cb636:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb63a:	48 8b 4b 58          	mov    0x58(%rbx),%rcx
ffffffff812cb63e:	48 8d 50 04          	lea    0x4(%rax),%rdx
ffffffff812cb642:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812cb645:	0f 87 43 02 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb64b:	8b 30                	mov    (%rax),%esi
ffffffff812cb64d:	48 89 53 60          	mov    %rdx,0x60(%rbx)
		insn->moffset1.nbytes = 4;
		insn->moffset2.value = get_next(int, insn);
ffffffff812cb651:	48 8d 50 08          	lea    0x8(%rax),%rdx
		insn->moffset1.value = get_next(int, insn);
		insn->moffset1.nbytes = 4;
		break;
	case 8:
		insn->moffset1.value = get_next(int, insn);
		insn->moffset1.nbytes = 4;
ffffffff812cb655:	c6 43 3d 04          	movb   $0x4,0x3d(%rbx)
		insn->moffset2.value = get_next(int, insn);
ffffffff812cb659:	48 39 d1             	cmp    %rdx,%rcx
	case 4:
		insn->moffset1.value = get_next(int, insn);
		insn->moffset1.nbytes = 4;
		break;
	case 8:
		insn->moffset1.value = get_next(int, insn);
ffffffff812cb65c:	89 73 38             	mov    %esi,0x38(%rbx)
		insn->moffset1.nbytes = 4;
		insn->moffset2.value = get_next(int, insn);
ffffffff812cb65f:	0f 82 29 02 00 00    	jb     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb665:	8b 40 04             	mov    0x4(%rax),%eax
ffffffff812cb668:	48 89 53 60          	mov    %rdx,0x60(%rbx)
		insn->moffset2.nbytes = 4;
ffffffff812cb66c:	c6 43 45 04          	movb   $0x4,0x45(%rbx)
		insn->moffset1.nbytes = 4;
		break;
	case 8:
		insn->moffset1.value = get_next(int, insn);
		insn->moffset1.nbytes = 4;
		insn->moffset2.value = get_next(int, insn);
ffffffff812cb670:	89 43 40             	mov    %eax,0x40(%rbx)
		insn->moffset2.nbytes = 4;
		break;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}
	insn->moffset1.got = insn->moffset2.got = 1;
ffffffff812cb673:	c6 43 44 01          	movb   $0x1,0x44(%rbx)
ffffffff812cb677:	e9 0e 02 00 00       	jmpq   ffffffff812cb88a <insn_get_immediate+0x2d7>
		if (!__get_moffset(insn))
			goto err_out;
		goto done;
	}

	if (!inat_has_immediate(insn->attr))
ffffffff812cb67c:	89 d0                	mov    %edx,%eax
ffffffff812cb67e:	25 00 38 00 00       	and    $0x3800,%eax
ffffffff812cb683:	0f 84 01 02 00 00    	je     ffffffff812cb88a <insn_get_immediate+0x2d7>
		/* no immediates */
		goto done;

	switch (inat_immediate_size(insn->attr)) {
ffffffff812cb689:	c1 e8 0b             	shr    $0xb,%eax
ffffffff812cb68c:	ff c8                	dec    %eax
ffffffff812cb68e:	83 f8 06             	cmp    $0x6,%eax
ffffffff812cb691:	0f 87 f7 01 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb697:	ff 24 c5 48 f5 63 81 	jmpq   *-0x7e9c0ab8(,%rax,8)
	case INAT_IMM_BYTE:
		insn->immediate.value = get_next(char, insn);
ffffffff812cb69e:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb6a2:	48 8d 48 01          	lea    0x1(%rax),%rcx
ffffffff812cb6a6:	48 3b 4b 58          	cmp    0x58(%rbx),%rcx
ffffffff812cb6aa:	0f 87 de 01 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb6b0:	0f be 00             	movsbl (%rax),%eax
ffffffff812cb6b3:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate.nbytes = 1;
ffffffff812cb6b7:	c6 43 3d 01          	movb   $0x1,0x3d(%rbx)
		/* no immediates */
		goto done;

	switch (inat_immediate_size(insn->attr)) {
	case INAT_IMM_BYTE:
		insn->immediate.value = get_next(char, insn);
ffffffff812cb6bb:	89 43 38             	mov    %eax,0x38(%rbx)
ffffffff812cb6be:	e9 a3 01 00 00       	jmpq   ffffffff812cb866 <insn_get_immediate+0x2b3>
	case INAT_IMM_DWORD:
		insn->immediate.value = get_next(int, insn);
		insn->immediate.nbytes = 4;
		break;
	case INAT_IMM_QWORD:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb6c3:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb6c7:	48 8b 73 58          	mov    0x58(%rbx),%rsi
ffffffff812cb6cb:	48 8d 48 04          	lea    0x4(%rax),%rcx
ffffffff812cb6cf:	48 39 f1             	cmp    %rsi,%rcx
ffffffff812cb6d2:	0f 87 b6 01 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb6d8:	8b 38                	mov    (%rax),%edi
ffffffff812cb6da:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate1.nbytes = 4;
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb6de:	48 8d 48 08          	lea    0x8(%rax),%rcx
		insn->immediate.value = get_next(int, insn);
		insn->immediate.nbytes = 4;
		break;
	case INAT_IMM_QWORD:
		insn->immediate1.value = get_next(int, insn);
		insn->immediate1.nbytes = 4;
ffffffff812cb6e2:	c6 43 3d 04          	movb   $0x4,0x3d(%rbx)
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb6e6:	48 39 ce             	cmp    %rcx,%rsi
	case INAT_IMM_DWORD:
		insn->immediate.value = get_next(int, insn);
		insn->immediate.nbytes = 4;
		break;
	case INAT_IMM_QWORD:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb6e9:	89 7b 38             	mov    %edi,0x38(%rbx)
		insn->immediate1.nbytes = 4;
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb6ec:	0f 82 9c 01 00 00    	jb     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb6f2:	8b 40 04             	mov    0x4(%rax),%eax
ffffffff812cb6f5:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate2.nbytes = 4;
ffffffff812cb6f9:	c6 43 45 04          	movb   $0x4,0x45(%rbx)
		insn->immediate.nbytes = 4;
		break;
	case INAT_IMM_QWORD:
		insn->immediate1.value = get_next(int, insn);
		insn->immediate1.nbytes = 4;
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb6fd:	89 43 40             	mov    %eax,0x40(%rbx)
ffffffff812cb700:	e9 61 01 00 00       	jmpq   ffffffff812cb866 <insn_get_immediate+0x2b3>
}

/* Decode ptr16:16/32(Ap) */
static int __get_immptr(struct insn *insn)
{
	switch (insn->opnd_bytes) {
ffffffff812cb705:	8a 43 4c             	mov    0x4c(%rbx),%al
ffffffff812cb708:	3c 02                	cmp    $0x2,%al
ffffffff812cb70a:	74 09                	je     ffffffff812cb715 <insn_get_immediate+0x162>
ffffffff812cb70c:	3c 04                	cmp    $0x4,%al
ffffffff812cb70e:	74 27                	je     ffffffff812cb737 <insn_get_immediate+0x184>
ffffffff812cb710:	e9 79 01 00 00       	jmpq   ffffffff812cb88e <insn_get_immediate+0x2db>
	case 2:
		insn->immediate1.value = get_next(short, insn);
ffffffff812cb715:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb719:	48 8d 48 02          	lea    0x2(%rax),%rcx
ffffffff812cb71d:	48 3b 4b 58          	cmp    0x58(%rbx),%rcx
ffffffff812cb721:	0f 87 67 01 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb727:	0f bf 00             	movswl (%rax),%eax
ffffffff812cb72a:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate1.nbytes = 2;
ffffffff812cb72e:	c6 43 3d 02          	movb   $0x2,0x3d(%rbx)
/* Decode ptr16:16/32(Ap) */
static int __get_immptr(struct insn *insn)
{
	switch (insn->opnd_bytes) {
	case 2:
		insn->immediate1.value = get_next(short, insn);
ffffffff812cb732:	89 43 38             	mov    %eax,0x38(%rbx)
ffffffff812cb735:	eb 1f                	jmp    ffffffff812cb756 <insn_get_immediate+0x1a3>
		insn->immediate1.nbytes = 2;
		break;
	case 4:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb737:	48 8b 4b 60          	mov    0x60(%rbx),%rcx
ffffffff812cb73b:	48 8d 41 04          	lea    0x4(%rcx),%rax
ffffffff812cb73f:	48 3b 43 58          	cmp    0x58(%rbx),%rax
ffffffff812cb743:	0f 87 45 01 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb749:	8b 09                	mov    (%rcx),%ecx
ffffffff812cb74b:	48 89 43 60          	mov    %rax,0x60(%rbx)
		insn->immediate1.nbytes = 4;
ffffffff812cb74f:	c6 43 3d 04          	movb   $0x4,0x3d(%rbx)
	case 2:
		insn->immediate1.value = get_next(short, insn);
		insn->immediate1.nbytes = 2;
		break;
	case 4:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb753:	89 4b 38             	mov    %ecx,0x38(%rbx)
		/* ptr16:64 is not exist (no segment) */
		return 0;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}
	insn->immediate2.value = get_next(unsigned short, insn);
ffffffff812cb756:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb75a:	48 8d 48 02          	lea    0x2(%rax),%rcx
ffffffff812cb75e:	48 3b 4b 58          	cmp    0x58(%rbx),%rcx
ffffffff812cb762:	0f 87 26 01 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb768:	0f b7 00             	movzwl (%rax),%eax
ffffffff812cb76b:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
	insn->immediate2.nbytes = 2;
ffffffff812cb76f:	c6 43 45 02          	movb   $0x2,0x45(%rbx)
		/* ptr16:64 is not exist (no segment) */
		return 0;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}
	insn->immediate2.value = get_next(unsigned short, insn);
ffffffff812cb773:	89 43 40             	mov    %eax,0x40(%rbx)
ffffffff812cb776:	e9 e3 00 00 00       	jmpq   ffffffff812cb85e <insn_get_immediate+0x2ab>
}

/* Decode imm v32(Iz). Return 0 if failed */
static int __get_immv32(struct insn *insn)
{
	switch (insn->opnd_bytes) {
ffffffff812cb77b:	8a 43 4c             	mov    0x4c(%rbx),%al
ffffffff812cb77e:	3c 04                	cmp    $0x4,%al
ffffffff812cb780:	74 31                	je     ffffffff812cb7b3 <insn_get_immediate+0x200>
ffffffff812cb782:	3c 08                	cmp    $0x8,%al
ffffffff812cb784:	74 2d                	je     ffffffff812cb7b3 <insn_get_immediate+0x200>
ffffffff812cb786:	3c 02                	cmp    $0x2,%al
ffffffff812cb788:	0f 85 00 01 00 00    	jne    ffffffff812cb88e <insn_get_immediate+0x2db>
	case 2:
		insn->immediate.value = get_next(short, insn);
ffffffff812cb78e:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb792:	48 8d 48 02          	lea    0x2(%rax),%rcx
ffffffff812cb796:	48 3b 4b 58          	cmp    0x58(%rbx),%rcx
ffffffff812cb79a:	0f 87 ee 00 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb7a0:	0f bf 00             	movswl (%rax),%eax
ffffffff812cb7a3:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate.nbytes = 2;
ffffffff812cb7a7:	c6 43 3d 02          	movb   $0x2,0x3d(%rbx)
/* Decode imm v32(Iz). Return 0 if failed */
static int __get_immv32(struct insn *insn)
{
	switch (insn->opnd_bytes) {
	case 2:
		insn->immediate.value = get_next(short, insn);
ffffffff812cb7ab:	89 43 38             	mov    %eax,0x38(%rbx)
ffffffff812cb7ae:	e9 b3 00 00 00       	jmpq   ffffffff812cb866 <insn_get_immediate+0x2b3>
		insn->immediate.nbytes = 2;
		break;
	case 4:
	case 8:
		insn->immediate.value = get_next(int, insn);
ffffffff812cb7b3:	48 8b 4b 60          	mov    0x60(%rbx),%rcx
ffffffff812cb7b7:	48 8d 41 04          	lea    0x4(%rcx),%rax
ffffffff812cb7bb:	48 3b 43 58          	cmp    0x58(%rbx),%rax
ffffffff812cb7bf:	0f 87 c9 00 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb7c5:	8b 09                	mov    (%rcx),%ecx
ffffffff812cb7c7:	48 89 43 60          	mov    %rax,0x60(%rbx)
		insn->immediate.nbytes = 4;
ffffffff812cb7cb:	c6 43 3d 04          	movb   $0x4,0x3d(%rbx)
		insn->immediate.value = get_next(short, insn);
		insn->immediate.nbytes = 2;
		break;
	case 4:
	case 8:
		insn->immediate.value = get_next(int, insn);
ffffffff812cb7cf:	89 4b 38             	mov    %ecx,0x38(%rbx)
ffffffff812cb7d2:	e9 8f 00 00 00       	jmpq   ffffffff812cb866 <insn_get_immediate+0x2b3>
}

/* Decode imm v64(Iv/Ov), Return 0 if failed */
static int __get_immv(struct insn *insn)
{
	switch (insn->opnd_bytes) {
ffffffff812cb7d7:	8a 43 4c             	mov    0x4c(%rbx),%al
ffffffff812cb7da:	3c 04                	cmp    $0x4,%al
ffffffff812cb7dc:	74 2e                	je     ffffffff812cb80c <insn_get_immediate+0x259>
ffffffff812cb7de:	3c 08                	cmp    $0x8,%al
ffffffff812cb7e0:	74 47                	je     ffffffff812cb829 <insn_get_immediate+0x276>
ffffffff812cb7e2:	3c 02                	cmp    $0x2,%al
ffffffff812cb7e4:	0f 85 a4 00 00 00    	jne    ffffffff812cb88e <insn_get_immediate+0x2db>
	case 2:
		insn->immediate1.value = get_next(short, insn);
ffffffff812cb7ea:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb7ee:	48 8d 48 02          	lea    0x2(%rax),%rcx
ffffffff812cb7f2:	48 3b 4b 58          	cmp    0x58(%rbx),%rcx
ffffffff812cb7f6:	0f 87 92 00 00 00    	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb7fc:	0f bf 00             	movswl (%rax),%eax
ffffffff812cb7ff:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate1.nbytes = 2;
ffffffff812cb803:	c6 43 3d 02          	movb   $0x2,0x3d(%rbx)
/* Decode imm v64(Iv/Ov), Return 0 if failed */
static int __get_immv(struct insn *insn)
{
	switch (insn->opnd_bytes) {
	case 2:
		insn->immediate1.value = get_next(short, insn);
ffffffff812cb807:	89 43 38             	mov    %eax,0x38(%rbx)
ffffffff812cb80a:	eb 52                	jmp    ffffffff812cb85e <insn_get_immediate+0x2ab>
		insn->immediate1.nbytes = 2;
		break;
	case 4:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb80c:	48 8b 4b 60          	mov    0x60(%rbx),%rcx
ffffffff812cb810:	48 8d 41 04          	lea    0x4(%rcx),%rax
ffffffff812cb814:	48 3b 43 58          	cmp    0x58(%rbx),%rax
ffffffff812cb818:	77 74                	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb81a:	8b 09                	mov    (%rcx),%ecx
ffffffff812cb81c:	48 89 43 60          	mov    %rax,0x60(%rbx)
		insn->immediate1.nbytes = 4;
ffffffff812cb820:	c6 43 3d 04          	movb   $0x4,0x3d(%rbx)
	case 2:
		insn->immediate1.value = get_next(short, insn);
		insn->immediate1.nbytes = 2;
		break;
	case 4:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb824:	89 4b 38             	mov    %ecx,0x38(%rbx)
ffffffff812cb827:	eb 35                	jmp    ffffffff812cb85e <insn_get_immediate+0x2ab>
		insn->immediate1.nbytes = 4;
		break;
	case 8:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb829:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb82d:	48 8b 73 58          	mov    0x58(%rbx),%rsi
ffffffff812cb831:	48 8d 48 04          	lea    0x4(%rax),%rcx
ffffffff812cb835:	48 39 f1             	cmp    %rsi,%rcx
ffffffff812cb838:	77 54                	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb83a:	8b 38                	mov    (%rax),%edi
ffffffff812cb83c:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate1.nbytes = 4;
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb840:	48 8d 48 08          	lea    0x8(%rax),%rcx
		insn->immediate1.value = get_next(int, insn);
		insn->immediate1.nbytes = 4;
		break;
	case 8:
		insn->immediate1.value = get_next(int, insn);
		insn->immediate1.nbytes = 4;
ffffffff812cb844:	c6 43 3d 04          	movb   $0x4,0x3d(%rbx)
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb848:	48 39 ce             	cmp    %rcx,%rsi
	case 4:
		insn->immediate1.value = get_next(int, insn);
		insn->immediate1.nbytes = 4;
		break;
	case 8:
		insn->immediate1.value = get_next(int, insn);
ffffffff812cb84b:	89 7b 38             	mov    %edi,0x38(%rbx)
		insn->immediate1.nbytes = 4;
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb84e:	72 3e                	jb     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb850:	8b 40 04             	mov    0x4(%rax),%eax
ffffffff812cb853:	48 89 4b 60          	mov    %rcx,0x60(%rbx)
		insn->immediate2.nbytes = 4;
ffffffff812cb857:	c6 43 45 04          	movb   $0x4,0x45(%rbx)
		insn->immediate1.nbytes = 4;
		break;
	case 8:
		insn->immediate1.value = get_next(int, insn);
		insn->immediate1.nbytes = 4;
		insn->immediate2.value = get_next(int, insn);
ffffffff812cb85b:	89 43 40             	mov    %eax,0x40(%rbx)
		insn->immediate2.nbytes = 4;
		break;
	default:	/* opnd_bytes must be modified manually */
		goto err_out;
	}
	insn->immediate1.got = insn->immediate2.got = 1;
ffffffff812cb85e:	c6 43 44 01          	movb   $0x1,0x44(%rbx)
ffffffff812cb862:	c6 43 3c 01          	movb   $0x1,0x3c(%rbx)
		break;
	default:
		/* Here, insn must have an immediate, but failed */
		goto err_out;
	}
	if (inat_has_second_immediate(insn->attr)) {
ffffffff812cb866:	81 e2 00 00 01 00    	and    $0x10000,%edx
ffffffff812cb86c:	74 1c                	je     ffffffff812cb88a <insn_get_immediate+0x2d7>
		insn->immediate2.value = get_next(char, insn);
ffffffff812cb86e:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb872:	48 8d 50 01          	lea    0x1(%rax),%rdx
ffffffff812cb876:	48 3b 53 58          	cmp    0x58(%rbx),%rdx
ffffffff812cb87a:	77 12                	ja     ffffffff812cb88e <insn_get_immediate+0x2db>
ffffffff812cb87c:	0f be 00             	movsbl (%rax),%eax
ffffffff812cb87f:	48 89 53 60          	mov    %rdx,0x60(%rbx)
		insn->immediate2.nbytes = 1;
ffffffff812cb883:	c6 43 45 01          	movb   $0x1,0x45(%rbx)
	default:
		/* Here, insn must have an immediate, but failed */
		goto err_out;
	}
	if (inat_has_second_immediate(insn->attr)) {
		insn->immediate2.value = get_next(char, insn);
ffffffff812cb887:	89 43 40             	mov    %eax,0x40(%rbx)
		insn->immediate2.nbytes = 1;
	}
done:
	insn->immediate.got = 1;
ffffffff812cb88a:	c6 43 3c 01          	movb   $0x1,0x3c(%rbx)

err_out:
	return;
}
ffffffff812cb88e:	58                   	pop    %rax
ffffffff812cb88f:	5b                   	pop    %rbx
ffffffff812cb890:	5d                   	pop    %rbp
ffffffff812cb891:	c3                   	retq   

ffffffff812cb892 <insn_get_length>:
 * If necessary, first collects the instruction up to and including the
 * immediates bytes.
 */
void insn_get_length(struct insn *insn)
{
	if (insn->length)
ffffffff812cb892:	80 7f 4e 00          	cmpb   $0x0,0x4e(%rdi)
ffffffff812cb896:	75 22                	jne    ffffffff812cb8ba <insn_get_length+0x28>
 *
 * If necessary, first collects the instruction up to and including the
 * immediates bytes.
 */
void insn_get_length(struct insn *insn)
{
ffffffff812cb898:	55                   	push   %rbp
ffffffff812cb899:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cb89c:	53                   	push   %rbx
ffffffff812cb89d:	52                   	push   %rdx
	if (insn->length)
		return;
	if (!insn->immediate.got)
ffffffff812cb89e:	80 7f 3c 00          	cmpb   $0x0,0x3c(%rdi)
ffffffff812cb8a2:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cb8a5:	75 05                	jne    ffffffff812cb8ac <insn_get_length+0x1a>
		insn_get_immediate(insn);
ffffffff812cb8a7:	e8 07 fd ff ff       	callq  ffffffff812cb5b3 <insn_get_immediate>
	insn->length = (unsigned char)((unsigned long)insn->next_byte
ffffffff812cb8ac:	48 8b 43 60          	mov    0x60(%rbx),%rax
ffffffff812cb8b0:	48 2b 43 50          	sub    0x50(%rbx),%rax
ffffffff812cb8b4:	88 43 4e             	mov    %al,0x4e(%rbx)
				     - (unsigned long)insn->kaddr);
}
ffffffff812cb8b7:	58                   	pop    %rax
ffffffff812cb8b8:	5b                   	pop    %rbx
ffffffff812cb8b9:	5d                   	pop    %rbp
ffffffff812cb8ba:	c3                   	retq   
ffffffff812cb8bb:	90                   	nop
ffffffff812cb8bc:	90                   	nop
ffffffff812cb8bd:	90                   	nop
ffffffff812cb8be:	90                   	nop
ffffffff812cb8bf:	90                   	nop

ffffffff812cb8c0 <__memcpy>:
 * Output:
 * rax original destination
 */
ENTRY(__memcpy)
ENTRY(memcpy)
	ALTERNATIVE_2 "jmp memcpy_orig", "", X86_FEATURE_REP_GOOD, \
ffffffff812cb8c0:	e9 2b 00 00 00       	jmpq   ffffffff812cb8f0 <memcpy_orig>
		      "jmp memcpy_erms", X86_FEATURE_ERMS

	movq %rdi, %rax
ffffffff812cb8c5:	48 89 f8             	mov    %rdi,%rax
	movq %rdx, %rcx
ffffffff812cb8c8:	48 89 d1             	mov    %rdx,%rcx
	shrq $3, %rcx
ffffffff812cb8cb:	48 c1 e9 03          	shr    $0x3,%rcx
	andl $7, %edx
ffffffff812cb8cf:	83 e2 07             	and    $0x7,%edx
	rep movsq
ffffffff812cb8d2:	f3 48 a5             	rep movsq %ds:(%rsi),%es:(%rdi)
	movl %edx, %ecx
ffffffff812cb8d5:	89 d1                	mov    %edx,%ecx
	rep movsb
ffffffff812cb8d7:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
	ret
ffffffff812cb8d9:	c3                   	retq   
ffffffff812cb8da:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

ffffffff812cb8e0 <memcpy_erms>:
/*
 * memcpy_erms() - enhanced fast string memcpy. This is faster and
 * simpler than memcpy. Use memcpy_erms when possible.
 */
ENTRY(memcpy_erms)
	movq %rdi, %rax
ffffffff812cb8e0:	48 89 f8             	mov    %rdi,%rax
	movq %rdx, %rcx
ffffffff812cb8e3:	48 89 d1             	mov    %rdx,%rcx
	rep movsb
ffffffff812cb8e6:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
	ret
ffffffff812cb8e8:	c3                   	retq   
ffffffff812cb8e9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

ffffffff812cb8f0 <memcpy_orig>:
ENDPROC(memcpy_erms)

ENTRY(memcpy_orig)
	CFI_STARTPROC
	movq %rdi, %rax
ffffffff812cb8f0:	48 89 f8             	mov    %rdi,%rax

	cmpq $0x20, %rdx
ffffffff812cb8f3:	48 83 fa 20          	cmp    $0x20,%rdx
	jb .Lhandle_tail
ffffffff812cb8f7:	72 7e                	jb     ffffffff812cb977 <memcpy_orig+0x87>

	/*
	 * We check whether memory false dependence could occur,
	 * then jump to corresponding copy mode.
	 */
	cmp  %dil, %sil
ffffffff812cb8f9:	40 38 fe             	cmp    %dil,%sil
	jl .Lcopy_backward
ffffffff812cb8fc:	7c 35                	jl     ffffffff812cb933 <memcpy_orig+0x43>
	subq $0x20, %rdx
ffffffff812cb8fe:	48 83 ea 20          	sub    $0x20,%rdx
.Lcopy_forward_loop:
	subq $0x20,	%rdx
ffffffff812cb902:	48 83 ea 20          	sub    $0x20,%rdx

	/*
	 * Move in blocks of 4x8 bytes:
	 */
	movq 0*8(%rsi),	%r8
ffffffff812cb906:	4c 8b 06             	mov    (%rsi),%r8
	movq 1*8(%rsi),	%r9
ffffffff812cb909:	4c 8b 4e 08          	mov    0x8(%rsi),%r9
	movq 2*8(%rsi),	%r10
ffffffff812cb90d:	4c 8b 56 10          	mov    0x10(%rsi),%r10
	movq 3*8(%rsi),	%r11
ffffffff812cb911:	4c 8b 5e 18          	mov    0x18(%rsi),%r11
	leaq 4*8(%rsi),	%rsi
ffffffff812cb915:	48 8d 76 20          	lea    0x20(%rsi),%rsi

	movq %r8,	0*8(%rdi)
ffffffff812cb919:	4c 89 07             	mov    %r8,(%rdi)
	movq %r9,	1*8(%rdi)
ffffffff812cb91c:	4c 89 4f 08          	mov    %r9,0x8(%rdi)
	movq %r10,	2*8(%rdi)
ffffffff812cb920:	4c 89 57 10          	mov    %r10,0x10(%rdi)
	movq %r11,	3*8(%rdi)
ffffffff812cb924:	4c 89 5f 18          	mov    %r11,0x18(%rdi)
	leaq 4*8(%rdi),	%rdi
ffffffff812cb928:	48 8d 7f 20          	lea    0x20(%rdi),%rdi
	jae  .Lcopy_forward_loop
ffffffff812cb92c:	73 d4                	jae    ffffffff812cb902 <memcpy_orig+0x12>
	addl $0x20,	%edx
ffffffff812cb92e:	83 c2 20             	add    $0x20,%edx
	jmp  .Lhandle_tail
ffffffff812cb931:	eb 44                	jmp    ffffffff812cb977 <memcpy_orig+0x87>

.Lcopy_backward:
	/*
	 * Calculate copy position to tail.
	 */
	addq %rdx,	%rsi
ffffffff812cb933:	48 01 d6             	add    %rdx,%rsi
	addq %rdx,	%rdi
ffffffff812cb936:	48 01 d7             	add    %rdx,%rdi
	subq $0x20,	%rdx
ffffffff812cb939:	48 83 ea 20          	sub    $0x20,%rdx
ffffffff812cb93d:	0f 1f 00             	nopl   (%rax)
	 * At most 3 ALU operations in one cycle,
	 * so append NOPS in the same 16 bytes trunk.
	 */
	.p2align 4
.Lcopy_backward_loop:
	subq $0x20,	%rdx
ffffffff812cb940:	48 83 ea 20          	sub    $0x20,%rdx
	movq -1*8(%rsi),	%r8
ffffffff812cb944:	4c 8b 46 f8          	mov    -0x8(%rsi),%r8
	movq -2*8(%rsi),	%r9
ffffffff812cb948:	4c 8b 4e f0          	mov    -0x10(%rsi),%r9
	movq -3*8(%rsi),	%r10
ffffffff812cb94c:	4c 8b 56 e8          	mov    -0x18(%rsi),%r10
	movq -4*8(%rsi),	%r11
ffffffff812cb950:	4c 8b 5e e0          	mov    -0x20(%rsi),%r11
	leaq -4*8(%rsi),	%rsi
ffffffff812cb954:	48 8d 76 e0          	lea    -0x20(%rsi),%rsi
	movq %r8,		-1*8(%rdi)
ffffffff812cb958:	4c 89 47 f8          	mov    %r8,-0x8(%rdi)
	movq %r9,		-2*8(%rdi)
ffffffff812cb95c:	4c 89 4f f0          	mov    %r9,-0x10(%rdi)
	movq %r10,		-3*8(%rdi)
ffffffff812cb960:	4c 89 57 e8          	mov    %r10,-0x18(%rdi)
	movq %r11,		-4*8(%rdi)
ffffffff812cb964:	4c 89 5f e0          	mov    %r11,-0x20(%rdi)
	leaq -4*8(%rdi),	%rdi
ffffffff812cb968:	48 8d 7f e0          	lea    -0x20(%rdi),%rdi
	jae  .Lcopy_backward_loop
ffffffff812cb96c:	73 d2                	jae    ffffffff812cb940 <memcpy_orig+0x50>

	/*
	 * Calculate copy position to head.
	 */
	addl $0x20,	%edx
ffffffff812cb96e:	83 c2 20             	add    $0x20,%edx
	subq %rdx,	%rsi
ffffffff812cb971:	48 29 d6             	sub    %rdx,%rsi
	subq %rdx,	%rdi
ffffffff812cb974:	48 29 d7             	sub    %rdx,%rdi
.Lhandle_tail:
	cmpl $16,	%edx
ffffffff812cb977:	83 fa 10             	cmp    $0x10,%edx
	jb   .Lless_16bytes
ffffffff812cb97a:	72 24                	jb     ffffffff812cb9a0 <memcpy_orig+0xb0>

	/*
	 * Move data from 16 bytes to 31 bytes.
	 */
	movq 0*8(%rsi), %r8
ffffffff812cb97c:	4c 8b 06             	mov    (%rsi),%r8
	movq 1*8(%rsi),	%r9
ffffffff812cb97f:	4c 8b 4e 08          	mov    0x8(%rsi),%r9
	movq -2*8(%rsi, %rdx),	%r10
ffffffff812cb983:	4c 8b 54 16 f0       	mov    -0x10(%rsi,%rdx,1),%r10
	movq -1*8(%rsi, %rdx),	%r11
ffffffff812cb988:	4c 8b 5c 16 f8       	mov    -0x8(%rsi,%rdx,1),%r11
	movq %r8,	0*8(%rdi)
ffffffff812cb98d:	4c 89 07             	mov    %r8,(%rdi)
	movq %r9,	1*8(%rdi)
ffffffff812cb990:	4c 89 4f 08          	mov    %r9,0x8(%rdi)
	movq %r10,	-2*8(%rdi, %rdx)
ffffffff812cb994:	4c 89 54 17 f0       	mov    %r10,-0x10(%rdi,%rdx,1)
	movq %r11,	-1*8(%rdi, %rdx)
ffffffff812cb999:	4c 89 5c 17 f8       	mov    %r11,-0x8(%rdi,%rdx,1)
	retq
ffffffff812cb99e:	c3                   	retq   
ffffffff812cb99f:	90                   	nop
	.p2align 4
.Lless_16bytes:
	cmpl $8,	%edx
ffffffff812cb9a0:	83 fa 08             	cmp    $0x8,%edx
	jb   .Lless_8bytes
ffffffff812cb9a3:	72 1b                	jb     ffffffff812cb9c0 <memcpy_orig+0xd0>
	/*
	 * Move data from 8 bytes to 15 bytes.
	 */
	movq 0*8(%rsi),	%r8
ffffffff812cb9a5:	4c 8b 06             	mov    (%rsi),%r8
	movq -1*8(%rsi, %rdx),	%r9
ffffffff812cb9a8:	4c 8b 4c 16 f8       	mov    -0x8(%rsi,%rdx,1),%r9
	movq %r8,	0*8(%rdi)
ffffffff812cb9ad:	4c 89 07             	mov    %r8,(%rdi)
	movq %r9,	-1*8(%rdi, %rdx)
ffffffff812cb9b0:	4c 89 4c 17 f8       	mov    %r9,-0x8(%rdi,%rdx,1)
	retq
ffffffff812cb9b5:	c3                   	retq   
ffffffff812cb9b6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cb9bd:	00 00 00 
	.p2align 4
.Lless_8bytes:
	cmpl $4,	%edx
ffffffff812cb9c0:	83 fa 04             	cmp    $0x4,%edx
	jb   .Lless_3bytes
ffffffff812cb9c3:	72 1b                	jb     ffffffff812cb9e0 <memcpy_orig+0xf0>

	/*
	 * Move data from 4 bytes to 7 bytes.
	 */
	movl (%rsi), %ecx
ffffffff812cb9c5:	8b 0e                	mov    (%rsi),%ecx
	movl -4(%rsi, %rdx), %r8d
ffffffff812cb9c7:	44 8b 44 16 fc       	mov    -0x4(%rsi,%rdx,1),%r8d
	movl %ecx, (%rdi)
ffffffff812cb9cc:	89 0f                	mov    %ecx,(%rdi)
	movl %r8d, -4(%rdi, %rdx)
ffffffff812cb9ce:	44 89 44 17 fc       	mov    %r8d,-0x4(%rdi,%rdx,1)
	retq
ffffffff812cb9d3:	c3                   	retq   
ffffffff812cb9d4:	66 90                	xchg   %ax,%ax
ffffffff812cb9d6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cb9dd:	00 00 00 
	.p2align 4
.Lless_3bytes:
	subl $1, %edx
ffffffff812cb9e0:	83 ea 01             	sub    $0x1,%edx
	jb .Lend
ffffffff812cb9e3:	72 19                	jb     ffffffff812cb9fe <memcpy_orig+0x10e>
	/*
	 * Move data from 1 bytes to 3 bytes.
	 */
	movzbl (%rsi), %ecx
ffffffff812cb9e5:	0f b6 0e             	movzbl (%rsi),%ecx
	jz .Lstore_1byte
ffffffff812cb9e8:	74 12                	je     ffffffff812cb9fc <memcpy_orig+0x10c>
	movzbq 1(%rsi), %r8
ffffffff812cb9ea:	4c 0f b6 46 01       	movzbq 0x1(%rsi),%r8
	movzbq (%rsi, %rdx), %r9
ffffffff812cb9ef:	4c 0f b6 0c 16       	movzbq (%rsi,%rdx,1),%r9
	movb %r8b, 1(%rdi)
ffffffff812cb9f4:	44 88 47 01          	mov    %r8b,0x1(%rdi)
	movb %r9b, (%rdi, %rdx)
ffffffff812cb9f8:	44 88 0c 17          	mov    %r9b,(%rdi,%rdx,1)
.Lstore_1byte:
	movb %cl, (%rdi)
ffffffff812cb9fc:	88 0f                	mov    %cl,(%rdi)

.Lend:
	retq
ffffffff812cb9fe:	c3                   	retq   
ffffffff812cb9ff:	90                   	nop

ffffffff812cba00 <__memmove>:
ENTRY(memmove)
ENTRY(__memmove)
	CFI_STARTPROC

	/* Handle more 32 bytes in loop */
	mov %rdi, %rax
ffffffff812cba00:	48 89 f8             	mov    %rdi,%rax
	cmp $0x20, %rdx
ffffffff812cba03:	48 83 fa 20          	cmp    $0x20,%rdx
	jb	1f
ffffffff812cba07:	0f 82 03 01 00 00    	jb     ffffffff812cbb10 <__memmove+0x110>

	/* Decide forward/backward copy mode */
	cmp %rdi, %rsi
ffffffff812cba0d:	48 39 fe             	cmp    %rdi,%rsi
	jge .Lmemmove_begin_forward
ffffffff812cba10:	7d 0f                	jge    ffffffff812cba21 <__memmove+0x21>
	mov %rsi, %r8
ffffffff812cba12:	49 89 f0             	mov    %rsi,%r8
	add %rdx, %r8
ffffffff812cba15:	49 01 d0             	add    %rdx,%r8
	cmp %rdi, %r8
ffffffff812cba18:	49 39 f8             	cmp    %rdi,%r8
	jg 2f
ffffffff812cba1b:	0f 8f 9f 00 00 00    	jg     ffffffff812cbac0 <__memmove+0xc0>
ffffffff812cba21:	90                   	nop
ffffffff812cba22:	90                   	nop
ffffffff812cba23:	90                   	nop
ffffffff812cba24:	90                   	nop
ffffffff812cba25:	90                   	nop
ffffffff812cba26:	90                   	nop

	/*
	 * movsq instruction have many startup latency
	 * so we handle small size by general register.
	 */
	cmp  $680, %rdx
ffffffff812cba27:	48 81 fa a8 02 00 00 	cmp    $0x2a8,%rdx
	jb	3f
ffffffff812cba2e:	72 05                	jb     ffffffff812cba35 <__memmove+0x35>
	/*
	 * movsq instruction is only good for aligned case.
	 */

	cmpb %dil, %sil
ffffffff812cba30:	40 38 fe             	cmp    %dil,%sil
	je 4f
ffffffff812cba33:	74 3b                	je     ffffffff812cba70 <__memmove+0x70>
3:
	sub $0x20, %rdx
ffffffff812cba35:	48 83 ea 20          	sub    $0x20,%rdx
	/*
	 * We gobble 32 bytes forward in each loop.
	 */
5:
	sub $0x20, %rdx
ffffffff812cba39:	48 83 ea 20          	sub    $0x20,%rdx
	movq 0*8(%rsi), %r11
ffffffff812cba3d:	4c 8b 1e             	mov    (%rsi),%r11
	movq 1*8(%rsi), %r10
ffffffff812cba40:	4c 8b 56 08          	mov    0x8(%rsi),%r10
	movq 2*8(%rsi), %r9
ffffffff812cba44:	4c 8b 4e 10          	mov    0x10(%rsi),%r9
	movq 3*8(%rsi), %r8
ffffffff812cba48:	4c 8b 46 18          	mov    0x18(%rsi),%r8
	leaq 4*8(%rsi), %rsi
ffffffff812cba4c:	48 8d 76 20          	lea    0x20(%rsi),%rsi

	movq %r11, 0*8(%rdi)
ffffffff812cba50:	4c 89 1f             	mov    %r11,(%rdi)
	movq %r10, 1*8(%rdi)
ffffffff812cba53:	4c 89 57 08          	mov    %r10,0x8(%rdi)
	movq %r9, 2*8(%rdi)
ffffffff812cba57:	4c 89 4f 10          	mov    %r9,0x10(%rdi)
	movq %r8, 3*8(%rdi)
ffffffff812cba5b:	4c 89 47 18          	mov    %r8,0x18(%rdi)
	leaq 4*8(%rdi), %rdi
ffffffff812cba5f:	48 8d 7f 20          	lea    0x20(%rdi),%rdi
	jae 5b
ffffffff812cba63:	73 d4                	jae    ffffffff812cba39 <__memmove+0x39>
	addq $0x20, %rdx
ffffffff812cba65:	48 83 c2 20          	add    $0x20,%rdx
	jmp 1f
ffffffff812cba69:	e9 a2 00 00 00       	jmpq   ffffffff812cbb10 <__memmove+0x110>
ffffffff812cba6e:	66 90                	xchg   %ax,%ax
	/*
	 * Handle data forward by movsq.
	 */
	.p2align 4
4:
	movq %rdx, %rcx
ffffffff812cba70:	48 89 d1             	mov    %rdx,%rcx
	movq -8(%rsi, %rdx), %r11
ffffffff812cba73:	4c 8b 5c 16 f8       	mov    -0x8(%rsi,%rdx,1),%r11
	lea -8(%rdi, %rdx), %r10
ffffffff812cba78:	4c 8d 54 17 f8       	lea    -0x8(%rdi,%rdx,1),%r10
	shrq $3, %rcx
ffffffff812cba7d:	48 c1 e9 03          	shr    $0x3,%rcx
	rep movsq
ffffffff812cba81:	f3 48 a5             	rep movsq %ds:(%rsi),%es:(%rdi)
	movq %r11, (%r10)
ffffffff812cba84:	4d 89 1a             	mov    %r11,(%r10)
	jmp 13f
ffffffff812cba87:	e9 0c 01 00 00       	jmpq   ffffffff812cbb98 <__memmove+0x198>
ffffffff812cba8c:	0f 1f 40 00          	nopl   0x0(%rax)
	/*
	 * Handle data backward by movsq.
	 */
	.p2align 4
7:
	movq %rdx, %rcx
ffffffff812cba90:	48 89 d1             	mov    %rdx,%rcx
	movq (%rsi), %r11
ffffffff812cba93:	4c 8b 1e             	mov    (%rsi),%r11
	movq %rdi, %r10
ffffffff812cba96:	49 89 fa             	mov    %rdi,%r10
	leaq -8(%rsi, %rdx), %rsi
ffffffff812cba99:	48 8d 74 16 f8       	lea    -0x8(%rsi,%rdx,1),%rsi
	leaq -8(%rdi, %rdx), %rdi
ffffffff812cba9e:	48 8d 7c 17 f8       	lea    -0x8(%rdi,%rdx,1),%rdi
	shrq $3, %rcx
ffffffff812cbaa3:	48 c1 e9 03          	shr    $0x3,%rcx
	std
ffffffff812cbaa7:	fd                   	std    
	rep movsq
ffffffff812cbaa8:	f3 48 a5             	rep movsq %ds:(%rsi),%es:(%rdi)
	cld
ffffffff812cbaab:	fc                   	cld    
	movq %r11, (%r10)
ffffffff812cbaac:	4d 89 1a             	mov    %r11,(%r10)
	jmp 13f
ffffffff812cbaaf:	e9 e4 00 00 00       	jmpq   ffffffff812cbb98 <__memmove+0x198>
ffffffff812cbab4:	66 90                	xchg   %ax,%ax
ffffffff812cbab6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cbabd:	00 00 00 
	/*
	 * Start to prepare for backward copy.
	 */
	.p2align 4
2:
	cmp $680, %rdx
ffffffff812cbac0:	48 81 fa a8 02 00 00 	cmp    $0x2a8,%rdx
	jb 6f
ffffffff812cbac7:	72 05                	jb     ffffffff812cbace <__memmove+0xce>
	cmp %dil, %sil
ffffffff812cbac9:	40 38 fe             	cmp    %dil,%sil
	je 7b
ffffffff812cbacc:	74 c2                	je     ffffffff812cba90 <__memmove+0x90>
6:
	/*
	 * Calculate copy position to tail.
	 */
	addq %rdx, %rsi
ffffffff812cbace:	48 01 d6             	add    %rdx,%rsi
	addq %rdx, %rdi
ffffffff812cbad1:	48 01 d7             	add    %rdx,%rdi
	subq $0x20, %rdx
ffffffff812cbad4:	48 83 ea 20          	sub    $0x20,%rdx
	/*
	 * We gobble 32 bytes backward in each loop.
	 */
8:
	subq $0x20, %rdx
ffffffff812cbad8:	48 83 ea 20          	sub    $0x20,%rdx
	movq -1*8(%rsi), %r11
ffffffff812cbadc:	4c 8b 5e f8          	mov    -0x8(%rsi),%r11
	movq -2*8(%rsi), %r10
ffffffff812cbae0:	4c 8b 56 f0          	mov    -0x10(%rsi),%r10
	movq -3*8(%rsi), %r9
ffffffff812cbae4:	4c 8b 4e e8          	mov    -0x18(%rsi),%r9
	movq -4*8(%rsi), %r8
ffffffff812cbae8:	4c 8b 46 e0          	mov    -0x20(%rsi),%r8
	leaq -4*8(%rsi), %rsi
ffffffff812cbaec:	48 8d 76 e0          	lea    -0x20(%rsi),%rsi

	movq %r11, -1*8(%rdi)
ffffffff812cbaf0:	4c 89 5f f8          	mov    %r11,-0x8(%rdi)
	movq %r10, -2*8(%rdi)
ffffffff812cbaf4:	4c 89 57 f0          	mov    %r10,-0x10(%rdi)
	movq %r9, -3*8(%rdi)
ffffffff812cbaf8:	4c 89 4f e8          	mov    %r9,-0x18(%rdi)
	movq %r8, -4*8(%rdi)
ffffffff812cbafc:	4c 89 47 e0          	mov    %r8,-0x20(%rdi)
	leaq -4*8(%rdi), %rdi
ffffffff812cbb00:	48 8d 7f e0          	lea    -0x20(%rdi),%rdi
	jae 8b
ffffffff812cbb04:	73 d2                	jae    ffffffff812cbad8 <__memmove+0xd8>
	/*
	 * Calculate copy position to head.
	 */
	addq $0x20, %rdx
ffffffff812cbb06:	48 83 c2 20          	add    $0x20,%rdx
	subq %rdx, %rsi
ffffffff812cbb0a:	48 29 d6             	sub    %rdx,%rsi
	subq %rdx, %rdi
ffffffff812cbb0d:	48 29 d7             	sub    %rdx,%rdi
1:
	cmpq $16, %rdx
ffffffff812cbb10:	48 83 fa 10          	cmp    $0x10,%rdx
	jb 9f
ffffffff812cbb14:	72 2a                	jb     ffffffff812cbb40 <__memmove+0x140>
	/*
	 * Move data from 16 bytes to 31 bytes.
	 */
	movq 0*8(%rsi), %r11
ffffffff812cbb16:	4c 8b 1e             	mov    (%rsi),%r11
	movq 1*8(%rsi), %r10
ffffffff812cbb19:	4c 8b 56 08          	mov    0x8(%rsi),%r10
	movq -2*8(%rsi, %rdx), %r9
ffffffff812cbb1d:	4c 8b 4c 16 f0       	mov    -0x10(%rsi,%rdx,1),%r9
	movq -1*8(%rsi, %rdx), %r8
ffffffff812cbb22:	4c 8b 44 16 f8       	mov    -0x8(%rsi,%rdx,1),%r8
	movq %r11, 0*8(%rdi)
ffffffff812cbb27:	4c 89 1f             	mov    %r11,(%rdi)
	movq %r10, 1*8(%rdi)
ffffffff812cbb2a:	4c 89 57 08          	mov    %r10,0x8(%rdi)
	movq %r9, -2*8(%rdi, %rdx)
ffffffff812cbb2e:	4c 89 4c 17 f0       	mov    %r9,-0x10(%rdi,%rdx,1)
	movq %r8, -1*8(%rdi, %rdx)
ffffffff812cbb33:	4c 89 44 17 f8       	mov    %r8,-0x8(%rdi,%rdx,1)
	jmp 13f
ffffffff812cbb38:	eb 5e                	jmp    ffffffff812cbb98 <__memmove+0x198>
ffffffff812cbb3a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
	.p2align 4
9:
	cmpq $8, %rdx
ffffffff812cbb40:	48 83 fa 08          	cmp    $0x8,%rdx
	jb 10f
ffffffff812cbb44:	72 12                	jb     ffffffff812cbb58 <__memmove+0x158>
	/*
	 * Move data from 8 bytes to 15 bytes.
	 */
	movq 0*8(%rsi), %r11
ffffffff812cbb46:	4c 8b 1e             	mov    (%rsi),%r11
	movq -1*8(%rsi, %rdx), %r10
ffffffff812cbb49:	4c 8b 54 16 f8       	mov    -0x8(%rsi,%rdx,1),%r10
	movq %r11, 0*8(%rdi)
ffffffff812cbb4e:	4c 89 1f             	mov    %r11,(%rdi)
	movq %r10, -1*8(%rdi, %rdx)
ffffffff812cbb51:	4c 89 54 17 f8       	mov    %r10,-0x8(%rdi,%rdx,1)
	jmp 13f
ffffffff812cbb56:	eb 40                	jmp    ffffffff812cbb98 <__memmove+0x198>
10:
	cmpq $4, %rdx
ffffffff812cbb58:	48 83 fa 04          	cmp    $0x4,%rdx
	jb 11f
ffffffff812cbb5c:	72 12                	jb     ffffffff812cbb70 <__memmove+0x170>
	/*
	 * Move data from 4 bytes to 7 bytes.
	 */
	movl (%rsi), %r11d
ffffffff812cbb5e:	44 8b 1e             	mov    (%rsi),%r11d
	movl -4(%rsi, %rdx), %r10d
ffffffff812cbb61:	44 8b 54 16 fc       	mov    -0x4(%rsi,%rdx,1),%r10d
	movl %r11d, (%rdi)
ffffffff812cbb66:	44 89 1f             	mov    %r11d,(%rdi)
	movl %r10d, -4(%rdi, %rdx)
ffffffff812cbb69:	44 89 54 17 fc       	mov    %r10d,-0x4(%rdi,%rdx,1)
	jmp 13f
ffffffff812cbb6e:	eb 28                	jmp    ffffffff812cbb98 <__memmove+0x198>
11:
	cmp $2, %rdx
ffffffff812cbb70:	48 83 fa 02          	cmp    $0x2,%rdx
	jb 12f
ffffffff812cbb74:	72 16                	jb     ffffffff812cbb8c <__memmove+0x18c>
	/*
	 * Move data from 2 bytes to 3 bytes.
	 */
	movw (%rsi), %r11w
ffffffff812cbb76:	66 44 8b 1e          	mov    (%rsi),%r11w
	movw -2(%rsi, %rdx), %r10w
ffffffff812cbb7a:	66 44 8b 54 16 fe    	mov    -0x2(%rsi,%rdx,1),%r10w
	movw %r11w, (%rdi)
ffffffff812cbb80:	66 44 89 1f          	mov    %r11w,(%rdi)
	movw %r10w, -2(%rdi, %rdx)
ffffffff812cbb84:	66 44 89 54 17 fe    	mov    %r10w,-0x2(%rdi,%rdx,1)
	jmp 13f
ffffffff812cbb8a:	eb 0c                	jmp    ffffffff812cbb98 <__memmove+0x198>
12:
	cmp $1, %rdx
ffffffff812cbb8c:	48 83 fa 01          	cmp    $0x1,%rdx
	jb 13f
ffffffff812cbb90:	72 06                	jb     ffffffff812cbb98 <__memmove+0x198>
	/*
	 * Move data for 1 byte.
	 */
	movb (%rsi), %r11b
ffffffff812cbb92:	44 8a 1e             	mov    (%rsi),%r11b
	movb %r11b, (%rdi)
ffffffff812cbb95:	44 88 1f             	mov    %r11b,(%rdi)
13:
	retq
ffffffff812cbb98:	c3                   	retq   
ffffffff812cbb99:	90                   	nop
ffffffff812cbb9a:	90                   	nop
ffffffff812cbb9b:	90                   	nop
ffffffff812cbb9c:	90                   	nop
ffffffff812cbb9d:	90                   	nop
ffffffff812cbb9e:	90                   	nop
ffffffff812cbb9f:	90                   	nop

ffffffff812cbba0 <__memset>:
	 * Some CPUs support enhanced REP MOVSB/STOSB feature. It is recommended
	 * to use it when possible. If not available, use fast string instructions.
	 *
	 * Otherwise, use original memset function.
	 */
	ALTERNATIVE_2 "jmp memset_orig", "", X86_FEATURE_REP_GOOD, \
ffffffff812cbba0:	e9 3b 00 00 00       	jmpq   ffffffff812cbbe0 <memset_orig>
		      "jmp memset_erms", X86_FEATURE_ERMS

	movq %rdi,%r9
ffffffff812cbba5:	49 89 f9             	mov    %rdi,%r9
	movq %rdx,%rcx
ffffffff812cbba8:	48 89 d1             	mov    %rdx,%rcx
	andl $7,%edx
ffffffff812cbbab:	83 e2 07             	and    $0x7,%edx
	shrq $3,%rcx
ffffffff812cbbae:	48 c1 e9 03          	shr    $0x3,%rcx
	/* expand byte value  */
	movzbl %sil,%esi
ffffffff812cbbb2:	40 0f b6 f6          	movzbl %sil,%esi
	movabs $0x0101010101010101,%rax
ffffffff812cbbb6:	48 b8 01 01 01 01 01 	movabs $0x101010101010101,%rax
ffffffff812cbbbd:	01 01 01 
	imulq %rsi,%rax
ffffffff812cbbc0:	48 0f af c6          	imul   %rsi,%rax
	rep stosq
ffffffff812cbbc4:	f3 48 ab             	rep stos %rax,%es:(%rdi)
	movl %edx,%ecx
ffffffff812cbbc7:	89 d1                	mov    %edx,%ecx
	rep stosb
ffffffff812cbbc9:	f3 aa                	rep stos %al,%es:(%rdi)
	movq %r9,%rax
ffffffff812cbbcb:	4c 89 c8             	mov    %r9,%rax
	ret
ffffffff812cbbce:	c3                   	retq   
ffffffff812cbbcf:	90                   	nop

ffffffff812cbbd0 <memset_erms>:
 * rdx   count (bytes)
 *
 * rax   original destination
 */
ENTRY(memset_erms)
	movq %rdi,%r9
ffffffff812cbbd0:	49 89 f9             	mov    %rdi,%r9
	movb %sil,%al
ffffffff812cbbd3:	40 88 f0             	mov    %sil,%al
	movq %rdx,%rcx
ffffffff812cbbd6:	48 89 d1             	mov    %rdx,%rcx
	rep stosb
ffffffff812cbbd9:	f3 aa                	rep stos %al,%es:(%rdi)
	movq %r9,%rax
ffffffff812cbbdb:	4c 89 c8             	mov    %r9,%rax
	ret
ffffffff812cbbde:	c3                   	retq   
ffffffff812cbbdf:	90                   	nop

ffffffff812cbbe0 <memset_orig>:
ENDPROC(memset_erms)

ENTRY(memset_orig)
	CFI_STARTPROC
	movq %rdi,%r10
ffffffff812cbbe0:	49 89 fa             	mov    %rdi,%r10

	/* expand byte value  */
	movzbl %sil,%ecx
ffffffff812cbbe3:	40 0f b6 ce          	movzbl %sil,%ecx
	movabs $0x0101010101010101,%rax
ffffffff812cbbe7:	48 b8 01 01 01 01 01 	movabs $0x101010101010101,%rax
ffffffff812cbbee:	01 01 01 
	imulq  %rcx,%rax
ffffffff812cbbf1:	48 0f af c1          	imul   %rcx,%rax

	/* align dst */
	movl  %edi,%r9d
ffffffff812cbbf5:	41 89 f9             	mov    %edi,%r9d
	andl  $7,%r9d
ffffffff812cbbf8:	41 83 e1 07          	and    $0x7,%r9d
	jnz  .Lbad_alignment
ffffffff812cbbfc:	75 70                	jne    ffffffff812cbc6e <memset_orig+0x8e>
	CFI_REMEMBER_STATE
.Lafter_bad_alignment:

	movq  %rdx,%rcx
ffffffff812cbbfe:	48 89 d1             	mov    %rdx,%rcx
	shrq  $6,%rcx
ffffffff812cbc01:	48 c1 e9 06          	shr    $0x6,%rcx
	jz	 .Lhandle_tail
ffffffff812cbc05:	74 39                	je     ffffffff812cbc40 <memset_orig+0x60>
ffffffff812cbc07:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
ffffffff812cbc0e:	00 00 

	.p2align 4
.Lloop_64:
	decq  %rcx
ffffffff812cbc10:	48 ff c9             	dec    %rcx
	movq  %rax,(%rdi)
ffffffff812cbc13:	48 89 07             	mov    %rax,(%rdi)
	movq  %rax,8(%rdi)
ffffffff812cbc16:	48 89 47 08          	mov    %rax,0x8(%rdi)
	movq  %rax,16(%rdi)
ffffffff812cbc1a:	48 89 47 10          	mov    %rax,0x10(%rdi)
	movq  %rax,24(%rdi)
ffffffff812cbc1e:	48 89 47 18          	mov    %rax,0x18(%rdi)
	movq  %rax,32(%rdi)
ffffffff812cbc22:	48 89 47 20          	mov    %rax,0x20(%rdi)
	movq  %rax,40(%rdi)
ffffffff812cbc26:	48 89 47 28          	mov    %rax,0x28(%rdi)
	movq  %rax,48(%rdi)
ffffffff812cbc2a:	48 89 47 30          	mov    %rax,0x30(%rdi)
	movq  %rax,56(%rdi)
ffffffff812cbc2e:	48 89 47 38          	mov    %rax,0x38(%rdi)
	leaq  64(%rdi),%rdi
ffffffff812cbc32:	48 8d 7f 40          	lea    0x40(%rdi),%rdi
	jnz    .Lloop_64
ffffffff812cbc36:	75 d8                	jne    ffffffff812cbc10 <memset_orig+0x30>
ffffffff812cbc38:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
ffffffff812cbc3f:	00 

	/* Handle tail in loops. The loops should be faster than hard
	   to predict jump tables. */
	.p2align 4
.Lhandle_tail:
	movl	%edx,%ecx
ffffffff812cbc40:	89 d1                	mov    %edx,%ecx
	andl    $63&(~7),%ecx
ffffffff812cbc42:	83 e1 38             	and    $0x38,%ecx
	jz 		.Lhandle_7
ffffffff812cbc45:	74 14                	je     ffffffff812cbc5b <memset_orig+0x7b>
	shrl	$3,%ecx
ffffffff812cbc47:	c1 e9 03             	shr    $0x3,%ecx
ffffffff812cbc4a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
	.p2align 4
.Lloop_8:
	decl   %ecx
ffffffff812cbc50:	ff c9                	dec    %ecx
	movq  %rax,(%rdi)
ffffffff812cbc52:	48 89 07             	mov    %rax,(%rdi)
	leaq  8(%rdi),%rdi
ffffffff812cbc55:	48 8d 7f 08          	lea    0x8(%rdi),%rdi
	jnz    .Lloop_8
ffffffff812cbc59:	75 f5                	jne    ffffffff812cbc50 <memset_orig+0x70>

.Lhandle_7:
	andl	$7,%edx
ffffffff812cbc5b:	83 e2 07             	and    $0x7,%edx
	jz      .Lende
ffffffff812cbc5e:	74 0a                	je     ffffffff812cbc6a <memset_orig+0x8a>
	.p2align 4
.Lloop_1:
	decl    %edx
ffffffff812cbc60:	ff ca                	dec    %edx
	movb 	%al,(%rdi)
ffffffff812cbc62:	88 07                	mov    %al,(%rdi)
	leaq	1(%rdi),%rdi
ffffffff812cbc64:	48 8d 7f 01          	lea    0x1(%rdi),%rdi
	jnz     .Lloop_1
ffffffff812cbc68:	75 f6                	jne    ffffffff812cbc60 <memset_orig+0x80>

.Lende:
	movq	%r10,%rax
ffffffff812cbc6a:	4c 89 d0             	mov    %r10,%rax
	ret
ffffffff812cbc6d:	c3                   	retq   

	CFI_RESTORE_STATE
.Lbad_alignment:
	cmpq $7,%rdx
ffffffff812cbc6e:	48 83 fa 07          	cmp    $0x7,%rdx
	jbe	.Lhandle_7
ffffffff812cbc72:	76 e7                	jbe    ffffffff812cbc5b <memset_orig+0x7b>
	movq %rax,(%rdi)	/* unaligned store */
ffffffff812cbc74:	48 89 07             	mov    %rax,(%rdi)
	movq $8,%r8
ffffffff812cbc77:	49 c7 c0 08 00 00 00 	mov    $0x8,%r8
	subq %r9,%r8
ffffffff812cbc7e:	4d 29 c8             	sub    %r9,%r8
	addq %r8,%rdi
ffffffff812cbc81:	4c 01 c7             	add    %r8,%rdi
	subq %r8,%rdx
ffffffff812cbc84:	4c 29 c2             	sub    %r8,%rdx
	jmp .Lafter_bad_alignment
ffffffff812cbc87:	e9 72 ff ff ff       	jmpq   ffffffff812cbbfe <memset_orig+0x1e>

ffffffff812cbc8c <num_digits>:
 * Count the digits of @val including a possible sign.
 *
 * (Typed on and submitted from hpa's mobile phone.)
 */
int num_digits(int val)
{
ffffffff812cbc8c:	55                   	push   %rbp
	int m = 10;
	int d = 1;

	if (val < 0) {
ffffffff812cbc8d:	85 ff                	test   %edi,%edi
ffffffff812cbc8f:	b8 01 00 00 00       	mov    $0x1,%eax
 * Count the digits of @val including a possible sign.
 *
 * (Typed on and submitted from hpa's mobile phone.)
 */
int num_digits(int val)
{
ffffffff812cbc94:	48 89 e5             	mov    %rsp,%rbp
	int m = 10;
	int d = 1;

	if (val < 0) {
ffffffff812cbc97:	79 07                	jns    ffffffff812cbca0 <num_digits+0x14>
		d++;
		val = -val;
ffffffff812cbc99:	f7 df                	neg    %edi
{
	int m = 10;
	int d = 1;

	if (val < 0) {
		d++;
ffffffff812cbc9b:	b8 02 00 00 00       	mov    $0x2,%eax
 * (Typed on and submitted from hpa's mobile phone.)
 */
int num_digits(int val)
{
	int m = 10;
	int d = 1;
ffffffff812cbca0:	ba 0a 00 00 00       	mov    $0xa,%edx
	if (val < 0) {
		d++;
		val = -val;
	}

	while (val >= m) {
ffffffff812cbca5:	39 fa                	cmp    %edi,%edx
ffffffff812cbca7:	7f 07                	jg     ffffffff812cbcb0 <num_digits+0x24>
		m *= 10;
ffffffff812cbca9:	6b d2 0a             	imul   $0xa,%edx,%edx
		d++;
ffffffff812cbcac:	ff c0                	inc    %eax
ffffffff812cbcae:	eb f5                	jmp    ffffffff812cbca5 <num_digits+0x19>
	}
	return d;
}
ffffffff812cbcb0:	5d                   	pop    %rbp
ffffffff812cbcb1:	c3                   	retq   
ffffffff812cbcb2:	90                   	nop
ffffffff812cbcb3:	90                   	nop
ffffffff812cbcb4:	90                   	nop
ffffffff812cbcb5:	90                   	nop
ffffffff812cbcb6:	90                   	nop
ffffffff812cbcb7:	90                   	nop
ffffffff812cbcb8:	90                   	nop
ffffffff812cbcb9:	90                   	nop
ffffffff812cbcba:	90                   	nop
ffffffff812cbcbb:	90                   	nop
ffffffff812cbcbc:	90                   	nop
ffffffff812cbcbd:	90                   	nop
ffffffff812cbcbe:	90                   	nop
ffffffff812cbcbf:	90                   	nop

ffffffff812cbcc0 <__put_user_1>:
		ret ;		\
		CFI_ENDPROC

.text
ENTRY(__put_user_1)
	ENTER
ffffffff812cbcc0:	65 48 8b 1c 25 a0 a9 	mov    %gs:0xa9a0,%rbx
ffffffff812cbcc7:	00 00 
ffffffff812cbcc9:	48 81 eb 00 40 00 00 	sub    $0x4000,%rbx
	cmp TI_addr_limit(%_ASM_BX),%_ASM_CX
ffffffff812cbcd0:	48 3b 4b 18          	cmp    0x18(%rbx),%rcx
	jae bad_put_user
ffffffff812cbcd4:	0f 83 9f 00 00 00    	jae    ffffffff812cbd79 <bad_put_user>
ffffffff812cbcda:	90                   	nop
ffffffff812cbcdb:	90                   	nop
ffffffff812cbcdc:	90                   	nop
	ASM_STAC
1:	movb %al,(%_ASM_CX)
ffffffff812cbcdd:	88 01                	mov    %al,(%rcx)
	xor %eax,%eax
ffffffff812cbcdf:	31 c0                	xor    %eax,%eax
ffffffff812cbce1:	90                   	nop
ffffffff812cbce2:	90                   	nop
ffffffff812cbce3:	90                   	nop
	EXIT
ffffffff812cbce4:	c3                   	retq   
ffffffff812cbce5:	90                   	nop
ffffffff812cbce6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cbced:	00 00 00 

ffffffff812cbcf0 <__put_user_2>:
ENDPROC(__put_user_1)

ENTRY(__put_user_2)
	ENTER
ffffffff812cbcf0:	65 48 8b 1c 25 a0 a9 	mov    %gs:0xa9a0,%rbx
ffffffff812cbcf7:	00 00 
ffffffff812cbcf9:	48 81 eb 00 40 00 00 	sub    $0x4000,%rbx
	mov TI_addr_limit(%_ASM_BX),%_ASM_BX
ffffffff812cbd00:	48 8b 5b 18          	mov    0x18(%rbx),%rbx
	sub $1,%_ASM_BX
ffffffff812cbd04:	48 83 eb 01          	sub    $0x1,%rbx
	cmp %_ASM_BX,%_ASM_CX
ffffffff812cbd08:	48 39 d9             	cmp    %rbx,%rcx
	jae bad_put_user
ffffffff812cbd0b:	73 6c                	jae    ffffffff812cbd79 <bad_put_user>
ffffffff812cbd0d:	90                   	nop
ffffffff812cbd0e:	90                   	nop
ffffffff812cbd0f:	90                   	nop
	ASM_STAC
2:	movw %ax,(%_ASM_CX)
ffffffff812cbd10:	66 89 01             	mov    %ax,(%rcx)
	xor %eax,%eax
ffffffff812cbd13:	31 c0                	xor    %eax,%eax
ffffffff812cbd15:	90                   	nop
ffffffff812cbd16:	90                   	nop
ffffffff812cbd17:	90                   	nop
	EXIT
ffffffff812cbd18:	c3                   	retq   
ffffffff812cbd19:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

ffffffff812cbd20 <__put_user_4>:
ENDPROC(__put_user_2)

ENTRY(__put_user_4)
	ENTER
ffffffff812cbd20:	65 48 8b 1c 25 a0 a9 	mov    %gs:0xa9a0,%rbx
ffffffff812cbd27:	00 00 
ffffffff812cbd29:	48 81 eb 00 40 00 00 	sub    $0x4000,%rbx
	mov TI_addr_limit(%_ASM_BX),%_ASM_BX
ffffffff812cbd30:	48 8b 5b 18          	mov    0x18(%rbx),%rbx
	sub $3,%_ASM_BX
ffffffff812cbd34:	48 83 eb 03          	sub    $0x3,%rbx
	cmp %_ASM_BX,%_ASM_CX
ffffffff812cbd38:	48 39 d9             	cmp    %rbx,%rcx
	jae bad_put_user
ffffffff812cbd3b:	73 3c                	jae    ffffffff812cbd79 <bad_put_user>
ffffffff812cbd3d:	90                   	nop
ffffffff812cbd3e:	90                   	nop
ffffffff812cbd3f:	90                   	nop
	ASM_STAC
3:	movl %eax,(%_ASM_CX)
ffffffff812cbd40:	89 01                	mov    %eax,(%rcx)
	xor %eax,%eax
ffffffff812cbd42:	31 c0                	xor    %eax,%eax
ffffffff812cbd44:	90                   	nop
ffffffff812cbd45:	90                   	nop
ffffffff812cbd46:	90                   	nop
	EXIT
ffffffff812cbd47:	c3                   	retq   
ffffffff812cbd48:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
ffffffff812cbd4f:	00 

ffffffff812cbd50 <__put_user_8>:
ENDPROC(__put_user_4)

ENTRY(__put_user_8)
	ENTER
ffffffff812cbd50:	65 48 8b 1c 25 a0 a9 	mov    %gs:0xa9a0,%rbx
ffffffff812cbd57:	00 00 
ffffffff812cbd59:	48 81 eb 00 40 00 00 	sub    $0x4000,%rbx
	mov TI_addr_limit(%_ASM_BX),%_ASM_BX
ffffffff812cbd60:	48 8b 5b 18          	mov    0x18(%rbx),%rbx
	sub $7,%_ASM_BX
ffffffff812cbd64:	48 83 eb 07          	sub    $0x7,%rbx
	cmp %_ASM_BX,%_ASM_CX
ffffffff812cbd68:	48 39 d9             	cmp    %rbx,%rcx
	jae bad_put_user
ffffffff812cbd6b:	73 0c                	jae    ffffffff812cbd79 <bad_put_user>
ffffffff812cbd6d:	90                   	nop
ffffffff812cbd6e:	90                   	nop
ffffffff812cbd6f:	90                   	nop
	ASM_STAC
4:	mov %_ASM_AX,(%_ASM_CX)
ffffffff812cbd70:	48 89 01             	mov    %rax,(%rcx)
#ifdef CONFIG_X86_32
5:	movl %edx,4(%_ASM_CX)
#endif
	xor %eax,%eax
ffffffff812cbd73:	31 c0                	xor    %eax,%eax
ffffffff812cbd75:	90                   	nop
ffffffff812cbd76:	90                   	nop
ffffffff812cbd77:	90                   	nop
	EXIT
ffffffff812cbd78:	c3                   	retq   

ffffffff812cbd79 <bad_put_user>:
ENDPROC(__put_user_8)

bad_put_user:
	CFI_STARTPROC
	movl $-EFAULT,%eax
ffffffff812cbd79:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
ffffffff812cbd7e:	90                   	nop
ffffffff812cbd7f:	90                   	nop
ffffffff812cbd80:	90                   	nop
	EXIT
ffffffff812cbd81:	c3                   	retq   
ffffffff812cbd82:	90                   	nop
ffffffff812cbd83:	90                   	nop
ffffffff812cbd84:	90                   	nop
ffffffff812cbd85:	90                   	nop
ffffffff812cbd86:	90                   	nop
ffffffff812cbd87:	90                   	nop
ffffffff812cbd88:	90                   	nop
ffffffff812cbd89:	90                   	nop
ffffffff812cbd8a:	90                   	nop
ffffffff812cbd8b:	90                   	nop
ffffffff812cbd8c:	90                   	nop
ffffffff812cbd8d:	90                   	nop
ffffffff812cbd8e:	90                   	nop
ffffffff812cbd8f:	90                   	nop

ffffffff812cbd90 <call_rwsem_down_read_failed>:
#endif

/* Fix up special calling conventions */
ENTRY(call_rwsem_down_read_failed)
	CFI_STARTPROC
	save_common_regs
ffffffff812cbd90:	57                   	push   %rdi
ffffffff812cbd91:	56                   	push   %rsi
ffffffff812cbd92:	51                   	push   %rcx
ffffffff812cbd93:	41 50                	push   %r8
ffffffff812cbd95:	41 51                	push   %r9
ffffffff812cbd97:	41 52                	push   %r10
ffffffff812cbd99:	41 53                	push   %r11
	__ASM_SIZE(push,_cfi_reg) __ASM_REG(dx)
ffffffff812cbd9b:	52                   	push   %rdx
	movq %rax,%rdi
ffffffff812cbd9c:	48 89 c7             	mov    %rax,%rdi
	call rwsem_down_read_failed
ffffffff812cbd9f:	e8 71 c1 16 00       	callq  ffffffff81437f15 <rwsem_down_read_failed>
	__ASM_SIZE(pop,_cfi_reg) __ASM_REG(dx)
ffffffff812cbda4:	5a                   	pop    %rdx
	restore_common_regs
ffffffff812cbda5:	41 5b                	pop    %r11
ffffffff812cbda7:	41 5a                	pop    %r10
ffffffff812cbda9:	41 59                	pop    %r9
ffffffff812cbdab:	41 58                	pop    %r8
ffffffff812cbdad:	59                   	pop    %rcx
ffffffff812cbdae:	5e                   	pop    %rsi
ffffffff812cbdaf:	5f                   	pop    %rdi
	ret
ffffffff812cbdb0:	c3                   	retq   
ffffffff812cbdb1:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
ffffffff812cbdb6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cbdbd:	00 00 00 

ffffffff812cbdc0 <call_rwsem_down_write_failed>:
	CFI_ENDPROC
ENDPROC(call_rwsem_down_read_failed)

ENTRY(call_rwsem_down_write_failed)
	CFI_STARTPROC
	save_common_regs
ffffffff812cbdc0:	57                   	push   %rdi
ffffffff812cbdc1:	56                   	push   %rsi
ffffffff812cbdc2:	51                   	push   %rcx
ffffffff812cbdc3:	41 50                	push   %r8
ffffffff812cbdc5:	41 51                	push   %r9
ffffffff812cbdc7:	41 52                	push   %r10
ffffffff812cbdc9:	41 53                	push   %r11
	movq %rax,%rdi
ffffffff812cbdcb:	48 89 c7             	mov    %rax,%rdi
	call rwsem_down_write_failed
ffffffff812cbdce:	e8 c5 be 16 00       	callq  ffffffff81437c98 <rwsem_down_write_failed>
	restore_common_regs
ffffffff812cbdd3:	41 5b                	pop    %r11
ffffffff812cbdd5:	41 5a                	pop    %r10
ffffffff812cbdd7:	41 59                	pop    %r9
ffffffff812cbdd9:	41 58                	pop    %r8
ffffffff812cbddb:	59                   	pop    %rcx
ffffffff812cbddc:	5e                   	pop    %rsi
ffffffff812cbddd:	5f                   	pop    %rdi
	ret
ffffffff812cbdde:	c3                   	retq   
ffffffff812cbddf:	90                   	nop

ffffffff812cbde0 <call_rwsem_wake>:
ENDPROC(call_rwsem_down_write_failed)

ENTRY(call_rwsem_wake)
	CFI_STARTPROC
	/* do nothing if still outstanding active readers */
	__ASM_HALF_SIZE(dec) %__ASM_HALF_REG(dx)
ffffffff812cbde0:	ff ca                	dec    %edx
	jnz 1f
ffffffff812cbde2:	75 1e                	jne    ffffffff812cbe02 <call_rwsem_wake+0x22>
	save_common_regs
ffffffff812cbde4:	57                   	push   %rdi
ffffffff812cbde5:	56                   	push   %rsi
ffffffff812cbde6:	51                   	push   %rcx
ffffffff812cbde7:	41 50                	push   %r8
ffffffff812cbde9:	41 51                	push   %r9
ffffffff812cbdeb:	41 52                	push   %r10
ffffffff812cbded:	41 53                	push   %r11
	movq %rax,%rdi
ffffffff812cbdef:	48 89 c7             	mov    %rax,%rdi
	call rwsem_wake
ffffffff812cbdf2:	e8 ce 45 dc ff       	callq  ffffffff810903c5 <rwsem_wake>
	restore_common_regs
ffffffff812cbdf7:	41 5b                	pop    %r11
ffffffff812cbdf9:	41 5a                	pop    %r10
ffffffff812cbdfb:	41 59                	pop    %r9
ffffffff812cbdfd:	41 58                	pop    %r8
ffffffff812cbdff:	59                   	pop    %rcx
ffffffff812cbe00:	5e                   	pop    %rsi
ffffffff812cbe01:	5f                   	pop    %rdi
1:	ret
ffffffff812cbe02:	c3                   	retq   
ffffffff812cbe03:	0f 1f 00             	nopl   (%rax)
ffffffff812cbe06:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
ffffffff812cbe0d:	00 00 00 

ffffffff812cbe10 <call_rwsem_downgrade_wake>:
	CFI_ENDPROC
ENDPROC(call_rwsem_wake)

ENTRY(call_rwsem_downgrade_wake)
	CFI_STARTPROC
	save_common_regs
ffffffff812cbe10:	57                   	push   %rdi
ffffffff812cbe11:	56                   	push   %rsi
ffffffff812cbe12:	51                   	push   %rcx
ffffffff812cbe13:	41 50                	push   %r8
ffffffff812cbe15:	41 51                	push   %r9
ffffffff812cbe17:	41 52                	push   %r10
ffffffff812cbe19:	41 53                	push   %r11
	__ASM_SIZE(push,_cfi_reg) __ASM_REG(dx)
ffffffff812cbe1b:	52                   	push   %rdx
	movq %rax,%rdi
ffffffff812cbe1c:	48 89 c7             	mov    %rax,%rdi
	call rwsem_downgrade_wake
ffffffff812cbe1f:	e8 e2 45 dc ff       	callq  ffffffff81090406 <rwsem_downgrade_wake>
	__ASM_SIZE(pop,_cfi_reg) __ASM_REG(dx)
ffffffff812cbe24:	5a                   	pop    %rdx
	restore_common_regs
ffffffff812cbe25:	41 5b                	pop    %r11
ffffffff812cbe27:	41 5a                	pop    %r10
ffffffff812cbe29:	41 59                	pop    %r9
ffffffff812cbe2b:	41 58                	pop    %r8
ffffffff812cbe2d:	59                   	pop    %rcx
ffffffff812cbe2e:	5e                   	pop    %rsi
ffffffff812cbe2f:	5f                   	pop    %rdi
	ret
ffffffff812cbe30:	c3                   	retq   

ffffffff812cbe31 <copy_from_user_nmi>:
 * We rely on the nested NMI work to allow atomic faults from the NMI path; the
 * nested NMI paths are careful to preserve CR2.
 */
unsigned long
copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
{
ffffffff812cbe31:	55                   	push   %rbp
ffffffff812cbe32:	48 b9 00 f0 ff ff ff 	movabs $0x7ffffffff000,%rcx
ffffffff812cbe39:	7f 00 00 
}

static inline unsigned long current_top_of_stack(void)
{
#ifdef CONFIG_X86_64
	return this_cpu_read_stable(cpu_tss.x86_tss.sp0);
ffffffff812cbe3c:	65 48 8b 04 25 04 22 	mov    %gs:0x12204,%rax
ffffffff812cbe43:	01 00 
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
ffffffff812cbe45:	48 8b 80 08 c0 ff ff 	mov    -0x3ff8(%rax),%rax
ffffffff812cbe4c:	48 89 e5             	mov    %rsp,%rbp
	unsigned long ret;

	if (__range_not_ok(from, n, TASK_SIZE))
ffffffff812cbe4f:	a9 00 00 00 20       	test   $0x20000000,%eax
ffffffff812cbe54:	74 1e                	je     ffffffff812cbe74 <copy_from_user_nmi+0x43>
ffffffff812cbe56:	65 48 8b 04 25 00 aa 	mov    %gs:0xaa00,%rax
ffffffff812cbe5d:	00 00 
ffffffff812cbe5f:	f6 80 df 02 00 00 08 	testb  $0x8,0x2df(%rax)
ffffffff812cbe66:	b9 00 e0 ff ff       	mov    $0xffffe000,%ecx
ffffffff812cbe6b:	b8 00 00 00 c0       	mov    $0xc0000000,%eax
ffffffff812cbe70:	48 0f 45 c8          	cmovne %rax,%rcx
	if (__builtin_constant_p(size))
		return addr > limit - size;

	/* Arbitrary sizes? Be careful about overflow */
	addr += size;
	if (addr < size)
ffffffff812cbe74:	49 89 f0             	mov    %rsi,%r8
		return 0;
ffffffff812cbe77:	31 c0                	xor    %eax,%eax
ffffffff812cbe79:	49 01 d0             	add    %rdx,%r8
ffffffff812cbe7c:	72 1a                	jb     ffffffff812cbe98 <copy_from_user_nmi+0x67>
unsigned long
copy_from_user_nmi(void *to, const void __user *from, unsigned long n)
{
	unsigned long ret;

	if (__range_not_ok(from, n, TASK_SIZE))
ffffffff812cbe7e:	4c 39 c1             	cmp    %r8,%rcx
ffffffff812cbe81:	72 15                	jb     ffffffff812cbe98 <copy_from_user_nmi+0x67>
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	raw_cpu_add_4(__preempt_count, val);
ffffffff812cbe83:	65 ff 05 0e eb d3 7e 	incl   %gs:0x7ed3eb0e(%rip)        # a998 <__preempt_count>
	/*
	 * If CPU has ERMS feature, use copy_user_enhanced_fast_string.
	 * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.
	 * Otherwise, use copy_user_generic_unrolled.
	 */
	alternative_call_2(copy_user_generic_unrolled,
ffffffff812cbe8a:	e8 a1 ec ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
	 * Even though this function is typically called from NMI/IRQ context
	 * disable pagefaults so that its behaviour is consistent even when
	 * called form other contexts.
	 */
	pagefault_disable();
	ret = __copy_from_user_inatomic(to, from, n);
ffffffff812cbe8f:	48 98                	cltq   
}

static __always_inline void __preempt_count_sub(int val)
{
	raw_cpu_add_4(__preempt_count, -val);
ffffffff812cbe91:	65 ff 0d 00 eb d3 7e 	decl   %gs:0x7ed3eb00(%rip)        # a998 <__preempt_count>
	pagefault_enable();

	return ret;
}
ffffffff812cbe98:	5d                   	pop    %rbp
ffffffff812cbe99:	c3                   	retq   

ffffffff812cbe9a <__clear_user>:
/*
 * Zero Userspace
 */

unsigned long __clear_user(void __user *addr, unsigned long size)
{
ffffffff812cbe9a:	55                   	push   %rbp
ffffffff812cbe9b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cbe9e:	90                   	nop
ffffffff812cbe9f:	90                   	nop
ffffffff812cbea0:	90                   	nop
		"	jmp 2b\n"
		".previous\n"
		_ASM_EXTABLE(0b,3b)
		_ASM_EXTABLE(1b,2b)
		: [size8] "=&c"(size), [dst] "=&D" (__d0)
		: [size1] "r"(size & 7), "[size8]" (size / 8), "[dst]"(addr),
ffffffff812cbea1:	48 89 f0             	mov    %rsi,%rax
ffffffff812cbea4:	48 89 f1             	mov    %rsi,%rcx
	long __d0;
	might_fault();
	/* no memory constraint because it doesn't change any memory gcc knows
	   about */
	stac();
	asm volatile(
ffffffff812cbea7:	31 d2                	xor    %edx,%edx
		"	jmp 2b\n"
		".previous\n"
		_ASM_EXTABLE(0b,3b)
		_ASM_EXTABLE(1b,2b)
		: [size8] "=&c"(size), [dst] "=&D" (__d0)
		: [size1] "r"(size & 7), "[size8]" (size / 8), "[dst]"(addr),
ffffffff812cbea9:	83 e0 07             	and    $0x7,%eax
ffffffff812cbeac:	48 c1 e9 03          	shr    $0x3,%rcx
	long __d0;
	might_fault();
	/* no memory constraint because it doesn't change any memory gcc knows
	   about */
	stac();
	asm volatile(
ffffffff812cbeb0:	be 08 00 00 00       	mov    $0x8,%esi
ffffffff812cbeb5:	48 85 c9             	test   %rcx,%rcx
ffffffff812cbeb8:	74 0a                	je     ffffffff812cbec4 <__clear_user+0x2a>
ffffffff812cbeba:	48 89 17             	mov    %rdx,(%rdi)
ffffffff812cbebd:	48 01 f7             	add    %rsi,%rdi
ffffffff812cbec0:	ff c9                	dec    %ecx
ffffffff812cbec2:	75 f6                	jne    ffffffff812cbeba <__clear_user+0x20>
ffffffff812cbec4:	48 89 c1             	mov    %rax,%rcx
ffffffff812cbec7:	85 c9                	test   %ecx,%ecx
ffffffff812cbec9:	74 09                	je     ffffffff812cbed4 <__clear_user+0x3a>
ffffffff812cbecb:	88 17                	mov    %dl,(%rdi)
ffffffff812cbecd:	48 ff c7             	inc    %rdi
ffffffff812cbed0:	ff c9                	dec    %ecx
ffffffff812cbed2:	75 f7                	jne    ffffffff812cbecb <__clear_user+0x31>
ffffffff812cbed4:	90                   	nop
ffffffff812cbed5:	90                   	nop
ffffffff812cbed6:	90                   	nop
		: [size8] "=&c"(size), [dst] "=&D" (__d0)
		: [size1] "r"(size & 7), "[size8]" (size / 8), "[dst]"(addr),
		  [zero] "r" (0UL), [eight] "r" (8UL));
	clac();
	return size;
}
ffffffff812cbed7:	48 89 c8             	mov    %rcx,%rax
ffffffff812cbeda:	5d                   	pop    %rbp
ffffffff812cbedb:	c3                   	retq   

ffffffff812cbedc <clear_user>:
	 */
	if (__builtin_constant_p(size))
		return addr > limit - size;

	/* Arbitrary sizes? Be careful about overflow */
	addr += size;
ffffffff812cbedc:	48 8d 14 37          	lea    (%rdi,%rsi,1),%rdx
	if (addr < size)
ffffffff812cbee0:	48 89 f0             	mov    %rsi,%rax
ffffffff812cbee3:	65 48 8b 0c 25 04 22 	mov    %gs:0x12204,%rcx
ffffffff812cbeea:	01 00 
ffffffff812cbeec:	48 39 d6             	cmp    %rdx,%rsi
ffffffff812cbeef:	77 13                	ja     ffffffff812cbf04 <clear_user+0x28>
EXPORT_SYMBOL(__clear_user);

unsigned long clear_user(void __user *to, unsigned long n)
{
	if (access_ok(VERIFY_WRITE, to, n))
ffffffff812cbef1:	48 39 91 18 c0 ff ff 	cmp    %rdx,-0x3fe8(%rcx)
ffffffff812cbef8:	72 0a                	jb     ffffffff812cbf04 <clear_user+0x28>
	return size;
}
EXPORT_SYMBOL(__clear_user);

unsigned long clear_user(void __user *to, unsigned long n)
{
ffffffff812cbefa:	55                   	push   %rbp
ffffffff812cbefb:	48 89 e5             	mov    %rsp,%rbp
	if (access_ok(VERIFY_WRITE, to, n))
		return __clear_user(to, n);
ffffffff812cbefe:	e8 97 ff ff ff       	callq  ffffffff812cbe9a <__clear_user>
	return n;
}
ffffffff812cbf03:	5d                   	pop    %rbp
ffffffff812cbf04:	c3                   	retq   

ffffffff812cbf05 <copy_in_user>:
EXPORT_SYMBOL(clear_user);

unsigned long copy_in_user(void __user *to, const void __user *from, unsigned len)
{
ffffffff812cbf05:	55                   	push   %rbp
	if (access_ok(VERIFY_WRITE, to, len) && access_ok(VERIFY_READ, from, len)) { 
ffffffff812cbf06:	89 d0                	mov    %edx,%eax
ffffffff812cbf08:	49 89 f8             	mov    %rdi,%r8
ffffffff812cbf0b:	65 48 8b 0c 25 04 22 	mov    %gs:0x12204,%rcx
ffffffff812cbf12:	01 00 
ffffffff812cbf14:	49 01 c0             	add    %rax,%r8
	return n;
}
EXPORT_SYMBOL(clear_user);

unsigned long copy_in_user(void __user *to, const void __user *from, unsigned len)
{
ffffffff812cbf17:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cbf1a:	72 24                	jb     ffffffff812cbf40 <copy_in_user+0x3b>
	if (access_ok(VERIFY_WRITE, to, len) && access_ok(VERIFY_READ, from, len)) { 
ffffffff812cbf1c:	4c 39 81 18 c0 ff ff 	cmp    %r8,-0x3fe8(%rcx)
ffffffff812cbf23:	72 1b                	jb     ffffffff812cbf40 <copy_in_user+0x3b>
ffffffff812cbf25:	4c 8b 81 18 c0 ff ff 	mov    -0x3fe8(%rcx),%r8
ffffffff812cbf2c:	48 89 f1             	mov    %rsi,%rcx
ffffffff812cbf2f:	48 01 c1             	add    %rax,%rcx
ffffffff812cbf32:	72 0c                	jb     ffffffff812cbf40 <copy_in_user+0x3b>
ffffffff812cbf34:	49 39 c8             	cmp    %rcx,%r8
ffffffff812cbf37:	72 07                	jb     ffffffff812cbf40 <copy_in_user+0x3b>
ffffffff812cbf39:	e8 f2 eb ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
			 X86_FEATURE_ERMS,
			 ASM_OUTPUT2("=a" (ret), "=D" (to), "=S" (from),
				     "=d" (len)),
			 "1" (to), "2" (from), "3" (len)
			 : "memory", "rcx", "r8", "r9", "r10", "r11");
	return ret;
ffffffff812cbf3e:	89 c0                	mov    %eax,%eax
		return copy_user_generic((__force void *)to, (__force void *)from, len);
	} 
	return len;		
}
ffffffff812cbf40:	5d                   	pop    %rbp
ffffffff812cbf41:	c3                   	retq   

ffffffff812cbf42 <copy_user_handle_tail>:
 * Since protection fault in copy_from/to_user is not a normal situation,
 * it is not necessary to optimize tail handling.
 */
__visible unsigned long
copy_user_handle_tail(char *to, char *from, unsigned len)
{
ffffffff812cbf42:	55                   	push   %rbp
ffffffff812cbf43:	49 89 f8             	mov    %rdi,%r8
	for (; len; --len, to++) {
		char c;

		if (__get_user_nocheck(c, from++, sizeof(char)))
ffffffff812cbf46:	31 c0                	xor    %eax,%eax
 * Since protection fault in copy_from/to_user is not a normal situation,
 * it is not necessary to optimize tail handling.
 */
__visible unsigned long
copy_user_handle_tail(char *to, char *from, unsigned len)
{
ffffffff812cbf48:	48 89 e5             	mov    %rsp,%rbp
	for (; len; --len, to++) {
ffffffff812cbf4b:	85 d2                	test   %edx,%edx
ffffffff812cbf4d:	74 2a                	je     ffffffff812cbf79 <copy_user_handle_tail+0x37>
		char c;

		if (__get_user_nocheck(c, from++, sizeof(char)))
ffffffff812cbf4f:	48 8d 4e 01          	lea    0x1(%rsi),%rcx
ffffffff812cbf53:	89 c7                	mov    %eax,%edi
ffffffff812cbf55:	90                   	nop
ffffffff812cbf56:	90                   	nop
ffffffff812cbf57:	90                   	nop
ffffffff812cbf58:	40 8a 36             	mov    (%rsi),%sil
ffffffff812cbf5b:	90                   	nop
ffffffff812cbf5c:	90                   	nop
ffffffff812cbf5d:	90                   	nop
ffffffff812cbf5e:	85 ff                	test   %edi,%edi
ffffffff812cbf60:	75 17                	jne    ffffffff812cbf79 <copy_user_handle_tail+0x37>
ffffffff812cbf62:	90                   	nop
ffffffff812cbf63:	90                   	nop
ffffffff812cbf64:	90                   	nop
			break;
		if (__put_user_nocheck(c, to, sizeof(char)))
ffffffff812cbf65:	41 88 30             	mov    %sil,(%r8)
ffffffff812cbf68:	90                   	nop
ffffffff812cbf69:	90                   	nop
ffffffff812cbf6a:	90                   	nop
ffffffff812cbf6b:	85 ff                	test   %edi,%edi
ffffffff812cbf6d:	75 0a                	jne    ffffffff812cbf79 <copy_user_handle_tail+0x37>
 * it is not necessary to optimize tail handling.
 */
__visible unsigned long
copy_user_handle_tail(char *to, char *from, unsigned len)
{
	for (; len; --len, to++) {
ffffffff812cbf6f:	ff ca                	dec    %edx
ffffffff812cbf71:	49 ff c0             	inc    %r8
		char c;

		if (__get_user_nocheck(c, from++, sizeof(char)))
ffffffff812cbf74:	48 89 ce             	mov    %rcx,%rsi
ffffffff812cbf77:	eb d2                	jmp    ffffffff812cbf4b <copy_user_handle_tail+0x9>
ffffffff812cbf79:	90                   	nop
ffffffff812cbf7a:	90                   	nop
ffffffff812cbf7b:	90                   	nop
ffffffff812cbf7c:	65 48 8b 04 25 04 22 	mov    %gs:0x12204,%rax
ffffffff812cbf83:	01 00 
			break;
	}
	clac();

	/* If the destination is a kernel buffer, we always clear the end */
	if (!__addr_ok(to))
ffffffff812cbf85:	4c 3b 80 18 c0 ff ff 	cmp    -0x3fe8(%rax),%r8
ffffffff812cbf8c:	89 d2                	mov    %edx,%edx
ffffffff812cbf8e:	72 0a                	jb     ffffffff812cbf9a <copy_user_handle_tail+0x58>
		memset(to, 0, len);
ffffffff812cbf90:	31 c0                	xor    %eax,%eax
ffffffff812cbf92:	4c 89 c7             	mov    %r8,%rdi
ffffffff812cbf95:	48 89 d1             	mov    %rdx,%rcx
ffffffff812cbf98:	f3 aa                	rep stos %al,%es:(%rdi)
	return len;
}
ffffffff812cbf9a:	48 89 d0             	mov    %rdx,%rax
ffffffff812cbf9d:	5d                   	pop    %rbp
ffffffff812cbf9e:	c3                   	retq   

ffffffff812cbf9f <inat_get_opcode_attribute>:
/* Attribute tables are generated from opcode map */
#include "inat-tables.c"

/* Attribute search APIs */
insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
{
ffffffff812cbf9f:	55                   	push   %rbp
	return inat_primary_table[opcode];
ffffffff812cbfa0:	40 0f b6 ff          	movzbl %dil,%edi
ffffffff812cbfa4:	8b 04 bd a0 2c 64 81 	mov    -0x7e9bd360(,%rdi,4),%eax
/* Attribute tables are generated from opcode map */
#include "inat-tables.c"

/* Attribute search APIs */
insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
{
ffffffff812cbfab:	48 89 e5             	mov    %rsp,%rbp
	return inat_primary_table[opcode];
}
ffffffff812cbfae:	5d                   	pop    %rbp
ffffffff812cbfaf:	c3                   	retq   

ffffffff812cbfb0 <inat_get_last_prefix_id>:
#include "inat-tables.c"

/* Attribute search APIs */
insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
{
	return inat_primary_table[opcode];
ffffffff812cbfb0:	40 0f b6 ff          	movzbl %dil,%edi
}

int inat_get_last_prefix_id(insn_byte_t last_pfx)
{
ffffffff812cbfb4:	55                   	push   %rbp
static inline int inat_last_prefix_id(insn_attr_t attr)
{
	if ((attr & INAT_PFX_MASK) > INAT_LSTPFX_MAX)
		return 0;
	else
		return attr & INAT_PFX_MASK;
ffffffff812cbfb5:	ba 00 00 00 00       	mov    $0x0,%edx
#include "inat-tables.c"

/* Attribute search APIs */
insn_attr_t inat_get_opcode_attribute(insn_byte_t opcode)
{
	return inat_primary_table[opcode];
ffffffff812cbfba:	8b 04 bd a0 2c 64 81 	mov    -0x7e9bd360(,%rdi,4),%eax
}

int inat_get_last_prefix_id(insn_byte_t last_pfx)
{
ffffffff812cbfc1:	48 89 e5             	mov    %rsp,%rbp
	insn_attr_t lpfx_attr;

	lpfx_attr = inat_get_opcode_attribute(last_pfx);
	return inat_last_prefix_id(lpfx_attr);
}
ffffffff812cbfc4:	5d                   	pop    %rbp
	return (attr & INAT_PFX_MASK) == INAT_PFX_REX;
}

static inline int inat_last_prefix_id(insn_attr_t attr)
{
	if ((attr & INAT_PFX_MASK) > INAT_LSTPFX_MAX)
ffffffff812cbfc5:	83 e0 0f             	and    $0xf,%eax
		return 0;
	else
		return attr & INAT_PFX_MASK;
ffffffff812cbfc8:	83 f8 03             	cmp    $0x3,%eax
ffffffff812cbfcb:	0f 47 c2             	cmova  %edx,%eax
ffffffff812cbfce:	c3                   	retq   

ffffffff812cbfcf <inat_get_escape_attribute>:
	return attr & INAT_ESC_MASK;
}

static inline int inat_escape_id(insn_attr_t attr)
{
	return (attr & INAT_ESC_MASK) >> INAT_ESC_OFFS;
ffffffff812cbfcf:	48 c1 ea 04          	shr    $0x4,%rdx

insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
				      insn_attr_t esc_attr)
{
ffffffff812cbfd3:	55                   	push   %rbp
	const insn_attr_t *table;
	int n;

	n = inat_escape_id(esc_attr);

	table = inat_escape_tables[n][0];
ffffffff812cbfd4:	83 e2 03             	and    $0x3,%edx
ffffffff812cbfd7:	48 89 d0             	mov    %rdx,%rax
	return inat_last_prefix_id(lpfx_attr);
}

insn_attr_t inat_get_escape_attribute(insn_byte_t opcode, int lpfx_id,
				      insn_attr_t esc_attr)
{
ffffffff812cbfda:	48 89 e5             	mov    %rsp,%rbp
	const insn_attr_t *table;
	int n;

	n = inat_escape_id(esc_attr);

	table = inat_escape_tables[n][0];
ffffffff812cbfdd:	48 c1 e0 05          	shl    $0x5,%rax
ffffffff812cbfe1:	48 8b 88 80 fd 63 81 	mov    -0x7e9c0280(%rax),%rcx
	if (!table)
		return 0;
ffffffff812cbfe8:	31 c0                	xor    %eax,%eax
	int n;

	n = inat_escape_id(esc_attr);

	table = inat_escape_tables[n][0];
	if (!table)
ffffffff812cbfea:	48 85 c9             	test   %rcx,%rcx
ffffffff812cbfed:	74 28                	je     ffffffff812cc017 <inat_get_escape_attribute+0x48>
		return 0;
	if (inat_has_variant(table[opcode]) && lpfx_id) {
ffffffff812cbfef:	40 0f b6 ff          	movzbl %dil,%edi
ffffffff812cbff3:	f6 44 b9 02 04       	testb  $0x4,0x2(%rcx,%rdi,4)
ffffffff812cbff8:	74 1a                	je     ffffffff812cc014 <inat_get_escape_attribute+0x45>
ffffffff812cbffa:	85 f6                	test   %esi,%esi
ffffffff812cbffc:	74 16                	je     ffffffff812cc014 <inat_get_escape_attribute+0x45>
		table = inat_escape_tables[n][lpfx_id];
ffffffff812cbffe:	48 63 f6             	movslq %esi,%rsi
ffffffff812cc001:	48 8d 04 96          	lea    (%rsi,%rdx,4),%rax
ffffffff812cc005:	48 8b 0c c5 80 fd 63 	mov    -0x7e9c0280(,%rax,8),%rcx
ffffffff812cc00c:	81 

	n = inat_escape_id(esc_attr);

	table = inat_escape_tables[n][0];
	if (!table)
		return 0;
ffffffff812cc00d:	31 c0                	xor    %eax,%eax
	if (inat_has_variant(table[opcode]) && lpfx_id) {
		table = inat_escape_tables[n][lpfx_id];
		if (!table)
ffffffff812cc00f:	48 85 c9             	test   %rcx,%rcx
ffffffff812cc012:	74 03                	je     ffffffff812cc017 <inat_get_escape_attribute+0x48>
			return 0;
	}
	return table[opcode];
ffffffff812cc014:	8b 04 b9             	mov    (%rcx,%rdi,4),%eax
}
ffffffff812cc017:	5d                   	pop    %rbp
ffffffff812cc018:	c3                   	retq   

ffffffff812cc019 <inat_get_group_attribute>:
	return attr & INAT_GRP_MASK;
}

static inline int inat_group_id(insn_attr_t attr)
{
	return (attr & INAT_GRP_MASK) >> INAT_GRP_OFFS;
ffffffff812cc019:	48 89 d1             	mov    %rdx,%rcx

insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
				     insn_attr_t grp_attr)
{
ffffffff812cc01c:	55                   	push   %rbp
ffffffff812cc01d:	48 c1 e9 06          	shr    $0x6,%rcx
	const insn_attr_t *table;
	int n;

	n = inat_group_id(grp_attr);

	table = inat_group_tables[n][0];
ffffffff812cc021:	83 e1 1f             	and    $0x1f,%ecx
	return table[opcode];
}

insn_attr_t inat_get_group_attribute(insn_byte_t modrm, int lpfx_id,
				     insn_attr_t grp_attr)
{
ffffffff812cc024:	48 89 e5             	mov    %rsp,%rbp
	const insn_attr_t *table;
	int n;

	n = inat_group_id(grp_attr);

	table = inat_group_tables[n][0];
ffffffff812cc027:	48 89 c8             	mov    %rcx,%rax
ffffffff812cc02a:	48 c1 e0 05          	shl    $0x5,%rax
ffffffff812cc02e:	48 8b 80 80 f9 63 81 	mov    -0x7e9c0680(%rax),%rax
	if (!table)
ffffffff812cc035:	48 85 c0             	test   %rax,%rax
ffffffff812cc038:	75 09                	jne    ffffffff812cc043 <inat_get_group_attribute+0x2a>
		return inat_group_common_attribute(grp_attr);
ffffffff812cc03a:	89 d0                	mov    %edx,%eax
ffffffff812cc03c:	25 3f f8 ff ff       	and    $0xfffff83f,%eax
ffffffff812cc041:	eb 31                	jmp    ffffffff812cc074 <inat_get_group_attribute+0x5b>
	if (inat_has_variant(table[X86_MODRM_REG(modrm)]) && lpfx_id) {
ffffffff812cc043:	48 c1 ef 03          	shr    $0x3,%rdi
ffffffff812cc047:	83 e7 07             	and    $0x7,%edi
ffffffff812cc04a:	f6 44 b8 02 04       	testb  $0x4,0x2(%rax,%rdi,4)
ffffffff812cc04f:	74 18                	je     ffffffff812cc069 <inat_get_group_attribute+0x50>
ffffffff812cc051:	85 f6                	test   %esi,%esi
ffffffff812cc053:	74 14                	je     ffffffff812cc069 <inat_get_group_attribute+0x50>
		table = inat_group_tables[n][lpfx_id];
ffffffff812cc055:	48 63 f6             	movslq %esi,%rsi
ffffffff812cc058:	48 8d 04 8e          	lea    (%rsi,%rcx,4),%rax
ffffffff812cc05c:	48 8b 04 c5 80 f9 63 	mov    -0x7e9c0680(,%rax,8),%rax
ffffffff812cc063:	81 
		if (!table)
ffffffff812cc064:	48 85 c0             	test   %rax,%rax
ffffffff812cc067:	74 d1                	je     ffffffff812cc03a <inat_get_group_attribute+0x21>
			return inat_group_common_attribute(grp_attr);
	}
	return table[X86_MODRM_REG(modrm)] |
ffffffff812cc069:	81 e2 3f f8 ff ff    	and    $0xfffff83f,%edx
ffffffff812cc06f:	0b 14 b8             	or     (%rax,%rdi,4),%edx
ffffffff812cc072:	89 d0                	mov    %edx,%eax
	       inat_group_common_attribute(grp_attr);
}
ffffffff812cc074:	5d                   	pop    %rbp
ffffffff812cc075:	c3                   	retq   

ffffffff812cc076 <inat_get_avx_attribute>:

insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
				   insn_byte_t vex_p)
{
ffffffff812cc076:	55                   	push   %rbp
	const insn_attr_t *table;
	if (vex_m > X86_VEX_M_MAX || vex_p > INAT_LSTPFX_MAX)
ffffffff812cc077:	40 80 fe 1f          	cmp    $0x1f,%sil
	       inat_group_common_attribute(grp_attr);
}

insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
				   insn_byte_t vex_p)
{
ffffffff812cc07b:	48 89 e5             	mov    %rsp,%rbp
	const insn_attr_t *table;
	if (vex_m > X86_VEX_M_MAX || vex_p > INAT_LSTPFX_MAX)
ffffffff812cc07e:	77 4a                	ja     ffffffff812cc0ca <inat_get_avx_attribute+0x54>
ffffffff812cc080:	80 fa 03             	cmp    $0x3,%dl
ffffffff812cc083:	77 45                	ja     ffffffff812cc0ca <inat_get_avx_attribute+0x54>
		return 0;
	/* At first, this checks the master table */
	table = inat_avx_tables[vex_m][0];
ffffffff812cc085:	40 0f b6 f6          	movzbl %sil,%esi
ffffffff812cc089:	48 89 f0             	mov    %rsi,%rax
ffffffff812cc08c:	48 c1 e0 05          	shl    $0x5,%rax
ffffffff812cc090:	48 8b 88 80 f5 63 81 	mov    -0x7e9c0a80(%rax),%rcx
insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
				   insn_byte_t vex_p)
{
	const insn_attr_t *table;
	if (vex_m > X86_VEX_M_MAX || vex_p > INAT_LSTPFX_MAX)
		return 0;
ffffffff812cc097:	31 c0                	xor    %eax,%eax
	/* At first, this checks the master table */
	table = inat_avx_tables[vex_m][0];
	if (!table)
ffffffff812cc099:	48 85 c9             	test   %rcx,%rcx
ffffffff812cc09c:	74 2e                	je     ffffffff812cc0cc <inat_get_avx_attribute+0x56>
		return 0;
	if (!inat_is_group(table[opcode]) && vex_p) {
ffffffff812cc09e:	40 0f b6 ff          	movzbl %dil,%edi
ffffffff812cc0a2:	f7 04 b9 c0 07 00 00 	testl  $0x7c0,(%rcx,%rdi,4)
ffffffff812cc0a9:	75 1a                	jne    ffffffff812cc0c5 <inat_get_avx_attribute+0x4f>
ffffffff812cc0ab:	84 d2                	test   %dl,%dl
ffffffff812cc0ad:	74 16                	je     ffffffff812cc0c5 <inat_get_avx_attribute+0x4f>
		/* If this is not a group, get attribute directly */
		table = inat_avx_tables[vex_m][vex_p];
ffffffff812cc0af:	0f b6 d2             	movzbl %dl,%edx
ffffffff812cc0b2:	48 8d 04 b2          	lea    (%rdx,%rsi,4),%rax
ffffffff812cc0b6:	48 8b 0c c5 80 f5 63 	mov    -0x7e9c0a80(,%rax,8),%rcx
ffffffff812cc0bd:	81 
insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
				   insn_byte_t vex_p)
{
	const insn_attr_t *table;
	if (vex_m > X86_VEX_M_MAX || vex_p > INAT_LSTPFX_MAX)
		return 0;
ffffffff812cc0be:	31 c0                	xor    %eax,%eax
	if (!table)
		return 0;
	if (!inat_is_group(table[opcode]) && vex_p) {
		/* If this is not a group, get attribute directly */
		table = inat_avx_tables[vex_m][vex_p];
		if (!table)
ffffffff812cc0c0:	48 85 c9             	test   %rcx,%rcx
ffffffff812cc0c3:	74 07                	je     ffffffff812cc0cc <inat_get_avx_attribute+0x56>
			return 0;
	}
	return table[opcode];
ffffffff812cc0c5:	8b 04 b9             	mov    (%rcx,%rdi,4),%eax
ffffffff812cc0c8:	eb 02                	jmp    ffffffff812cc0cc <inat_get_avx_attribute+0x56>
insn_attr_t inat_get_avx_attribute(insn_byte_t opcode, insn_byte_t vex_m,
				   insn_byte_t vex_p)
{
	const insn_attr_t *table;
	if (vex_m > X86_VEX_M_MAX || vex_p > INAT_LSTPFX_MAX)
		return 0;
ffffffff812cc0ca:	31 c0                	xor    %eax,%eax
		table = inat_avx_tables[vex_m][vex_p];
		if (!table)
			return 0;
	}
	return table[opcode];
}
ffffffff812cc0cc:	5d                   	pop    %rbp
ffffffff812cc0cd:	c3                   	retq   

ffffffff812cc0ce <copy_from_user_overflow>:
#include <linux/export.h>
#include <linux/bug.h>
#include <linux/uaccess.h>

void copy_from_user_overflow(void)
{
ffffffff812cc0ce:	55                   	push   %rbp
	WARN(1, "Buffer overflow detected!\n");
ffffffff812cc0cf:	48 c7 c2 da 6d 7b 81 	mov    $0xffffffff817b6dda,%rdx
ffffffff812cc0d6:	be 07 00 00 00       	mov    $0x7,%esi
ffffffff812cc0db:	48 c7 c7 f5 6d 7b 81 	mov    $0xffffffff817b6df5,%rdi
ffffffff812cc0e2:	31 c0                	xor    %eax,%eax
#include <linux/export.h>
#include <linux/bug.h>
#include <linux/uaccess.h>

void copy_from_user_overflow(void)
{
ffffffff812cc0e4:	48 89 e5             	mov    %rsp,%rbp
	WARN(1, "Buffer overflow detected!\n");
ffffffff812cc0e7:	e8 4a a2 d9 ff       	callq  ffffffff81066336 <warn_slowpath_fmt>
}
ffffffff812cc0ec:	5d                   	pop    %rbp
ffffffff812cc0ed:	c3                   	retq   

ffffffff812cc0ee <lockref_get>:
 *
 * This operation is only valid if you already hold a reference
 * to the object, so you know the count cannot be zero.
 */
void lockref_get(struct lockref *lockref)
{
ffffffff812cc0ee:	55                   	push   %rbp
ffffffff812cc0ef:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc0f2:	53                   	push   %rbx
ffffffff812cc0f3:	50                   	push   %rax
ffffffff812cc0f4:	48 89 fb             	mov    %rdi,%rbx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812cc0f7:	e8 34 c5 16 00       	callq  ffffffff81438630 <_raw_spin_lock>
	,
		return;
	);

	spin_lock(&lockref->lock);
	lockref->count++;
ffffffff812cc0fc:	ff 43 18             	incl   0x18(%rbx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812cc0ff:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc102:	e8 a5 c5 16 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&lockref->lock);
}
ffffffff812cc107:	5a                   	pop    %rdx
ffffffff812cc108:	5b                   	pop    %rbx
ffffffff812cc109:	5d                   	pop    %rbp
ffffffff812cc10a:	c3                   	retq   

ffffffff812cc10b <lockref_get_not_zero>:
 * lockref_get_not_zero - Increments count unless the count is 0 or dead
 * @lockref: pointer to lockref structure
 * Return: 1 if count updated successfully or 0 if count was zero
 */
int lockref_get_not_zero(struct lockref *lockref)
{
ffffffff812cc10b:	55                   	push   %rbp
ffffffff812cc10c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc10f:	41 54                	push   %r12
ffffffff812cc111:	53                   	push   %rbx
ffffffff812cc112:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cc115:	45 31 e4             	xor    %r12d,%r12d
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812cc118:	e8 13 c5 16 00       	callq  ffffffff81438630 <_raw_spin_lock>
		return 1;
	);

	spin_lock(&lockref->lock);
	retval = 0;
	if (lockref->count > 0) {
ffffffff812cc11d:	8b 53 18             	mov    0x18(%rbx),%edx
ffffffff812cc120:	85 d2                	test   %edx,%edx
ffffffff812cc122:	7e 0b                	jle    ffffffff812cc12f <lockref_get_not_zero+0x24>
		lockref->count++;
ffffffff812cc124:	ff c2                	inc    %edx
		retval = 1;
ffffffff812cc126:	41 bc 01 00 00 00    	mov    $0x1,%r12d
	);

	spin_lock(&lockref->lock);
	retval = 0;
	if (lockref->count > 0) {
		lockref->count++;
ffffffff812cc12c:	89 53 18             	mov    %edx,0x18(%rbx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812cc12f:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc132:	e8 75 c5 16 00       	callq  ffffffff814386ac <_raw_spin_unlock>
		retval = 1;
	}
	spin_unlock(&lockref->lock);
	return retval;
}
ffffffff812cc137:	44 89 e0             	mov    %r12d,%eax
ffffffff812cc13a:	5b                   	pop    %rbx
ffffffff812cc13b:	41 5c                	pop    %r12
ffffffff812cc13d:	5d                   	pop    %rbp
ffffffff812cc13e:	c3                   	retq   

ffffffff812cc13f <lockref_get_or_lock>:
 * @lockref: pointer to lockref structure
 * Return: 1 if count updated successfully or 0 if count was zero
 * and we got the lock instead.
 */
int lockref_get_or_lock(struct lockref *lockref)
{
ffffffff812cc13f:	55                   	push   %rbp
ffffffff812cc140:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc143:	53                   	push   %rbx
ffffffff812cc144:	51                   	push   %rcx
ffffffff812cc145:	48 89 fb             	mov    %rdi,%rbx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812cc148:	e8 e3 c4 16 00       	callq  ffffffff81438630 <_raw_spin_lock>
	,
		return 1;
	);

	spin_lock(&lockref->lock);
	if (lockref->count <= 0)
ffffffff812cc14d:	8b 53 18             	mov    0x18(%rbx),%edx
ffffffff812cc150:	31 c0                	xor    %eax,%eax
ffffffff812cc152:	85 d2                	test   %edx,%edx
ffffffff812cc154:	7e 12                	jle    ffffffff812cc168 <lockref_get_or_lock+0x29>
		return 0;
	lockref->count++;
ffffffff812cc156:	ff c2                	inc    %edx
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812cc158:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc15b:	89 53 18             	mov    %edx,0x18(%rbx)
ffffffff812cc15e:	e8 49 c5 16 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&lockref->lock);
	return 1;
ffffffff812cc163:	b8 01 00 00 00       	mov    $0x1,%eax
}
ffffffff812cc168:	5a                   	pop    %rdx
ffffffff812cc169:	5b                   	pop    %rbx
ffffffff812cc16a:	5d                   	pop    %rbp
ffffffff812cc16b:	c3                   	retq   

ffffffff812cc16c <lockref_put_return>:
 *
 * Decrement the reference count and return the new value.
 * If the lockref was dead or locked, return an error.
 */
int lockref_put_return(struct lockref *lockref)
{
ffffffff812cc16c:	55                   	push   %rbp
			return -1;
	,
		return new.count;
	);
	return -1;
}
ffffffff812cc16d:	83 c8 ff             	or     $0xffffffff,%eax
 *
 * Decrement the reference count and return the new value.
 * If the lockref was dead or locked, return an error.
 */
int lockref_put_return(struct lockref *lockref)
{
ffffffff812cc170:	48 89 e5             	mov    %rsp,%rbp
			return -1;
	,
		return new.count;
	);
	return -1;
}
ffffffff812cc173:	5d                   	pop    %rbp
ffffffff812cc174:	c3                   	retq   

ffffffff812cc175 <lockref_put_or_lock>:
 * lockref_put_or_lock - decrements count unless count <= 1 before decrement
 * @lockref: pointer to lockref structure
 * Return: 1 if count updated successfully or 0 if count <= 1 and lock taken
 */
int lockref_put_or_lock(struct lockref *lockref)
{
ffffffff812cc175:	55                   	push   %rbp
ffffffff812cc176:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc179:	53                   	push   %rbx
ffffffff812cc17a:	51                   	push   %rcx
ffffffff812cc17b:	48 89 fb             	mov    %rdi,%rbx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812cc17e:	e8 ad c4 16 00       	callq  ffffffff81438630 <_raw_spin_lock>
	,
		return 1;
	);

	spin_lock(&lockref->lock);
	if (lockref->count <= 1)
ffffffff812cc183:	8b 53 18             	mov    0x18(%rbx),%edx
ffffffff812cc186:	31 c0                	xor    %eax,%eax
ffffffff812cc188:	83 fa 01             	cmp    $0x1,%edx
ffffffff812cc18b:	7e 12                	jle    ffffffff812cc19f <lockref_put_or_lock+0x2a>
		return 0;
	lockref->count--;
ffffffff812cc18d:	ff ca                	dec    %edx
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812cc18f:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc192:	89 53 18             	mov    %edx,0x18(%rbx)
ffffffff812cc195:	e8 12 c5 16 00       	callq  ffffffff814386ac <_raw_spin_unlock>
	spin_unlock(&lockref->lock);
	return 1;
ffffffff812cc19a:	b8 01 00 00 00       	mov    $0x1,%eax
}
ffffffff812cc19f:	5a                   	pop    %rdx
ffffffff812cc1a0:	5b                   	pop    %rbx
ffffffff812cc1a1:	5d                   	pop    %rbp
ffffffff812cc1a2:	c3                   	retq   

ffffffff812cc1a3 <lockref_get_not_dead>:
 * lockref_get_not_dead - Increments count unless the ref is dead
 * @lockref: pointer to lockref structure
 * Return: 1 if count updated successfully or 0 if lockref was dead
 */
int lockref_get_not_dead(struct lockref *lockref)
{
ffffffff812cc1a3:	55                   	push   %rbp
ffffffff812cc1a4:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc1a7:	41 54                	push   %r12
ffffffff812cc1a9:	53                   	push   %rbx
ffffffff812cc1aa:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cc1ad:	45 31 e4             	xor    %r12d,%r12d
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
ffffffff812cc1b0:	e8 7b c4 16 00       	callq  ffffffff81438630 <_raw_spin_lock>
		return 1;
	);

	spin_lock(&lockref->lock);
	retval = 0;
	if (lockref->count >= 0) {
ffffffff812cc1b5:	8b 53 18             	mov    0x18(%rbx),%edx
ffffffff812cc1b8:	85 d2                	test   %edx,%edx
ffffffff812cc1ba:	78 0b                	js     ffffffff812cc1c7 <lockref_get_not_dead+0x24>
		lockref->count++;
ffffffff812cc1bc:	ff c2                	inc    %edx
		retval = 1;
ffffffff812cc1be:	41 bc 01 00 00 00    	mov    $0x1,%r12d
	);

	spin_lock(&lockref->lock);
	retval = 0;
	if (lockref->count >= 0) {
		lockref->count++;
ffffffff812cc1c4:	89 53 18             	mov    %edx,0x18(%rbx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
ffffffff812cc1c7:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc1ca:	e8 dd c4 16 00       	callq  ffffffff814386ac <_raw_spin_unlock>
		retval = 1;
	}
	spin_unlock(&lockref->lock);
	return retval;
}
ffffffff812cc1cf:	44 89 e0             	mov    %r12d,%eax
ffffffff812cc1d2:	5b                   	pop    %rbx
ffffffff812cc1d3:	41 5c                	pop    %r12
ffffffff812cc1d5:	5d                   	pop    %rbp
ffffffff812cc1d6:	c3                   	retq   

ffffffff812cc1d7 <lockref_mark_dead>:

static __always_inline void __read_once_size(const volatile void *p, void *res, int size)
{
	switch (size) {
	case 1: *(__u8 *)res = *(volatile __u8 *)p; break;
	case 2: *(__u16 *)res = *(volatile __u16 *)p; break;
ffffffff812cc1d7:	66 8b 07             	mov    (%rdi),%ax
/**
 * lockref_mark_dead - mark lockref dead
 * @lockref: pointer to lockref structure
 */
void lockref_mark_dead(struct lockref *lockref)
{
ffffffff812cc1da:	55                   	push   %rbp
ffffffff812cc1db:	48 89 e5             	mov    %rsp,%rbp
	assert_spin_locked(&lockref->lock);
ffffffff812cc1de:	38 c4                	cmp    %al,%ah
ffffffff812cc1e0:	75 02                	jne    ffffffff812cc1e4 <lockref_mark_dead+0xd>
ffffffff812cc1e2:	0f 0b                	ud2    
	lockref->count = -128;
ffffffff812cc1e4:	c7 47 18 80 ff ff ff 	movl   $0xffffff80,0x18(%rdi)
}
ffffffff812cc1eb:	5d                   	pop    %rbp
ffffffff812cc1ec:	c3                   	retq   

ffffffff812cc1ed <_bcd2bin>:
#include <linux/bcd.h>
#include <linux/export.h>

unsigned _bcd2bin(unsigned char val)
{
	return (val & 0x0f) + (val >> 4) * 10;
ffffffff812cc1ed:	40 88 f8             	mov    %dil,%al
#include <linux/bcd.h>
#include <linux/export.h>

unsigned _bcd2bin(unsigned char val)
{
ffffffff812cc1f0:	55                   	push   %rbp
	return (val & 0x0f) + (val >> 4) * 10;
ffffffff812cc1f1:	83 e7 0f             	and    $0xf,%edi
ffffffff812cc1f4:	c0 e8 04             	shr    $0x4,%al
ffffffff812cc1f7:	0f b6 c0             	movzbl %al,%eax
#include <linux/bcd.h>
#include <linux/export.h>

unsigned _bcd2bin(unsigned char val)
{
ffffffff812cc1fa:	48 89 e5             	mov    %rsp,%rbp
	return (val & 0x0f) + (val >> 4) * 10;
ffffffff812cc1fd:	6b c0 0a             	imul   $0xa,%eax,%eax
}
ffffffff812cc200:	5d                   	pop    %rbp
#include <linux/bcd.h>
#include <linux/export.h>

unsigned _bcd2bin(unsigned char val)
{
	return (val & 0x0f) + (val >> 4) * 10;
ffffffff812cc201:	01 f8                	add    %edi,%eax
}
ffffffff812cc203:	c3                   	retq   

ffffffff812cc204 <_bin2bcd>:
EXPORT_SYMBOL(_bcd2bin);

unsigned char _bin2bcd(unsigned val)
{
	return ((val / 10) << 4) + val % 10;
ffffffff812cc204:	b9 0a 00 00 00       	mov    $0xa,%ecx
ffffffff812cc209:	89 f8                	mov    %edi,%eax
ffffffff812cc20b:	31 d2                	xor    %edx,%edx
ffffffff812cc20d:	f7 f1                	div    %ecx
	return (val & 0x0f) + (val >> 4) * 10;
}
EXPORT_SYMBOL(_bcd2bin);

unsigned char _bin2bcd(unsigned val)
{
ffffffff812cc20f:	55                   	push   %rbp
ffffffff812cc210:	48 89 e5             	mov    %rsp,%rbp
	return ((val / 10) << 4) + val % 10;
}
ffffffff812cc213:	5d                   	pop    %rbp
}
EXPORT_SYMBOL(_bcd2bin);

unsigned char _bin2bcd(unsigned val)
{
	return ((val / 10) << 4) + val % 10;
ffffffff812cc214:	c1 e0 04             	shl    $0x4,%eax
ffffffff812cc217:	01 d0                	add    %edx,%eax
}
ffffffff812cc219:	c3                   	retq   

ffffffff812cc21a <iter_div_u64_rem>:
/*
 * Iterative div/mod for use when dividend is not expected to be much
 * bigger than divisor.
 */
u32 iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{
ffffffff812cc21a:	55                   	push   %rbp
u32 iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder);

static __always_inline u32
__iter_div_u64_rem(u64 dividend, u32 divisor, u64 *remainder)
{
	u32 ret = 0;
ffffffff812cc21b:	31 c0                	xor    %eax,%eax

	while (dividend >= divisor) {
ffffffff812cc21d:	89 f6                	mov    %esi,%esi
ffffffff812cc21f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc222:	48 39 f7             	cmp    %rsi,%rdi
ffffffff812cc225:	72 07                	jb     ffffffff812cc22e <iter_div_u64_rem+0x14>
		/* The following asm() prevents the compiler from
		   optimising this loop into a modulo operation.  */
		asm("" : "+rm"(dividend));

		dividend -= divisor;
		ret++;
ffffffff812cc227:	ff c0                	inc    %eax
	while (dividend >= divisor) {
		/* The following asm() prevents the compiler from
		   optimising this loop into a modulo operation.  */
		asm("" : "+rm"(dividend));

		dividend -= divisor;
ffffffff812cc229:	48 29 f7             	sub    %rsi,%rdi
ffffffff812cc22c:	eb f4                	jmp    ffffffff812cc222 <iter_div_u64_rem+0x8>
		ret++;
	}

	*remainder = dividend;
ffffffff812cc22e:	48 89 3a             	mov    %rdi,(%rdx)
	return __iter_div_u64_rem(dividend, divisor, remainder);
}
ffffffff812cc231:	5d                   	pop    %rbp
ffffffff812cc232:	c3                   	retq   

ffffffff812cc233 <u32_swap>:
#include <linux/export.h>
#include <linux/sort.h>

static void u32_swap(void *a, void *b, int size)
{
	u32 t = *(u32 *)a;
ffffffff812cc233:	8b 07                	mov    (%rdi),%eax
	*(u32 *)a = *(u32 *)b;
ffffffff812cc235:	8b 16                	mov    (%rsi),%edx
#include <linux/types.h>
#include <linux/export.h>
#include <linux/sort.h>

static void u32_swap(void *a, void *b, int size)
{
ffffffff812cc237:	55                   	push   %rbp
	u32 t = *(u32 *)a;
	*(u32 *)a = *(u32 *)b;
ffffffff812cc238:	89 17                	mov    %edx,(%rdi)
#include <linux/types.h>
#include <linux/export.h>
#include <linux/sort.h>

static void u32_swap(void *a, void *b, int size)
{
ffffffff812cc23a:	48 89 e5             	mov    %rsp,%rbp
	u32 t = *(u32 *)a;
	*(u32 *)a = *(u32 *)b;
	*(u32 *)b = t;
ffffffff812cc23d:	89 06                	mov    %eax,(%rsi)
}
ffffffff812cc23f:	5d                   	pop    %rbp
ffffffff812cc240:	c3                   	retq   

ffffffff812cc241 <generic_swap>:

static void generic_swap(void *a, void *b, int size)
{
ffffffff812cc241:	55                   	push   %rbp
ffffffff812cc242:	31 c0                	xor    %eax,%eax
ffffffff812cc244:	48 89 e5             	mov    %rsp,%rbp
	char t;

	do {
		t = *(char *)a;
ffffffff812cc247:	8a 0c 07             	mov    (%rdi,%rax,1),%cl
		*(char *)a++ = *(char *)b;
ffffffff812cc24a:	44 8a 04 06          	mov    (%rsi,%rax,1),%r8b
ffffffff812cc24e:	44 88 04 07          	mov    %r8b,(%rdi,%rax,1)
		*(char *)b++ = t;
ffffffff812cc252:	88 0c 06             	mov    %cl,(%rsi,%rax,1)
ffffffff812cc255:	48 ff c0             	inc    %rax
ffffffff812cc258:	89 d1                	mov    %edx,%ecx
ffffffff812cc25a:	29 c1                	sub    %eax,%ecx
	} while (--size > 0);
ffffffff812cc25c:	85 c9                	test   %ecx,%ecx
ffffffff812cc25e:	7f e7                	jg     ffffffff812cc247 <generic_swap+0x6>
}
ffffffff812cc260:	5d                   	pop    %rbp
ffffffff812cc261:	c3                   	retq   

ffffffff812cc262 <sort>:
void sort(void *base, size_t num, size_t size,
	  int (*cmp_func)(const void *, const void *),
	  void (*swap_func)(void *, void *, int size))
{
	/* pre-scale counters for performance */
	int i = (num/2 - 1) * size, n = num * size, c, r;
ffffffff812cc262:	48 89 f0             	mov    %rsi,%rax
 */

void sort(void *base, size_t num, size_t size,
	  int (*cmp_func)(const void *, const void *),
	  void (*swap_func)(void *, void *, int size))
{
ffffffff812cc265:	55                   	push   %rbp
	/* pre-scale counters for performance */
	int i = (num/2 - 1) * size, n = num * size, c, r;
ffffffff812cc266:	48 d1 e8             	shr    %rax
ffffffff812cc269:	48 ff c8             	dec    %rax
 */

void sort(void *base, size_t num, size_t size,
	  int (*cmp_func)(const void *, const void *),
	  void (*swap_func)(void *, void *, int size))
{
ffffffff812cc26c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc26f:	41 57                	push   %r15
	/* pre-scale counters for performance */
	int i = (num/2 - 1) * size, n = num * size, c, r;
ffffffff812cc271:	0f af c2             	imul   %edx,%eax
 */

void sort(void *base, size_t num, size_t size,
	  int (*cmp_func)(const void *, const void *),
	  void (*swap_func)(void *, void *, int size))
{
ffffffff812cc274:	41 56                	push   %r14
ffffffff812cc276:	41 55                	push   %r13
ffffffff812cc278:	41 54                	push   %r12
ffffffff812cc27a:	53                   	push   %rbx
ffffffff812cc27b:	49 89 fc             	mov    %rdi,%r12
ffffffff812cc27e:	48 89 d3             	mov    %rdx,%rbx
ffffffff812cc281:	49 89 cf             	mov    %rcx,%r15
	/* pre-scale counters for performance */
	int i = (num/2 - 1) * size, n = num * size, c, r;
ffffffff812cc284:	41 89 d6             	mov    %edx,%r14d
 */

void sort(void *base, size_t num, size_t size,
	  int (*cmp_func)(const void *, const void *),
	  void (*swap_func)(void *, void *, int size))
{
ffffffff812cc287:	48 83 ec 38          	sub    $0x38,%rsp
	/* pre-scale counters for performance */
	int i = (num/2 - 1) * size, n = num * size, c, r;
ffffffff812cc28b:	89 45 c8             	mov    %eax,-0x38(%rbp)
ffffffff812cc28e:	89 f0                	mov    %esi,%eax
ffffffff812cc290:	0f af c2             	imul   %edx,%eax

	if (!swap_func)
ffffffff812cc293:	4d 85 c0             	test   %r8,%r8
ffffffff812cc296:	4d 89 c5             	mov    %r8,%r13
void sort(void *base, size_t num, size_t size,
	  int (*cmp_func)(const void *, const void *),
	  void (*swap_func)(void *, void *, int size))
{
	/* pre-scale counters for performance */
	int i = (num/2 - 1) * size, n = num * size, c, r;
ffffffff812cc299:	89 45 c0             	mov    %eax,-0x40(%rbp)

	if (!swap_func)
ffffffff812cc29c:	75 16                	jne    ffffffff812cc2b4 <sort+0x52>
		swap_func = (size == 4 ? u32_swap : generic_swap);
ffffffff812cc29e:	48 83 fa 04          	cmp    $0x4,%rdx
ffffffff812cc2a2:	48 c7 c0 41 c2 2c 81 	mov    $0xffffffff812cc241,%rax
ffffffff812cc2a9:	49 c7 c5 33 c2 2c 81 	mov    $0xffffffff812cc233,%r13
ffffffff812cc2b0:	4c 0f 45 e8          	cmovne %rax,%r13

	/* heapify */
	for ( ; i >= 0; i -= size) {
		for (r = i; r * 2 + size < n; r  = c) {
ffffffff812cc2b4:	48 63 45 c0          	movslq -0x40(%rbp),%rax
ffffffff812cc2b8:	48 89 45 a8          	mov    %rax,-0x58(%rbp)

	if (!swap_func)
		swap_func = (size == 4 ? u32_swap : generic_swap);

	/* heapify */
	for ( ; i >= 0; i -= size) {
ffffffff812cc2bc:	83 7d c8 00          	cmpl   $0x0,-0x38(%rbp)
ffffffff812cc2c0:	0f 88 88 00 00 00    	js     ffffffff812cc34e <sort+0xec>
ffffffff812cc2c6:	48 63 55 c8          	movslq -0x38(%rbp),%rdx
		for (r = i; r * 2 + size < n; r  = c) {
ffffffff812cc2ca:	8d 04 12             	lea    (%rdx,%rdx,1),%eax
ffffffff812cc2cd:	48 63 c8             	movslq %eax,%rcx
ffffffff812cc2d0:	48 01 d9             	add    %rbx,%rcx
ffffffff812cc2d3:	48 3b 4d a8          	cmp    -0x58(%rbp),%rcx
ffffffff812cc2d7:	73 59                	jae    ffffffff812cc332 <sort+0xd0>
			c = r * 2 + size;
ffffffff812cc2d9:	44 01 f0             	add    %r14d,%eax
			if (c < n - size &&
ffffffff812cc2dc:	48 63 f8             	movslq %eax,%rdi
		swap_func = (size == 4 ? u32_swap : generic_swap);

	/* heapify */
	for ( ; i >= 0; i -= size) {
		for (r = i; r * 2 + size < n; r  = c) {
			c = r * 2 + size;
ffffffff812cc2df:	89 45 cc             	mov    %eax,-0x34(%rbp)
			if (c < n - size &&
ffffffff812cc2e2:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
ffffffff812cc2e6:	48 29 d8             	sub    %rbx,%rax
ffffffff812cc2e9:	48 39 c7             	cmp    %rax,%rdi
ffffffff812cc2ec:	73 23                	jae    ffffffff812cc311 <sort+0xaf>
					cmp_func(base + c, base + c + size) < 0)
ffffffff812cc2ee:	48 8d 34 3b          	lea    (%rbx,%rdi,1),%rsi
ffffffff812cc2f2:	89 55 b8             	mov    %edx,-0x48(%rbp)
ffffffff812cc2f5:	4c 01 e7             	add    %r12,%rdi
ffffffff812cc2f8:	4c 01 e6             	add    %r12,%rsi
ffffffff812cc2fb:	41 ff d7             	callq  *%r15
				c += size;
ffffffff812cc2fe:	8b 4d cc             	mov    -0x34(%rbp),%ecx
ffffffff812cc301:	48 63 55 b8          	movslq -0x48(%rbp),%rdx
ffffffff812cc305:	44 01 f1             	add    %r14d,%ecx
ffffffff812cc308:	85 c0                	test   %eax,%eax
ffffffff812cc30a:	0f 49 4d cc          	cmovns -0x34(%rbp),%ecx
ffffffff812cc30e:	89 4d cc             	mov    %ecx,-0x34(%rbp)
			if (cmp_func(base + r, base + c) >= 0)
ffffffff812cc311:	48 63 45 cc          	movslq -0x34(%rbp),%rax
ffffffff812cc315:	4c 01 e0             	add    %r12,%rax
ffffffff812cc318:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812cc31c:	49 8d 04 14          	lea    (%r12,%rdx,1),%rax
ffffffff812cc320:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cc324:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
ffffffff812cc328:	48 89 c7             	mov    %rax,%rdi
ffffffff812cc32b:	41 ff d7             	callq  *%r15
ffffffff812cc32e:	85 c0                	test   %eax,%eax
ffffffff812cc330:	78 06                	js     ffffffff812cc338 <sort+0xd6>
ffffffff812cc332:	44 29 75 c8          	sub    %r14d,-0x38(%rbp)
ffffffff812cc336:	eb 84                	jmp    ffffffff812cc2bc <sort+0x5a>
				break;
			swap_func(base + r, base + c, size);
ffffffff812cc338:	89 da                	mov    %ebx,%edx
ffffffff812cc33a:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cc33e:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
ffffffff812cc342:	41 ff d5             	callq  *%r13
	if (!swap_func)
		swap_func = (size == 4 ? u32_swap : generic_swap);

	/* heapify */
	for ( ; i >= 0; i -= size) {
		for (r = i; r * 2 + size < n; r  = c) {
ffffffff812cc345:	48 63 55 cc          	movslq -0x34(%rbp),%rdx
ffffffff812cc349:	e9 7c ff ff ff       	jmpq   ffffffff812cc2ca <sort+0x68>
			swap_func(base + r, base + c, size);
		}
	}

	/* sort */
	for (i = n - size; i > 0; i -= size) {
ffffffff812cc34e:	8b 45 c0             	mov    -0x40(%rbp),%eax
		swap_func(base, base + i, size);
ffffffff812cc351:	89 5d a8             	mov    %ebx,-0x58(%rbp)
			swap_func(base + r, base + c, size);
		}
	}

	/* sort */
	for (i = n - size; i > 0; i -= size) {
ffffffff812cc354:	29 d8                	sub    %ebx,%eax
ffffffff812cc356:	89 45 c8             	mov    %eax,-0x38(%rbp)
ffffffff812cc359:	83 7d c8 00          	cmpl   $0x0,-0x38(%rbp)
ffffffff812cc35d:	0f 8e a1 00 00 00    	jle    ffffffff812cc404 <sort+0x1a2>
		swap_func(base, base + i, size);
ffffffff812cc363:	48 63 45 c8          	movslq -0x38(%rbp),%rax
ffffffff812cc367:	8b 55 a8             	mov    -0x58(%rbp),%edx
ffffffff812cc36a:	4c 89 e7             	mov    %r12,%rdi
ffffffff812cc36d:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812cc371:	49 8d 34 04          	lea    (%r12,%rax,1),%rsi
ffffffff812cc375:	41 ff d5             	callq  *%r13
		for (r = 0; r * 2 + size < i; r = c) {
			c = r * 2 + size;
			if (c < i - size &&
ffffffff812cc378:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
	}

	/* sort */
	for (i = n - size; i > 0; i -= size) {
		swap_func(base, base + i, size);
		for (r = 0; r * 2 + size < i; r = c) {
ffffffff812cc37c:	31 d2                	xor    %edx,%edx
			c = r * 2 + size;
			if (c < i - size &&
ffffffff812cc37e:	48 29 d8             	sub    %rbx,%rax
ffffffff812cc381:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
	}

	/* sort */
	for (i = n - size; i > 0; i -= size) {
		swap_func(base, base + i, size);
		for (r = 0; r * 2 + size < i; r = c) {
ffffffff812cc385:	8d 04 12             	lea    (%rdx,%rdx,1),%eax
ffffffff812cc388:	48 63 c8             	movslq %eax,%rcx
ffffffff812cc38b:	48 01 d9             	add    %rbx,%rcx
ffffffff812cc38e:	48 39 4d c0          	cmp    %rcx,-0x40(%rbp)
ffffffff812cc392:	76 53                	jbe    ffffffff812cc3e7 <sort+0x185>
			c = r * 2 + size;
ffffffff812cc394:	44 01 f0             	add    %r14d,%eax
			if (c < i - size &&
ffffffff812cc397:	48 63 f8             	movslq %eax,%rdi
ffffffff812cc39a:	48 3b 7d a0          	cmp    -0x60(%rbp),%rdi

	/* sort */
	for (i = n - size; i > 0; i -= size) {
		swap_func(base, base + i, size);
		for (r = 0; r * 2 + size < i; r = c) {
			c = r * 2 + size;
ffffffff812cc39e:	89 45 cc             	mov    %eax,-0x34(%rbp)
			if (c < i - size &&
ffffffff812cc3a1:	73 23                	jae    ffffffff812cc3c6 <sort+0x164>
					cmp_func(base + c, base + c + size) < 0)
ffffffff812cc3a3:	48 8d 34 3b          	lea    (%rbx,%rdi,1),%rsi
ffffffff812cc3a7:	89 55 b8             	mov    %edx,-0x48(%rbp)
ffffffff812cc3aa:	4c 01 e7             	add    %r12,%rdi
ffffffff812cc3ad:	4c 01 e6             	add    %r12,%rsi
ffffffff812cc3b0:	41 ff d7             	callq  *%r15
				c += size;
ffffffff812cc3b3:	8b 55 cc             	mov    -0x34(%rbp),%edx
ffffffff812cc3b6:	85 c0                	test   %eax,%eax
ffffffff812cc3b8:	41 8d 0c 16          	lea    (%r14,%rdx,1),%ecx
ffffffff812cc3bc:	0f 49 ca             	cmovns %edx,%ecx
ffffffff812cc3bf:	48 63 55 b8          	movslq -0x48(%rbp),%rdx
ffffffff812cc3c3:	89 4d cc             	mov    %ecx,-0x34(%rbp)
			if (cmp_func(base + r, base + c) >= 0)
ffffffff812cc3c6:	48 63 45 cc          	movslq -0x34(%rbp),%rax
ffffffff812cc3ca:	4c 01 e0             	add    %r12,%rax
ffffffff812cc3cd:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812cc3d1:	49 8d 04 14          	lea    (%r12,%rdx,1),%rax
ffffffff812cc3d5:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cc3d9:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
ffffffff812cc3dd:	48 89 c7             	mov    %rax,%rdi
ffffffff812cc3e0:	41 ff d7             	callq  *%r15
ffffffff812cc3e3:	85 c0                	test   %eax,%eax
ffffffff812cc3e5:	78 09                	js     ffffffff812cc3f0 <sort+0x18e>
ffffffff812cc3e7:	44 29 75 c8          	sub    %r14d,-0x38(%rbp)
ffffffff812cc3eb:	e9 69 ff ff ff       	jmpq   ffffffff812cc359 <sort+0xf7>
				break;
			swap_func(base + r, base + c, size);
ffffffff812cc3f0:	8b 55 a8             	mov    -0x58(%rbp),%edx
ffffffff812cc3f3:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cc3f7:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
ffffffff812cc3fb:	41 ff d5             	callq  *%r13
	}

	/* sort */
	for (i = n - size; i > 0; i -= size) {
		swap_func(base, base + i, size);
		for (r = 0; r * 2 + size < i; r = c) {
ffffffff812cc3fe:	48 63 55 cc          	movslq -0x34(%rbp),%rdx
ffffffff812cc402:	eb 81                	jmp    ffffffff812cc385 <sort+0x123>
			if (cmp_func(base + r, base + c) >= 0)
				break;
			swap_func(base + r, base + c, size);
		}
	}
}
ffffffff812cc404:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812cc408:	5b                   	pop    %rbx
ffffffff812cc409:	41 5c                	pop    %r12
ffffffff812cc40b:	41 5d                	pop    %r13
ffffffff812cc40d:	41 5e                	pop    %r14
ffffffff812cc40f:	41 5f                	pop    %r15
ffffffff812cc411:	5d                   	pop    %rbp
ffffffff812cc412:	c3                   	retq   

ffffffff812cc413 <match_wildcard>:
 *   '*' - matches zero or more characters
 *   '?' - matches one character
 * If it's matched, return true, else return false.
 */
bool match_wildcard(const char *pattern, const char *str)
{
ffffffff812cc413:	55                   	push   %rbp
	const char *s = str;
	const char *p = pattern;
ffffffff812cc414:	48 89 f8             	mov    %rdi,%rax
 *   '?' - matches one character
 * If it's matched, return true, else return false.
 */
bool match_wildcard(const char *pattern, const char *str)
{
	const char *s = str;
ffffffff812cc417:	48 89 f2             	mov    %rsi,%rdx
	const char *p = pattern;
	bool star = false;
ffffffff812cc41a:	45 31 c9             	xor    %r9d,%r9d
 *   '*' - matches zero or more characters
 *   '?' - matches one character
 * If it's matched, return true, else return false.
 */
bool match_wildcard(const char *pattern, const char *str)
{
ffffffff812cc41d:	48 89 e5             	mov    %rsp,%rbp
	const char *s = str;
	const char *p = pattern;
	bool star = false;

	while (*s) {
ffffffff812cc420:	44 8a 02             	mov    (%rdx),%r8b
ffffffff812cc423:	45 84 c0             	test   %r8b,%r8b
ffffffff812cc426:	74 45                	je     ffffffff812cc46d <match_wildcard+0x5a>
		switch (*p) {
ffffffff812cc428:	8a 08                	mov    (%rax),%cl
ffffffff812cc42a:	80 f9 2a             	cmp    $0x2a,%cl
ffffffff812cc42d:	74 07                	je     ffffffff812cc436 <match_wildcard+0x23>
ffffffff812cc42f:	80 f9 3f             	cmp    $0x3f,%cl
ffffffff812cc432:	75 17                	jne    ffffffff812cc44b <match_wildcard+0x38>
ffffffff812cc434:	eb 1a                	jmp    ffffffff812cc450 <match_wildcard+0x3d>
			p++;
			break;
		case '*':
			star = true;
			str = s;
			if (!*++p)
ffffffff812cc436:	80 78 01 00          	cmpb   $0x0,0x1(%rax)
ffffffff812cc43a:	48 8d 48 01          	lea    0x1(%rax),%rcx
ffffffff812cc43e:	74 3e                	je     ffffffff812cc47e <match_wildcard+0x6b>
			s++;
			p++;
			break;
		case '*':
			star = true;
			str = s;
ffffffff812cc440:	48 89 d6             	mov    %rdx,%rsi
			if (!*++p)
				return true;
			pattern = p;
ffffffff812cc443:	48 89 cf             	mov    %rcx,%rdi
		case '?':
			s++;
			p++;
			break;
		case '*':
			star = true;
ffffffff812cc446:	41 b1 01             	mov    $0x1,%r9b
ffffffff812cc449:	eb 1d                	jmp    ffffffff812cc468 <match_wildcard+0x55>
			if (!*++p)
				return true;
			pattern = p;
			break;
		default:
			if (*s == *p) {
ffffffff812cc44b:	41 38 c8             	cmp    %cl,%r8b
ffffffff812cc44e:	75 09                	jne    ffffffff812cc459 <match_wildcard+0x46>
				s++;
ffffffff812cc450:	48 ff c2             	inc    %rdx
				p++;
ffffffff812cc453:	48 8d 48 01          	lea    0x1(%rax),%rcx
ffffffff812cc457:	eb 0f                	jmp    ffffffff812cc468 <match_wildcard+0x55>
			} else {
				if (!star)
ffffffff812cc459:	45 84 c9             	test   %r9b,%r9b
ffffffff812cc45c:	74 24                	je     ffffffff812cc482 <match_wildcard+0x6f>
					return false;
				str++;
ffffffff812cc45e:	48 8d 56 01          	lea    0x1(%rsi),%rdx
				s = str;
				p = pattern;
ffffffff812cc462:	48 89 f9             	mov    %rdi,%rcx
				s++;
				p++;
			} else {
				if (!star)
					return false;
				str++;
ffffffff812cc465:	48 89 d6             	mov    %rdx,%rsi
ffffffff812cc468:	48 89 c8             	mov    %rcx,%rax
ffffffff812cc46b:	eb b3                	jmp    ffffffff812cc420 <match_wildcard+0xd>
			break;
		}
	}

	if (*p == '*')
		++p;
ffffffff812cc46d:	31 d2                	xor    %edx,%edx
ffffffff812cc46f:	80 38 2a             	cmpb   $0x2a,(%rax)
ffffffff812cc472:	0f 94 c2             	sete   %dl
	return !*p;
ffffffff812cc475:	80 3c 10 00          	cmpb   $0x0,(%rax,%rdx,1)
ffffffff812cc479:	0f 94 c0             	sete   %al
ffffffff812cc47c:	eb 06                	jmp    ffffffff812cc484 <match_wildcard+0x71>
			break;
		case '*':
			star = true;
			str = s;
			if (!*++p)
				return true;
ffffffff812cc47e:	b0 01                	mov    $0x1,%al
ffffffff812cc480:	eb 02                	jmp    ffffffff812cc484 <match_wildcard+0x71>
			if (*s == *p) {
				s++;
				p++;
			} else {
				if (!star)
					return false;
ffffffff812cc482:	31 c0                	xor    %eax,%eax
	}

	if (*p == '*')
		++p;
	return !*p;
}
ffffffff812cc484:	5d                   	pop    %rbp
ffffffff812cc485:	c3                   	retq   

ffffffff812cc486 <match_token>:
 * to it. Tokens can include up to MAX_OPT_ARGS instances of basic c-style
 * format identifiers which will be taken into account when matching the
 * tokens, and whose locations will be returned in the @args array.
 */
int match_token(char *s, const match_table_t table, substring_t args[])
{
ffffffff812cc486:	55                   	push   %rbp
ffffffff812cc487:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc48a:	41 57                	push   %r15
ffffffff812cc48c:	41 56                	push   %r14
ffffffff812cc48e:	41 55                	push   %r13
ffffffff812cc490:	41 54                	push   %r12
ffffffff812cc492:	49 89 f4             	mov    %rsi,%r12
ffffffff812cc495:	53                   	push   %rbx
ffffffff812cc496:	48 83 ec 38          	sub    $0x38,%rsp
ffffffff812cc49a:	48 89 7d b0          	mov    %rdi,-0x50(%rbp)
ffffffff812cc49e:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
	const struct match_token *p;

	for (p = table; !match_one(s, p->pattern, args) ; p++)
ffffffff812cc4a2:	49 8b 44 24 08       	mov    0x8(%r12),%rax
static int match_one(char *s, const char *p, substring_t args[])
{
	char *meta;
	int argc = 0;

	if (!p)
ffffffff812cc4a7:	48 85 c0             	test   %rax,%rax
ffffffff812cc4aa:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
ffffffff812cc4ae:	0f 84 60 01 00 00    	je     ffffffff812cc614 <match_token+0x18e>
ffffffff812cc4b4:	48 8b 5d b0          	mov    -0x50(%rbp),%rbx
ffffffff812cc4b8:	45 31 ed             	xor    %r13d,%r13d
		return 1;

	while(1) {
		int len = -1;
		meta = strchr(p, '%');
ffffffff812cc4bb:	4c 8b 7d c8          	mov    -0x38(%rbp),%r15
ffffffff812cc4bf:	be 25 00 00 00       	mov    $0x25,%esi
ffffffff812cc4c4:	4c 89 ff             	mov    %r15,%rdi
ffffffff812cc4c7:	e8 d1 b7 ff ff       	callq  ffffffff812c7c9d <strchr>
		if (!meta)
ffffffff812cc4cc:	48 85 c0             	test   %rax,%rax
	if (!p)
		return 1;

	while(1) {
		int len = -1;
		meta = strchr(p, '%');
ffffffff812cc4cf:	49 89 c6             	mov    %rax,%r14
		if (!meta)
ffffffff812cc4d2:	75 19                	jne    ffffffff812cc4ed <match_token+0x67>
			return strcmp(p, s) == 0;
ffffffff812cc4d4:	48 89 de             	mov    %rbx,%rsi
ffffffff812cc4d7:	4c 89 ff             	mov    %r15,%rdi
ffffffff812cc4da:	e8 79 b7 ff ff       	callq  ffffffff812c7c58 <strcmp>
 */
int match_token(char *s, const match_table_t table, substring_t args[])
{
	const struct match_token *p;

	for (p = table; !match_one(s, p->pattern, args) ; p++)
ffffffff812cc4df:	85 c0                	test   %eax,%eax
ffffffff812cc4e1:	0f 84 2d 01 00 00    	je     ffffffff812cc614 <match_token+0x18e>
ffffffff812cc4e7:	49 83 c4 10          	add    $0x10,%r12
ffffffff812cc4eb:	eb b5                	jmp    ffffffff812cc4a2 <match_token+0x1c>
		int len = -1;
		meta = strchr(p, '%');
		if (!meta)
			return strcmp(p, s) == 0;

		if (strncmp(p, s, meta-p))
ffffffff812cc4ed:	4c 29 f8             	sub    %r15,%rax
ffffffff812cc4f0:	48 89 de             	mov    %rbx,%rsi
ffffffff812cc4f3:	4c 89 ff             	mov    %r15,%rdi
ffffffff812cc4f6:	48 89 c2             	mov    %rax,%rdx
ffffffff812cc4f9:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
ffffffff812cc4fd:	e8 76 b7 ff ff       	callq  ffffffff812c7c78 <strncmp>
ffffffff812cc502:	85 c0                	test   %eax,%eax
ffffffff812cc504:	75 e1                	jne    ffffffff812cc4e7 <match_token+0x61>
			return 0;

		s += meta - p;
		p = meta + 1;
ffffffff812cc506:	49 8d 7e 01          	lea    0x1(%r14),%rdi
			return strcmp(p, s) == 0;

		if (strncmp(p, s, meta-p))
			return 0;

		s += meta - p;
ffffffff812cc50a:	48 03 5d b8          	add    -0x48(%rbp),%rbx
		p = meta + 1;
ffffffff812cc50e:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)

		if (isdigit(*p))
ffffffff812cc512:	41 0f b6 56 01       	movzbl 0x1(%r14),%edx
ffffffff812cc517:	f6 82 60 ef 63 81 04 	testb  $0x4,-0x7e9c10a0(%rdx)
ffffffff812cc51e:	48 89 d0             	mov    %rdx,%rax
ffffffff812cc521:	74 13                	je     ffffffff812cc536 <match_token+0xb0>
			len = simple_strtoul(p, (char **) &p, 10);
ffffffff812cc523:	48 8d 75 c8          	lea    -0x38(%rbp),%rsi
ffffffff812cc527:	ba 0a 00 00 00       	mov    $0xa,%edx
ffffffff812cc52c:	e8 9b c0 ff ff       	callq  ffffffff812c85cc <simple_strtoul>
ffffffff812cc531:	48 63 d0             	movslq %eax,%rdx
ffffffff812cc534:	eb 1d                	jmp    ffffffff812cc553 <match_token+0xcd>

	if (!p)
		return 1;

	while(1) {
		int len = -1;
ffffffff812cc536:	48 83 ca ff          	or     $0xffffffffffffffff,%rdx
		s += meta - p;
		p = meta + 1;

		if (isdigit(*p))
			len = simple_strtoul(p, (char **) &p, 10);
		else if (*p == '%') {
ffffffff812cc53a:	3c 25                	cmp    $0x25,%al
ffffffff812cc53c:	75 15                	jne    ffffffff812cc553 <match_token+0xcd>
			if (*s++ != '%')
ffffffff812cc53e:	80 3b 25             	cmpb   $0x25,(%rbx)
ffffffff812cc541:	75 a4                	jne    ffffffff812cc4e7 <match_token+0x61>
				return 0;
			p++;
ffffffff812cc543:	4d 8d 46 02          	lea    0x2(%r14),%r8
		p = meta + 1;

		if (isdigit(*p))
			len = simple_strtoul(p, (char **) &p, 10);
		else if (*p == '%') {
			if (*s++ != '%')
ffffffff812cc547:	48 ff c3             	inc    %rbx
				return 0;
			p++;
ffffffff812cc54a:	4c 89 45 c8          	mov    %r8,-0x38(%rbp)
ffffffff812cc54e:	e9 68 ff ff ff       	jmpq   ffffffff812cc4bb <match_token+0x35>
			continue;
		}

		if (argc >= MAX_OPT_ARGS)
ffffffff812cc553:	41 83 fd 02          	cmp    $0x2,%r13d
ffffffff812cc557:	7f 8e                	jg     ffffffff812cc4e7 <match_token+0x61>
			return 0;

		args[argc].from = s;
ffffffff812cc559:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
ffffffff812cc55d:	4d 63 c5             	movslq %r13d,%r8
ffffffff812cc560:	49 c1 e0 04          	shl    $0x4,%r8
ffffffff812cc564:	4e 8d 34 00          	lea    (%rax,%r8,1),%r14
		switch (*p++) {
ffffffff812cc568:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
		}

		if (argc >= MAX_OPT_ARGS)
			return 0;

		args[argc].from = s;
ffffffff812cc56c:	49 89 1e             	mov    %rbx,(%r14)
		switch (*p++) {
ffffffff812cc56f:	48 8d 48 01          	lea    0x1(%rax),%rcx
ffffffff812cc573:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
ffffffff812cc577:	8a 00                	mov    (%rax),%al
ffffffff812cc579:	3c 73                	cmp    $0x73,%al
ffffffff812cc57b:	74 30                	je     ffffffff812cc5ad <match_token+0x127>
ffffffff812cc57d:	7f 17                	jg     ffffffff812cc596 <match_token+0x110>
ffffffff812cc57f:	3c 64                	cmp    $0x64,%al
ffffffff812cc581:	74 5a                	je     ffffffff812cc5dd <match_token+0x157>
ffffffff812cc583:	3c 6f                	cmp    $0x6f,%al
ffffffff812cc585:	0f 85 5c ff ff ff    	jne    ffffffff812cc4e7 <match_token+0x61>
			goto num;
		case 'u':
			simple_strtoul(s, &args[argc].to, 0);
			goto num;
		case 'o':
			simple_strtoul(s, &args[argc].to, 8);
ffffffff812cc58b:	49 8d 76 08          	lea    0x8(%r14),%rsi
ffffffff812cc58f:	ba 08 00 00 00       	mov    $0x8,%edx
ffffffff812cc594:	eb 5d                	jmp    ffffffff812cc5f3 <match_token+0x16d>

		if (argc >= MAX_OPT_ARGS)
			return 0;

		args[argc].from = s;
		switch (*p++) {
ffffffff812cc596:	3c 75                	cmp    $0x75,%al
ffffffff812cc598:	74 53                	je     ffffffff812cc5ed <match_token+0x167>
ffffffff812cc59a:	3c 78                	cmp    $0x78,%al
ffffffff812cc59c:	0f 85 45 ff ff ff    	jne    ffffffff812cc4e7 <match_token+0x61>
			goto num;
		case 'o':
			simple_strtoul(s, &args[argc].to, 8);
			goto num;
		case 'x':
			simple_strtoul(s, &args[argc].to, 16);
ffffffff812cc5a2:	49 8d 76 08          	lea    0x8(%r14),%rsi
ffffffff812cc5a6:	ba 10 00 00 00       	mov    $0x10,%edx
ffffffff812cc5ab:	eb 46                	jmp    ffffffff812cc5f3 <match_token+0x16d>
			return 0;

		args[argc].from = s;
		switch (*p++) {
		case 's': {
			size_t str_len = strlen(s);
ffffffff812cc5ad:	31 c0                	xor    %eax,%eax
ffffffff812cc5af:	48 83 c9 ff          	or     $0xffffffffffffffff,%rcx
ffffffff812cc5b3:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc5b6:	f2 ae                	repnz scas %es:(%rdi),%al
ffffffff812cc5b8:	48 f7 d1             	not    %rcx

			if (str_len == 0)
ffffffff812cc5bb:	48 ff c9             	dec    %rcx
ffffffff812cc5be:	0f 84 23 ff ff ff    	je     ffffffff812cc4e7 <match_token+0x61>
				return 0;
			if (len == -1 || len > str_len)
ffffffff812cc5c4:	83 fa ff             	cmp    $0xffffffff,%edx
ffffffff812cc5c7:	74 08                	je     ffffffff812cc5d1 <match_token+0x14b>
ffffffff812cc5c9:	48 63 c2             	movslq %edx,%rax
ffffffff812cc5cc:	48 39 c1             	cmp    %rax,%rcx
ffffffff812cc5cf:	73 03                	jae    ffffffff812cc5d4 <match_token+0x14e>
				len = str_len;
ffffffff812cc5d1:	48 63 d1             	movslq %ecx,%rdx
			args[argc].to = s + len;
ffffffff812cc5d4:	48 01 d3             	add    %rdx,%rbx
ffffffff812cc5d7:	49 89 5e 08          	mov    %rbx,0x8(%r14)
ffffffff812cc5db:	eb 2b                	jmp    ffffffff812cc608 <match_token+0x182>
			break;
		}
		case 'd':
			simple_strtol(s, &args[argc].to, 0);
ffffffff812cc5dd:	49 8d 76 08          	lea    0x8(%r14),%rsi
ffffffff812cc5e1:	31 d2                	xor    %edx,%edx
ffffffff812cc5e3:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc5e6:	e8 30 c8 ff ff       	callq  ffffffff812c8e1b <simple_strtol>
ffffffff812cc5eb:	eb 0e                	jmp    ffffffff812cc5fb <match_token+0x175>
			goto num;
		case 'u':
			simple_strtoul(s, &args[argc].to, 0);
ffffffff812cc5ed:	49 8d 76 08          	lea    0x8(%r14),%rsi
ffffffff812cc5f1:	31 d2                	xor    %edx,%edx
			goto num;
		case 'o':
			simple_strtoul(s, &args[argc].to, 8);
			goto num;
		case 'x':
			simple_strtoul(s, &args[argc].to, 16);
ffffffff812cc5f3:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc5f6:	e8 d1 bf ff ff       	callq  ffffffff812c85cc <simple_strtoul>
		num:
			if (args[argc].to == args[argc].from)
ffffffff812cc5fb:	49 8b 06             	mov    (%r14),%rax
ffffffff812cc5fe:	49 39 46 08          	cmp    %rax,0x8(%r14)
ffffffff812cc602:	0f 84 df fe ff ff    	je     ffffffff812cc4e7 <match_token+0x61>
				return 0;
			break;
		default:
			return 0;
		}
		s = args[argc].to;
ffffffff812cc608:	49 8b 5e 08          	mov    0x8(%r14),%rbx
		argc++;
ffffffff812cc60c:	41 ff c5             	inc    %r13d
ffffffff812cc60f:	e9 a7 fe ff ff       	jmpq   ffffffff812cc4bb <match_token+0x35>
	const struct match_token *p;

	for (p = table; !match_one(s, p->pattern, args) ; p++)
		;

	return p->token;
ffffffff812cc614:	41 8b 04 24          	mov    (%r12),%eax
}
ffffffff812cc618:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812cc61c:	5b                   	pop    %rbx
ffffffff812cc61d:	41 5c                	pop    %r12
ffffffff812cc61f:	41 5d                	pop    %r13
ffffffff812cc621:	41 5e                	pop    %r14
ffffffff812cc623:	41 5f                	pop    %r15
ffffffff812cc625:	5d                   	pop    %rbp
ffffffff812cc626:	c3                   	retq   

ffffffff812cc627 <match_strlcpy>:
 * Description: Copy the characters in &substring_t @src to the
 * c-style string @dest.  Copy no more than @size - 1 characters, plus
 * the terminating NUL.  Return length of @src.
 */
size_t match_strlcpy(char *dest, const substring_t *src, size_t size)
{
ffffffff812cc627:	55                   	push   %rbp
	size_t ret = src->to - src->from;
ffffffff812cc628:	4c 8b 06             	mov    (%rsi),%r8
ffffffff812cc62b:	48 8b 46 08          	mov    0x8(%rsi),%rax
 * Description: Copy the characters in &substring_t @src to the
 * c-style string @dest.  Copy no more than @size - 1 characters, plus
 * the terminating NUL.  Return length of @src.
 */
size_t match_strlcpy(char *dest, const substring_t *src, size_t size)
{
ffffffff812cc62f:	48 89 e5             	mov    %rsp,%rbp
	size_t ret = src->to - src->from;
ffffffff812cc632:	4c 29 c0             	sub    %r8,%rax

	if (size) {
ffffffff812cc635:	48 85 d2             	test   %rdx,%rdx
ffffffff812cc638:	74 1b                	je     ffffffff812cc655 <match_strlcpy+0x2e>
		size_t len = ret >= size ? size - 1 : ret;
ffffffff812cc63a:	48 8d 4a ff          	lea    -0x1(%rdx),%rcx
ffffffff812cc63e:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cc641:	49 89 f9             	mov    %rdi,%r9
		memcpy(dest, src->from, len);
ffffffff812cc644:	4c 89 c6             	mov    %r8,%rsi
size_t match_strlcpy(char *dest, const substring_t *src, size_t size)
{
	size_t ret = src->to - src->from;

	if (size) {
		size_t len = ret >= size ? size - 1 : ret;
ffffffff812cc647:	48 0f 42 c8          	cmovb  %rax,%rcx
ffffffff812cc64b:	48 89 ca             	mov    %rcx,%rdx
		memcpy(dest, src->from, len);
ffffffff812cc64e:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
		dest[len] = '\0';
ffffffff812cc650:	41 c6 04 11 00       	movb   $0x0,(%r9,%rdx,1)
	}
	return ret;
}
ffffffff812cc655:	5d                   	pop    %rbp
ffffffff812cc656:	c3                   	retq   

ffffffff812cc657 <match_number>:
 * Description: Given a &substring_t and a base, attempts to parse the substring
 * as a number in that base. On success, sets @result to the integer represented
 * by the string and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
static int match_number(substring_t *s, int *result, int base)
{
ffffffff812cc657:	55                   	push   %rbp
ffffffff812cc658:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc65b:	41 56                	push   %r14
ffffffff812cc65d:	41 55                	push   %r13
ffffffff812cc65f:	41 54                	push   %r12
ffffffff812cc661:	53                   	push   %rbx
ffffffff812cc662:	49 89 fe             	mov    %rdi,%r14
ffffffff812cc665:	49 89 f5             	mov    %rsi,%r13
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812cc668:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812cc66d:	48 83 ec 20          	sub    $0x20,%rsp
	char *endp;
	char *buf;
	int ret;
	long val;
	size_t len = s->to - s->from;
ffffffff812cc671:	4c 8b 67 08          	mov    0x8(%rdi),%r12
ffffffff812cc675:	4c 2b 27             	sub    (%rdi),%r12
 * Description: Given a &substring_t and a base, attempts to parse the substring
 * as a number in that base. On success, sets @result to the integer represented
 * by the string and returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
static int match_number(substring_t *s, int *result, int base)
{
ffffffff812cc678:	89 55 cc             	mov    %edx,-0x34(%rbp)
ffffffff812cc67b:	49 8d 7c 24 01       	lea    0x1(%r12),%rdi
ffffffff812cc680:	e8 06 4b e3 ff       	callq  ffffffff8110118b <__kmalloc>
ffffffff812cc685:	48 89 c3             	mov    %rax,%rbx
ffffffff812cc688:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
	int ret;
	long val;
	size_t len = s->to - s->from;

	buf = kmalloc(len + 1, GFP_KERNEL);
	if (!buf)
ffffffff812cc68d:	48 85 db             	test   %rbx,%rbx
ffffffff812cc690:	74 55                	je     ffffffff812cc6e7 <match_number+0x90>
		return -ENOMEM;
	memcpy(buf, s->from, len);
ffffffff812cc692:	49 8b 36             	mov    (%r14),%rsi
ffffffff812cc695:	4c 89 e1             	mov    %r12,%rcx
ffffffff812cc698:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc69b:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
	buf[len] = '\0';

	ret = 0;
	val = simple_strtol(buf, &endp, base);
ffffffff812cc69d:	8b 55 cc             	mov    -0x34(%rbp),%edx
ffffffff812cc6a0:	48 8d 75 d8          	lea    -0x28(%rbp),%rsi

	buf = kmalloc(len + 1, GFP_KERNEL);
	if (!buf)
		return -ENOMEM;
	memcpy(buf, s->from, len);
	buf[len] = '\0';
ffffffff812cc6a4:	42 c6 04 23 00       	movb   $0x0,(%rbx,%r12,1)

	ret = 0;
	val = simple_strtol(buf, &endp, base);
ffffffff812cc6a9:	48 89 df             	mov    %rbx,%rdi
	if (endp == buf)
		ret = -EINVAL;
ffffffff812cc6ac:	41 bc ea ff ff ff    	mov    $0xffffffea,%r12d
		return -ENOMEM;
	memcpy(buf, s->from, len);
	buf[len] = '\0';

	ret = 0;
	val = simple_strtol(buf, &endp, base);
ffffffff812cc6b2:	e8 64 c7 ff ff       	callq  ffffffff812c8e1b <simple_strtol>
	if (endp == buf)
ffffffff812cc6b7:	48 39 5d d8          	cmp    %rbx,-0x28(%rbp)
ffffffff812cc6bb:	74 1f                	je     ffffffff812cc6dc <match_number+0x85>
		ret = -EINVAL;
	else if (val < (long)INT_MIN || val > (long)INT_MAX)
ffffffff812cc6bd:	ba 00 00 00 80       	mov    $0x80000000,%edx
ffffffff812cc6c2:	b9 ff ff ff ff       	mov    $0xffffffff,%ecx
		ret = -ERANGE;
ffffffff812cc6c7:	41 bc de ff ff ff    	mov    $0xffffffde,%r12d

	ret = 0;
	val = simple_strtol(buf, &endp, base);
	if (endp == buf)
		ret = -EINVAL;
	else if (val < (long)INT_MIN || val > (long)INT_MAX)
ffffffff812cc6cd:	48 01 c2             	add    %rax,%rdx
ffffffff812cc6d0:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812cc6d3:	77 07                	ja     ffffffff812cc6dc <match_number+0x85>
		ret = -ERANGE;
	else
		*result = (int) val;
ffffffff812cc6d5:	41 89 45 00          	mov    %eax,0x0(%r13)
	if (!buf)
		return -ENOMEM;
	memcpy(buf, s->from, len);
	buf[len] = '\0';

	ret = 0;
ffffffff812cc6d9:	45 31 e4             	xor    %r12d,%r12d
		ret = -EINVAL;
	else if (val < (long)INT_MIN || val > (long)INT_MAX)
		ret = -ERANGE;
	else
		*result = (int) val;
	kfree(buf);
ffffffff812cc6dc:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc6df:	e8 a5 42 e3 ff       	callq  ffffffff81100989 <kfree>
	return ret;
ffffffff812cc6e4:	44 89 e0             	mov    %r12d,%eax
}
ffffffff812cc6e7:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812cc6eb:	5b                   	pop    %rbx
ffffffff812cc6ec:	41 5c                	pop    %r12
ffffffff812cc6ee:	41 5d                	pop    %r13
ffffffff812cc6f0:	41 5e                	pop    %r14
ffffffff812cc6f2:	5d                   	pop    %rbp
ffffffff812cc6f3:	c3                   	retq   

ffffffff812cc6f4 <match_int>:
 * Description: Attempts to parse the &substring_t @s as a decimal integer. On
 * success, sets @result to the integer represented by the string and returns 0.
 * Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
int match_int(substring_t *s, int *result)
{
ffffffff812cc6f4:	55                   	push   %rbp
	return match_number(s, result, 0);
ffffffff812cc6f5:	31 d2                	xor    %edx,%edx
 * Description: Attempts to parse the &substring_t @s as a decimal integer. On
 * success, sets @result to the integer represented by the string and returns 0.
 * Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
int match_int(substring_t *s, int *result)
{
ffffffff812cc6f7:	48 89 e5             	mov    %rsp,%rbp
	return match_number(s, result, 0);
ffffffff812cc6fa:	e8 58 ff ff ff       	callq  ffffffff812cc657 <match_number>
}
ffffffff812cc6ff:	5d                   	pop    %rbp
ffffffff812cc700:	c3                   	retq   

ffffffff812cc701 <match_octal>:
 * Description: Attempts to parse the &substring_t @s as an octal integer. On
 * success, sets @result to the integer represented by the string and returns
 * 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
int match_octal(substring_t *s, int *result)
{
ffffffff812cc701:	55                   	push   %rbp
	return match_number(s, result, 8);
ffffffff812cc702:	ba 08 00 00 00       	mov    $0x8,%edx
 * Description: Attempts to parse the &substring_t @s as an octal integer. On
 * success, sets @result to the integer represented by the string and returns
 * 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
int match_octal(substring_t *s, int *result)
{
ffffffff812cc707:	48 89 e5             	mov    %rsp,%rbp
	return match_number(s, result, 8);
ffffffff812cc70a:	e8 48 ff ff ff       	callq  ffffffff812cc657 <match_number>
}
ffffffff812cc70f:	5d                   	pop    %rbp
ffffffff812cc710:	c3                   	retq   

ffffffff812cc711 <match_hex>:
 * Description: Attempts to parse the &substring_t @s as a hexadecimal integer.
 * On success, sets @result to the integer represented by the string and
 * returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
int match_hex(substring_t *s, int *result)
{
ffffffff812cc711:	55                   	push   %rbp
	return match_number(s, result, 16);
ffffffff812cc712:	ba 10 00 00 00       	mov    $0x10,%edx
 * Description: Attempts to parse the &substring_t @s as a hexadecimal integer.
 * On success, sets @result to the integer represented by the string and
 * returns 0. Returns -ENOMEM, -EINVAL, or -ERANGE on failure.
 */
int match_hex(substring_t *s, int *result)
{
ffffffff812cc717:	48 89 e5             	mov    %rsp,%rbp
	return match_number(s, result, 16);
ffffffff812cc71a:	e8 38 ff ff ff       	callq  ffffffff812cc657 <match_number>
}
ffffffff812cc71f:	5d                   	pop    %rbp
ffffffff812cc720:	c3                   	retq   

ffffffff812cc721 <match_strdup>:
 * Description: Allocates and returns a string filled with the contents of
 * the &substring_t @s. The caller is responsible for freeing the returned
 * string with kfree().
 */
char *match_strdup(const substring_t *s)
{
ffffffff812cc721:	55                   	push   %rbp
ffffffff812cc722:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812cc727:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc72a:	41 54                	push   %r12
ffffffff812cc72c:	53                   	push   %rbx
	size_t sz = s->to - s->from + 1;
ffffffff812cc72d:	48 8b 5f 08          	mov    0x8(%rdi),%rbx
ffffffff812cc731:	48 2b 1f             	sub    (%rdi),%rbx
 * Description: Allocates and returns a string filled with the contents of
 * the &substring_t @s. The caller is responsible for freeing the returned
 * string with kfree().
 */
char *match_strdup(const substring_t *s)
{
ffffffff812cc734:	49 89 fc             	mov    %rdi,%r12
	size_t sz = s->to - s->from + 1;
ffffffff812cc737:	48 ff c3             	inc    %rbx
ffffffff812cc73a:	48 89 df             	mov    %rbx,%rdi
ffffffff812cc73d:	e8 49 4a e3 ff       	callq  ffffffff8110118b <__kmalloc>
	char *p = kmalloc(sz, GFP_KERNEL);
	if (p)
ffffffff812cc742:	48 85 c0             	test   %rax,%rax
ffffffff812cc745:	49 89 c2             	mov    %rax,%r10
ffffffff812cc748:	74 0e                	je     ffffffff812cc758 <match_strdup+0x37>
		match_strlcpy(p, s, sz);
ffffffff812cc74a:	48 89 da             	mov    %rbx,%rdx
ffffffff812cc74d:	4c 89 e6             	mov    %r12,%rsi
ffffffff812cc750:	48 89 c7             	mov    %rax,%rdi
ffffffff812cc753:	e8 cf fe ff ff       	callq  ffffffff812cc627 <match_strlcpy>
	return p;
}
ffffffff812cc758:	5b                   	pop    %rbx
ffffffff812cc759:	4c 89 d0             	mov    %r10,%rax
ffffffff812cc75c:	41 5c                	pop    %r12
ffffffff812cc75e:	5d                   	pop    %rbp
ffffffff812cc75f:	c3                   	retq   

ffffffff812cc760 <half_md4_transform>:

/*
 * Basic cut-down MD4 transform.  Returns only 32 bits of result.
 */
__u32 half_md4_transform(__u32 buf[4], __u32 const in[8])
{
ffffffff812cc760:	55                   	push   %rbp
ffffffff812cc761:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc764:	41 57                	push   %r15
ffffffff812cc766:	41 56                	push   %r14
ffffffff812cc768:	41 55                	push   %r13
ffffffff812cc76a:	41 54                	push   %r12
ffffffff812cc76c:	53                   	push   %rbx
ffffffff812cc76d:	50                   	push   %rax
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
ffffffff812cc76e:	8b 4f 08             	mov    0x8(%rdi),%ecx
ffffffff812cc771:	33 4f 0c             	xor    0xc(%rdi),%ecx
ffffffff812cc774:	23 4f 04             	and    0x4(%rdi),%ecx
ffffffff812cc777:	44 8b 0e             	mov    (%rsi),%r9d
ffffffff812cc77a:	33 4f 0c             	xor    0xc(%rdi),%ecx
ffffffff812cc77d:	8b 17                	mov    (%rdi),%edx
	ROUND(F, d, a, b, c, in[1] + K1,  7);
ffffffff812cc77f:	44 8b 5e 04          	mov    0x4(%rsi),%r11d
ffffffff812cc783:	44 8b 7f 0c          	mov    0xc(%rdi),%r15d
	ROUND(F, c, d, a, b, in[2] + K1, 11);
ffffffff812cc787:	44 8b 47 04          	mov    0x4(%rdi),%r8d
ffffffff812cc78b:	44 8b 66 08          	mov    0x8(%rsi),%r12d
ffffffff812cc78f:	44 01 ca             	add    %r9d,%edx
	ROUND(F, b, c, d, a, in[3] + K1, 19);
ffffffff812cc792:	44 8b 76 0c          	mov    0xc(%rsi),%r14d
	ROUND(F, a, b, c, d, in[4] + K1,  3);
ffffffff812cc796:	8b 5e 10             	mov    0x10(%rsi),%ebx
__u32 half_md4_transform(__u32 buf[4], __u32 const in[8])
{
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
ffffffff812cc799:	01 d1                	add    %edx,%ecx
ffffffff812cc79b:	45 01 df             	add    %r11d,%r15d
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
	ROUND(F, a, b, c, d, in[4] + K1,  3);
	ROUND(F, d, a, b, c, in[5] + K1,  7);
ffffffff812cc79e:	44 8b 56 14          	mov    0x14(%rsi),%r10d
__u32 half_md4_transform(__u32 buf[4], __u32 const in[8])
{
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
ffffffff812cc7a2:	89 ca                	mov    %ecx,%edx
	ROUND(F, d, a, b, c, in[1] + K1,  7);
ffffffff812cc7a4:	8b 4f 04             	mov    0x4(%rdi),%ecx
ffffffff812cc7a7:	33 4f 08             	xor    0x8(%rdi),%ecx
__u32 half_md4_transform(__u32 buf[4], __u32 const in[8])
{
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
ffffffff812cc7aa:	c1 c2 03             	rol    $0x3,%edx
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
	ROUND(F, a, b, c, d, in[4] + K1,  3);
ffffffff812cc7ad:	89 5d d4             	mov    %ebx,-0x2c(%rbp)
	ROUND(F, d, a, b, c, in[5] + K1,  7);
	ROUND(F, c, d, a, b, in[6] + K1, 11);
	ROUND(F, b, c, d, a, in[7] + K1, 19);
ffffffff812cc7b0:	44 8b 6e 1c          	mov    0x1c(%rsi),%r13d
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
ffffffff812cc7b4:	41 31 d0             	xor    %edx,%r8d
	ROUND(F, b, c, d, a, in[3] + K1, 19);
ffffffff812cc7b7:	89 d0                	mov    %edx,%eax
{
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
ffffffff812cc7b9:	21 d1                	and    %edx,%ecx
ffffffff812cc7bb:	33 4f 08             	xor    0x8(%rdi),%ecx
ffffffff812cc7be:	44 01 f9             	add    %r15d,%ecx
ffffffff812cc7c1:	c1 c1 07             	rol    $0x7,%ecx
ffffffff812cc7c4:	41 89 cf             	mov    %ecx,%r15d
ffffffff812cc7c7:	8b 4f 08             	mov    0x8(%rdi),%ecx
	ROUND(F, c, d, a, b, in[2] + K1, 11);
ffffffff812cc7ca:	45 21 f8             	and    %r15d,%r8d
ffffffff812cc7cd:	44 33 47 04          	xor    0x4(%rdi),%r8d
	ROUND(F, b, c, d, a, in[3] + K1, 19);
ffffffff812cc7d1:	44 31 f8             	xor    %r15d,%eax
ffffffff812cc7d4:	44 01 e1             	add    %r12d,%ecx
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
ffffffff812cc7d7:	41 01 c8             	add    %ecx,%r8d
ffffffff812cc7da:	8b 4f 04             	mov    0x4(%rdi),%ecx
ffffffff812cc7dd:	41 c1 c0 0b          	rol    $0xb,%r8d
	ROUND(F, b, c, d, a, in[3] + K1, 19);
ffffffff812cc7e1:	44 21 c0             	and    %r8d,%eax
ffffffff812cc7e4:	44 01 f1             	add    %r14d,%ecx
ffffffff812cc7e7:	31 d0                	xor    %edx,%eax
ffffffff812cc7e9:	01 c8                	add    %ecx,%eax
ffffffff812cc7eb:	8d 0c 1a             	lea    (%rdx,%rbx,1),%ecx
	ROUND(F, a, b, c, d, in[4] + K1,  3);
ffffffff812cc7ee:	44 89 fa             	mov    %r15d,%edx

	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
ffffffff812cc7f1:	c1 c8 0d             	ror    $0xd,%eax
	ROUND(F, a, b, c, d, in[4] + K1,  3);
ffffffff812cc7f4:	44 31 c2             	xor    %r8d,%edx
	ROUND(F, d, a, b, c, in[5] + K1,  7);
	ROUND(F, c, d, a, b, in[6] + K1, 11);
ffffffff812cc7f7:	8b 5e 18             	mov    0x18(%rsi),%ebx
	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
	ROUND(F, a, b, c, d, in[4] + K1,  3);
ffffffff812cc7fa:	21 c2                	and    %eax,%edx
ffffffff812cc7fc:	44 31 fa             	xor    %r15d,%edx
ffffffff812cc7ff:	01 ca                	add    %ecx,%edx
ffffffff812cc801:	43 8d 0c 17          	lea    (%r15,%r10,1),%ecx
	ROUND(F, d, a, b, c, in[5] + K1,  7);
ffffffff812cc805:	45 89 c7             	mov    %r8d,%r15d
	/* Round 1 */
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
	ROUND(F, a, b, c, d, in[4] + K1,  3);
ffffffff812cc808:	c1 c2 03             	rol    $0x3,%edx
	ROUND(F, d, a, b, c, in[5] + K1,  7);
ffffffff812cc80b:	41 31 c7             	xor    %eax,%r15d
ffffffff812cc80e:	41 21 d7             	and    %edx,%r15d
	ROUND(F, c, d, a, b, in[6] + K1, 11);
	ROUND(F, b, c, d, a, in[7] + K1, 19);
ffffffff812cc811:	89 d6                	mov    %edx,%esi
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
	ROUND(F, a, b, c, d, in[4] + K1,  3);
	ROUND(F, d, a, b, c, in[5] + K1,  7);
ffffffff812cc813:	45 31 c7             	xor    %r8d,%r15d
ffffffff812cc816:	41 01 cf             	add    %ecx,%r15d
	ROUND(F, c, d, a, b, in[6] + K1, 11);
ffffffff812cc819:	89 c1                	mov    %eax,%ecx
ffffffff812cc81b:	41 01 d8             	add    %ebx,%r8d
	ROUND(F, a, b, c, d, in[0] + K1,  3);
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
	ROUND(F, a, b, c, d, in[4] + K1,  3);
	ROUND(F, d, a, b, c, in[5] + K1,  7);
ffffffff812cc81e:	41 c1 c7 07          	rol    $0x7,%r15d
	ROUND(F, c, d, a, b, in[6] + K1, 11);
ffffffff812cc822:	31 d1                	xor    %edx,%ecx
ffffffff812cc824:	44 21 f9             	and    %r15d,%ecx
	ROUND(F, b, c, d, a, in[7] + K1, 19);
ffffffff812cc827:	44 31 fe             	xor    %r15d,%esi
	ROUND(F, d, a, b, c, in[1] + K1,  7);
	ROUND(F, c, d, a, b, in[2] + K1, 11);
	ROUND(F, b, c, d, a, in[3] + K1, 19);
	ROUND(F, a, b, c, d, in[4] + K1,  3);
	ROUND(F, d, a, b, c, in[5] + K1,  7);
	ROUND(F, c, d, a, b, in[6] + K1, 11);
ffffffff812cc82a:	31 c1                	xor    %eax,%ecx
ffffffff812cc82c:	44 01 c1             	add    %r8d,%ecx
ffffffff812cc82f:	46 8d 04 28          	lea    (%rax,%r13,1),%r8d
ffffffff812cc833:	c1 c1 0b             	rol    $0xb,%ecx
	ROUND(F, b, c, d, a, in[7] + K1, 19);
ffffffff812cc836:	21 ce                	and    %ecx,%esi
ffffffff812cc838:	31 d6                	xor    %edx,%esi
ffffffff812cc83a:	42 8d 04 06          	lea    (%rsi,%r8,1),%eax
ffffffff812cc83e:	45 8d 84 13 99 79 82 	lea    0x5a827999(%r11,%rdx,1),%r8d
ffffffff812cc845:	5a 
ffffffff812cc846:	89 ca                	mov    %ecx,%edx
ffffffff812cc848:	43 8d b4 3e 99 79 82 	lea    0x5a827999(%r14,%r15,1),%esi
ffffffff812cc84f:	5a 
ffffffff812cc850:	c1 c8 0d             	ror    $0xd,%eax
ffffffff812cc853:	21 c2                	and    %eax,%edx
ffffffff812cc855:	44 01 c2             	add    %r8d,%edx

	/* Round 2 */
	ROUND(G, a, b, c, d, in[1] + K2,  3);
ffffffff812cc858:	41 89 c8             	mov    %ecx,%r8d
ffffffff812cc85b:	41 31 c0             	xor    %eax,%r8d
ffffffff812cc85e:	45 21 f8             	and    %r15d,%r8d
ffffffff812cc861:	44 8b 7d d4          	mov    -0x2c(%rbp),%r15d
ffffffff812cc865:	41 01 d0             	add    %edx,%r8d
ffffffff812cc868:	89 c2                	mov    %eax,%edx
ffffffff812cc86a:	41 c1 c0 03          	rol    $0x3,%r8d
ffffffff812cc86e:	44 21 c2             	and    %r8d,%edx
ffffffff812cc871:	01 d6                	add    %edx,%esi
	ROUND(G, d, a, b, c, in[3] + K2,  5);
ffffffff812cc873:	89 c2                	mov    %eax,%edx
ffffffff812cc875:	44 31 c2             	xor    %r8d,%edx
ffffffff812cc878:	21 ca                	and    %ecx,%edx
ffffffff812cc87a:	01 f2                	add    %esi,%edx
ffffffff812cc87c:	41 8d b4 0a 99 79 82 	lea    0x5a827999(%r10,%rcx,1),%esi
ffffffff812cc883:	5a 
ffffffff812cc884:	44 89 c1             	mov    %r8d,%ecx
ffffffff812cc887:	c1 c2 05             	rol    $0x5,%edx
ffffffff812cc88a:	21 d1                	and    %edx,%ecx
ffffffff812cc88c:	01 f1                	add    %esi,%ecx
	ROUND(G, c, d, a, b, in[5] + K2,  9);
ffffffff812cc88e:	44 89 c6             	mov    %r8d,%esi
ffffffff812cc891:	31 d6                	xor    %edx,%esi
ffffffff812cc893:	21 c6                	and    %eax,%esi
ffffffff812cc895:	01 ce                	add    %ecx,%esi
ffffffff812cc897:	41 8d 8c 05 99 79 82 	lea    0x5a827999(%r13,%rax,1),%ecx
ffffffff812cc89e:	5a 
ffffffff812cc89f:	89 d0                	mov    %edx,%eax
ffffffff812cc8a1:	c1 c6 09             	rol    $0x9,%esi
ffffffff812cc8a4:	21 f0                	and    %esi,%eax
ffffffff812cc8a6:	01 c8                	add    %ecx,%eax
	ROUND(G, b, c, d, a, in[7] + K2, 13);
ffffffff812cc8a8:	89 d1                	mov    %edx,%ecx
ffffffff812cc8aa:	31 f1                	xor    %esi,%ecx
ffffffff812cc8ac:	44 21 c1             	and    %r8d,%ecx
ffffffff812cc8af:	47 8d 84 01 99 79 82 	lea    0x5a827999(%r9,%r8,1),%r8d
ffffffff812cc8b6:	5a 
ffffffff812cc8b7:	01 c1                	add    %eax,%ecx
ffffffff812cc8b9:	89 f0                	mov    %esi,%eax
ffffffff812cc8bb:	c1 c1 0d             	rol    $0xd,%ecx
ffffffff812cc8be:	21 c8                	and    %ecx,%eax
ffffffff812cc8c0:	41 01 c0             	add    %eax,%r8d
	ROUND(G, a, b, c, d, in[0] + K2,  3);
ffffffff812cc8c3:	89 f0                	mov    %esi,%eax
ffffffff812cc8c5:	31 c8                	xor    %ecx,%eax
ffffffff812cc8c7:	21 d0                	and    %edx,%eax
ffffffff812cc8c9:	44 01 c0             	add    %r8d,%eax
ffffffff812cc8cc:	45 8d 84 14 99 79 82 	lea    0x5a827999(%r12,%rdx,1),%r8d
ffffffff812cc8d3:	5a 
ffffffff812cc8d4:	89 ca                	mov    %ecx,%edx
ffffffff812cc8d6:	c1 c0 03             	rol    $0x3,%eax
ffffffff812cc8d9:	21 c2                	and    %eax,%edx
ffffffff812cc8db:	41 01 d0             	add    %edx,%r8d
	ROUND(G, d, a, b, c, in[2] + K2,  5);
ffffffff812cc8de:	89 ca                	mov    %ecx,%edx
ffffffff812cc8e0:	31 c2                	xor    %eax,%edx
ffffffff812cc8e2:	21 f2                	and    %esi,%edx
ffffffff812cc8e4:	44 01 c2             	add    %r8d,%edx
ffffffff812cc8e7:	45 8d 84 37 99 79 82 	lea    0x5a827999(%r15,%rsi,1),%r8d
ffffffff812cc8ee:	5a 
ffffffff812cc8ef:	89 c6                	mov    %eax,%esi
ffffffff812cc8f1:	c1 c2 05             	rol    $0x5,%edx
ffffffff812cc8f4:	44 8d bc 0b 99 79 82 	lea    0x5a827999(%rbx,%rcx,1),%r15d
ffffffff812cc8fb:	5a 
ffffffff812cc8fc:	21 d6                	and    %edx,%esi
ffffffff812cc8fe:	41 01 f0             	add    %esi,%r8d
	ROUND(G, c, d, a, b, in[4] + K2,  9);
ffffffff812cc901:	89 c6                	mov    %eax,%esi
ffffffff812cc903:	31 d6                	xor    %edx,%esi
ffffffff812cc905:	21 ce                	and    %ecx,%esi
ffffffff812cc907:	89 d1                	mov    %edx,%ecx
ffffffff812cc909:	44 01 c6             	add    %r8d,%esi
	ROUND(G, b, c, d, a, in[6] + K2, 13);
ffffffff812cc90c:	41 89 d0             	mov    %edx,%r8d
	ROUND(G, d, a, b, c, in[3] + K2,  5);
	ROUND(G, c, d, a, b, in[5] + K2,  9);
	ROUND(G, b, c, d, a, in[7] + K2, 13);
	ROUND(G, a, b, c, d, in[0] + K2,  3);
	ROUND(G, d, a, b, c, in[2] + K2,  5);
	ROUND(G, c, d, a, b, in[4] + K2,  9);
ffffffff812cc90f:	c1 c6 09             	rol    $0x9,%esi
ffffffff812cc912:	21 f1                	and    %esi,%ecx
	ROUND(G, b, c, d, a, in[6] + K2, 13);
ffffffff812cc914:	41 31 f0             	xor    %esi,%r8d
ffffffff812cc917:	44 01 f9             	add    %r15d,%ecx
ffffffff812cc91a:	41 89 c7             	mov    %eax,%r15d
ffffffff812cc91d:	41 8d 84 06 a1 eb d9 	lea    0x6ed9eba1(%r14,%rax,1),%eax
ffffffff812cc924:	6e 
ffffffff812cc925:	45 21 c7             	and    %r8d,%r15d
ffffffff812cc928:	44 01 f9             	add    %r15d,%ecx
ffffffff812cc92b:	c1 c1 0d             	rol    $0xd,%ecx

	/* Round 3 */
	ROUND(H, a, b, c, d, in[3] + K3,  3);
ffffffff812cc92e:	41 31 c8             	xor    %ecx,%r8d
ffffffff812cc931:	41 01 c0             	add    %eax,%r8d
ffffffff812cc934:	41 8d 84 15 a1 eb d9 	lea    0x6ed9eba1(%r13,%rdx,1),%eax
ffffffff812cc93b:	6e 
	ROUND(H, d, a, b, c, in[7] + K3,  9);
ffffffff812cc93c:	89 f2                	mov    %esi,%edx
	ROUND(G, d, a, b, c, in[2] + K2,  5);
	ROUND(G, c, d, a, b, in[4] + K2,  9);
	ROUND(G, b, c, d, a, in[6] + K2, 13);

	/* Round 3 */
	ROUND(H, a, b, c, d, in[3] + K3,  3);
ffffffff812cc93e:	41 c1 c0 03          	rol    $0x3,%r8d
	ROUND(H, d, a, b, c, in[7] + K3,  9);
ffffffff812cc942:	31 ca                	xor    %ecx,%edx
ffffffff812cc944:	44 31 c2             	xor    %r8d,%edx
ffffffff812cc947:	01 d0                	add    %edx,%eax
ffffffff812cc949:	41 8d 94 34 a1 eb d9 	lea    0x6ed9eba1(%r12,%rsi,1),%edx
ffffffff812cc950:	6e 
	ROUND(H, c, d, a, b, in[2] + K3, 11);
ffffffff812cc951:	89 ce                	mov    %ecx,%esi
	ROUND(G, c, d, a, b, in[4] + K2,  9);
	ROUND(G, b, c, d, a, in[6] + K2, 13);

	/* Round 3 */
	ROUND(H, a, b, c, d, in[3] + K3,  3);
	ROUND(H, d, a, b, c, in[7] + K3,  9);
ffffffff812cc953:	c1 c0 09             	rol    $0x9,%eax
	ROUND(H, c, d, a, b, in[2] + K3, 11);
ffffffff812cc956:	44 31 c6             	xor    %r8d,%esi
ffffffff812cc959:	8d 8c 0b a1 eb d9 6e 	lea    0x6ed9eba1(%rbx,%rcx,1),%ecx
ffffffff812cc960:	31 c6                	xor    %eax,%esi
ffffffff812cc962:	01 f2                	add    %esi,%edx
	ROUND(H, b, c, d, a, in[6] + K3, 15);
ffffffff812cc964:	44 89 c6             	mov    %r8d,%esi
ffffffff812cc967:	47 8d 84 03 a1 eb d9 	lea    0x6ed9eba1(%r11,%r8,1),%r8d
ffffffff812cc96e:	6e 
	ROUND(G, b, c, d, a, in[6] + K2, 13);

	/* Round 3 */
	ROUND(H, a, b, c, d, in[3] + K3,  3);
	ROUND(H, d, a, b, c, in[7] + K3,  9);
	ROUND(H, c, d, a, b, in[2] + K3, 11);
ffffffff812cc96f:	c1 c2 0b             	rol    $0xb,%edx
	ROUND(H, b, c, d, a, in[6] + K3, 15);
ffffffff812cc972:	31 c6                	xor    %eax,%esi
ffffffff812cc974:	31 d6                	xor    %edx,%esi
ffffffff812cc976:	01 ce                	add    %ecx,%esi
	ROUND(H, a, b, c, d, in[1] + K3,  3);
ffffffff812cc978:	89 c1                	mov    %eax,%ecx
ffffffff812cc97a:	41 8d 84 02 a1 eb d9 	lea    0x6ed9eba1(%r10,%rax,1),%eax
ffffffff812cc981:	6e 

	/* Round 3 */
	ROUND(H, a, b, c, d, in[3] + K3,  3);
	ROUND(H, d, a, b, c, in[7] + K3,  9);
	ROUND(H, c, d, a, b, in[2] + K3, 11);
	ROUND(H, b, c, d, a, in[6] + K3, 15);
ffffffff812cc982:	c1 c6 0f             	rol    $0xf,%esi
	ROUND(H, a, b, c, d, in[1] + K3,  3);
ffffffff812cc985:	31 d1                	xor    %edx,%ecx
ffffffff812cc987:	31 f1                	xor    %esi,%ecx
ffffffff812cc989:	44 01 c1             	add    %r8d,%ecx
	ROUND(H, d, a, b, c, in[5] + K3,  9);
ffffffff812cc98c:	41 89 d0             	mov    %edx,%r8d
	/* Round 3 */
	ROUND(H, a, b, c, d, in[3] + K3,  3);
	ROUND(H, d, a, b, c, in[7] + K3,  9);
	ROUND(H, c, d, a, b, in[2] + K3, 11);
	ROUND(H, b, c, d, a, in[6] + K3, 15);
	ROUND(H, a, b, c, d, in[1] + K3,  3);
ffffffff812cc98f:	c1 c1 03             	rol    $0x3,%ecx
	ROUND(H, d, a, b, c, in[5] + K3,  9);
ffffffff812cc992:	41 31 f0             	xor    %esi,%r8d
ffffffff812cc995:	41 31 c8             	xor    %ecx,%r8d
ffffffff812cc998:	41 01 c0             	add    %eax,%r8d
ffffffff812cc99b:	41 8d 84 11 a1 eb d9 	lea    0x6ed9eba1(%r9,%rdx,1),%eax
ffffffff812cc9a2:	6e 
	ROUND(H, c, d, a, b, in[0] + K3, 11);
ffffffff812cc9a3:	89 f2                	mov    %esi,%edx
	ROUND(H, a, b, c, d, in[3] + K3,  3);
	ROUND(H, d, a, b, c, in[7] + K3,  9);
	ROUND(H, c, d, a, b, in[2] + K3, 11);
	ROUND(H, b, c, d, a, in[6] + K3, 15);
	ROUND(H, a, b, c, d, in[1] + K3,  3);
	ROUND(H, d, a, b, c, in[5] + K3,  9);
ffffffff812cc9a5:	41 c1 c0 09          	rol    $0x9,%r8d
	ROUND(H, c, d, a, b, in[0] + K3, 11);
ffffffff812cc9a9:	31 ca                	xor    %ecx,%edx
ffffffff812cc9ab:	44 31 c2             	xor    %r8d,%edx
ffffffff812cc9ae:	01 c2                	add    %eax,%edx
ffffffff812cc9b0:	8b 45 d4             	mov    -0x2c(%rbp),%eax
ffffffff812cc9b3:	c1 c2 0b             	rol    $0xb,%edx
ffffffff812cc9b6:	8d b4 30 a1 eb d9 6e 	lea    0x6ed9eba1(%rax,%rsi,1),%esi
	ROUND(H, b, c, d, a, in[4] + K3, 15);
ffffffff812cc9bd:	89 c8                	mov    %ecx,%eax

	buf[0] += a;
ffffffff812cc9bf:	03 0f                	add    (%rdi),%ecx
	ROUND(H, c, d, a, b, in[2] + K3, 11);
	ROUND(H, b, c, d, a, in[6] + K3, 15);
	ROUND(H, a, b, c, d, in[1] + K3,  3);
	ROUND(H, d, a, b, c, in[5] + K3,  9);
	ROUND(H, c, d, a, b, in[0] + K3, 11);
	ROUND(H, b, c, d, a, in[4] + K3, 15);
ffffffff812cc9c1:	44 31 c0             	xor    %r8d,%eax

	buf[0] += a;
	buf[1] += b;
	buf[2] += c;
	buf[3] += d;
ffffffff812cc9c4:	44 03 47 0c          	add    0xc(%rdi),%r8d
	ROUND(H, c, d, a, b, in[2] + K3, 11);
	ROUND(H, b, c, d, a, in[6] + K3, 15);
	ROUND(H, a, b, c, d, in[1] + K3,  3);
	ROUND(H, d, a, b, c, in[5] + K3,  9);
	ROUND(H, c, d, a, b, in[0] + K3, 11);
	ROUND(H, b, c, d, a, in[4] + K3, 15);
ffffffff812cc9c8:	31 d0                	xor    %edx,%eax

	buf[0] += a;
	buf[1] += b;
	buf[2] += c;
ffffffff812cc9ca:	03 57 08             	add    0x8(%rdi),%edx
	ROUND(H, c, d, a, b, in[2] + K3, 11);
	ROUND(H, b, c, d, a, in[6] + K3, 15);
	ROUND(H, a, b, c, d, in[1] + K3,  3);
	ROUND(H, d, a, b, c, in[5] + K3,  9);
	ROUND(H, c, d, a, b, in[0] + K3, 11);
	ROUND(H, b, c, d, a, in[4] + K3, 15);
ffffffff812cc9cd:	01 f0                	add    %esi,%eax

	buf[0] += a;
	buf[1] += b;
ffffffff812cc9cf:	c1 c0 0f             	rol    $0xf,%eax
ffffffff812cc9d2:	03 47 04             	add    0x4(%rdi),%eax
	ROUND(H, a, b, c, d, in[1] + K3,  3);
	ROUND(H, d, a, b, c, in[5] + K3,  9);
	ROUND(H, c, d, a, b, in[0] + K3, 11);
	ROUND(H, b, c, d, a, in[4] + K3, 15);

	buf[0] += a;
ffffffff812cc9d5:	89 0f                	mov    %ecx,(%rdi)
	buf[1] += b;
	buf[2] += c;
	buf[3] += d;
ffffffff812cc9d7:	44 89 47 0c          	mov    %r8d,0xc(%rdi)
	ROUND(H, c, d, a, b, in[0] + K3, 11);
	ROUND(H, b, c, d, a, in[4] + K3, 15);

	buf[0] += a;
	buf[1] += b;
	buf[2] += c;
ffffffff812cc9db:	89 57 08             	mov    %edx,0x8(%rdi)
	ROUND(H, d, a, b, c, in[5] + K3,  9);
	ROUND(H, c, d, a, b, in[0] + K3, 11);
	ROUND(H, b, c, d, a, in[4] + K3, 15);

	buf[0] += a;
	buf[1] += b;
ffffffff812cc9de:	89 47 04             	mov    %eax,0x4(%rdi)
	buf[2] += c;
	buf[3] += d;

	return buf[1]; /* "most hashed" word */
}
ffffffff812cc9e1:	5a                   	pop    %rdx
ffffffff812cc9e2:	5b                   	pop    %rbx
ffffffff812cc9e3:	41 5c                	pop    %r12
ffffffff812cc9e5:	41 5d                	pop    %r13
ffffffff812cc9e7:	41 5e                	pop    %r14
ffffffff812cc9e9:	41 5f                	pop    %r15
ffffffff812cc9eb:	5d                   	pop    %rbp
ffffffff812cc9ec:	c3                   	retq   

ffffffff812cc9ed <debug_locks_off>:

/*
 * Generic 'turn off all lock debugging' function:
 */
int debug_locks_off(void)
{
ffffffff812cc9ed:	55                   	push   %rbp
extern int debug_locks_silent;


static inline int __debug_locks_off(void)
{
	return xchg(&debug_locks, 0);
ffffffff812cc9ee:	31 d2                	xor    %edx,%edx
ffffffff812cc9f0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cc9f3:	87 15 87 96 76 00    	xchg   %edx,0x769687(%rip)        # ffffffff81a36080 <debug_locks>
		if (!debug_locks_silent) {
			console_verbose();
			return 1;
		}
	}
	return 0;
ffffffff812cc9f9:	31 c0                	xor    %eax,%eax
/*
 * Generic 'turn off all lock debugging' function:
 */
int debug_locks_off(void)
{
	if (__debug_locks_off()) {
ffffffff812cc9fb:	85 d2                	test   %edx,%edx
ffffffff812cc9fd:	74 21                	je     ffffffff812cca20 <debug_locks_off+0x33>
		if (!debug_locks_silent) {
ffffffff812cc9ff:	83 3d fa 4e 8e 00 00 	cmpl   $0x0,0x8e4efa(%rip)        # ffffffff81bb1900 <debug_locks_silent>
ffffffff812cca06:	75 18                	jne    ffffffff812cca20 <debug_locks_off+0x33>
	console_loglevel = CONSOLE_LOGLEVEL_SILENT;
}

static inline void console_verbose(void)
{
	if (console_loglevel)
ffffffff812cca08:	83 3d 41 83 75 00 00 	cmpl   $0x0,0x758341(%rip)        # ffffffff81a24d50 <console_printk>
			console_verbose();
			return 1;
ffffffff812cca0f:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812cca14:	74 0a                	je     ffffffff812cca20 <debug_locks_off+0x33>
		console_loglevel = CONSOLE_LOGLEVEL_MOTORMOUTH;
ffffffff812cca16:	c7 05 30 83 75 00 0f 	movl   $0xf,0x758330(%rip)        # ffffffff81a24d50 <console_printk>
ffffffff812cca1d:	00 00 00 
		}
	}
	return 0;
}
ffffffff812cca20:	5d                   	pop    %rbp
ffffffff812cca21:	c3                   	retq   

ffffffff812cca22 <prandom_u32_state>:
 *	For more random results, use prandom_u32().
 */
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
ffffffff812cca22:	44 8b 0f             	mov    (%rdi),%r9d
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
ffffffff812cca25:	44 8b 47 04          	mov    0x4(%rdi),%r8d
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
ffffffff812cca29:	8b 4f 08             	mov    0x8(%rdi),%ecx
	state->s4 = TAUSWORTHE(state->s4,  3U, 12U, 4294967168U, 13U);
ffffffff812cca2c:	8b 57 0c             	mov    0xc(%rdi),%edx
 *
 *	This is used for pseudo-randomness with no outside seeding.
 *	For more random results, use prandom_u32().
 */
u32 prandom_u32_state(struct rnd_state *state)
{
ffffffff812cca2f:	55                   	push   %rbp
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
ffffffff812cca30:	45 89 ca             	mov    %r9d,%r10d
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
ffffffff812cca33:	42 8d 04 85 00 00 00 	lea    0x0(,%r8,4),%eax
ffffffff812cca3a:	00 
 *	For more random results, use prandom_u32().
 */
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
ffffffff812cca3b:	41 c1 e2 06          	shl    $0x6,%r10d
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
ffffffff812cca3f:	89 ce                	mov    %ecx,%esi
 *
 *	This is used for pseudo-randomness with no outside seeding.
 *	For more random results, use prandom_u32().
 */
u32 prandom_u32_state(struct rnd_state *state)
{
ffffffff812cca41:	48 89 e5             	mov    %rsp,%rbp
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
ffffffff812cca44:	45 31 ca             	xor    %r9d,%r10d
ffffffff812cca47:	41 83 e1 fe          	and    $0xfffffffe,%r9d
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
ffffffff812cca4b:	44 31 c0             	xor    %r8d,%eax
 *	For more random results, use prandom_u32().
 */
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
ffffffff812cca4e:	41 c1 e1 12          	shl    $0x12,%r9d
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
ffffffff812cca52:	41 83 e0 f8          	and    $0xfffffff8,%r8d
 *	For more random results, use prandom_u32().
 */
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
ffffffff812cca56:	41 c1 ea 0d          	shr    $0xd,%r10d
ffffffff812cca5a:	45 31 ca             	xor    %r9d,%r10d
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
ffffffff812cca5d:	45 89 c1             	mov    %r8d,%r9d
ffffffff812cca60:	c1 e8 1b             	shr    $0x1b,%eax
ffffffff812cca63:	41 c1 e1 02          	shl    $0x2,%r9d
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
ffffffff812cca67:	c1 e6 0d             	shl    $0xd,%esi
 *	For more random results, use prandom_u32().
 */
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
ffffffff812cca6a:	44 89 17             	mov    %r10d,(%rdi)
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
ffffffff812cca6d:	44 31 c8             	xor    %r9d,%eax
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
ffffffff812cca70:	31 ce                	xor    %ecx,%esi
ffffffff812cca72:	83 e1 f0             	and    $0xfffffff0,%ecx
 */
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
ffffffff812cca75:	41 89 c0             	mov    %eax,%r8d
ffffffff812cca78:	89 47 04             	mov    %eax,0x4(%rdi)
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
	state->s4 = TAUSWORTHE(state->s4,  3U, 12U, 4294967168U, 13U);
ffffffff812cca7b:	8d 04 d5 00 00 00 00 	lea    0x0(,%rdx,8),%eax
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
ffffffff812cca82:	c1 e1 07             	shl    $0x7,%ecx
ffffffff812cca85:	c1 ee 15             	shr    $0x15,%esi
	state->s4 = TAUSWORTHE(state->s4,  3U, 12U, 4294967168U, 13U);
ffffffff812cca88:	31 d0                	xor    %edx,%eax
ffffffff812cca8a:	83 e2 80             	and    $0xffffff80,%edx
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
ffffffff812cca8d:	31 ce                	xor    %ecx,%esi
	state->s4 = TAUSWORTHE(state->s4,  3U, 12U, 4294967168U, 13U);
ffffffff812cca8f:	89 d1                	mov    %edx,%ecx
ffffffff812cca91:	c1 e8 0c             	shr    $0xc,%eax
u32 prandom_u32_state(struct rnd_state *state)
{
#define TAUSWORTHE(s, a, b, c, d) ((s & c) << d) ^ (((s << a) ^ s) >> b)
	state->s1 = TAUSWORTHE(state->s1,  6U, 13U, 4294967294U, 18U);
	state->s2 = TAUSWORTHE(state->s2,  2U, 27U, 4294967288U,  2U);
	state->s3 = TAUSWORTHE(state->s3, 13U, 21U, 4294967280U,  7U);
ffffffff812cca94:	89 77 08             	mov    %esi,0x8(%rdi)
	state->s4 = TAUSWORTHE(state->s4,  3U, 12U, 4294967168U, 13U);
ffffffff812cca97:	c1 e1 0d             	shl    $0xd,%ecx
ffffffff812cca9a:	31 c8                	xor    %ecx,%eax
ffffffff812cca9c:	89 c2                	mov    %eax,%edx
ffffffff812cca9e:	89 47 0c             	mov    %eax,0xc(%rdi)

	return (state->s1 ^ state->s2 ^ state->s3 ^ state->s4);
ffffffff812ccaa1:	44 89 d0             	mov    %r10d,%eax
ffffffff812ccaa4:	44 31 c0             	xor    %r8d,%eax
ffffffff812ccaa7:	31 f0                	xor    %esi,%eax
ffffffff812ccaa9:	31 d0                	xor    %edx,%eax
}
ffffffff812ccaab:	5d                   	pop    %rbp
ffffffff812ccaac:	c3                   	retq   

ffffffff812ccaad <prandom_u32>:
 *	A 32 bit pseudo-random number is generated using a fast
 *	algorithm suitable for simulation. This algorithm is NOT
 *	considered safe for cryptographic use.
 */
u32 prandom_u32(void)
{
ffffffff812ccaad:	55                   	push   %rbp
ffffffff812ccaae:	48 89 e5             	mov    %rsp,%rbp
	struct rnd_state *state = &get_cpu_var(net_rand_state);
ffffffff812ccab1:	48 c7 c7 10 0a 01 00 	mov    $0x10a10,%rdi
ffffffff812ccab8:	65 48 03 3d 58 d6 d3 	add    %gs:0x7ed3d658(%rip),%rdi        # a118 <this_cpu_off>
ffffffff812ccabf:	7e 
	u32 res;

	res = prandom_u32_state(state);
ffffffff812ccac0:	e8 5d ff ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	put_cpu_var(state);

	return res;
}
ffffffff812ccac5:	5d                   	pop    %rbp
ffffffff812ccac6:	c3                   	retq   

ffffffff812ccac7 <prandom_bytes_state>:
 *
 *	This is used for pseudo-randomness with no outside seeding.
 *	For more random results, use prandom_bytes().
 */
void prandom_bytes_state(struct rnd_state *state, void *buf, size_t bytes)
{
ffffffff812ccac7:	55                   	push   %rbp
ffffffff812ccac8:	49 89 d3             	mov    %rdx,%r11
ffffffff812ccacb:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ccace:	41 55                	push   %r13
ffffffff812ccad0:	49 89 fd             	mov    %rdi,%r13
ffffffff812ccad3:	41 54                	push   %r12
ffffffff812ccad5:	49 89 f4             	mov    %rsi,%r12
ffffffff812ccad8:	53                   	push   %rbx
	u8 *ptr = buf;
ffffffff812ccad9:	48 89 f3             	mov    %rsi,%rbx
ffffffff812ccadc:	4c 89 d8             	mov    %r11,%rax
ffffffff812ccadf:	48 29 d8             	sub    %rbx,%rax
ffffffff812ccae2:	4c 01 e0             	add    %r12,%rax

	while (bytes >= sizeof(u32)) {
ffffffff812ccae5:	48 83 f8 03          	cmp    $0x3,%rax
ffffffff812ccae9:	76 11                	jbe    ffffffff812ccafc <prandom_bytes_state+0x35>
		put_unaligned(prandom_u32_state(state), (u32 *) ptr);
ffffffff812ccaeb:	4c 89 ef             	mov    %r13,%rdi
		ptr += sizeof(u32);
ffffffff812ccaee:	48 83 c3 04          	add    $0x4,%rbx
void prandom_bytes_state(struct rnd_state *state, void *buf, size_t bytes)
{
	u8 *ptr = buf;

	while (bytes >= sizeof(u32)) {
		put_unaligned(prandom_u32_state(state), (u32 *) ptr);
ffffffff812ccaf2:	e8 2b ff ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	*((__le16 *)p) = cpu_to_le16(val);
}

static inline void put_unaligned_le32(u32 val, void *p)
{
	*((__le32 *)p) = cpu_to_le32(val);
ffffffff812ccaf7:	89 43 fc             	mov    %eax,-0x4(%rbx)
ffffffff812ccafa:	eb e0                	jmp    ffffffff812ccadc <prandom_bytes_state+0x15>
ffffffff812ccafc:	4c 89 d8             	mov    %r11,%rax
ffffffff812ccaff:	48 c1 e8 02          	shr    $0x2,%rax
		ptr += sizeof(u32);
		bytes -= sizeof(u32);
	}

	if (bytes > 0) {
ffffffff812ccb03:	41 83 e3 03          	and    $0x3,%r11d
ffffffff812ccb07:	49 8d 1c 84          	lea    (%r12,%rax,4),%rbx
ffffffff812ccb0b:	74 18                	je     ffffffff812ccb25 <prandom_bytes_state+0x5e>
		u32 rem = prandom_u32_state(state);
ffffffff812ccb0d:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ccb10:	e8 0d ff ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
ffffffff812ccb15:	31 d2                	xor    %edx,%edx
		do {
			*ptr++ = (u8) rem;
ffffffff812ccb17:	88 04 13             	mov    %al,(%rbx,%rdx,1)
ffffffff812ccb1a:	48 ff c2             	inc    %rdx
			bytes--;
			rem >>= BITS_PER_BYTE;
ffffffff812ccb1d:	c1 e8 08             	shr    $0x8,%eax
		} while (bytes > 0);
ffffffff812ccb20:	49 39 d3             	cmp    %rdx,%r11
ffffffff812ccb23:	75 f2                	jne    ffffffff812ccb17 <prandom_bytes_state+0x50>
	}
}
ffffffff812ccb25:	5b                   	pop    %rbx
ffffffff812ccb26:	41 5c                	pop    %r12
ffffffff812ccb28:	41 5d                	pop    %r13
ffffffff812ccb2a:	5d                   	pop    %rbp
ffffffff812ccb2b:	c3                   	retq   

ffffffff812ccb2c <prandom_bytes>:
 *	prandom_bytes - get the requested number of pseudo-random bytes
 *	@buf: where to copy the pseudo-random bytes to
 *	@bytes: the requested number of bytes
 */
void prandom_bytes(void *buf, size_t bytes)
{
ffffffff812ccb2c:	55                   	push   %rbp
ffffffff812ccb2d:	48 89 f8             	mov    %rdi,%rax
ffffffff812ccb30:	48 89 f2             	mov    %rsi,%rdx
ffffffff812ccb33:	48 89 e5             	mov    %rsp,%rbp
	struct rnd_state *state = &get_cpu_var(net_rand_state);
ffffffff812ccb36:	48 c7 c7 10 0a 01 00 	mov    $0x10a10,%rdi
ffffffff812ccb3d:	65 48 03 3d d3 d5 d3 	add    %gs:0x7ed3d5d3(%rip),%rdi        # a118 <this_cpu_off>
ffffffff812ccb44:	7e 

	prandom_bytes_state(state, buf, bytes);
ffffffff812ccb45:	48 89 c6             	mov    %rax,%rsi
ffffffff812ccb48:	e8 7a ff ff ff       	callq  ffffffff812ccac7 <prandom_bytes_state>
	put_cpu_var(state);
}
ffffffff812ccb4d:	5d                   	pop    %rbp
ffffffff812ccb4e:	c3                   	retq   

ffffffff812ccb4f <prandom_warmup>:
EXPORT_SYMBOL(prandom_bytes);

static void prandom_warmup(struct rnd_state *state)
{
ffffffff812ccb4f:	55                   	push   %rbp
ffffffff812ccb50:	48 89 e5             	mov    %rsp,%rbp
	/* Calling RNG ten times to satisfy recurrence condition */
	prandom_u32_state(state);
ffffffff812ccb53:	e8 ca fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb58:	e8 c5 fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb5d:	e8 c0 fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb62:	e8 bb fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb67:	e8 b6 fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb6c:	e8 b1 fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb71:	e8 ac fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb76:	e8 a7 fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb7b:	e8 a2 fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
	prandom_u32_state(state);
ffffffff812ccb80:	e8 9d fe ff ff       	callq  ffffffff812cca22 <prandom_u32_state>
}
ffffffff812ccb85:	5d                   	pop    %rbp
ffffffff812ccb86:	c3                   	retq   

ffffffff812ccb87 <__prandom_reseed>:
/*
 *	Generate better values after random number generator
 *	is fully initialized.
 */
static void __prandom_reseed(bool late)
{
ffffffff812ccb87:	55                   	push   %rbp
ffffffff812ccb88:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ccb8b:	41 56                	push   %r14
ffffffff812ccb8d:	41 55                	push   %r13
ffffffff812ccb8f:	41 54                	push   %r12
ffffffff812ccb91:	53                   	push   %rbx
ffffffff812ccb92:	89 fb                	mov    %edi,%ebx
ffffffff812ccb94:	48 83 ec 10          	sub    $0x10,%rsp
	/*
	 * "=rm" is safe here, because "pop" adjusts the stack before
	 * it evaluates its effective address -- this is part of the
	 * documented behavior of the "pop" instruction.
	 */
	asm volatile("# __raw_save_flags\n\t"
ffffffff812ccb98:	9c                   	pushfq 
ffffffff812ccb99:	41 5d                	pop    %r13
		     :"memory", "cc");
}

static inline void native_irq_disable(void)
{
	asm volatile("cli": : :"memory");
ffffffff812ccb9b:	fa                   	cli    
	 * already waiting for bytes when the nonblocking pool
	 * got initialized.
	 */

	/* only allow initial seeding (late == false) once */
	if (!spin_trylock_irqsave(&lock, flags))
ffffffff812ccb9c:	48 c7 c7 a0 60 a3 81 	mov    $0xffffffff81a360a0,%rdi
ffffffff812ccba3:	e8 b5 ba 16 00       	callq  ffffffff8143865d <_raw_spin_trylock>
ffffffff812ccba8:	85 c0                	test   %eax,%eax
ffffffff812ccbaa:	75 08                	jne    ffffffff812ccbb4 <__prandom_reseed+0x2d>
	return flags;
}

static inline void native_restore_fl(unsigned long flags)
{
	asm volatile("push %0 ; popf"
ffffffff812ccbac:	41 55                	push   %r13
ffffffff812ccbae:	9d                   	popfq  
ffffffff812ccbaf:	e9 b7 00 00 00       	jmpq   ffffffff812ccc6b <__prandom_reseed+0xe4>
		return;

	if (latch && !late)
ffffffff812ccbb4:	38 1d 4a 4d 8e 00    	cmp    %bl,0x8e4d4a(%rip)        # ffffffff81bb1904 <latch.15818>
ffffffff812ccbba:	0f 87 9c 00 00 00    	ja     ffffffff812ccc5c <__prandom_reseed+0xd5>
		goto out;

	latch = true;
ffffffff812ccbc0:	c6 05 3d 4d 8e 00 01 	movb   $0x1,0x8e4d3d(%rip)        # ffffffff81bb1904 <latch.15818>

	for_each_possible_cpu(i) {
ffffffff812ccbc7:	83 cb ff             	or     $0xffffffff,%ebx
		struct rnd_state *state = &per_cpu(net_rand_state,i);
ffffffff812ccbca:	49 c7 c4 10 0a 01 00 	mov    $0x10a10,%r12
ffffffff812ccbd1:	8d 53 01             	lea    0x1(%rbx),%edx
ffffffff812ccbd4:	48 8b 3d 55 f6 34 00 	mov    0x34f655(%rip),%rdi        # ffffffff8161c230 <cpu_possible_mask>
ffffffff812ccbdb:	be 20 00 00 00       	mov    $0x20,%esi
ffffffff812ccbe0:	48 63 d2             	movslq %edx,%rdx
ffffffff812ccbe3:	e8 30 41 00 00       	callq  ffffffff812d0d18 <find_next_bit>
	if (latch && !late)
		goto out;

	latch = true;

	for_each_possible_cpu(i) {
ffffffff812ccbe8:	3b 05 fe c6 78 00    	cmp    0x78c6fe(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812ccbee:	89 c3                	mov    %eax,%ebx
ffffffff812ccbf0:	7d 6a                	jge    ffffffff812ccc5c <__prandom_reseed+0xd5>
		struct rnd_state *state = &per_cpu(net_rand_state,i);
		u32 seeds[4];

		get_random_bytes(&seeds, sizeof(seeds));
ffffffff812ccbf2:	48 8d 7d d0          	lea    -0x30(%rbp),%rdi
		goto out;

	latch = true;

	for_each_possible_cpu(i) {
		struct rnd_state *state = &per_cpu(net_rand_state,i);
ffffffff812ccbf6:	48 98                	cltq   
		u32 seeds[4];

		get_random_bytes(&seeds, sizeof(seeds));
ffffffff812ccbf8:	be 10 00 00 00       	mov    $0x10,%esi
		goto out;

	latch = true;

	for_each_possible_cpu(i) {
		struct rnd_state *state = &per_cpu(net_rand_state,i);
ffffffff812ccbfd:	4d 89 e6             	mov    %r12,%r14
ffffffff812ccc00:	4c 03 34 c5 00 8e a5 	add    -0x7e5a7200(,%rax,8),%r14
ffffffff812ccc07:	81 
		u32 seeds[4];

		get_random_bytes(&seeds, sizeof(seeds));
ffffffff812ccc08:	e8 cc a6 04 00       	callq  ffffffff813172d9 <get_random_bytes>
		state->s1 = __seed(seeds[0],   2U);
ffffffff812ccc0d:	8b 45 d0             	mov    -0x30(%rbp),%eax
		state->s2 = __seed(seeds[1],   8U);
		state->s3 = __seed(seeds[2],  16U);
		state->s4 = __seed(seeds[3], 128U);

		prandom_warmup(state);
ffffffff812ccc10:	4c 89 f7             	mov    %r14,%rdi
/*
 * Handle minimum values for seeds
 */
static inline u32 __seed(u32 x, u32 m)
{
	return (x < m) ? x + m : x;
ffffffff812ccc13:	8d 50 02             	lea    0x2(%rax),%edx
ffffffff812ccc16:	83 f8 01             	cmp    $0x1,%eax
ffffffff812ccc19:	0f 46 c2             	cmovbe %edx,%eax
	for_each_possible_cpu(i) {
		struct rnd_state *state = &per_cpu(net_rand_state,i);
		u32 seeds[4];

		get_random_bytes(&seeds, sizeof(seeds));
		state->s1 = __seed(seeds[0],   2U);
ffffffff812ccc1c:	41 89 06             	mov    %eax,(%r14)
		state->s2 = __seed(seeds[1],   8U);
ffffffff812ccc1f:	8b 45 d4             	mov    -0x2c(%rbp),%eax
ffffffff812ccc22:	8d 50 08             	lea    0x8(%rax),%edx
ffffffff812ccc25:	83 f8 07             	cmp    $0x7,%eax
ffffffff812ccc28:	0f 46 c2             	cmovbe %edx,%eax
ffffffff812ccc2b:	41 89 46 04          	mov    %eax,0x4(%r14)
		state->s3 = __seed(seeds[2],  16U);
ffffffff812ccc2f:	8b 45 d8             	mov    -0x28(%rbp),%eax
ffffffff812ccc32:	8d 50 10             	lea    0x10(%rax),%edx
ffffffff812ccc35:	83 f8 0f             	cmp    $0xf,%eax
ffffffff812ccc38:	0f 46 c2             	cmovbe %edx,%eax
ffffffff812ccc3b:	41 89 46 08          	mov    %eax,0x8(%r14)
		state->s4 = __seed(seeds[3], 128U);
ffffffff812ccc3f:	8b 45 dc             	mov    -0x24(%rbp),%eax
ffffffff812ccc42:	8d 90 80 00 00 00    	lea    0x80(%rax),%edx
ffffffff812ccc48:	83 f8 7f             	cmp    $0x7f,%eax
ffffffff812ccc4b:	0f 46 c2             	cmovbe %edx,%eax
ffffffff812ccc4e:	41 89 46 0c          	mov    %eax,0xc(%r14)

		prandom_warmup(state);
ffffffff812ccc52:	e8 f8 fe ff ff       	callq  ffffffff812ccb4f <prandom_warmup>
ffffffff812ccc57:	e9 75 ff ff ff       	jmpq   ffffffff812ccbd1 <__prandom_reseed+0x4a>
	raw_spin_unlock_irq(&lock->rlock);
}

static inline void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags)
{
	raw_spin_unlock_irqrestore(&lock->rlock, flags);
ffffffff812ccc5c:	4c 89 ee             	mov    %r13,%rsi
ffffffff812ccc5f:	48 c7 c7 a0 60 a3 81 	mov    $0xffffffff81a360a0,%rdi
ffffffff812ccc66:	e8 71 ba 16 00       	callq  ffffffff814386dc <_raw_spin_unlock_irqrestore>
	}
out:
	spin_unlock_irqrestore(&lock, flags);
}
ffffffff812ccc6b:	58                   	pop    %rax
ffffffff812ccc6c:	5a                   	pop    %rdx
ffffffff812ccc6d:	5b                   	pop    %rbx
ffffffff812ccc6e:	41 5c                	pop    %r12
ffffffff812ccc70:	41 5d                	pop    %r13
ffffffff812ccc72:	41 5e                	pop    %r14
ffffffff812ccc74:	5d                   	pop    %rbp
ffffffff812ccc75:	c3                   	retq   

ffffffff812ccc76 <prandom_seed>:
 *	@seed: seed value
 *
 *	Add some additional seeding to the prandom pool.
 */
void prandom_seed(u32 entropy)
{
ffffffff812ccc76:	55                   	push   %rbp
	int i;
	/*
	 * No locking on the CPUs, but then somewhat random results are, well,
	 * expected.
	 */
	for_each_possible_cpu (i) {
ffffffff812ccc77:	41 83 cb ff          	or     $0xffffffff,%r11d
 *	@seed: seed value
 *
 *	Add some additional seeding to the prandom pool.
 */
void prandom_seed(u32 entropy)
{
ffffffff812ccc7b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ccc7e:	41 54                	push   %r12
ffffffff812ccc80:	41 89 fc             	mov    %edi,%r12d
ffffffff812ccc83:	53                   	push   %rbx
	/*
	 * No locking on the CPUs, but then somewhat random results are, well,
	 * expected.
	 */
	for_each_possible_cpu (i) {
		struct rnd_state *state = &per_cpu(net_rand_state, i);
ffffffff812ccc84:	48 c7 c3 10 0a 01 00 	mov    $0x10a10,%rbx
ffffffff812ccc8b:	41 8d 53 01          	lea    0x1(%r11),%edx
ffffffff812ccc8f:	48 8b 3d 9a f5 34 00 	mov    0x34f59a(%rip),%rdi        # ffffffff8161c230 <cpu_possible_mask>
ffffffff812ccc96:	be 20 00 00 00       	mov    $0x20,%esi
ffffffff812ccc9b:	48 63 d2             	movslq %edx,%rdx
ffffffff812ccc9e:	e8 75 40 00 00       	callq  ffffffff812d0d18 <find_next_bit>
	int i;
	/*
	 * No locking on the CPUs, but then somewhat random results are, well,
	 * expected.
	 */
	for_each_possible_cpu (i) {
ffffffff812ccca3:	3b 05 43 c6 78 00    	cmp    0x78c643(%rip),%eax        # ffffffff81a592ec <nr_cpu_ids>
ffffffff812ccca9:	41 89 c3             	mov    %eax,%r11d
ffffffff812cccac:	7d 24                	jge    ffffffff812cccd2 <prandom_seed+0x5c>
		struct rnd_state *state = &per_cpu(net_rand_state, i);
ffffffff812cccae:	48 98                	cltq   
ffffffff812cccb0:	48 89 df             	mov    %rbx,%rdi
ffffffff812cccb3:	48 03 3c c5 00 8e a5 	add    -0x7e5a7200(,%rax,8),%rdi
ffffffff812cccba:	81 

		state->s1 = __seed(state->s1 ^ entropy, 2U);
ffffffff812cccbb:	8b 07                	mov    (%rdi),%eax
ffffffff812cccbd:	44 31 e0             	xor    %r12d,%eax
ffffffff812cccc0:	8d 50 02             	lea    0x2(%rax),%edx
ffffffff812cccc3:	83 f8 01             	cmp    $0x1,%eax
ffffffff812cccc6:	0f 46 c2             	cmovbe %edx,%eax
ffffffff812cccc9:	89 07                	mov    %eax,(%rdi)
		prandom_warmup(state);
ffffffff812ccccb:	e8 7f fe ff ff       	callq  ffffffff812ccb4f <prandom_warmup>
ffffffff812cccd0:	eb b9                	jmp    ffffffff812ccc8b <prandom_seed+0x15>
	}
}
ffffffff812cccd2:	5b                   	pop    %rbx
ffffffff812cccd3:	41 5c                	pop    %r12
ffffffff812cccd5:	5d                   	pop    %rbp
ffffffff812cccd6:	c3                   	retq   

ffffffff812cccd7 <__prandom_timer>:
static void __prandom_timer(unsigned long dontcare);

static DEFINE_TIMER(seed_timer, __prandom_timer, 0, 0);

static void __prandom_timer(unsigned long dontcare)
{
ffffffff812cccd7:	55                   	push   %rbp
	u32 entropy;
	unsigned long expires;

	get_random_bytes(&entropy, sizeof(entropy));
ffffffff812cccd8:	be 04 00 00 00       	mov    $0x4,%esi
static void __prandom_timer(unsigned long dontcare);

static DEFINE_TIMER(seed_timer, __prandom_timer, 0, 0);

static void __prandom_timer(unsigned long dontcare)
{
ffffffff812cccdd:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ccce0:	48 83 ec 10          	sub    $0x10,%rsp
	u32 entropy;
	unsigned long expires;

	get_random_bytes(&entropy, sizeof(entropy));
ffffffff812ccce4:	48 8d 7d fc          	lea    -0x4(%rbp),%rdi
ffffffff812ccce8:	e8 ec a5 04 00       	callq  ffffffff813172d9 <get_random_bytes>
	prandom_seed(entropy);
ffffffff812ccced:	8b 7d fc             	mov    -0x4(%rbp),%edi
ffffffff812cccf0:	e8 81 ff ff ff       	callq  ffffffff812ccc76 <prandom_seed>
 *
 * Returns: pseudo-random number in interval [0, ep_ro)
 */
static inline u32 prandom_u32_max(u32 ep_ro)
{
	return (u32)(((u64) prandom_u32() * ep_ro) >> 32);
ffffffff812cccf5:	e8 b3 fd ff ff       	callq  ffffffff812ccaad <prandom_u32>

	/* reseed every ~60 seconds, in [40 .. 80) interval with slack */
	expires = 40 + prandom_u32_max(40);
	seed_timer.expires = jiffies + msecs_to_jiffies(expires * MSEC_PER_SEC);
ffffffff812cccfa:	89 c7                	mov    %eax,%edi
ffffffff812cccfc:	48 6b ff 28          	imul   $0x28,%rdi,%rdi
ffffffff812ccd00:	48 c1 ef 20          	shr    $0x20,%rdi
ffffffff812ccd04:	83 c7 28             	add    $0x28,%edi
ffffffff812ccd07:	69 ff e8 03 00 00    	imul   $0x3e8,%edi,%edi
ffffffff812ccd0d:	e8 b8 0c dd ff       	callq  ffffffff8109d9ca <msecs_to_jiffies>
ffffffff812ccd12:	48 8b 15 e7 a2 73 00 	mov    0x73a2e7(%rip),%rdx        # ffffffff81a07000 <jiffies>

	add_timer(&seed_timer);
ffffffff812ccd19:	48 c7 c7 c0 60 a3 81 	mov    $0xffffffff81a360c0,%rdi
	get_random_bytes(&entropy, sizeof(entropy));
	prandom_seed(entropy);

	/* reseed every ~60 seconds, in [40 .. 80) interval with slack */
	expires = 40 + prandom_u32_max(40);
	seed_timer.expires = jiffies + msecs_to_jiffies(expires * MSEC_PER_SEC);
ffffffff812ccd20:	48 01 d0             	add    %rdx,%rax
ffffffff812ccd23:	48 89 05 a6 93 76 00 	mov    %rax,0x7693a6(%rip)        # ffffffff81a360d0 <seed_timer+0x10>

	add_timer(&seed_timer);
ffffffff812ccd2a:	e8 cb 1b dd ff       	callq  ffffffff8109e8fa <add_timer>
}
ffffffff812ccd2f:	c9                   	leaveq 
ffffffff812ccd30:	c3                   	retq   

ffffffff812ccd31 <prandom_reseed_late>:
out:
	spin_unlock_irqrestore(&lock, flags);
}

void prandom_reseed_late(void)
{
ffffffff812ccd31:	55                   	push   %rbp
	__prandom_reseed(true);
ffffffff812ccd32:	bf 01 00 00 00       	mov    $0x1,%edi
out:
	spin_unlock_irqrestore(&lock, flags);
}

void prandom_reseed_late(void)
{
ffffffff812ccd37:	48 89 e5             	mov    %rsp,%rbp
	__prandom_reseed(true);
ffffffff812ccd3a:	e8 48 fe ff ff       	callq  ffffffff812ccb87 <__prandom_reseed>
}
ffffffff812ccd3f:	5d                   	pop    %rbp
ffffffff812ccd40:	c3                   	retq   

ffffffff812ccd41 <bust_spinlocks>:
#include <linux/console.h>


void __attribute__((weak)) bust_spinlocks(int yes)
{
	if (yes) {
ffffffff812ccd41:	85 ff                	test   %edi,%edi
ffffffff812ccd43:	74 07                	je     ffffffff812ccd4c <bust_spinlocks+0xb>
		++oops_in_progress;
ffffffff812ccd45:	ff 05 cd c8 87 00    	incl   0x87c8cd(%rip)        # ffffffff81b49618 <oops_in_progress>
ffffffff812ccd4b:	c3                   	retq   
#include <linux/vt_kern.h>
#include <linux/console.h>


void __attribute__((weak)) bust_spinlocks(int yes)
{
ffffffff812ccd4c:	55                   	push   %rbp
ffffffff812ccd4d:	48 89 e5             	mov    %rsp,%rbp
	if (yes) {
		++oops_in_progress;
	} else {
#ifdef CONFIG_VT
		unblank_screen();
ffffffff812ccd50:	e8 3e c9 03 00       	callq  ffffffff81309693 <unblank_screen>
#endif
		console_unblank();
ffffffff812ccd55:	e8 01 70 dc ff       	callq  ffffffff81093d5b <console_unblank>
		if (--oops_in_progress == 0)
ffffffff812ccd5a:	ff 0d b8 c8 87 00    	decl   0x87c8b8(%rip)        # ffffffff81b49618 <oops_in_progress>
ffffffff812ccd60:	75 05                	jne    ffffffff812ccd67 <bust_spinlocks+0x26>
			wake_up_klogd();
ffffffff812ccd62:	e8 90 67 dc ff       	callq  ffffffff810934f7 <wake_up_klogd>
	}
}
ffffffff812ccd67:	5d                   	pop    %rbp
ffffffff812ccd68:	c3                   	retq   

ffffffff812ccd69 <kvasprintf>:
#include <linux/types.h>
#include <linux/string.h>

/* Simplified asprintf. */
char *kvasprintf(gfp_t gfp, const char *fmt, va_list ap)
{
ffffffff812ccd69:	55                   	push   %rbp
	unsigned int len;
	char *p;
	va_list aq;

	va_copy(aq, ap);
ffffffff812ccd6a:	b9 06 00 00 00       	mov    $0x6,%ecx
#include <linux/types.h>
#include <linux/string.h>

/* Simplified asprintf. */
char *kvasprintf(gfp_t gfp, const char *fmt, va_list ap)
{
ffffffff812ccd6f:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ccd72:	41 56                	push   %r14
ffffffff812ccd74:	41 55                	push   %r13
	unsigned int len;
	char *p;
	va_list aq;

	va_copy(aq, ap);
ffffffff812ccd76:	48 8d 45 c8          	lea    -0x38(%rbp),%rax
#include <linux/types.h>
#include <linux/string.h>

/* Simplified asprintf. */
char *kvasprintf(gfp_t gfp, const char *fmt, va_list ap)
{
ffffffff812ccd7a:	41 54                	push   %r12
ffffffff812ccd7c:	53                   	push   %rbx
ffffffff812ccd7d:	49 89 f6             	mov    %rsi,%r14
ffffffff812ccd80:	89 fb                	mov    %edi,%ebx
	unsigned int len;
	char *p;
	va_list aq;

	va_copy(aq, ap);
ffffffff812ccd82:	48 89 d6             	mov    %rdx,%rsi
#include <linux/types.h>
#include <linux/string.h>

/* Simplified asprintf. */
char *kvasprintf(gfp_t gfp, const char *fmt, va_list ap)
{
ffffffff812ccd85:	48 83 ec 20          	sub    $0x20,%rsp
	unsigned int len;
	char *p;
	va_list aq;

	va_copy(aq, ap);
ffffffff812ccd89:	48 89 c7             	mov    %rax,%rdi
#include <linux/types.h>
#include <linux/string.h>

/* Simplified asprintf. */
char *kvasprintf(gfp_t gfp, const char *fmt, va_list ap)
{
ffffffff812ccd8c:	49 89 d4             	mov    %rdx,%r12
	unsigned int len;
	char *p;
	va_list aq;

	va_copy(aq, ap);
ffffffff812ccd8f:	f3 a5                	rep movsl %ds:(%rsi),%es:(%rdi)
	len = vsnprintf(NULL, 0, fmt, aq);
ffffffff812ccd91:	4c 89 f2             	mov    %r14,%rdx
ffffffff812ccd94:	31 f6                	xor    %esi,%esi
ffffffff812ccd96:	31 ff                	xor    %edi,%edi
ffffffff812ccd98:	48 89 c1             	mov    %rax,%rcx
ffffffff812ccd9b:	e8 20 d6 ff ff       	callq  ffffffff812ca3c0 <vsnprintf>
	va_end(aq);

	p = kmalloc_track_caller(len+1, gfp);
ffffffff812ccda0:	48 8b 55 08          	mov    0x8(%rbp),%rdx
ffffffff812ccda4:	44 8d 68 01          	lea    0x1(%rax),%r13d
ffffffff812ccda8:	89 de                	mov    %ebx,%esi
ffffffff812ccdaa:	4c 89 ef             	mov    %r13,%rdi
ffffffff812ccdad:	e8 51 42 e3 ff       	callq  ffffffff81101003 <__kmalloc_track_caller>
	if (!p)
ffffffff812ccdb2:	48 85 c0             	test   %rax,%rax

	va_copy(aq, ap);
	len = vsnprintf(NULL, 0, fmt, aq);
	va_end(aq);

	p = kmalloc_track_caller(len+1, gfp);
ffffffff812ccdb5:	48 89 c3             	mov    %rax,%rbx
	if (!p)
ffffffff812ccdb8:	74 11                	je     ffffffff812ccdcb <kvasprintf+0x62>
		return NULL;

	vsnprintf(p, len+1, fmt, ap);
ffffffff812ccdba:	4c 89 e1             	mov    %r12,%rcx
ffffffff812ccdbd:	4c 89 f2             	mov    %r14,%rdx
ffffffff812ccdc0:	4c 89 ee             	mov    %r13,%rsi
ffffffff812ccdc3:	48 89 c7             	mov    %rax,%rdi
ffffffff812ccdc6:	e8 f5 d5 ff ff       	callq  ffffffff812ca3c0 <vsnprintf>

	return p;
}
ffffffff812ccdcb:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812ccdcf:	48 89 d8             	mov    %rbx,%rax
ffffffff812ccdd2:	5b                   	pop    %rbx
ffffffff812ccdd3:	41 5c                	pop    %r12
ffffffff812ccdd5:	41 5d                	pop    %r13
ffffffff812ccdd7:	41 5e                	pop    %r14
ffffffff812ccdd9:	5d                   	pop    %rbp
ffffffff812ccdda:	c3                   	retq   

ffffffff812ccddb <kasprintf>:
EXPORT_SYMBOL(kvasprintf);

char *kasprintf(gfp_t gfp, const char *fmt, ...)
{
ffffffff812ccddb:	55                   	push   %rbp
ffffffff812ccddc:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ccddf:	48 83 ec 50          	sub    $0x50,%rsp
	va_list ap;
	char *p;

	va_start(ap, fmt);
ffffffff812ccde3:	48 8d 45 10          	lea    0x10(%rbp),%rax
	return p;
}
EXPORT_SYMBOL(kvasprintf);

char *kasprintf(gfp_t gfp, const char *fmt, ...)
{
ffffffff812ccde7:	48 89 55 e0          	mov    %rdx,-0x20(%rbp)
	va_list ap;
	char *p;

	va_start(ap, fmt);
	p = kvasprintf(gfp, fmt, ap);
ffffffff812ccdeb:	48 8d 55 b8          	lea    -0x48(%rbp),%rdx
	return p;
}
EXPORT_SYMBOL(kvasprintf);

char *kasprintf(gfp_t gfp, const char *fmt, ...)
{
ffffffff812ccdef:	48 89 4d e8          	mov    %rcx,-0x18(%rbp)
ffffffff812ccdf3:	4c 89 45 f0          	mov    %r8,-0x10(%rbp)
	va_list ap;
	char *p;

	va_start(ap, fmt);
ffffffff812ccdf7:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812ccdfb:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
	return p;
}
EXPORT_SYMBOL(kvasprintf);

char *kasprintf(gfp_t gfp, const char *fmt, ...)
{
ffffffff812ccdff:	4c 89 4d f8          	mov    %r9,-0x8(%rbp)
	va_list ap;
	char *p;

	va_start(ap, fmt);
ffffffff812cce03:	c7 45 b8 10 00 00 00 	movl   $0x10,-0x48(%rbp)
ffffffff812cce0a:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	p = kvasprintf(gfp, fmt, ap);
ffffffff812cce0e:	e8 56 ff ff ff       	callq  ffffffff812ccd69 <kvasprintf>
	va_end(ap);

	return p;
}
ffffffff812cce13:	c9                   	leaveq 
ffffffff812cce14:	c3                   	retq   

ffffffff812cce15 <__bitmap_equal>:
 * for the best explanations of this ordering.
 */

int __bitmap_equal(const unsigned long *bitmap1,
		const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812cce15:	55                   	push   %rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812cce16:	89 d0                	mov    %edx,%eax
	for (k = 0; k < lim; ++k)
ffffffff812cce18:	45 31 c0             	xor    %r8d,%r8d
 */

int __bitmap_equal(const unsigned long *bitmap1,
		const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812cce1b:	c1 e8 06             	shr    $0x6,%eax
 * for the best explanations of this ordering.
 */

int __bitmap_equal(const unsigned long *bitmap1,
		const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812cce1e:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
ffffffff812cce21:	44 39 c0             	cmp    %r8d,%eax
ffffffff812cce24:	76 12                	jbe    ffffffff812cce38 <__bitmap_equal+0x23>
		if (bitmap1[k] != bitmap2[k])
ffffffff812cce26:	4a 8b 0c c7          	mov    (%rdi,%r8,8),%rcx
ffffffff812cce2a:	49 ff c0             	inc    %r8
ffffffff812cce2d:	4a 3b 4c c6 f8       	cmp    -0x8(%rsi,%r8,8),%rcx
ffffffff812cce32:	74 ed                	je     ffffffff812cce21 <__bitmap_equal+0xc>
			return 0;
ffffffff812cce34:	31 c0                	xor    %eax,%eax
ffffffff812cce36:	eb 2c                	jmp    ffffffff812cce64 <__bitmap_equal+0x4f>

	if (bits % BITS_PER_LONG)
ffffffff812cce38:	f6 c2 3f             	test   $0x3f,%dl
		if ((bitmap1[k] ^ bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
			return 0;

	return 1;
ffffffff812cce3b:	b8 01 00 00 00       	mov    $0x1,%eax
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] != bitmap2[k])
			return 0;

	if (bits % BITS_PER_LONG)
ffffffff812cce40:	74 22                	je     ffffffff812cce64 <__bitmap_equal+0x4f>
		if ((bitmap1[k] ^ bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
ffffffff812cce42:	45 89 c0             	mov    %r8d,%r8d
		const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] != bitmap2[k])
			return 0;
ffffffff812cce45:	89 d1                	mov    %edx,%ecx
ffffffff812cce47:	4a 8b 04 c7          	mov    (%rdi,%r8,8),%rax
ffffffff812cce4b:	4a 33 04 c6          	xor    (%rsi,%r8,8),%rax
ffffffff812cce4f:	f7 d9                	neg    %ecx
ffffffff812cce51:	48 89 c6             	mov    %rax,%rsi
ffffffff812cce54:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax
ffffffff812cce58:	48 d3 e8             	shr    %cl,%rax
ffffffff812cce5b:	48 85 c6             	test   %rax,%rsi
ffffffff812cce5e:	0f 94 c0             	sete   %al
ffffffff812cce61:	0f b6 c0             	movzbl %al,%eax
	if (bits % BITS_PER_LONG)
		if ((bitmap1[k] ^ bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
			return 0;

	return 1;
}
ffffffff812cce64:	5d                   	pop    %rbp
ffffffff812cce65:	c3                   	retq   

ffffffff812cce66 <__bitmap_complement>:
EXPORT_SYMBOL(__bitmap_equal);

void __bitmap_complement(unsigned long *dst, const unsigned long *src, unsigned int bits)
{
ffffffff812cce66:	55                   	push   %rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812cce67:	89 d0                	mov    %edx,%eax
	for (k = 0; k < lim; ++k)
ffffffff812cce69:	31 c9                	xor    %ecx,%ecx
}
EXPORT_SYMBOL(__bitmap_equal);

void __bitmap_complement(unsigned long *dst, const unsigned long *src, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812cce6b:	c1 e8 06             	shr    $0x6,%eax
	return 1;
}
EXPORT_SYMBOL(__bitmap_equal);

void __bitmap_complement(unsigned long *dst, const unsigned long *src, unsigned int bits)
{
ffffffff812cce6e:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
ffffffff812cce71:	39 c8                	cmp    %ecx,%eax
ffffffff812cce73:	76 10                	jbe    ffffffff812cce85 <__bitmap_complement+0x1f>
		dst[k] = ~src[k];
ffffffff812cce75:	4c 8b 04 ce          	mov    (%rsi,%rcx,8),%r8
ffffffff812cce79:	49 f7 d0             	not    %r8
ffffffff812cce7c:	4c 89 04 cf          	mov    %r8,(%rdi,%rcx,8)
ffffffff812cce80:	48 ff c1             	inc    %rcx
ffffffff812cce83:	eb ec                	jmp    ffffffff812cce71 <__bitmap_complement+0xb>

	if (bits % BITS_PER_LONG)
ffffffff812cce85:	80 e2 3f             	and    $0x3f,%dl
ffffffff812cce88:	74 0b                	je     ffffffff812cce95 <__bitmap_complement+0x2f>
		dst[k] = ~src[k];
ffffffff812cce8a:	48 8b 14 c6          	mov    (%rsi,%rax,8),%rdx
ffffffff812cce8e:	48 f7 d2             	not    %rdx
ffffffff812cce91:	48 89 14 c7          	mov    %rdx,(%rdi,%rax,8)
}
ffffffff812cce95:	5d                   	pop    %rbp
ffffffff812cce96:	c3                   	retq   

ffffffff812cce97 <__bitmap_and>:
}
EXPORT_SYMBOL(__bitmap_shift_left);

int __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812cce97:	55                   	push   %rbp
	unsigned int k;
	unsigned int lim = bits/BITS_PER_LONG;
ffffffff812cce98:	41 89 c8             	mov    %ecx,%r8d
	unsigned long result = 0;

	for (k = 0; k < lim; k++)
ffffffff812cce9b:	31 c0                	xor    %eax,%eax

int __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k;
	unsigned int lim = bits/BITS_PER_LONG;
ffffffff812cce9d:	41 c1 e8 06          	shr    $0x6,%r8d
	unsigned long result = 0;
ffffffff812ccea1:	45 31 c9             	xor    %r9d,%r9d
}
EXPORT_SYMBOL(__bitmap_shift_left);

int __bitmap_and(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccea4:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k;
	unsigned int lim = bits/BITS_PER_LONG;
	unsigned long result = 0;

	for (k = 0; k < lim; k++)
ffffffff812ccea7:	41 39 c0             	cmp    %eax,%r8d
ffffffff812cceaa:	76 14                	jbe    ffffffff812ccec0 <__bitmap_and+0x29>
		result |= (dst[k] = bitmap1[k] & bitmap2[k]);
ffffffff812cceac:	4c 8b 14 c6          	mov    (%rsi,%rax,8),%r10
ffffffff812cceb0:	4c 23 14 c2          	and    (%rdx,%rax,8),%r10
ffffffff812cceb4:	4c 89 14 c7          	mov    %r10,(%rdi,%rax,8)
ffffffff812cceb8:	4d 09 d1             	or     %r10,%r9
ffffffff812ccebb:	48 ff c0             	inc    %rax
ffffffff812ccebe:	eb e7                	jmp    ffffffff812ccea7 <__bitmap_and+0x10>
	if (bits % BITS_PER_LONG)
ffffffff812ccec0:	f6 c1 3f             	test   $0x3f,%cl
ffffffff812ccec3:	74 1b                	je     ffffffff812ccee0 <__bitmap_and+0x49>
		result |= (dst[k] = bitmap1[k] & bitmap2[k] &
ffffffff812ccec5:	4a 8b 04 c6          	mov    (%rsi,%r8,8),%rax
ffffffff812ccec9:	4a 23 04 c2          	and    (%rdx,%r8,8),%rax
ffffffff812ccecd:	f7 d9                	neg    %ecx
ffffffff812ccecf:	48 83 ca ff          	or     $0xffffffffffffffff,%rdx
ffffffff812cced3:	48 d3 ea             	shr    %cl,%rdx
ffffffff812cced6:	48 21 d0             	and    %rdx,%rax
ffffffff812cced9:	4a 89 04 c7          	mov    %rax,(%rdi,%r8,8)
ffffffff812ccedd:	49 09 c1             	or     %rax,%r9
			   BITMAP_LAST_WORD_MASK(bits));
	return result != 0;
ffffffff812ccee0:	31 c0                	xor    %eax,%eax
ffffffff812ccee2:	4d 85 c9             	test   %r9,%r9
ffffffff812ccee5:	0f 95 c0             	setne  %al
}
ffffffff812ccee8:	5d                   	pop    %rbp
ffffffff812ccee9:	c3                   	retq   

ffffffff812cceea <__bitmap_or>:
EXPORT_SYMBOL(__bitmap_and);

void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812cceea:	55                   	push   %rbp
	unsigned int k;
	unsigned int nr = BITS_TO_LONGS(bits);
ffffffff812cceeb:	89 c9                	mov    %ecx,%ecx

	for (k = 0; k < nr; k++)
ffffffff812cceed:	31 c0                	xor    %eax,%eax

void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k;
	unsigned int nr = BITS_TO_LONGS(bits);
ffffffff812cceef:	48 83 c1 3f          	add    $0x3f,%rcx
}
EXPORT_SYMBOL(__bitmap_and);

void __bitmap_or(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccef3:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k;
	unsigned int nr = BITS_TO_LONGS(bits);
ffffffff812ccef6:	48 c1 e9 06          	shr    $0x6,%rcx

	for (k = 0; k < nr; k++)
ffffffff812ccefa:	39 c1                	cmp    %eax,%ecx
ffffffff812ccefc:	76 11                	jbe    ffffffff812ccf0f <__bitmap_or+0x25>
		dst[k] = bitmap1[k] | bitmap2[k];
ffffffff812ccefe:	4c 8b 04 c6          	mov    (%rsi,%rax,8),%r8
ffffffff812ccf02:	4c 0b 04 c2          	or     (%rdx,%rax,8),%r8
ffffffff812ccf06:	4c 89 04 c7          	mov    %r8,(%rdi,%rax,8)
ffffffff812ccf0a:	48 ff c0             	inc    %rax
ffffffff812ccf0d:	eb eb                	jmp    ffffffff812ccefa <__bitmap_or+0x10>
}
ffffffff812ccf0f:	5d                   	pop    %rbp
ffffffff812ccf10:	c3                   	retq   

ffffffff812ccf11 <__bitmap_xor>:
EXPORT_SYMBOL(__bitmap_or);

void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccf11:	55                   	push   %rbp
	unsigned int k;
	unsigned int nr = BITS_TO_LONGS(bits);
ffffffff812ccf12:	89 c9                	mov    %ecx,%ecx

	for (k = 0; k < nr; k++)
ffffffff812ccf14:	31 c0                	xor    %eax,%eax

void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k;
	unsigned int nr = BITS_TO_LONGS(bits);
ffffffff812ccf16:	48 83 c1 3f          	add    $0x3f,%rcx
}
EXPORT_SYMBOL(__bitmap_or);

void __bitmap_xor(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccf1a:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k;
	unsigned int nr = BITS_TO_LONGS(bits);
ffffffff812ccf1d:	48 c1 e9 06          	shr    $0x6,%rcx

	for (k = 0; k < nr; k++)
ffffffff812ccf21:	39 c1                	cmp    %eax,%ecx
ffffffff812ccf23:	76 11                	jbe    ffffffff812ccf36 <__bitmap_xor+0x25>
		dst[k] = bitmap1[k] ^ bitmap2[k];
ffffffff812ccf25:	4c 8b 04 c6          	mov    (%rsi,%rax,8),%r8
ffffffff812ccf29:	4c 33 04 c2          	xor    (%rdx,%rax,8),%r8
ffffffff812ccf2d:	4c 89 04 c7          	mov    %r8,(%rdi,%rax,8)
ffffffff812ccf31:	48 ff c0             	inc    %rax
ffffffff812ccf34:	eb eb                	jmp    ffffffff812ccf21 <__bitmap_xor+0x10>
}
ffffffff812ccf36:	5d                   	pop    %rbp
ffffffff812ccf37:	c3                   	retq   

ffffffff812ccf38 <__bitmap_andnot>:
EXPORT_SYMBOL(__bitmap_xor);

int __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccf38:	55                   	push   %rbp
	unsigned int k;
	unsigned int lim = bits/BITS_PER_LONG;
ffffffff812ccf39:	41 89 c8             	mov    %ecx,%r8d
	unsigned long result = 0;

	for (k = 0; k < lim; k++)
ffffffff812ccf3c:	31 c0                	xor    %eax,%eax

int __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k;
	unsigned int lim = bits/BITS_PER_LONG;
ffffffff812ccf3e:	41 c1 e8 06          	shr    $0x6,%r8d
	unsigned long result = 0;
ffffffff812ccf42:	45 31 d2             	xor    %r10d,%r10d
}
EXPORT_SYMBOL(__bitmap_xor);

int __bitmap_andnot(unsigned long *dst, const unsigned long *bitmap1,
				const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccf45:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k;
	unsigned int lim = bits/BITS_PER_LONG;
	unsigned long result = 0;

	for (k = 0; k < lim; k++)
ffffffff812ccf48:	41 39 c0             	cmp    %eax,%r8d
ffffffff812ccf4b:	76 17                	jbe    ffffffff812ccf64 <__bitmap_andnot+0x2c>
		result |= (dst[k] = bitmap1[k] & ~bitmap2[k]);
ffffffff812ccf4d:	4c 8b 0c c2          	mov    (%rdx,%rax,8),%r9
ffffffff812ccf51:	49 f7 d1             	not    %r9
ffffffff812ccf54:	4c 23 0c c6          	and    (%rsi,%rax,8),%r9
ffffffff812ccf58:	4c 89 0c c7          	mov    %r9,(%rdi,%rax,8)
ffffffff812ccf5c:	4d 09 ca             	or     %r9,%r10
ffffffff812ccf5f:	48 ff c0             	inc    %rax
ffffffff812ccf62:	eb e4                	jmp    ffffffff812ccf48 <__bitmap_andnot+0x10>
	if (bits % BITS_PER_LONG)
ffffffff812ccf64:	f6 c1 3f             	test   $0x3f,%cl
ffffffff812ccf67:	74 1e                	je     ffffffff812ccf87 <__bitmap_andnot+0x4f>
		result |= (dst[k] = bitmap1[k] & ~bitmap2[k] &
ffffffff812ccf69:	f7 d9                	neg    %ecx
ffffffff812ccf6b:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax
ffffffff812ccf6f:	4a 8b 14 c2          	mov    (%rdx,%r8,8),%rdx
ffffffff812ccf73:	48 d3 e8             	shr    %cl,%rax
ffffffff812ccf76:	4a 23 04 c6          	and    (%rsi,%r8,8),%rax
ffffffff812ccf7a:	48 f7 d2             	not    %rdx
ffffffff812ccf7d:	48 21 d0             	and    %rdx,%rax
ffffffff812ccf80:	4a 89 04 c7          	mov    %rax,(%rdi,%r8,8)
ffffffff812ccf84:	49 09 c2             	or     %rax,%r10
			   BITMAP_LAST_WORD_MASK(bits));
	return result != 0;
ffffffff812ccf87:	31 c0                	xor    %eax,%eax
ffffffff812ccf89:	4d 85 d2             	test   %r10,%r10
ffffffff812ccf8c:	0f 95 c0             	setne  %al
}
ffffffff812ccf8f:	5d                   	pop    %rbp
ffffffff812ccf90:	c3                   	retq   

ffffffff812ccf91 <__bitmap_intersects>:
EXPORT_SYMBOL(__bitmap_andnot);

int __bitmap_intersects(const unsigned long *bitmap1,
			const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccf91:	55                   	push   %rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812ccf92:	89 d1                	mov    %edx,%ecx
	for (k = 0; k < lim; ++k)
ffffffff812ccf94:	45 31 c0             	xor    %r8d,%r8d
EXPORT_SYMBOL(__bitmap_andnot);

int __bitmap_intersects(const unsigned long *bitmap1,
			const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812ccf97:	c1 e9 06             	shr    $0x6,%ecx
}
EXPORT_SYMBOL(__bitmap_andnot);

int __bitmap_intersects(const unsigned long *bitmap1,
			const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccf9a:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
ffffffff812ccf9d:	44 39 c1             	cmp    %r8d,%ecx
ffffffff812ccfa0:	76 17                	jbe    ffffffff812ccfb9 <__bitmap_intersects+0x28>
		if (bitmap1[k] & bitmap2[k])
ffffffff812ccfa2:	4a 8b 04 c7          	mov    (%rdi,%r8,8),%rax
ffffffff812ccfa6:	4a 23 04 c6          	and    (%rsi,%r8,8),%rax
ffffffff812ccfaa:	49 ff c0             	inc    %r8
ffffffff812ccfad:	48 85 c0             	test   %rax,%rax
ffffffff812ccfb0:	74 eb                	je     ffffffff812ccf9d <__bitmap_intersects+0xc>
			return 1;
ffffffff812ccfb2:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812ccfb7:	eb 29                	jmp    ffffffff812ccfe2 <__bitmap_intersects+0x51>

	if (bits % BITS_PER_LONG)
		if ((bitmap1[k] & bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
			return 1;
	return 0;
ffffffff812ccfb9:	31 c0                	xor    %eax,%eax
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] & bitmap2[k])
			return 1;

	if (bits % BITS_PER_LONG)
ffffffff812ccfbb:	f6 c2 3f             	test   $0x3f,%dl
ffffffff812ccfbe:	74 22                	je     ffffffff812ccfe2 <__bitmap_intersects+0x51>
		if ((bitmap1[k] & bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
ffffffff812ccfc0:	45 89 c0             	mov    %r8d,%r8d
			const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] & bitmap2[k])
			return 1;
ffffffff812ccfc3:	f7 da                	neg    %edx
ffffffff812ccfc5:	4a 8b 04 c7          	mov    (%rdi,%r8,8),%rax
ffffffff812ccfc9:	4a 23 04 c6          	and    (%rsi,%r8,8),%rax
ffffffff812ccfcd:	89 d1                	mov    %edx,%ecx
ffffffff812ccfcf:	48 89 c6             	mov    %rax,%rsi
ffffffff812ccfd2:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax
ffffffff812ccfd6:	48 d3 e8             	shr    %cl,%rax
ffffffff812ccfd9:	48 85 c6             	test   %rax,%rsi
ffffffff812ccfdc:	0f 95 c0             	setne  %al
ffffffff812ccfdf:	0f b6 c0             	movzbl %al,%eax

	if (bits % BITS_PER_LONG)
		if ((bitmap1[k] & bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
			return 1;
	return 0;
}
ffffffff812ccfe2:	5d                   	pop    %rbp
ffffffff812ccfe3:	c3                   	retq   

ffffffff812ccfe4 <__bitmap_subset>:
EXPORT_SYMBOL(__bitmap_intersects);

int __bitmap_subset(const unsigned long *bitmap1,
		    const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccfe4:	55                   	push   %rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812ccfe5:	89 d1                	mov    %edx,%ecx
	for (k = 0; k < lim; ++k)
ffffffff812ccfe7:	45 31 c0             	xor    %r8d,%r8d
EXPORT_SYMBOL(__bitmap_intersects);

int __bitmap_subset(const unsigned long *bitmap1,
		    const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
ffffffff812ccfea:	c1 e9 06             	shr    $0x6,%ecx
}
EXPORT_SYMBOL(__bitmap_intersects);

int __bitmap_subset(const unsigned long *bitmap1,
		    const unsigned long *bitmap2, unsigned int bits)
{
ffffffff812ccfed:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
ffffffff812ccff0:	44 39 c1             	cmp    %r8d,%ecx
ffffffff812ccff3:	76 17                	jbe    ffffffff812cd00c <__bitmap_subset+0x28>
		if (bitmap1[k] & ~bitmap2[k])
ffffffff812ccff5:	4a 8b 04 c6          	mov    (%rsi,%r8,8),%rax
ffffffff812ccff9:	48 f7 d0             	not    %rax
ffffffff812ccffc:	4a 23 04 c7          	and    (%rdi,%r8,8),%rax
ffffffff812cd000:	49 ff c0             	inc    %r8
ffffffff812cd003:	48 85 c0             	test   %rax,%rax
ffffffff812cd006:	74 e8                	je     ffffffff812ccff0 <__bitmap_subset+0xc>
			return 0;
ffffffff812cd008:	31 c0                	xor    %eax,%eax
ffffffff812cd00a:	eb 2c                	jmp    ffffffff812cd038 <__bitmap_subset+0x54>

	if (bits % BITS_PER_LONG)
ffffffff812cd00c:	f6 c2 3f             	test   $0x3f,%dl
		if ((bitmap1[k] & ~bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
			return 0;
	return 1;
ffffffff812cd00f:	b8 01 00 00 00       	mov    $0x1,%eax
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] & ~bitmap2[k])
			return 0;

	if (bits % BITS_PER_LONG)
ffffffff812cd014:	74 22                	je     ffffffff812cd038 <__bitmap_subset+0x54>
		    const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] & ~bitmap2[k])
			return 0;
ffffffff812cd016:	89 d1                	mov    %edx,%ecx

	if (bits % BITS_PER_LONG)
		if ((bitmap1[k] & ~bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
ffffffff812cd018:	45 89 c0             	mov    %r8d,%r8d
		    const unsigned long *bitmap2, unsigned int bits)
{
	unsigned int k, lim = bits/BITS_PER_LONG;
	for (k = 0; k < lim; ++k)
		if (bitmap1[k] & ~bitmap2[k])
			return 0;
ffffffff812cd01b:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax
ffffffff812cd01f:	f7 d9                	neg    %ecx
ffffffff812cd021:	4a 8b 14 c6          	mov    (%rsi,%r8,8),%rdx
ffffffff812cd025:	48 d3 e8             	shr    %cl,%rax
ffffffff812cd028:	4a 23 04 c7          	and    (%rdi,%r8,8),%rax
ffffffff812cd02c:	48 f7 d2             	not    %rdx
ffffffff812cd02f:	48 85 d0             	test   %rdx,%rax
ffffffff812cd032:	0f 94 c0             	sete   %al
ffffffff812cd035:	0f b6 c0             	movzbl %al,%eax

	if (bits % BITS_PER_LONG)
		if ((bitmap1[k] & ~bitmap2[k]) & BITMAP_LAST_WORD_MASK(bits))
			return 0;
	return 1;
}
ffffffff812cd038:	5d                   	pop    %rbp
ffffffff812cd039:	c3                   	retq   

ffffffff812cd03a <bitmap_set>:
}
EXPORT_SYMBOL(__bitmap_weight);

void bitmap_set(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
ffffffff812cd03a:	89 f0                	mov    %esi,%eax
	return w;
}
EXPORT_SYMBOL(__bitmap_weight);

void bitmap_set(unsigned long *map, unsigned int start, int len)
{
ffffffff812cd03c:	55                   	push   %rbp
	unsigned long *p = map + BIT_WORD(start);
	const unsigned int size = start + len;
	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
ffffffff812cd03d:	89 f1                	mov    %esi,%ecx
}
EXPORT_SYMBOL(__bitmap_weight);

void bitmap_set(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
ffffffff812cd03f:	c1 e8 06             	shr    $0x6,%eax
	const unsigned int size = start + len;
	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
ffffffff812cd042:	83 e1 3f             	and    $0x3f,%ecx
ffffffff812cd045:	41 b8 40 00 00 00    	mov    $0x40,%r8d
}
EXPORT_SYMBOL(__bitmap_weight);

void bitmap_set(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
ffffffff812cd04b:	48 8d 3c c7          	lea    (%rdi,%rax,8),%rdi
	const unsigned int size = start + len;
	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
	unsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);
ffffffff812cd04f:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax

void bitmap_set(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
	const unsigned int size = start + len;
	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
ffffffff812cd053:	41 29 c8             	sub    %ecx,%r8d
	unsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);
ffffffff812cd056:	48 d3 e0             	shl    %cl,%rax
	return w;
}
EXPORT_SYMBOL(__bitmap_weight);

void bitmap_set(unsigned long *map, unsigned int start, int len)
{
ffffffff812cd059:	48 89 e5             	mov    %rsp,%rbp
	unsigned long *p = map + BIT_WORD(start);
	const unsigned int size = start + len;
	int bits_to_set = BITS_PER_LONG - (start % BITS_PER_LONG);
	unsigned long mask_to_set = BITMAP_FIRST_WORD_MASK(start);

	while (len - bits_to_set >= 0) {
ffffffff812cd05c:	89 d1                	mov    %edx,%ecx
ffffffff812cd05e:	41 89 c9             	mov    %ecx,%r9d
ffffffff812cd061:	45 29 c1             	sub    %r8d,%r9d
ffffffff812cd064:	78 16                	js     ffffffff812cd07c <bitmap_set+0x42>
		*p |= mask_to_set;
ffffffff812cd066:	48 09 07             	or     %rax,(%rdi)
		len -= bits_to_set;
		bits_to_set = BITS_PER_LONG;
ffffffff812cd069:	41 b8 40 00 00 00    	mov    $0x40,%r8d
		mask_to_set = ~0UL;
		p++;
ffffffff812cd06f:	48 83 c7 08          	add    $0x8,%rdi

	while (len - bits_to_set >= 0) {
		*p |= mask_to_set;
		len -= bits_to_set;
		bits_to_set = BITS_PER_LONG;
		mask_to_set = ~0UL;
ffffffff812cd073:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax
ffffffff812cd077:	44 89 c9             	mov    %r9d,%ecx
ffffffff812cd07a:	eb e2                	jmp    ffffffff812cd05e <bitmap_set+0x24>
		p++;
	}
	if (len) {
ffffffff812cd07c:	85 c9                	test   %ecx,%ecx
ffffffff812cd07e:	74 12                	je     ffffffff812cd092 <bitmap_set+0x58>
		mask_to_set &= BITMAP_LAST_WORD_MASK(size);
		*p |= mask_to_set;
ffffffff812cd080:	8d 0c 32             	lea    (%rdx,%rsi,1),%ecx
ffffffff812cd083:	48 83 ca ff          	or     $0xffffffffffffffff,%rdx
ffffffff812cd087:	f7 d9                	neg    %ecx
ffffffff812cd089:	48 d3 ea             	shr    %cl,%rdx
ffffffff812cd08c:	48 21 d0             	and    %rdx,%rax
ffffffff812cd08f:	48 09 07             	or     %rax,(%rdi)
	}
}
ffffffff812cd092:	5d                   	pop    %rbp
ffffffff812cd093:	c3                   	retq   

ffffffff812cd094 <bitmap_clear>:
EXPORT_SYMBOL(bitmap_set);

void bitmap_clear(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
ffffffff812cd094:	89 f0                	mov    %esi,%eax
	}
}
EXPORT_SYMBOL(bitmap_set);

void bitmap_clear(unsigned long *map, unsigned int start, int len)
{
ffffffff812cd096:	55                   	push   %rbp
	unsigned long *p = map + BIT_WORD(start);
	const unsigned int size = start + len;
	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
ffffffff812cd097:	89 f1                	mov    %esi,%ecx
}
EXPORT_SYMBOL(bitmap_set);

void bitmap_clear(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
ffffffff812cd099:	c1 e8 06             	shr    $0x6,%eax
	const unsigned int size = start + len;
	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
ffffffff812cd09c:	83 e1 3f             	and    $0x3f,%ecx
ffffffff812cd09f:	41 b8 40 00 00 00    	mov    $0x40,%r8d
}
EXPORT_SYMBOL(bitmap_set);

void bitmap_clear(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
ffffffff812cd0a5:	48 8d 3c c7          	lea    (%rdi,%rax,8),%rdi
	const unsigned int size = start + len;
	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
	unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);
ffffffff812cd0a9:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax

void bitmap_clear(unsigned long *map, unsigned int start, int len)
{
	unsigned long *p = map + BIT_WORD(start);
	const unsigned int size = start + len;
	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
ffffffff812cd0ad:	41 29 c8             	sub    %ecx,%r8d
	unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);
ffffffff812cd0b0:	48 d3 e0             	shl    %cl,%rax
	}
}
EXPORT_SYMBOL(bitmap_set);

void bitmap_clear(unsigned long *map, unsigned int start, int len)
{
ffffffff812cd0b3:	48 89 e5             	mov    %rsp,%rbp
	unsigned long *p = map + BIT_WORD(start);
	const unsigned int size = start + len;
	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
	unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);

	while (len - bits_to_clear >= 0) {
ffffffff812cd0b6:	89 d1                	mov    %edx,%ecx
ffffffff812cd0b8:	41 89 c9             	mov    %ecx,%r9d
ffffffff812cd0bb:	45 29 c1             	sub    %r8d,%r9d
ffffffff812cd0be:	78 19                	js     ffffffff812cd0d9 <bitmap_clear+0x45>
		*p &= ~mask_to_clear;
ffffffff812cd0c0:	48 f7 d0             	not    %rax
		len -= bits_to_clear;
		bits_to_clear = BITS_PER_LONG;
ffffffff812cd0c3:	41 b8 40 00 00 00    	mov    $0x40,%r8d
	const unsigned int size = start + len;
	int bits_to_clear = BITS_PER_LONG - (start % BITS_PER_LONG);
	unsigned long mask_to_clear = BITMAP_FIRST_WORD_MASK(start);

	while (len - bits_to_clear >= 0) {
		*p &= ~mask_to_clear;
ffffffff812cd0c9:	48 21 07             	and    %rax,(%rdi)
		len -= bits_to_clear;
		bits_to_clear = BITS_PER_LONG;
		mask_to_clear = ~0UL;
ffffffff812cd0cc:	44 89 c9             	mov    %r9d,%ecx
		p++;
ffffffff812cd0cf:	48 83 c7 08          	add    $0x8,%rdi

	while (len - bits_to_clear >= 0) {
		*p &= ~mask_to_clear;
		len -= bits_to_clear;
		bits_to_clear = BITS_PER_LONG;
		mask_to_clear = ~0UL;
ffffffff812cd0d3:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax
ffffffff812cd0d7:	eb df                	jmp    ffffffff812cd0b8 <bitmap_clear+0x24>
		p++;
	}
	if (len) {
ffffffff812cd0d9:	85 c9                	test   %ecx,%ecx
ffffffff812cd0db:	74 15                	je     ffffffff812cd0f2 <bitmap_clear+0x5e>
		mask_to_clear &= BITMAP_LAST_WORD_MASK(size);
		*p &= ~mask_to_clear;
ffffffff812cd0dd:	8d 0c 32             	lea    (%rdx,%rsi,1),%ecx
ffffffff812cd0e0:	48 83 ca ff          	or     $0xffffffffffffffff,%rdx
ffffffff812cd0e4:	f7 d9                	neg    %ecx
ffffffff812cd0e6:	48 d3 ea             	shr    %cl,%rdx
ffffffff812cd0e9:	48 21 d0             	and    %rdx,%rax
ffffffff812cd0ec:	48 f7 d0             	not    %rax
ffffffff812cd0ef:	48 21 07             	and    %rax,(%rdi)
	}
}
ffffffff812cd0f2:	5d                   	pop    %rbp
ffffffff812cd0f3:	c3                   	retq   

ffffffff812cd0f4 <__reg_op>:
	REG_OP_ALLOC,		/* set all bits in region */
	REG_OP_RELEASE,		/* clear all bits in region */
};

static int __reg_op(unsigned long *bitmap, unsigned int pos, int order, int reg_op)
{
ffffffff812cd0f4:	41 89 c9             	mov    %ecx,%r9d

	/*
	 * Either nlongs_reg == 1 (for small orders that fit in one long)
	 * or (offset == 0 && mask == ~0UL) (for larger multiword orders.)
	 */
	nbits_reg = 1 << order;
ffffffff812cd0f7:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812cd0fc:	88 d1                	mov    %dl,%cl
ffffffff812cd0fe:	d3 e0                	shl    %cl,%eax
	REG_OP_ALLOC,		/* set all bits in region */
	REG_OP_RELEASE,		/* clear all bits in region */
};

static int __reg_op(unsigned long *bitmap, unsigned int pos, int order, int reg_op)
{
ffffffff812cd100:	55                   	push   %rbp
	 * Can't do "mask = (1UL << nbitsinlong) - 1", as that
	 * overflows if nbitsinlong == BITS_PER_LONG.
	 */
	mask = (1UL << (nbitsinlong - 1));
	mask += mask - 1;
	mask <<= offset;
ffffffff812cd101:	41 89 f0             	mov    %esi,%r8d

	/*
	 * Either nlongs_reg == 1 (for small orders that fit in one long)
	 * or (offset == 0 && mask == ~0UL) (for larger multiword orders.)
	 */
	nbits_reg = 1 << order;
ffffffff812cd104:	48 63 d0             	movslq %eax,%rdx

	/*
	 * Can't do "mask = (1UL << nbitsinlong) - 1", as that
	 * overflows if nbitsinlong == BITS_PER_LONG.
	 */
	mask = (1UL << (nbitsinlong - 1));
ffffffff812cd107:	b8 40 00 00 00       	mov    $0x40,%eax

	/*
	 * Either nlongs_reg == 1 (for small orders that fit in one long)
	 * or (offset == 0 && mask == ~0UL) (for larger multiword orders.)
	 */
	nbits_reg = 1 << order;
ffffffff812cd10c:	48 89 d1             	mov    %rdx,%rcx
	index = pos / BITS_PER_LONG;
	offset = pos - (index * BITS_PER_LONG);
	nlongs_reg = BITS_TO_LONGS(nbits_reg);
ffffffff812cd10f:	48 83 c2 3f          	add    $0x3f,%rdx
	REG_OP_ALLOC,		/* set all bits in region */
	REG_OP_RELEASE,		/* clear all bits in region */
};

static int __reg_op(unsigned long *bitmap, unsigned int pos, int order, int reg_op)
{
ffffffff812cd113:	48 89 e5             	mov    %rsp,%rbp
	 * or (offset == 0 && mask == ~0UL) (for larger multiword orders.)
	 */
	nbits_reg = 1 << order;
	index = pos / BITS_PER_LONG;
	offset = pos - (index * BITS_PER_LONG);
	nlongs_reg = BITS_TO_LONGS(nbits_reg);
ffffffff812cd116:	48 c1 ea 06          	shr    $0x6,%rdx

	/*
	 * Can't do "mask = (1UL << nbitsinlong) - 1", as that
	 * overflows if nbitsinlong == BITS_PER_LONG.
	 */
	mask = (1UL << (nbitsinlong - 1));
ffffffff812cd11a:	83 f9 40             	cmp    $0x40,%ecx
ffffffff812cd11d:	0f 4f c8             	cmovg  %eax,%ecx
ffffffff812cd120:	b8 02 00 00 00       	mov    $0x2,%eax
	mask += mask - 1;
	mask <<= offset;
ffffffff812cd125:	41 c1 e8 06          	shr    $0x6,%r8d

	/*
	 * Can't do "mask = (1UL << nbitsinlong) - 1", as that
	 * overflows if nbitsinlong == BITS_PER_LONG.
	 */
	mask = (1UL << (nbitsinlong - 1));
ffffffff812cd129:	ff c9                	dec    %ecx
	mask += mask - 1;
ffffffff812cd12b:	48 d3 e0             	shl    %cl,%rax
	mask <<= offset;
ffffffff812cd12e:	89 f1                	mov    %esi,%ecx
	/*
	 * Can't do "mask = (1UL << nbitsinlong) - 1", as that
	 * overflows if nbitsinlong == BITS_PER_LONG.
	 */
	mask = (1UL << (nbitsinlong - 1));
	mask += mask - 1;
ffffffff812cd130:	48 ff c8             	dec    %rax
	mask <<= offset;
ffffffff812cd133:	48 d3 e0             	shl    %cl,%rax

	switch (reg_op) {
ffffffff812cd136:	41 83 f9 01          	cmp    $0x1,%r9d
ffffffff812cd13a:	74 16                	je     ffffffff812cd152 <__reg_op+0x5e>
ffffffff812cd13c:	41 83 f9 02          	cmp    $0x2,%r9d
ffffffff812cd140:	74 09                	je     ffffffff812cd14b <__reg_op+0x57>
ffffffff812cd142:	45 85 c9             	test   %r9d,%r9d
ffffffff812cd145:	75 4f                	jne    ffffffff812cd196 <__reg_op+0xa2>
ffffffff812cd147:	31 c9                	xor    %ecx,%ecx
ffffffff812cd149:	eb 1a                	jmp    ffffffff812cd165 <__reg_op+0x71>
ffffffff812cd14b:	31 c9                	xor    %ecx,%ecx
			bitmap[index + i] |= mask;
		break;

	case REG_OP_RELEASE:
		for (i = 0; i < nlongs_reg; i++)
			bitmap[index + i] &= ~mask;
ffffffff812cd14d:	48 f7 d0             	not    %rax
ffffffff812cd150:	eb 31                	jmp    ffffffff812cd183 <__reg_op+0x8f>
	 */
	mask = (1UL << (nbitsinlong - 1));
	mask += mask - 1;
	mask <<= offset;

	switch (reg_op) {
ffffffff812cd152:	31 c9                	xor    %ecx,%ecx
ffffffff812cd154:	eb 1a                	jmp    ffffffff812cd170 <__reg_op+0x7c>
	case REG_OP_ISFREE:
		for (i = 0; i < nlongs_reg; i++) {
			if (bitmap[index + i] & mask)
ffffffff812cd156:	42 8d 34 01          	lea    (%rcx,%r8,1),%esi
ffffffff812cd15a:	48 63 f6             	movslq %esi,%rsi
ffffffff812cd15d:	48 85 04 f7          	test   %rax,(%rdi,%rsi,8)
ffffffff812cd161:	75 33                	jne    ffffffff812cd196 <__reg_op+0xa2>
	mask += mask - 1;
	mask <<= offset;

	switch (reg_op) {
	case REG_OP_ISFREE:
		for (i = 0; i < nlongs_reg; i++) {
ffffffff812cd163:	ff c1                	inc    %ecx
ffffffff812cd165:	39 d1                	cmp    %edx,%ecx
ffffffff812cd167:	7c ed                	jl     ffffffff812cd156 <__reg_op+0x62>
			if (bitmap[index + i] & mask)
				goto done;
		}
		ret = 1;	/* all bits in region free (zero) */
ffffffff812cd169:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812cd16e:	eb 28                	jmp    ffffffff812cd198 <__reg_op+0xa4>
		break;

	case REG_OP_ALLOC:
		for (i = 0; i < nlongs_reg; i++)
ffffffff812cd170:	39 d1                	cmp    %edx,%ecx
ffffffff812cd172:	7d 22                	jge    ffffffff812cd196 <__reg_op+0xa2>
			bitmap[index + i] |= mask;
ffffffff812cd174:	42 8d 34 01          	lea    (%rcx,%r8,1),%esi
		}
		ret = 1;	/* all bits in region free (zero) */
		break;

	case REG_OP_ALLOC:
		for (i = 0; i < nlongs_reg; i++)
ffffffff812cd178:	ff c1                	inc    %ecx
			bitmap[index + i] |= mask;
ffffffff812cd17a:	48 63 f6             	movslq %esi,%rsi
ffffffff812cd17d:	48 09 04 f7          	or     %rax,(%rdi,%rsi,8)
ffffffff812cd181:	eb ed                	jmp    ffffffff812cd170 <__reg_op+0x7c>
		break;

	case REG_OP_RELEASE:
		for (i = 0; i < nlongs_reg; i++)
ffffffff812cd183:	39 d1                	cmp    %edx,%ecx
ffffffff812cd185:	7d 0f                	jge    ffffffff812cd196 <__reg_op+0xa2>
			bitmap[index + i] &= ~mask;
ffffffff812cd187:	42 8d 34 01          	lea    (%rcx,%r8,1),%esi
		for (i = 0; i < nlongs_reg; i++)
			bitmap[index + i] |= mask;
		break;

	case REG_OP_RELEASE:
		for (i = 0; i < nlongs_reg; i++)
ffffffff812cd18b:	ff c1                	inc    %ecx
			bitmap[index + i] &= ~mask;
ffffffff812cd18d:	48 63 f6             	movslq %esi,%rsi
ffffffff812cd190:	48 21 04 f7          	and    %rax,(%rdi,%rsi,8)
ffffffff812cd194:	eb ed                	jmp    ffffffff812cd183 <__reg_op+0x8f>
	int offset;		/* bit offset region in bitmap[index] */
	int nlongs_reg;		/* num longs spanned by region in bitmap */
	int nbitsinlong;	/* num bits of region in each spanned long */
	unsigned long mask;	/* bitmask for one long of region */
	int i;			/* scans bitmap by longs */
	int ret = 0;		/* return value */
ffffffff812cd196:	31 c0                	xor    %eax,%eax
			bitmap[index + i] &= ~mask;
		break;
	}
done:
	return ret;
}
ffffffff812cd198:	5d                   	pop    %rbp
ffffffff812cd199:	c3                   	retq   

ffffffff812cd19a <bitmap_find_free_region>:
 *
 * Return the bit offset in bitmap of the allocated region,
 * or -errno on failure.
 */
int bitmap_find_free_region(unsigned long *bitmap, unsigned int bits, int order)
{
ffffffff812cd19a:	55                   	push   %rbp
	unsigned int pos, end;		/* scans bitmap by regions of size order */

	for (pos = 0 ; (end = pos + (1U << order)) <= bits; pos = end) {
ffffffff812cd19b:	41 bb 01 00 00 00    	mov    $0x1,%r11d
ffffffff812cd1a1:	88 d1                	mov    %dl,%cl
ffffffff812cd1a3:	41 d3 e3             	shl    %cl,%r11d
ffffffff812cd1a6:	45 31 d2             	xor    %r10d,%r10d
 *
 * Return the bit offset in bitmap of the allocated region,
 * or -errno on failure.
 */
int bitmap_find_free_region(unsigned long *bitmap, unsigned int bits, int order)
{
ffffffff812cd1a9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd1ac:	41 56                	push   %r14
ffffffff812cd1ae:	41 89 f6             	mov    %esi,%r14d
ffffffff812cd1b1:	41 55                	push   %r13
ffffffff812cd1b3:	49 89 fd             	mov    %rdi,%r13
ffffffff812cd1b6:	41 54                	push   %r12
ffffffff812cd1b8:	53                   	push   %rbx
ffffffff812cd1b9:	89 d3                	mov    %edx,%ebx
	unsigned int pos, end;		/* scans bitmap by regions of size order */

	for (pos = 0 ; (end = pos + (1U << order)) <= bits; pos = end) {
ffffffff812cd1bb:	47 8d 24 1a          	lea    (%r10,%r11,1),%r12d
ffffffff812cd1bf:	45 39 f4             	cmp    %r14d,%r12d
ffffffff812cd1c2:	77 2f                	ja     ffffffff812cd1f3 <bitmap_find_free_region+0x59>
		if (!__reg_op(bitmap, pos, order, REG_OP_ISFREE))
ffffffff812cd1c4:	31 c9                	xor    %ecx,%ecx
ffffffff812cd1c6:	89 da                	mov    %ebx,%edx
ffffffff812cd1c8:	44 89 d6             	mov    %r10d,%esi
ffffffff812cd1cb:	4c 89 ef             	mov    %r13,%rdi
ffffffff812cd1ce:	e8 21 ff ff ff       	callq  ffffffff812cd0f4 <__reg_op>
ffffffff812cd1d3:	85 c0                	test   %eax,%eax
ffffffff812cd1d5:	74 17                	je     ffffffff812cd1ee <bitmap_find_free_region+0x54>
			continue;
		__reg_op(bitmap, pos, order, REG_OP_ALLOC);
ffffffff812cd1d7:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812cd1dc:	89 da                	mov    %ebx,%edx
ffffffff812cd1de:	44 89 d6             	mov    %r10d,%esi
ffffffff812cd1e1:	4c 89 ef             	mov    %r13,%rdi
ffffffff812cd1e4:	e8 0b ff ff ff       	callq  ffffffff812cd0f4 <__reg_op>
		return pos;
ffffffff812cd1e9:	44 89 d0             	mov    %r10d,%eax
ffffffff812cd1ec:	eb 0a                	jmp    ffffffff812cd1f8 <bitmap_find_free_region+0x5e>
ffffffff812cd1ee:	45 89 e2             	mov    %r12d,%r10d
ffffffff812cd1f1:	eb c8                	jmp    ffffffff812cd1bb <bitmap_find_free_region+0x21>
	}
	return -ENOMEM;
ffffffff812cd1f3:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
}
ffffffff812cd1f8:	5b                   	pop    %rbx
ffffffff812cd1f9:	41 5c                	pop    %r12
ffffffff812cd1fb:	41 5d                	pop    %r13
ffffffff812cd1fd:	41 5e                	pop    %r14
ffffffff812cd1ff:	5d                   	pop    %rbp
ffffffff812cd200:	c3                   	retq   

ffffffff812cd201 <bitmap_release_region>:
 * the found region (by clearing it in the bitmap).
 *
 * No return value.
 */
void bitmap_release_region(unsigned long *bitmap, unsigned int pos, int order)
{
ffffffff812cd201:	55                   	push   %rbp
	__reg_op(bitmap, pos, order, REG_OP_RELEASE);
ffffffff812cd202:	b9 02 00 00 00       	mov    $0x2,%ecx
 * the found region (by clearing it in the bitmap).
 *
 * No return value.
 */
void bitmap_release_region(unsigned long *bitmap, unsigned int pos, int order)
{
ffffffff812cd207:	48 89 e5             	mov    %rsp,%rbp
	__reg_op(bitmap, pos, order, REG_OP_RELEASE);
ffffffff812cd20a:	e8 e5 fe ff ff       	callq  ffffffff812cd0f4 <__reg_op>
}
ffffffff812cd20f:	5d                   	pop    %rbp
ffffffff812cd210:	c3                   	retq   

ffffffff812cd211 <bitmap_allocate_region>:
 *
 * Return 0 on success, or %-EBUSY if specified region wasn't
 * free (not all bits were zero).
 */
int bitmap_allocate_region(unsigned long *bitmap, unsigned int pos, int order)
{
ffffffff812cd211:	55                   	push   %rbp
	if (!__reg_op(bitmap, pos, order, REG_OP_ISFREE))
ffffffff812cd212:	31 c9                	xor    %ecx,%ecx
 *
 * Return 0 on success, or %-EBUSY if specified region wasn't
 * free (not all bits were zero).
 */
int bitmap_allocate_region(unsigned long *bitmap, unsigned int pos, int order)
{
ffffffff812cd214:	41 89 f3             	mov    %esi,%r11d
ffffffff812cd217:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd21a:	53                   	push   %rbx
ffffffff812cd21b:	89 d3                	mov    %edx,%ebx
	if (!__reg_op(bitmap, pos, order, REG_OP_ISFREE))
ffffffff812cd21d:	e8 d2 fe ff ff       	callq  ffffffff812cd0f4 <__reg_op>
ffffffff812cd222:	89 c2                	mov    %eax,%edx
ffffffff812cd224:	b8 f0 ff ff ff       	mov    $0xfffffff0,%eax
ffffffff812cd229:	85 d2                	test   %edx,%edx
ffffffff812cd22b:	74 0f                	je     ffffffff812cd23c <bitmap_allocate_region+0x2b>
		return -EBUSY;
	return __reg_op(bitmap, pos, order, REG_OP_ALLOC);
ffffffff812cd22d:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812cd232:	89 da                	mov    %ebx,%edx
ffffffff812cd234:	44 89 de             	mov    %r11d,%esi
ffffffff812cd237:	e8 b8 fe ff ff       	callq  ffffffff812cd0f4 <__reg_op>
}
ffffffff812cd23c:	5b                   	pop    %rbx
ffffffff812cd23d:	5d                   	pop    %rbp
ffffffff812cd23e:	c3                   	retq   

ffffffff812cd23f <__bitmap_shift_right>:
 * direction.  Zeros are fed into the vacated MS positions and the
 * LS bits shifted off the bottom are lost.
 */
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			unsigned shift, unsigned nbits)
{
ffffffff812cd23f:	55                   	push   %rbp
	unsigned k, lim = BITS_TO_LONGS(nbits);
	unsigned off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
ffffffff812cd240:	41 89 d2             	mov    %edx,%r10d
	unsigned long mask = BITMAP_LAST_WORD_MASK(nbits);
ffffffff812cd243:	48 83 c8 ff          	or     $0xffffffffffffffff,%rax
 */
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			unsigned shift, unsigned nbits)
{
	unsigned k, lim = BITS_TO_LONGS(nbits);
	unsigned off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
ffffffff812cd247:	41 c1 ea 06          	shr    $0x6,%r10d
ffffffff812cd24b:	83 e2 3f             	and    $0x3f,%edx
 * direction.  Zeros are fed into the vacated MS positions and the
 * LS bits shifted off the bottom are lost.
 */
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			unsigned shift, unsigned nbits)
{
ffffffff812cd24e:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd251:	41 57                	push   %r15
ffffffff812cd253:	41 56                	push   %r14
ffffffff812cd255:	41 55                	push   %r13
ffffffff812cd257:	41 54                	push   %r12
			upper = 0;
		else {
			upper = src[off + k + 1];
			if (off + k + 1 == lim - 1)
				upper &= mask;
			upper <<= (BITS_PER_LONG - rem);
ffffffff812cd259:	41 be 40 00 00 00    	mov    $0x40,%r14d
 * direction.  Zeros are fed into the vacated MS positions and the
 * LS bits shifted off the bottom are lost.
 */
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			unsigned shift, unsigned nbits)
{
ffffffff812cd25f:	53                   	push   %rbx
ffffffff812cd260:	41 50                	push   %r8
	unsigned k, lim = BITS_TO_LONGS(nbits);
ffffffff812cd262:	41 89 c8             	mov    %ecx,%r8d
ffffffff812cd265:	49 83 c0 3f          	add    $0x3f,%r8
	unsigned off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	unsigned long mask = BITMAP_LAST_WORD_MASK(nbits);
ffffffff812cd269:	f7 d9                	neg    %ecx
ffffffff812cd26b:	41 8d 5a 01          	lea    0x1(%r10),%ebx
 * LS bits shifted off the bottom are lost.
 */
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			unsigned shift, unsigned nbits)
{
	unsigned k, lim = BITS_TO_LONGS(nbits);
ffffffff812cd26f:	49 c1 e8 06          	shr    $0x6,%r8
	unsigned off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	unsigned long mask = BITMAP_LAST_WORD_MASK(nbits);
ffffffff812cd273:	48 d3 e8             	shr    %cl,%rax
ffffffff812cd276:	49 89 fd             	mov    %rdi,%r13
ffffffff812cd279:	45 8d 78 ff          	lea    -0x1(%r8),%r15d
 * LS bits shifted off the bottom are lost.
 */
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			unsigned shift, unsigned nbits)
{
	unsigned k, lim = BITS_TO_LONGS(nbits);
ffffffff812cd27d:	45 89 c4             	mov    %r8d,%r12d
	unsigned off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	unsigned long mask = BITMAP_LAST_WORD_MASK(nbits);
ffffffff812cd280:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
 */
void __bitmap_shift_right(unsigned long *dst, const unsigned long *src,
			unsigned shift, unsigned nbits)
{
	unsigned k, lim = BITS_TO_LONGS(nbits);
	unsigned off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
ffffffff812cd284:	45 89 d3             	mov    %r10d,%r11d
			upper = 0;
		else {
			upper = src[off + k + 1];
			if (off + k + 1 == lim - 1)
				upper &= mask;
			upper <<= (BITS_PER_LONG - rem);
ffffffff812cd287:	41 29 d6             	sub    %edx,%r14d
			unsigned shift, unsigned nbits)
{
	unsigned k, lim = BITS_TO_LONGS(nbits);
	unsigned off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	unsigned long mask = BITMAP_LAST_WORD_MASK(nbits);
	for (k = 0; off + k < lim; ++k) {
ffffffff812cd28a:	45 39 dc             	cmp    %r11d,%r12d
ffffffff812cd28d:	76 51                	jbe    ffffffff812cd2e0 <__bitmap_shift_right+0xa1>
		/*
		 * If shift is not word aligned, take lower rem bits of
		 * word above and make them the top rem bits of result.
		 */
		if (!rem || off + k + 1 >= lim)
			upper = 0;
ffffffff812cd28f:	31 c0                	xor    %eax,%eax

		/*
		 * If shift is not word aligned, take lower rem bits of
		 * word above and make them the top rem bits of result.
		 */
		if (!rem || off + k + 1 >= lim)
ffffffff812cd291:	85 d2                	test   %edx,%edx
ffffffff812cd293:	74 1f                	je     ffffffff812cd2b4 <__bitmap_shift_right+0x75>
ffffffff812cd295:	41 39 dc             	cmp    %ebx,%r12d
ffffffff812cd298:	76 1a                	jbe    ffffffff812cd2b4 <__bitmap_shift_right+0x75>
			upper = 0;
		else {
			upper = src[off + k + 1];
ffffffff812cd29a:	89 d8                	mov    %ebx,%eax
			if (off + k + 1 == lim - 1)
				upper &= mask;
ffffffff812cd29c:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
		 * word above and make them the top rem bits of result.
		 */
		if (!rem || off + k + 1 >= lim)
			upper = 0;
		else {
			upper = src[off + k + 1];
ffffffff812cd2a0:	48 8b 04 c6          	mov    (%rsi,%rax,8),%rax
			if (off + k + 1 == lim - 1)
				upper &= mask;
ffffffff812cd2a4:	48 21 c1             	and    %rax,%rcx
ffffffff812cd2a7:	41 39 df             	cmp    %ebx,%r15d
ffffffff812cd2aa:	48 0f 44 c1          	cmove  %rcx,%rax
			upper <<= (BITS_PER_LONG - rem);
ffffffff812cd2ae:	44 88 f1             	mov    %r14b,%cl
ffffffff812cd2b1:	48 d3 e0             	shl    %cl,%rax
		}
		lower = src[off + k];
ffffffff812cd2b4:	44 89 d9             	mov    %r11d,%ecx
ffffffff812cd2b7:	4c 8b 0c ce          	mov    (%rsi,%rcx,8),%r9
		if (off + k == lim - 1)
			lower &= mask;
ffffffff812cd2bb:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
ffffffff812cd2bf:	4c 21 c9             	and    %r9,%rcx
ffffffff812cd2c2:	45 39 df             	cmp    %r11d,%r15d
ffffffff812cd2c5:	4c 0f 44 c9          	cmove  %rcx,%r9
		lower >>= rem;
		dst[k] = lower | upper;
ffffffff812cd2c9:	88 d1                	mov    %dl,%cl
ffffffff812cd2cb:	41 ff c3             	inc    %r11d
ffffffff812cd2ce:	49 d3 e9             	shr    %cl,%r9
ffffffff812cd2d1:	49 83 c5 08          	add    $0x8,%r13
ffffffff812cd2d5:	ff c3                	inc    %ebx
ffffffff812cd2d7:	49 09 c1             	or     %rax,%r9
ffffffff812cd2da:	4d 89 4d f8          	mov    %r9,-0x8(%r13)
ffffffff812cd2de:	eb aa                	jmp    ffffffff812cd28a <__bitmap_shift_right+0x4b>
	}
	if (off)
ffffffff812cd2e0:	45 85 d2             	test   %r10d,%r10d
ffffffff812cd2e3:	74 15                	je     ffffffff812cd2fa <__bitmap_shift_right+0xbb>
		memset(&dst[lim - off], 0, off*sizeof(unsigned long));
ffffffff812cd2e5:	45 29 d0             	sub    %r10d,%r8d
ffffffff812cd2e8:	44 89 d1             	mov    %r10d,%ecx
ffffffff812cd2eb:	31 c0                	xor    %eax,%eax
ffffffff812cd2ed:	4a 8d 14 c7          	lea    (%rdi,%r8,8),%rdx
ffffffff812cd2f1:	48 c1 e1 03          	shl    $0x3,%rcx
ffffffff812cd2f5:	48 89 d7             	mov    %rdx,%rdi
ffffffff812cd2f8:	f3 aa                	rep stos %al,%es:(%rdi)
}
ffffffff812cd2fa:	58                   	pop    %rax
ffffffff812cd2fb:	5b                   	pop    %rbx
ffffffff812cd2fc:	41 5c                	pop    %r12
ffffffff812cd2fe:	41 5d                	pop    %r13
ffffffff812cd300:	41 5e                	pop    %r14
ffffffff812cd302:	41 5f                	pop    %r15
ffffffff812cd304:	5d                   	pop    %rbp
ffffffff812cd305:	c3                   	retq   

ffffffff812cd306 <__bitmap_shift_left>:
 * and those MS bits shifted off the top are lost.
 */

void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
			unsigned int shift, unsigned int nbits)
{
ffffffff812cd306:	49 89 f8             	mov    %rdi,%r8
	int k;
	unsigned int lim = BITS_TO_LONGS(nbits);
	unsigned int off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
ffffffff812cd309:	89 d7                	mov    %edx,%edi
 * and those MS bits shifted off the top are lost.
 */

void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
			unsigned int shift, unsigned int nbits)
{
ffffffff812cd30b:	55                   	push   %rbp
	int k;
	unsigned int lim = BITS_TO_LONGS(nbits);
	unsigned int off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	for (k = lim - off - 1; k >= 0; --k) {
ffffffff812cd30c:	89 c8                	mov    %ecx,%eax
void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
			unsigned int shift, unsigned int nbits)
{
	int k;
	unsigned int lim = BITS_TO_LONGS(nbits);
	unsigned int off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
ffffffff812cd30e:	c1 ef 06             	shr    $0x6,%edi
ffffffff812cd311:	83 e2 3f             	and    $0x3f,%edx
	for (k = lim - off - 1; k >= 0; --k) {
ffffffff812cd314:	48 83 c0 3f          	add    $0x3f,%rax
ffffffff812cd318:	89 f9                	mov    %edi,%ecx
 * and those MS bits shifted off the top are lost.
 */

void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
			unsigned int shift, unsigned int nbits)
{
ffffffff812cd31a:	48 89 e5             	mov    %rsp,%rbp
	int k;
	unsigned int lim = BITS_TO_LONGS(nbits);
	unsigned int off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	for (k = lim - off - 1; k >= 0; --k) {
ffffffff812cd31d:	48 c1 e8 06          	shr    $0x6,%rax
ffffffff812cd321:	f7 d1                	not    %ecx
		/*
		 * If shift is not word aligned, take upper rem bits of
		 * word below and make them the bottom rem bits of result.
		 */
		if (rem && k > 0)
			lower = src[k - 1] >> (BITS_PER_LONG - rem);
ffffffff812cd323:	41 ba 40 00 00 00    	mov    $0x40,%r10d
 * and those MS bits shifted off the top are lost.
 */

void __bitmap_shift_left(unsigned long *dst, const unsigned long *src,
			unsigned int shift, unsigned int nbits)
{
ffffffff812cd329:	53                   	push   %rbx
	int k;
	unsigned int lim = BITS_TO_LONGS(nbits);
	unsigned int off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	for (k = lim - off - 1; k >= 0; --k) {
ffffffff812cd32a:	01 c8                	add    %ecx,%eax
		/*
		 * If shift is not word aligned, take upper rem bits of
		 * word below and make them the bottom rem bits of result.
		 */
		if (rem && k > 0)
			lower = src[k - 1] >> (BITS_PER_LONG - rem);
ffffffff812cd32c:	41 29 d2             	sub    %edx,%r10d
			unsigned int shift, unsigned int nbits)
{
	int k;
	unsigned int lim = BITS_TO_LONGS(nbits);
	unsigned int off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	for (k = lim - off - 1; k >= 0; --k) {
ffffffff812cd32f:	85 c0                	test   %eax,%eax
ffffffff812cd331:	78 32                	js     ffffffff812cd365 <__bitmap_shift_left+0x5f>

		/*
		 * If shift is not word aligned, take upper rem bits of
		 * word below and make them the bottom rem bits of result.
		 */
		if (rem && k > 0)
ffffffff812cd333:	85 d2                	test   %edx,%edx
ffffffff812cd335:	4c 63 d8             	movslq %eax,%r11
ffffffff812cd338:	74 11                	je     ffffffff812cd34b <__bitmap_shift_left+0x45>
ffffffff812cd33a:	85 c0                	test   %eax,%eax
ffffffff812cd33c:	7e 0d                	jle    ffffffff812cd34b <__bitmap_shift_left+0x45>
			lower = src[k - 1] >> (BITS_PER_LONG - rem);
ffffffff812cd33e:	4e 8b 4c de f8       	mov    -0x8(%rsi,%r11,8),%r9
ffffffff812cd343:	44 88 d1             	mov    %r10b,%cl
ffffffff812cd346:	49 d3 e9             	shr    %cl,%r9
ffffffff812cd349:	eb 03                	jmp    ffffffff812cd34e <__bitmap_shift_left+0x48>
		else
			lower = 0;
ffffffff812cd34b:	45 31 c9             	xor    %r9d,%r9d
		upper = src[k] << rem;
		dst[k + off] = lower | upper;
ffffffff812cd34e:	4e 8b 1c de          	mov    (%rsi,%r11,8),%r11
ffffffff812cd352:	88 d1                	mov    %dl,%cl
ffffffff812cd354:	8d 1c 38             	lea    (%rax,%rdi,1),%ebx
			unsigned int shift, unsigned int nbits)
{
	int k;
	unsigned int lim = BITS_TO_LONGS(nbits);
	unsigned int off = shift/BITS_PER_LONG, rem = shift % BITS_PER_LONG;
	for (k = lim - off - 1; k >= 0; --k) {
ffffffff812cd357:	ff c8                	dec    %eax
		if (rem && k > 0)
			lower = src[k - 1] >> (BITS_PER_LONG - rem);
		else
			lower = 0;
		upper = src[k] << rem;
		dst[k + off] = lower | upper;
ffffffff812cd359:	49 d3 e3             	shl    %cl,%r11
ffffffff812cd35c:	4d 09 d9             	or     %r11,%r9
ffffffff812cd35f:	4d 89 0c d8          	mov    %r9,(%r8,%rbx,8)
ffffffff812cd363:	eb ca                	jmp    ffffffff812cd32f <__bitmap_shift_left+0x29>
	}
	if (off)
ffffffff812cd365:	85 ff                	test   %edi,%edi
ffffffff812cd367:	74 0d                	je     ffffffff812cd376 <__bitmap_shift_left+0x70>
		memset(dst, 0, off*sizeof(unsigned long));
ffffffff812cd369:	89 f9                	mov    %edi,%ecx
ffffffff812cd36b:	31 c0                	xor    %eax,%eax
ffffffff812cd36d:	4c 89 c7             	mov    %r8,%rdi
ffffffff812cd370:	48 c1 e1 03          	shl    $0x3,%rcx
ffffffff812cd374:	f3 aa                	rep stos %al,%es:(%rdi)
}
ffffffff812cd376:	5b                   	pop    %rbx
ffffffff812cd377:	5d                   	pop    %rbp
ffffffff812cd378:	c3                   	retq   

ffffffff812cd379 <hweight_long>:
		order++;
	return order;
}

static inline unsigned long hweight_long(unsigned long w)
{
ffffffff812cd379:	55                   	push   %rbp
ffffffff812cd37a:	e8 88 7a 00 00       	callq  ffffffff812d4e07 <__sw_hweight64>
ffffffff812cd37f:	48 89 e5             	mov    %rsp,%rbp
	return sizeof(w) == 4 ? hweight32(w) : hweight64(w);
}
ffffffff812cd382:	5d                   	pop    %rbp
ffffffff812cd383:	c3                   	retq   

ffffffff812cd384 <__bitmap_weight>:
	return 1;
}
EXPORT_SYMBOL(__bitmap_subset);

int __bitmap_weight(const unsigned long *bitmap, unsigned int bits)
{
ffffffff812cd384:	89 f0                	mov    %esi,%eax
ffffffff812cd386:	55                   	push   %rbp
ffffffff812cd387:	48 89 fa             	mov    %rdi,%rdx
ffffffff812cd38a:	c1 e8 06             	shr    $0x6,%eax
	unsigned int k, lim = bits/BITS_PER_LONG;
	int w = 0;
ffffffff812cd38d:	45 31 c0             	xor    %r8d,%r8d
ffffffff812cd390:	48 8d 0c c7          	lea    (%rdi,%rax,8),%rcx
	return 1;
}
EXPORT_SYMBOL(__bitmap_subset);

int __bitmap_weight(const unsigned long *bitmap, unsigned int bits)
{
ffffffff812cd394:	48 89 e5             	mov    %rsp,%rbp
	unsigned int k, lim = bits/BITS_PER_LONG;
	int w = 0;

	for (k = 0; k < lim; k++)
ffffffff812cd397:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812cd39a:	74 11                	je     ffffffff812cd3ad <__bitmap_weight+0x29>
		w += hweight_long(bitmap[k]);
ffffffff812cd39c:	48 8b 3a             	mov    (%rdx),%rdi
ffffffff812cd39f:	e8 d5 ff ff ff       	callq  ffffffff812cd379 <hweight_long>
ffffffff812cd3a4:	41 01 c0             	add    %eax,%r8d
ffffffff812cd3a7:	48 83 c2 08          	add    $0x8,%rdx
ffffffff812cd3ab:	eb ea                	jmp    ffffffff812cd397 <__bitmap_weight+0x13>

	if (bits % BITS_PER_LONG)
ffffffff812cd3ad:	40 f6 c6 3f          	test   $0x3f,%sil
ffffffff812cd3b1:	74 16                	je     ffffffff812cd3c9 <__bitmap_weight+0x45>
		w += hweight_long(bitmap[k] & BITMAP_LAST_WORD_MASK(bits));
ffffffff812cd3b3:	89 f1                	mov    %esi,%ecx
ffffffff812cd3b5:	48 83 cf ff          	or     $0xffffffffffffffff,%rdi
ffffffff812cd3b9:	f7 d9                	neg    %ecx
ffffffff812cd3bb:	48 d3 ef             	shr    %cl,%rdi
ffffffff812cd3be:	48 23 3a             	and    (%rdx),%rdi
ffffffff812cd3c1:	e8 b3 ff ff ff       	callq  ffffffff812cd379 <hweight_long>
ffffffff812cd3c6:	41 01 c0             	add    %eax,%r8d

	return w;
}
ffffffff812cd3c9:	44 89 c0             	mov    %r8d,%eax
ffffffff812cd3cc:	5d                   	pop    %rbp
ffffffff812cd3cd:	c3                   	retq   

ffffffff812cd3ce <bitmap_zero>:
{
	if (small_const_nbits(nbits))
		*dst = 0UL;
	else {
		unsigned int len = BITS_TO_LONGS(nbits) * sizeof(unsigned long);
		memset(dst, 0, len);
ffffffff812cd3ce:	89 f1                	mov    %esi,%ecx

#define small_const_nbits(nbits) \
	(__builtin_constant_p(nbits) && (nbits) <= BITS_PER_LONG)

static inline void bitmap_zero(unsigned long *dst, unsigned int nbits)
{
ffffffff812cd3d0:	55                   	push   %rbp
	if (small_const_nbits(nbits))
		*dst = 0UL;
	else {
		unsigned int len = BITS_TO_LONGS(nbits) * sizeof(unsigned long);
		memset(dst, 0, len);
ffffffff812cd3d1:	31 c0                	xor    %eax,%eax
ffffffff812cd3d3:	48 83 c1 3f          	add    $0x3f,%rcx
ffffffff812cd3d7:	48 c1 e9 06          	shr    $0x6,%rcx

#define small_const_nbits(nbits) \
	(__builtin_constant_p(nbits) && (nbits) <= BITS_PER_LONG)

static inline void bitmap_zero(unsigned long *dst, unsigned int nbits)
{
ffffffff812cd3db:	48 89 e5             	mov    %rsp,%rbp
	if (small_const_nbits(nbits))
		*dst = 0UL;
	else {
		unsigned int len = BITS_TO_LONGS(nbits) * sizeof(unsigned long);
		memset(dst, 0, len);
ffffffff812cd3de:	48 c1 e1 03          	shl    $0x3,%rcx
ffffffff812cd3e2:	f3 aa                	rep stos %al,%es:(%rdi)
	}
}
ffffffff812cd3e4:	5d                   	pop    %rbp
ffffffff812cd3e5:	c3                   	retq   

ffffffff812cd3e6 <__bitmap_parselist>:
 *    %-ERANGE: bit number specified too large for mask
 */
static int __bitmap_parselist(const char *buf, unsigned int buflen,
		int is_user, unsigned long *maskp,
		int nmaskbits)
{
ffffffff812cd3e6:	55                   	push   %rbp
ffffffff812cd3e7:	49 89 fa             	mov    %rdi,%r10
ffffffff812cd3ea:	41 89 f3             	mov    %esi,%r11d
	int c, old_c, totaldigits;
	const char __user __force *ubuf = (const char __user __force *)buf;
	int at_start, in_range;

	totaldigits = c = 0;
	bitmap_zero(maskp, nmaskbits);
ffffffff812cd3ed:	48 89 cf             	mov    %rcx,%rdi
ffffffff812cd3f0:	44 89 c6             	mov    %r8d,%esi
 *    %-ERANGE: bit number specified too large for mask
 */
static int __bitmap_parselist(const char *buf, unsigned int buflen,
		int is_user, unsigned long *maskp,
		int nmaskbits)
{
ffffffff812cd3f3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd3f6:	41 57                	push   %r15
ffffffff812cd3f8:	41 56                	push   %r14
ffffffff812cd3fa:	41 55                	push   %r13
ffffffff812cd3fc:	41 54                	push   %r12
ffffffff812cd3fe:	49 89 ce             	mov    %rcx,%r14
ffffffff812cd401:	53                   	push   %rbx
ffffffff812cd402:	41 51                	push   %r9
	unsigned a, b;
	int c, old_c, totaldigits;
	const char __user __force *ubuf = (const char __user __force *)buf;
	int at_start, in_range;

	totaldigits = c = 0;
ffffffff812cd404:	45 31 ed             	xor    %r13d,%r13d
	bitmap_zero(maskp, nmaskbits);
ffffffff812cd407:	e8 c2 ff ff ff       	callq  ffffffff812cd3ce <bitmap_zero>
	unsigned a, b;
	int c, old_c, totaldigits;
	const char __user __force *ubuf = (const char __user __force *)buf;
	int at_start, in_range;

	totaldigits = c = 0;
ffffffff812cd40c:	31 c0                	xor    %eax,%eax
		int is_user, unsigned long *maskp,
		int nmaskbits)
{
	unsigned a, b;
	int c, old_c, totaldigits;
	const char __user __force *ubuf = (const char __user __force *)buf;
ffffffff812cd40e:	4d 89 d1             	mov    %r10,%r9
		at_start = 1;
		in_range = 0;
		a = b = 0;

		/* Get the next cpu# or a range of cpu#'s */
		while (buflen) {
ffffffff812cd411:	41 89 c7             	mov    %eax,%r15d

	totaldigits = c = 0;
	bitmap_zero(maskp, nmaskbits);
	do {
		at_start = 1;
		in_range = 0;
ffffffff812cd414:	31 c9                	xor    %ecx,%ecx
	int at_start, in_range;

	totaldigits = c = 0;
	bitmap_zero(maskp, nmaskbits);
	do {
		at_start = 1;
ffffffff812cd416:	bb 01 00 00 00       	mov    $0x1,%ebx
		in_range = 0;
		a = b = 0;
ffffffff812cd41b:	31 f6                	xor    %esi,%esi
ffffffff812cd41d:	31 ff                	xor    %edi,%edi

		/* Get the next cpu# or a range of cpu#'s */
		while (buflen) {
ffffffff812cd41f:	45 85 db             	test   %r11d,%r11d
ffffffff812cd422:	0f 84 99 00 00 00    	je     ffffffff812cd4c1 <__bitmap_parselist+0xdb>
			old_c = c;
			if (is_user) {
ffffffff812cd428:	85 d2                	test   %edx,%edx
ffffffff812cd42a:	74 26                	je     ffffffff812cd452 <__bitmap_parselist+0x6c>
				if (__get_user(c, ubuf++))
ffffffff812cd42c:	49 8d 41 01          	lea    0x1(%r9),%rax
ffffffff812cd430:	45 31 e4             	xor    %r12d,%r12d
ffffffff812cd433:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
ffffffff812cd437:	90                   	nop
ffffffff812cd438:	90                   	nop
ffffffff812cd439:	90                   	nop
ffffffff812cd43a:	41 8a 01             	mov    (%r9),%al
ffffffff812cd43d:	90                   	nop
ffffffff812cd43e:	90                   	nop
ffffffff812cd43f:	90                   	nop
ffffffff812cd440:	45 85 e4             	test   %r12d,%r12d
ffffffff812cd443:	0f be c0             	movsbl %al,%eax
ffffffff812cd446:	0f 85 a3 00 00 00    	jne    ffffffff812cd4ef <__bitmap_parselist+0x109>
ffffffff812cd44c:	4c 8b 4d d0          	mov    -0x30(%rbp),%r9
ffffffff812cd450:	eb 07                	jmp    ffffffff812cd459 <__bitmap_parselist+0x73>
					return -EFAULT;
			} else
				c = *buf++;
ffffffff812cd452:	41 0f be 02          	movsbl (%r10),%eax
ffffffff812cd456:	49 ff c2             	inc    %r10
			buflen--;
			if (isspace(c))
ffffffff812cd459:	44 0f b6 e0          	movzbl %al,%r12d
			if (is_user) {
				if (__get_user(c, ubuf++))
					return -EFAULT;
			} else
				c = *buf++;
			buflen--;
ffffffff812cd45d:	41 ff cb             	dec    %r11d
			if (isspace(c))
ffffffff812cd460:	45 8a a4 24 60 ef 63 	mov    -0x7e9c10a0(%r12),%r12b
ffffffff812cd467:	81 
ffffffff812cd468:	41 f6 c4 20          	test   $0x20,%r12b
ffffffff812cd46c:	75 4b                	jne    ffffffff812cd4b9 <__bitmap_parselist+0xd3>
			/*
			 * If the last character was a space and the current
			 * character isn't '\0', we've got embedded whitespace.
			 * This is a no-no, so throw an error.
			 */
			if (totaldigits && c && isspace(old_c))
ffffffff812cd46e:	45 85 ed             	test   %r13d,%r13d
ffffffff812cd471:	74 12                	je     ffffffff812cd485 <__bitmap_parselist+0x9f>
ffffffff812cd473:	85 c0                	test   %eax,%eax
ffffffff812cd475:	74 0e                	je     ffffffff812cd485 <__bitmap_parselist+0x9f>
ffffffff812cd477:	45 0f b6 ff          	movzbl %r15b,%r15d
ffffffff812cd47b:	41 f6 87 60 ef 63 81 	testb  $0x20,-0x7e9c10a0(%r15)
ffffffff812cd482:	20 
ffffffff812cd483:	75 71                	jne    ffffffff812cd4f6 <__bitmap_parselist+0x110>
				return -EINVAL;

			/* A '\0' or a ',' signal the end of a cpu# or range */
			if (c == '\0' || c == ',')
ffffffff812cd485:	85 c0                	test   %eax,%eax
ffffffff812cd487:	74 38                	je     ffffffff812cd4c1 <__bitmap_parselist+0xdb>
ffffffff812cd489:	83 f8 2c             	cmp    $0x2c,%eax
ffffffff812cd48c:	74 33                	je     ffffffff812cd4c1 <__bitmap_parselist+0xdb>
				break;

			if (c == '-') {
ffffffff812cd48e:	83 f8 2d             	cmp    $0x2d,%eax
ffffffff812cd491:	75 0f                	jne    ffffffff812cd4a2 <__bitmap_parselist+0xbc>
				if (at_start || in_range)
ffffffff812cd493:	09 cb                	or     %ecx,%ebx
ffffffff812cd495:	75 5f                	jne    ffffffff812cd4f6 <__bitmap_parselist+0x110>
					return -EINVAL;
				b = 0;
				in_range = 1;
ffffffff812cd497:	b9 01 00 00 00       	mov    $0x1,%ecx
ffffffff812cd49c:	31 db                	xor    %ebx,%ebx
				break;

			if (c == '-') {
				if (at_start || in_range)
					return -EINVAL;
				b = 0;
ffffffff812cd49e:	31 f6                	xor    %esi,%esi
ffffffff812cd4a0:	eb 17                	jmp    ffffffff812cd4b9 <__bitmap_parselist+0xd3>
				in_range = 1;
				continue;
			}

			if (!isdigit(c))
ffffffff812cd4a2:	41 80 e4 04          	and    $0x4,%r12b
ffffffff812cd4a6:	74 4e                	je     ffffffff812cd4f6 <__bitmap_parselist+0x110>
ffffffff812cd4a8:	6b f6 0a             	imul   $0xa,%esi,%esi
				return -EINVAL;

			b = b * 10 + (c - '0');
			if (!in_range)
				a = b;
ffffffff812cd4ab:	85 c9                	test   %ecx,%ecx
			}

			if (!isdigit(c))
				return -EINVAL;

			b = b * 10 + (c - '0');
ffffffff812cd4ad:	8d 74 30 d0          	lea    -0x30(%rax,%rsi,1),%esi
			if (!in_range)
				a = b;
ffffffff812cd4b1:	0f 44 fe             	cmove  %esi,%edi
			at_start = 0;
			totaldigits++;
ffffffff812cd4b4:	41 ff c5             	inc    %r13d
				return -EINVAL;

			b = b * 10 + (c - '0');
			if (!in_range)
				a = b;
			at_start = 0;
ffffffff812cd4b7:	31 db                	xor    %ebx,%ebx
ffffffff812cd4b9:	41 89 c7             	mov    %eax,%r15d
ffffffff812cd4bc:	e9 5e ff ff ff       	jmpq   ffffffff812cd41f <__bitmap_parselist+0x39>
			totaldigits++;
		}
		if (!(a <= b))
ffffffff812cd4c1:	39 f7                	cmp    %esi,%edi
ffffffff812cd4c3:	77 31                	ja     ffffffff812cd4f6 <__bitmap_parselist+0x110>
			return -EINVAL;
		if (b >= nmaskbits)
ffffffff812cd4c5:	44 39 c6             	cmp    %r8d,%esi
ffffffff812cd4c8:	73 33                	jae    ffffffff812cd4fd <__bitmap_parselist+0x117>
			return -ERANGE;
		if (!at_start) {
ffffffff812cd4ca:	85 db                	test   %ebx,%ebx
ffffffff812cd4cc:	74 12                	je     ffffffff812cd4e0 <__bitmap_parselist+0xfa>
			while (a <= b) {
				set_bit(a, maskp);
				a++;
			}
		}
	} while (buflen && c == ',');
ffffffff812cd4ce:	45 85 db             	test   %r11d,%r11d
ffffffff812cd4d1:	74 09                	je     ffffffff812cd4dc <__bitmap_parselist+0xf6>
ffffffff812cd4d3:	83 f8 2c             	cmp    $0x2c,%eax
ffffffff812cd4d6:	0f 84 35 ff ff ff    	je     ffffffff812cd411 <__bitmap_parselist+0x2b>
	return 0;
ffffffff812cd4dc:	31 c0                	xor    %eax,%eax
ffffffff812cd4de:	eb 22                	jmp    ffffffff812cd502 <__bitmap_parselist+0x11c>
			return -EINVAL;
		if (b >= nmaskbits)
			return -ERANGE;
		if (!at_start) {
			while (a <= b) {
				set_bit(a, maskp);
ffffffff812cd4e0:	89 f9                	mov    %edi,%ecx
		asm volatile(LOCK_PREFIX "orb %1,%0"
			: CONST_MASK_ADDR(nr, addr)
			: "iq" ((u8)CONST_MASK(nr))
			: "memory");
	} else {
		asm volatile(LOCK_PREFIX "bts %1,%0"
ffffffff812cd4e2:	f0 49 0f ab 0e       	lock bts %rcx,(%r14)
				a++;
ffffffff812cd4e7:	ff c7                	inc    %edi
		if (!(a <= b))
			return -EINVAL;
		if (b >= nmaskbits)
			return -ERANGE;
		if (!at_start) {
			while (a <= b) {
ffffffff812cd4e9:	39 fe                	cmp    %edi,%esi
ffffffff812cd4eb:	73 f3                	jae    ffffffff812cd4e0 <__bitmap_parselist+0xfa>
ffffffff812cd4ed:	eb df                	jmp    ffffffff812cd4ce <__bitmap_parselist+0xe8>
		/* Get the next cpu# or a range of cpu#'s */
		while (buflen) {
			old_c = c;
			if (is_user) {
				if (__get_user(c, ubuf++))
					return -EFAULT;
ffffffff812cd4ef:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
ffffffff812cd4f4:	eb 0c                	jmp    ffffffff812cd502 <__bitmap_parselist+0x11c>
			 * If the last character was a space and the current
			 * character isn't '\0', we've got embedded whitespace.
			 * This is a no-no, so throw an error.
			 */
			if (totaldigits && c && isspace(old_c))
				return -EINVAL;
ffffffff812cd4f6:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
ffffffff812cd4fb:	eb 05                	jmp    ffffffff812cd502 <__bitmap_parselist+0x11c>
			totaldigits++;
		}
		if (!(a <= b))
			return -EINVAL;
		if (b >= nmaskbits)
			return -ERANGE;
ffffffff812cd4fd:	b8 de ff ff ff       	mov    $0xffffffde,%eax
				a++;
			}
		}
	} while (buflen && c == ',');
	return 0;
}
ffffffff812cd502:	5a                   	pop    %rdx
ffffffff812cd503:	5b                   	pop    %rbx
ffffffff812cd504:	41 5c                	pop    %r12
ffffffff812cd506:	41 5d                	pop    %r13
ffffffff812cd508:	41 5e                	pop    %r14
ffffffff812cd50a:	41 5f                	pop    %r15
ffffffff812cd50c:	5d                   	pop    %rbp
ffffffff812cd50d:	c3                   	retq   

ffffffff812cd50e <bitmap_weight>:

	return find_first_zero_bit(src, nbits) == nbits;
}

static inline int bitmap_weight(const unsigned long *src, unsigned int nbits)
{
ffffffff812cd50e:	55                   	push   %rbp
ffffffff812cd50f:	48 89 e5             	mov    %rsp,%rbp
	if (small_const_nbits(nbits))
		return hweight_long(*src & BITMAP_LAST_WORD_MASK(nbits));
	return __bitmap_weight(src, nbits);
ffffffff812cd512:	e8 6d fe ff ff       	callq  ffffffff812cd384 <__bitmap_weight>
}
ffffffff812cd517:	5d                   	pop    %rbp
ffffffff812cd518:	c3                   	retq   

ffffffff812cd519 <bitmap_pos_to_ord>:
 * The bit positions 0 through @bits are valid positions in @buf.
 */
static int bitmap_pos_to_ord(const unsigned long *buf, unsigned int pos, unsigned int nbits)
{
	if (pos >= nbits || !test_bit(pos, buf))
		return -1;
ffffffff812cd519:	83 c8 ff             	or     $0xffffffff,%eax
 *
 * The bit positions 0 through @bits are valid positions in @buf.
 */
static int bitmap_pos_to_ord(const unsigned long *buf, unsigned int pos, unsigned int nbits)
{
	if (pos >= nbits || !test_bit(pos, buf))
ffffffff812cd51c:	39 d6                	cmp    %edx,%esi
ffffffff812cd51e:	73 16                	jae    ffffffff812cd536 <bitmap_pos_to_ord+0x1d>

static inline int variable_test_bit(long nr, volatile const unsigned long *addr)
{
	int oldbit;

	asm volatile("bt %2,%1\n\t"
ffffffff812cd520:	89 f2                	mov    %esi,%edx
ffffffff812cd522:	48 0f a3 17          	bt     %rdx,(%rdi)
ffffffff812cd526:	19 d2                	sbb    %edx,%edx
ffffffff812cd528:	85 d2                	test   %edx,%edx
ffffffff812cd52a:	74 0a                	je     ffffffff812cd536 <bitmap_pos_to_ord+0x1d>
 * that bit 7 is the 3rd (starting with 0th) set bit in @buf.
 *
 * The bit positions 0 through @bits are valid positions in @buf.
 */
static int bitmap_pos_to_ord(const unsigned long *buf, unsigned int pos, unsigned int nbits)
{
ffffffff812cd52c:	55                   	push   %rbp
ffffffff812cd52d:	48 89 e5             	mov    %rsp,%rbp
	if (pos >= nbits || !test_bit(pos, buf))
		return -1;

	return __bitmap_weight(buf, pos);
ffffffff812cd530:	e8 4f fe ff ff       	callq  ffffffff812cd384 <__bitmap_weight>
}
ffffffff812cd535:	5d                   	pop    %rbp
ffffffff812cd536:	c3                   	retq   

ffffffff812cd537 <bitmap_find_next_zero_area_off>:
					     unsigned long size,
					     unsigned long start,
					     unsigned int nr,
					     unsigned long align_mask,
					     unsigned long align_offset)
{
ffffffff812cd537:	55                   	push   %rbp
	unsigned long index, end, i;
again:
	index = find_next_zero_bit(map, size, start);

	/* Align allocation */
	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;
ffffffff812cd538:	4b 8d 04 01          	lea    (%r9,%r8,1),%rax
					     unsigned long size,
					     unsigned long start,
					     unsigned int nr,
					     unsigned long align_mask,
					     unsigned long align_offset)
{
ffffffff812cd53c:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd53f:	41 57                	push   %r15
ffffffff812cd541:	41 56                	push   %r14
ffffffff812cd543:	41 55                	push   %r13
ffffffff812cd545:	41 54                	push   %r12
ffffffff812cd547:	4d 89 c5             	mov    %r8,%r13
ffffffff812cd54a:	53                   	push   %rbx
ffffffff812cd54b:	49 89 f4             	mov    %rsi,%r12
ffffffff812cd54e:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cd551:	48 83 ec 28          	sub    $0x28,%rsp
ffffffff812cd555:	89 4d cc             	mov    %ecx,-0x34(%rbp)
	unsigned long index, end, i;
again:
	index = find_next_zero_bit(map, size, start);

	/* Align allocation */
	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;
ffffffff812cd558:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
					     unsigned long align_mask,
					     unsigned long align_offset)
{
	unsigned long index, end, i;
again:
	index = find_next_zero_bit(map, size, start);
ffffffff812cd55c:	4c 89 e6             	mov    %r12,%rsi
ffffffff812cd55f:	48 89 df             	mov    %rbx,%rdi
ffffffff812cd562:	4c 89 4d b8          	mov    %r9,-0x48(%rbp)
ffffffff812cd566:	e8 0a 38 00 00       	callq  ffffffff812d0d75 <find_next_zero_bit>

	/* Align allocation */
	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;
ffffffff812cd56b:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
ffffffff812cd56f:	4c 89 ea             	mov    %r13,%rdx
ffffffff812cd572:	4c 8b 4d b8          	mov    -0x48(%rbp),%r9
ffffffff812cd576:	48 f7 d2             	not    %rdx
ffffffff812cd579:	48 8d 0c 06          	lea    (%rsi,%rax,1),%rcx

	end = index + nr;
ffffffff812cd57d:	8b 45 cc             	mov    -0x34(%rbp),%eax
	unsigned long index, end, i;
again:
	index = find_next_zero_bit(map, size, start);

	/* Align allocation */
	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;
ffffffff812cd580:	48 21 d1             	and    %rdx,%rcx
ffffffff812cd583:	4c 29 c9             	sub    %r9,%rcx

	end = index + nr;
ffffffff812cd586:	4c 8d 3c 08          	lea    (%rax,%rcx,1),%r15
	unsigned long index, end, i;
again:
	index = find_next_zero_bit(map, size, start);

	/* Align allocation */
	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;
ffffffff812cd58a:	49 89 ce             	mov    %rcx,%r14

	end = index + nr;
	if (end > size)
ffffffff812cd58d:	4d 39 fc             	cmp    %r15,%r12
ffffffff812cd590:	72 1d                	jb     ffffffff812cd5af <bitmap_find_next_zero_area_off+0x78>
		return end;
	i = find_next_bit(map, end, index);
ffffffff812cd592:	48 89 ca             	mov    %rcx,%rdx
ffffffff812cd595:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cd598:	48 89 df             	mov    %rbx,%rdi
ffffffff812cd59b:	e8 78 37 00 00       	callq  ffffffff812d0d18 <find_next_bit>
	if (i < end) {
ffffffff812cd5a0:	49 39 c7             	cmp    %rax,%r15
ffffffff812cd5a3:	76 0f                	jbe    ffffffff812cd5b4 <bitmap_find_next_zero_area_off+0x7d>
		start = i + 1;
ffffffff812cd5a5:	48 8d 50 01          	lea    0x1(%rax),%rdx
		goto again;
ffffffff812cd5a9:	4c 8b 4d b8          	mov    -0x48(%rbp),%r9
ffffffff812cd5ad:	eb ad                	jmp    ffffffff812cd55c <bitmap_find_next_zero_area_off+0x25>
	index = find_next_zero_bit(map, size, start);

	/* Align allocation */
	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;

	end = index + nr;
ffffffff812cd5af:	4c 89 f8             	mov    %r15,%rax
ffffffff812cd5b2:	eb 03                	jmp    ffffffff812cd5b7 <bitmap_find_next_zero_area_off+0x80>
	unsigned long index, end, i;
again:
	index = find_next_zero_bit(map, size, start);

	/* Align allocation */
	index = __ALIGN_MASK(index + align_offset, align_mask) - align_offset;
ffffffff812cd5b4:	4c 89 f0             	mov    %r14,%rax
	if (i < end) {
		start = i + 1;
		goto again;
	}
	return index;
}
ffffffff812cd5b7:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812cd5bb:	5b                   	pop    %rbx
ffffffff812cd5bc:	41 5c                	pop    %r12
ffffffff812cd5be:	41 5d                	pop    %r13
ffffffff812cd5c0:	41 5e                	pop    %r14
ffffffff812cd5c2:	41 5f                	pop    %r15
ffffffff812cd5c4:	5d                   	pop    %rbp
ffffffff812cd5c5:	c3                   	retq   

ffffffff812cd5c6 <__bitmap_parse>:
 * Leading and trailing whitespace accepted, but not embedded whitespace.
 */
int __bitmap_parse(const char *buf, unsigned int buflen,
		int is_user, unsigned long *maskp,
		int nmaskbits)
{
ffffffff812cd5c6:	55                   	push   %rbp
ffffffff812cd5c7:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd5ca:	41 57                	push   %r15
ffffffff812cd5cc:	41 56                	push   %r14
ffffffff812cd5ce:	41 55                	push   %r13
ffffffff812cd5d0:	41 54                	push   %r12
ffffffff812cd5d2:	41 89 f5             	mov    %esi,%r13d
ffffffff812cd5d5:	53                   	push   %rbx
ffffffff812cd5d6:	49 89 fc             	mov    %rdi,%r12
	int c, old_c, totaldigits, ndigits, nchunks, nbits;
	u32 chunk;
	const char __user __force *ubuf = (const char __user __force *)buf;

	bitmap_zero(maskp, nmaskbits);
ffffffff812cd5d9:	44 89 c6             	mov    %r8d,%esi
ffffffff812cd5dc:	48 89 cf             	mov    %rcx,%rdi
 * Leading and trailing whitespace accepted, but not embedded whitespace.
 */
int __bitmap_parse(const char *buf, unsigned int buflen,
		int is_user, unsigned long *maskp,
		int nmaskbits)
{
ffffffff812cd5df:	48 89 cb             	mov    %rcx,%rbx
	u32 chunk;
	const char __user __force *ubuf = (const char __user __force *)buf;

	bitmap_zero(maskp, nmaskbits);

	nchunks = nbits = totaldigits = c = 0;
ffffffff812cd5e2:	45 31 f6             	xor    %r14d,%r14d
 * Leading and trailing whitespace accepted, but not embedded whitespace.
 */
int __bitmap_parse(const char *buf, unsigned int buflen,
		int is_user, unsigned long *maskp,
		int nmaskbits)
{
ffffffff812cd5e5:	48 83 ec 38          	sub    $0x38,%rsp
ffffffff812cd5e9:	89 55 c0             	mov    %edx,-0x40(%rbp)
ffffffff812cd5ec:	44 89 45 cc          	mov    %r8d,-0x34(%rbp)
	int c, old_c, totaldigits, ndigits, nchunks, nbits;
	u32 chunk;
	const char __user __force *ubuf = (const char __user __force *)buf;

	bitmap_zero(maskp, nmaskbits);
ffffffff812cd5f0:	e8 d9 fd ff ff       	callq  ffffffff812cd3ce <bitmap_zero>
		int is_user, unsigned long *maskp,
		int nmaskbits)
{
	int c, old_c, totaldigits, ndigits, nchunks, nbits;
	u32 chunk;
	const char __user __force *ubuf = (const char __user __force *)buf;
ffffffff812cd5f5:	4d 89 e2             	mov    %r12,%r10

	bitmap_zero(maskp, nmaskbits);

	nchunks = nbits = totaldigits = c = 0;
ffffffff812cd5f8:	c7 45 c4 00 00 00 00 	movl   $0x0,-0x3c(%rbp)
ffffffff812cd5ff:	45 31 c9             	xor    %r9d,%r9d
ffffffff812cd602:	c7 45 c8 00 00 00 00 	movl   $0x0,-0x38(%rbp)
	do {
		chunk = ndigits = 0;

		/* Get the next chunk of the bitmap */
		while (buflen) {
ffffffff812cd609:	44 89 f0             	mov    %r14d,%eax

	bitmap_zero(maskp, nmaskbits);

	nchunks = nbits = totaldigits = c = 0;
	do {
		chunk = ndigits = 0;
ffffffff812cd60c:	45 31 ff             	xor    %r15d,%r15d
ffffffff812cd60f:	31 c9                	xor    %ecx,%ecx

		/* Get the next chunk of the bitmap */
		while (buflen) {
ffffffff812cd611:	45 85 ed             	test   %r13d,%r13d
ffffffff812cd614:	0f 84 b4 00 00 00    	je     ffffffff812cd6ce <__bitmap_parse+0x108>
			old_c = c;
			if (is_user) {
ffffffff812cd61a:	83 7d c0 00          	cmpl   $0x0,-0x40(%rbp)
ffffffff812cd61e:	74 20                	je     ffffffff812cd640 <__bitmap_parse+0x7a>
				if (__get_user(c, ubuf++))
ffffffff812cd620:	49 8d 72 01          	lea    0x1(%r10),%rsi
ffffffff812cd624:	31 d2                	xor    %edx,%edx
ffffffff812cd626:	90                   	nop
ffffffff812cd627:	90                   	nop
ffffffff812cd628:	90                   	nop
ffffffff812cd629:	45 8a 02             	mov    (%r10),%r8b
ffffffff812cd62c:	90                   	nop
ffffffff812cd62d:	90                   	nop
ffffffff812cd62e:	90                   	nop
ffffffff812cd62f:	85 d2                	test   %edx,%edx
ffffffff812cd631:	45 0f be f0          	movsbl %r8b,%r14d
ffffffff812cd635:	0f 85 07 01 00 00    	jne    ffffffff812cd742 <__bitmap_parse+0x17c>
ffffffff812cd63b:	49 89 f2             	mov    %rsi,%r10
ffffffff812cd63e:	eb 08                	jmp    ffffffff812cd648 <__bitmap_parse+0x82>
					return -EFAULT;
			}
			else
				c = *buf++;
ffffffff812cd640:	45 0f be 34 24       	movsbl (%r12),%r14d
ffffffff812cd645:	49 ff c4             	inc    %r12
			buflen--;
			if (isspace(c))
ffffffff812cd648:	41 0f b6 d6          	movzbl %r14b,%edx
				if (__get_user(c, ubuf++))
					return -EFAULT;
			}
			else
				c = *buf++;
			buflen--;
ffffffff812cd64c:	41 ff cd             	dec    %r13d
			if (isspace(c))
ffffffff812cd64f:	8a 92 60 ef 63 81    	mov    -0x7e9c10a0(%rdx),%dl
ffffffff812cd655:	f6 c2 20             	test   $0x20,%dl
ffffffff812cd658:	75 6c                	jne    ffffffff812cd6c6 <__bitmap_parse+0x100>
			/*
			 * If the last character was a space and the current
			 * character isn't '\0', we've got embedded whitespace.
			 * This is a no-no, so throw an error.
			 */
			if (totaldigits && c && isspace(old_c))
ffffffff812cd65a:	83 7d c8 00          	cmpl   $0x0,-0x38(%rbp)
ffffffff812cd65e:	74 15                	je     ffffffff812cd675 <__bitmap_parse+0xaf>
ffffffff812cd660:	45 85 f6             	test   %r14d,%r14d
ffffffff812cd663:	74 10                	je     ffffffff812cd675 <__bitmap_parse+0xaf>
ffffffff812cd665:	0f b6 c0             	movzbl %al,%eax
ffffffff812cd668:	f6 80 60 ef 63 81 20 	testb  $0x20,-0x7e9c10a0(%rax)
ffffffff812cd66f:	0f 85 d4 00 00 00    	jne    ffffffff812cd749 <__bitmap_parse+0x183>
				return -EINVAL;

			/* A '\0' or a ',' signal the end of the chunk */
			if (c == '\0' || c == ',')
ffffffff812cd675:	45 85 f6             	test   %r14d,%r14d
ffffffff812cd678:	74 54                	je     ffffffff812cd6ce <__bitmap_parse+0x108>
ffffffff812cd67a:	41 83 fe 2c          	cmp    $0x2c,%r14d
ffffffff812cd67e:	74 4e                	je     ffffffff812cd6ce <__bitmap_parse+0x108>
				break;

			if (!isxdigit(c))
ffffffff812cd680:	80 e2 44             	and    $0x44,%dl
ffffffff812cd683:	0f 84 c0 00 00 00    	je     ffffffff812cd749 <__bitmap_parse+0x183>
			/*
			 * Make sure there are at least 4 free bits in 'chunk'.
			 * If not, this hexdigit will overflow 'chunk', so
			 * throw an error.
			 */
			if (chunk & ~((1UL << (CHUNKSZ - 4)) - 1))
ffffffff812cd689:	41 f7 c7 00 00 00 f0 	test   $0xf0000000,%r15d
ffffffff812cd690:	74 0a                	je     ffffffff812cd69c <__bitmap_parse+0xd6>
				return -EOVERFLOW;
ffffffff812cd692:	b8 b5 ff ff ff       	mov    $0xffffffb5,%eax
ffffffff812cd697:	e9 b2 00 00 00       	jmpq   ffffffff812cd74e <__bitmap_parse+0x188>

			chunk = (chunk << 4) | hex_to_bin(c);
ffffffff812cd69c:	44 89 f7             	mov    %r14d,%edi
ffffffff812cd69f:	4c 89 55 a8          	mov    %r10,-0x58(%rbp)
ffffffff812cd6a3:	44 89 4d b0          	mov    %r9d,-0x50(%rbp)
ffffffff812cd6a7:	89 4d bc             	mov    %ecx,-0x44(%rbp)
ffffffff812cd6aa:	41 c1 e7 04          	shl    $0x4,%r15d
ffffffff812cd6ae:	e8 17 60 00 00       	callq  ffffffff812d36ca <hex_to_bin>
			ndigits++; totaldigits++;
ffffffff812cd6b3:	8b 4d bc             	mov    -0x44(%rbp),%ecx
ffffffff812cd6b6:	ff 45 c8             	incl   -0x38(%rbp)
			 * throw an error.
			 */
			if (chunk & ~((1UL << (CHUNKSZ - 4)) - 1))
				return -EOVERFLOW;

			chunk = (chunk << 4) | hex_to_bin(c);
ffffffff812cd6b9:	41 09 c7             	or     %eax,%r15d
			ndigits++; totaldigits++;
ffffffff812cd6bc:	4c 8b 55 a8          	mov    -0x58(%rbp),%r10
ffffffff812cd6c0:	44 8b 4d b0          	mov    -0x50(%rbp),%r9d
ffffffff812cd6c4:	ff c1                	inc    %ecx
			if (is_user) {
				if (__get_user(c, ubuf++))
					return -EFAULT;
			}
			else
				c = *buf++;
ffffffff812cd6c6:	44 89 f0             	mov    %r14d,%eax
ffffffff812cd6c9:	e9 43 ff ff ff       	jmpq   ffffffff812cd611 <__bitmap_parse+0x4b>
				return -EOVERFLOW;

			chunk = (chunk << 4) | hex_to_bin(c);
			ndigits++; totaldigits++;
		}
		if (ndigits == 0)
ffffffff812cd6ce:	85 c9                	test   %ecx,%ecx
ffffffff812cd6d0:	74 77                	je     ffffffff812cd749 <__bitmap_parse+0x183>
			return -EINVAL;
		if (nchunks == 0 && chunk == 0)
ffffffff812cd6d2:	45 85 c9             	test   %r9d,%r9d
ffffffff812cd6d5:	75 05                	jne    ffffffff812cd6dc <__bitmap_parse+0x116>
ffffffff812cd6d7:	45 85 ff             	test   %r15d,%r15d
ffffffff812cd6da:	74 50                	je     ffffffff812cd72c <__bitmap_parse+0x166>
			continue;

		__bitmap_shift_left(maskp, maskp, CHUNKSZ, nmaskbits);
ffffffff812cd6dc:	8b 4d cc             	mov    -0x34(%rbp),%ecx
ffffffff812cd6df:	ba 20 00 00 00       	mov    $0x20,%edx
ffffffff812cd6e4:	48 89 de             	mov    %rbx,%rsi
ffffffff812cd6e7:	48 89 df             	mov    %rbx,%rdi
ffffffff812cd6ea:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
ffffffff812cd6ee:	44 89 4d bc          	mov    %r9d,-0x44(%rbp)
ffffffff812cd6f2:	e8 0f fc ff ff       	callq  ffffffff812cd306 <__bitmap_shift_left>
		*maskp |= chunk;
		nchunks++;
ffffffff812cd6f7:	44 8b 4d bc          	mov    -0x44(%rbp),%r9d
			return -EINVAL;
		if (nchunks == 0 && chunk == 0)
			continue;

		__bitmap_shift_left(maskp, maskp, CHUNKSZ, nmaskbits);
		*maskp |= chunk;
ffffffff812cd6fb:	44 89 f8             	mov    %r15d,%eax
ffffffff812cd6fe:	48 09 03             	or     %rax,(%rbx)
		nchunks++;
		nbits += (nchunks == 1) ? nbits_to_hold_value(chunk) : CHUNKSZ;
ffffffff812cd701:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812cd705:	b8 20 00 00 00       	mov    $0x20,%eax
		if (nchunks == 0 && chunk == 0)
			continue;

		__bitmap_shift_left(maskp, maskp, CHUNKSZ, nmaskbits);
		*maskp |= chunk;
		nchunks++;
ffffffff812cd70a:	41 ff c1             	inc    %r9d
		nbits += (nchunks == 1) ? nbits_to_hold_value(chunk) : CHUNKSZ;
ffffffff812cd70d:	41 83 f9 01          	cmp    $0x1,%r9d
ffffffff812cd711:	75 09                	jne    ffffffff812cd71c <__bitmap_parse+0x156>
	 * top 32 bits will be cleared.
	 *
	 * We cannot do this on 32 bits because at the very least some
	 * 486 CPUs did not behave this way.
	 */
	asm("bsrl %1,%0"
ffffffff812cd713:	83 c8 ff             	or     $0xffffffff,%eax
ffffffff812cd716:	41 0f bd c7          	bsr    %r15d,%eax
	asm("bsrl %1,%0\n\t"
	    "jnz 1f\n\t"
	    "movl $-1,%0\n"
	    "1:" : "=r" (r) : "rm" (x));
#endif
	return r + 1;
ffffffff812cd71a:	ff c0                	inc    %eax
ffffffff812cd71c:	01 45 c4             	add    %eax,-0x3c(%rbp)
		if (nbits > nmaskbits)
ffffffff812cd71f:	8b 4d c4             	mov    -0x3c(%rbp),%ecx
ffffffff812cd722:	39 4d cc             	cmp    %ecx,-0x34(%rbp)
ffffffff812cd725:	7d 08                	jge    ffffffff812cd72f <__bitmap_parse+0x169>
ffffffff812cd727:	e9 66 ff ff ff       	jmpq   ffffffff812cd692 <__bitmap_parse+0xcc>
ffffffff812cd72c:	45 31 c9             	xor    %r9d,%r9d
			return -EOVERFLOW;
	} while (buflen && c == ',');
ffffffff812cd72f:	45 85 ed             	test   %r13d,%r13d
ffffffff812cd732:	74 0a                	je     ffffffff812cd73e <__bitmap_parse+0x178>
ffffffff812cd734:	41 83 fe 2c          	cmp    $0x2c,%r14d
ffffffff812cd738:	0f 84 cb fe ff ff    	je     ffffffff812cd609 <__bitmap_parse+0x43>

	return 0;
ffffffff812cd73e:	31 c0                	xor    %eax,%eax
ffffffff812cd740:	eb 0c                	jmp    ffffffff812cd74e <__bitmap_parse+0x188>
		/* Get the next chunk of the bitmap */
		while (buflen) {
			old_c = c;
			if (is_user) {
				if (__get_user(c, ubuf++))
					return -EFAULT;
ffffffff812cd742:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
ffffffff812cd747:	eb 05                	jmp    ffffffff812cd74e <__bitmap_parse+0x188>
			 * If the last character was a space and the current
			 * character isn't '\0', we've got embedded whitespace.
			 * This is a no-no, so throw an error.
			 */
			if (totaldigits && c && isspace(old_c))
				return -EINVAL;
ffffffff812cd749:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
		if (nbits > nmaskbits)
			return -EOVERFLOW;
	} while (buflen && c == ',');

	return 0;
}
ffffffff812cd74e:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812cd752:	5b                   	pop    %rbx
ffffffff812cd753:	41 5c                	pop    %r12
ffffffff812cd755:	41 5d                	pop    %r13
ffffffff812cd757:	41 5e                	pop    %r14
ffffffff812cd759:	41 5f                	pop    %r15
ffffffff812cd75b:	5d                   	pop    %rbp
ffffffff812cd75c:	c3                   	retq   

ffffffff812cd75d <bitmap_parse_user>:
 */
int bitmap_parse_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
	if (!access_ok(VERIFY_READ, ubuf, ulen))
ffffffff812cd75d:	41 89 f1             	mov    %esi,%r9d
		return -EFAULT;
ffffffff812cd760:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
ffffffff812cd765:	65 4c 8b 04 25 04 22 	mov    %gs:0x12204,%r8
ffffffff812cd76c:	01 00 
ffffffff812cd76e:	49 01 f9             	add    %rdi,%r9
ffffffff812cd771:	72 1e                	jb     ffffffff812cd791 <bitmap_parse_user+0x34>
 */
int bitmap_parse_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
	if (!access_ok(VERIFY_READ, ubuf, ulen))
ffffffff812cd773:	4d 39 88 18 c0 ff ff 	cmp    %r9,-0x3fe8(%r8)
ffffffff812cd77a:	72 15                	jb     ffffffff812cd791 <bitmap_parse_user+0x34>
 * cyclic dependencies.
 */
int bitmap_parse_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
ffffffff812cd77c:	55                   	push   %rbp
ffffffff812cd77d:	41 89 c8             	mov    %ecx,%r8d
ffffffff812cd780:	48 89 d1             	mov    %rdx,%rcx
	if (!access_ok(VERIFY_READ, ubuf, ulen))
		return -EFAULT;
	return __bitmap_parse((const char __force *)ubuf,
ffffffff812cd783:	ba 01 00 00 00       	mov    $0x1,%edx
 * cyclic dependencies.
 */
int bitmap_parse_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
ffffffff812cd788:	48 89 e5             	mov    %rsp,%rbp
	if (!access_ok(VERIFY_READ, ubuf, ulen))
		return -EFAULT;
	return __bitmap_parse((const char __force *)ubuf,
ffffffff812cd78b:	e8 36 fe ff ff       	callq  ffffffff812cd5c6 <__bitmap_parse>
				ulen, 1, maskp, nmaskbits);

}
ffffffff812cd790:	5d                   	pop    %rbp
ffffffff812cd791:	c3                   	retq   

ffffffff812cd792 <bitmap_print_to_pagebuf>:
 * sets of 8 digits/set. Returns the number of characters written to buf.
 */
int bitmap_print_to_pagebuf(bool list, char *buf, const unsigned long *maskp,
			    int nmaskbits)
{
	ptrdiff_t len = PTR_ALIGN(buf + PAGE_SIZE - 1, PAGE_SIZE) - buf - 2;
ffffffff812cd792:	48 8d 86 fe 1f 00 00 	lea    0x1ffe(%rsi),%rax
 * ranges if list is specified or hex digits grouped into comma-separated
 * sets of 8 digits/set. Returns the number of characters written to buf.
 */
int bitmap_print_to_pagebuf(bool list, char *buf, const unsigned long *maskp,
			    int nmaskbits)
{
ffffffff812cd799:	55                   	push   %rbp
	ptrdiff_t len = PTR_ALIGN(buf + PAGE_SIZE - 1, PAGE_SIZE) - buf - 2;
ffffffff812cd79a:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
 * ranges if list is specified or hex digits grouped into comma-separated
 * sets of 8 digits/set. Returns the number of characters written to buf.
 */
int bitmap_print_to_pagebuf(bool list, char *buf, const unsigned long *maskp,
			    int nmaskbits)
{
ffffffff812cd7a0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd7a3:	53                   	push   %rbx
	ptrdiff_t len = PTR_ALIGN(buf + PAGE_SIZE - 1, PAGE_SIZE) - buf - 2;
ffffffff812cd7a4:	48 29 f0             	sub    %rsi,%rax
 * ranges if list is specified or hex digits grouped into comma-separated
 * sets of 8 digits/set. Returns the number of characters written to buf.
 */
int bitmap_print_to_pagebuf(bool list, char *buf, const unsigned long *maskp,
			    int nmaskbits)
{
ffffffff812cd7a7:	48 89 f3             	mov    %rsi,%rbx
ffffffff812cd7aa:	41 50                	push   %r8
	ptrdiff_t len = PTR_ALIGN(buf + PAGE_SIZE - 1, PAGE_SIZE) - buf - 2;
ffffffff812cd7ac:	48 8d 70 fe          	lea    -0x2(%rax),%rsi
ffffffff812cd7b0:	45 31 c0             	xor    %r8d,%r8d
	int n = 0;

	if (len > 1) {
ffffffff812cd7b3:	48 83 fe 01          	cmp    $0x1,%rsi
ffffffff812cd7b7:	7e 32                	jle    ffffffff812cd7eb <bitmap_print_to_pagebuf+0x59>
		n = list ? scnprintf(buf, len, "%*pbl", nmaskbits, maskp) :
ffffffff812cd7b9:	40 84 ff             	test   %dil,%dil
ffffffff812cd7bc:	49 89 d0             	mov    %rdx,%r8
ffffffff812cd7bf:	48 c7 c2 c4 3f 79 81 	mov    $0xffffffff81793fc4,%rdx
ffffffff812cd7c6:	75 07                	jne    ffffffff812cd7cf <bitmap_print_to_pagebuf+0x3d>
ffffffff812cd7c8:	48 c7 c2 12 6e 7b 81 	mov    $0xffffffff817b6e12,%rdx
ffffffff812cd7cf:	48 89 df             	mov    %rbx,%rdi
ffffffff812cd7d2:	31 c0                	xor    %eax,%eax
ffffffff812cd7d4:	e8 ba cf ff ff       	callq  ffffffff812ca793 <scnprintf>
			   scnprintf(buf, len, "%*pb", nmaskbits, maskp);
		buf[n++] = '\n';
ffffffff812cd7d9:	44 8d 40 01          	lea    0x1(%rax),%r8d
ffffffff812cd7dd:	48 63 d0             	movslq %eax,%rdx
ffffffff812cd7e0:	c6 04 13 0a          	movb   $0xa,(%rbx,%rdx,1)
		buf[n] = '\0';
ffffffff812cd7e4:	49 63 c0             	movslq %r8d,%rax
ffffffff812cd7e7:	c6 04 03 00          	movb   $0x0,(%rbx,%rax,1)
	}
	return n;
}
ffffffff812cd7eb:	5a                   	pop    %rdx
ffffffff812cd7ec:	44 89 c0             	mov    %r8d,%eax
ffffffff812cd7ef:	5b                   	pop    %rbx
ffffffff812cd7f0:	5d                   	pop    %rbp
ffffffff812cd7f1:	c3                   	retq   

ffffffff812cd7f2 <bitmap_parselist>:
	} while (buflen && c == ',');
	return 0;
}

int bitmap_parselist(const char *bp, unsigned long *maskp, int nmaskbits)
{
ffffffff812cd7f2:	55                   	push   %rbp
ffffffff812cd7f3:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd7f6:	41 54                	push   %r12
ffffffff812cd7f8:	53                   	push   %rbx
ffffffff812cd7f9:	49 89 f4             	mov    %rsi,%r12
	char *nl  = strchrnul(bp, '\n');
ffffffff812cd7fc:	be 0a 00 00 00       	mov    $0xa,%esi
	} while (buflen && c == ',');
	return 0;
}

int bitmap_parselist(const char *bp, unsigned long *maskp, int nmaskbits)
{
ffffffff812cd801:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cd804:	48 83 ec 10          	sub    $0x10,%rsp
ffffffff812cd808:	89 55 ec             	mov    %edx,-0x14(%rbp)
	char *nl  = strchrnul(bp, '\n');
ffffffff812cd80b:	e8 aa a4 ff ff       	callq  ffffffff812c7cba <strchrnul>
	int len = nl - bp;

	return __bitmap_parselist(bp, len, 0, maskp, nmaskbits);
ffffffff812cd810:	44 8b 45 ec          	mov    -0x14(%rbp),%r8d
ffffffff812cd814:	48 29 d8             	sub    %rbx,%rax
ffffffff812cd817:	4c 89 e1             	mov    %r12,%rcx
ffffffff812cd81a:	31 d2                	xor    %edx,%edx
ffffffff812cd81c:	48 89 df             	mov    %rbx,%rdi
ffffffff812cd81f:	48 89 c6             	mov    %rax,%rsi
ffffffff812cd822:	e8 bf fb ff ff       	callq  ffffffff812cd3e6 <__bitmap_parselist>
}
ffffffff812cd827:	5a                   	pop    %rdx
ffffffff812cd828:	59                   	pop    %rcx
ffffffff812cd829:	5b                   	pop    %rbx
ffffffff812cd82a:	41 5c                	pop    %r12
ffffffff812cd82c:	5d                   	pop    %rbp
ffffffff812cd82d:	c3                   	retq   

ffffffff812cd82e <bitmap_onto>:
void bitmap_onto(unsigned long *dst, const unsigned long *orig,
			const unsigned long *relmap, unsigned int bits)
{
	unsigned int n, m;	/* same meaning as in above comment */

	if (dst == orig)	/* following doesn't handle inplace mappings */
ffffffff812cd82e:	48 39 f7             	cmp    %rsi,%rdi
ffffffff812cd831:	74 74                	je     ffffffff812cd8a7 <bitmap_onto+0x79>
 *
 * All bits in @dst not set by the above rule are cleared.
 */
void bitmap_onto(unsigned long *dst, const unsigned long *orig,
			const unsigned long *relmap, unsigned int bits)
{
ffffffff812cd833:	55                   	push   %rbp
ffffffff812cd834:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd837:	41 57                	push   %r15
ffffffff812cd839:	41 56                	push   %r14
ffffffff812cd83b:	41 55                	push   %r13
ffffffff812cd83d:	41 54                	push   %r12
ffffffff812cd83f:	41 89 cf             	mov    %ecx,%r15d
ffffffff812cd842:	53                   	push   %rbx
ffffffff812cd843:	49 89 f5             	mov    %rsi,%r13
	unsigned int n, m;	/* same meaning as in above comment */

	if (dst == orig)	/* following doesn't handle inplace mappings */
		return;
	bitmap_zero(dst, bits);
ffffffff812cd846:	44 89 fe             	mov    %r15d,%esi
ffffffff812cd849:	49 89 d6             	mov    %rdx,%r14
ffffffff812cd84c:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cd84f:	4d 89 fc             	mov    %r15,%r12
 *
 * All bits in @dst not set by the above rule are cleared.
 */
void bitmap_onto(unsigned long *dst, const unsigned long *orig,
			const unsigned long *relmap, unsigned int bits)
{
ffffffff812cd852:	48 83 ec 18          	sub    $0x18,%rsp
	unsigned int n, m;	/* same meaning as in above comment */

	if (dst == orig)	/* following doesn't handle inplace mappings */
		return;
	bitmap_zero(dst, bits);
ffffffff812cd856:	e8 73 fb ff ff       	callq  ffffffff812cd3ce <bitmap_zero>
	 *			set_bit(n, dst);
	 *	}
	 */

	m = 0;
	for_each_set_bit(n, relmap, bits) {
ffffffff812cd85b:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cd85e:	48 89 d7             	mov    %rdx,%rdi
ffffffff812cd861:	e8 e6 33 00 00       	callq  ffffffff812d0c4c <find_first_bit>
	 *		if (test_bit(m, orig))
	 *			set_bit(n, dst);
	 *	}
	 */

	m = 0;
ffffffff812cd866:	31 c9                	xor    %ecx,%ecx
	for_each_set_bit(n, relmap, bits) {
ffffffff812cd868:	44 39 e0             	cmp    %r12d,%eax
ffffffff812cd86b:	73 2c                	jae    ffffffff812cd899 <bitmap_onto+0x6b>

static inline int variable_test_bit(long nr, volatile const unsigned long *addr)
{
	int oldbit;

	asm volatile("bt %2,%1\n\t"
ffffffff812cd86d:	89 ca                	mov    %ecx,%edx
ffffffff812cd86f:	49 0f a3 55 00       	bt     %rdx,0x0(%r13)
ffffffff812cd874:	19 d2                	sbb    %edx,%edx
		/* m == bitmap_pos_to_ord(relmap, n, bits) */
		if (test_bit(m, orig))
ffffffff812cd876:	85 d2                	test   %edx,%edx
ffffffff812cd878:	74 07                	je     ffffffff812cd881 <bitmap_onto+0x53>
			set_bit(n, dst);
ffffffff812cd87a:	89 c2                	mov    %eax,%edx
		asm volatile(LOCK_PREFIX "orb %1,%0"
			: CONST_MASK_ADDR(nr, addr)
			: "iq" ((u8)CONST_MASK(nr))
			: "memory");
	} else {
		asm volatile(LOCK_PREFIX "bts %1,%0"
ffffffff812cd87c:	f0 48 0f ab 13       	lock bts %rdx,(%rbx)
	 *			set_bit(n, dst);
	 *	}
	 */

	m = 0;
	for_each_set_bit(n, relmap, bits) {
ffffffff812cd881:	8d 50 01             	lea    0x1(%rax),%edx
		/* m == bitmap_pos_to_ord(relmap, n, bits) */
		if (test_bit(m, orig))
			set_bit(n, dst);
		m++;
ffffffff812cd884:	ff c1                	inc    %ecx
	 *			set_bit(n, dst);
	 *	}
	 */

	m = 0;
	for_each_set_bit(n, relmap, bits) {
ffffffff812cd886:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cd889:	4c 89 f7             	mov    %r14,%rdi
		/* m == bitmap_pos_to_ord(relmap, n, bits) */
		if (test_bit(m, orig))
			set_bit(n, dst);
		m++;
ffffffff812cd88c:	89 4d cc             	mov    %ecx,-0x34(%rbp)
	 *			set_bit(n, dst);
	 *	}
	 */

	m = 0;
	for_each_set_bit(n, relmap, bits) {
ffffffff812cd88f:	e8 84 34 00 00       	callq  ffffffff812d0d18 <find_next_bit>
ffffffff812cd894:	8b 4d cc             	mov    -0x34(%rbp),%ecx
ffffffff812cd897:	eb cf                	jmp    ffffffff812cd868 <bitmap_onto+0x3a>
		/* m == bitmap_pos_to_ord(relmap, n, bits) */
		if (test_bit(m, orig))
			set_bit(n, dst);
		m++;
	}
}
ffffffff812cd899:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812cd89d:	5b                   	pop    %rbx
ffffffff812cd89e:	41 5c                	pop    %r12
ffffffff812cd8a0:	41 5d                	pop    %r13
ffffffff812cd8a2:	41 5e                	pop    %r14
ffffffff812cd8a4:	41 5f                	pop    %r15
ffffffff812cd8a6:	5d                   	pop    %rbp
ffffffff812cd8a7:	c3                   	retq   

ffffffff812cd8a8 <bitmap_fold>:
void bitmap_fold(unsigned long *dst, const unsigned long *orig,
			unsigned int sz, unsigned int nbits)
{
	unsigned int oldbit;

	if (dst == orig)	/* following doesn't handle inplace mappings */
ffffffff812cd8a8:	48 39 f7             	cmp    %rsi,%rdi
ffffffff812cd8ab:	74 61                	je     ffffffff812cd90e <bitmap_fold+0x66>
 * Clear all other bits in @dst.  See further the comment and
 * Example [2] for bitmap_onto() for why and how to use this.
 */
void bitmap_fold(unsigned long *dst, const unsigned long *orig,
			unsigned int sz, unsigned int nbits)
{
ffffffff812cd8ad:	55                   	push   %rbp
ffffffff812cd8ae:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd8b1:	41 57                	push   %r15
ffffffff812cd8b3:	41 89 cf             	mov    %ecx,%r15d
ffffffff812cd8b6:	41 56                	push   %r14
ffffffff812cd8b8:	41 55                	push   %r13
ffffffff812cd8ba:	49 89 f5             	mov    %rsi,%r13
ffffffff812cd8bd:	41 54                	push   %r12
ffffffff812cd8bf:	53                   	push   %rbx
	unsigned int oldbit;

	if (dst == orig)	/* following doesn't handle inplace mappings */
		return;
	bitmap_zero(dst, nbits);
ffffffff812cd8c0:	44 89 fe             	mov    %r15d,%esi
 * Clear all other bits in @dst.  See further the comment and
 * Example [2] for bitmap_onto() for why and how to use this.
 */
void bitmap_fold(unsigned long *dst, const unsigned long *orig,
			unsigned int sz, unsigned int nbits)
{
ffffffff812cd8c3:	41 50                	push   %r8
ffffffff812cd8c5:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cd8c8:	41 89 d6             	mov    %edx,%r14d
ffffffff812cd8cb:	4d 89 fc             	mov    %r15,%r12
	unsigned int oldbit;

	if (dst == orig)	/* following doesn't handle inplace mappings */
		return;
	bitmap_zero(dst, nbits);
ffffffff812cd8ce:	e8 fb fa ff ff       	callq  ffffffff812cd3ce <bitmap_zero>

	for_each_set_bit(oldbit, orig, nbits)
ffffffff812cd8d3:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cd8d6:	4c 89 ef             	mov    %r13,%rdi
ffffffff812cd8d9:	e8 6e 33 00 00       	callq  ffffffff812d0c4c <find_first_bit>
ffffffff812cd8de:	44 39 e0             	cmp    %r12d,%eax
ffffffff812cd8e1:	89 c1                	mov    %eax,%ecx
ffffffff812cd8e3:	73 1e                	jae    ffffffff812cd903 <bitmap_fold+0x5b>
		set_bit(oldbit % sz, dst);
ffffffff812cd8e5:	89 c8                	mov    %ecx,%eax
ffffffff812cd8e7:	31 d2                	xor    %edx,%edx
ffffffff812cd8e9:	41 f7 f6             	div    %r14d
ffffffff812cd8ec:	89 d2                	mov    %edx,%edx
ffffffff812cd8ee:	f0 48 0f ab 13       	lock bts %rdx,(%rbx)

	if (dst == orig)	/* following doesn't handle inplace mappings */
		return;
	bitmap_zero(dst, nbits);

	for_each_set_bit(oldbit, orig, nbits)
ffffffff812cd8f3:	8d 51 01             	lea    0x1(%rcx),%edx
ffffffff812cd8f6:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cd8f9:	4c 89 ef             	mov    %r13,%rdi
ffffffff812cd8fc:	e8 17 34 00 00       	callq  ffffffff812d0d18 <find_next_bit>
ffffffff812cd901:	eb db                	jmp    ffffffff812cd8de <bitmap_fold+0x36>
		set_bit(oldbit % sz, dst);
}
ffffffff812cd903:	58                   	pop    %rax
ffffffff812cd904:	5b                   	pop    %rbx
ffffffff812cd905:	41 5c                	pop    %r12
ffffffff812cd907:	41 5d                	pop    %r13
ffffffff812cd909:	41 5e                	pop    %r14
ffffffff812cd90b:	41 5f                	pop    %r15
ffffffff812cd90d:	5d                   	pop    %rbp
ffffffff812cd90e:	c3                   	retq   

ffffffff812cd90f <bitmap_parselist_user>:
 */
int bitmap_parselist_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
	if (!access_ok(VERIFY_READ, ubuf, ulen))
ffffffff812cd90f:	41 89 f1             	mov    %esi,%r9d
		return -EFAULT;
ffffffff812cd912:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
ffffffff812cd917:	65 4c 8b 04 25 04 22 	mov    %gs:0x12204,%r8
ffffffff812cd91e:	01 00 
ffffffff812cd920:	49 01 f9             	add    %rdi,%r9
ffffffff812cd923:	72 1e                	jb     ffffffff812cd943 <bitmap_parselist_user+0x34>
 */
int bitmap_parselist_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
	if (!access_ok(VERIFY_READ, ubuf, ulen))
ffffffff812cd925:	4d 39 88 18 c0 ff ff 	cmp    %r9,-0x3fe8(%r8)
ffffffff812cd92c:	72 15                	jb     ffffffff812cd943 <bitmap_parselist_user+0x34>
 * cyclic dependencies.
 */
int bitmap_parselist_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
ffffffff812cd92e:	55                   	push   %rbp
ffffffff812cd92f:	41 89 c8             	mov    %ecx,%r8d
ffffffff812cd932:	48 89 d1             	mov    %rdx,%rcx
	if (!access_ok(VERIFY_READ, ubuf, ulen))
		return -EFAULT;
	return __bitmap_parselist((const char __force *)ubuf,
ffffffff812cd935:	ba 01 00 00 00       	mov    $0x1,%edx
 * cyclic dependencies.
 */
int bitmap_parselist_user(const char __user *ubuf,
			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
ffffffff812cd93a:	48 89 e5             	mov    %rsp,%rbp
	if (!access_ok(VERIFY_READ, ubuf, ulen))
		return -EFAULT;
	return __bitmap_parselist((const char __force *)ubuf,
ffffffff812cd93d:	e8 a4 fa ff ff       	callq  ffffffff812cd3e6 <__bitmap_parselist>
					ulen, 1, maskp, nmaskbits);
}
ffffffff812cd942:	5d                   	pop    %rbp
ffffffff812cd943:	c3                   	retq   

ffffffff812cd944 <bitmap_ord_to_pos>:
 * that the 3rd set bit (starting with 0th) is at position 7 in @buf.
 *
 * The bit positions 0 through @nbits-1 are valid positions in @buf.
 */
unsigned int bitmap_ord_to_pos(const unsigned long *buf, unsigned int ord, unsigned int nbits)
{
ffffffff812cd944:	55                   	push   %rbp
ffffffff812cd945:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd948:	41 56                	push   %r14
ffffffff812cd94a:	41 55                	push   %r13
ffffffff812cd94c:	41 54                	push   %r12
ffffffff812cd94e:	41 89 d4             	mov    %edx,%r12d
ffffffff812cd951:	53                   	push   %rbx
ffffffff812cd952:	89 f3                	mov    %esi,%ebx
	unsigned int pos;

	for (pos = find_first_bit(buf, nbits);
ffffffff812cd954:	4c 89 e6             	mov    %r12,%rsi
 * that the 3rd set bit (starting with 0th) is at position 7 in @buf.
 *
 * The bit positions 0 through @nbits-1 are valid positions in @buf.
 */
unsigned int bitmap_ord_to_pos(const unsigned long *buf, unsigned int ord, unsigned int nbits)
{
ffffffff812cd957:	49 89 fd             	mov    %rdi,%r13
ffffffff812cd95a:	4d 89 e6             	mov    %r12,%r14
	unsigned int pos;

	for (pos = find_first_bit(buf, nbits);
ffffffff812cd95d:	e8 ea 32 00 00       	callq  ffffffff812d0c4c <find_first_bit>
ffffffff812cd962:	44 39 f0             	cmp    %r14d,%eax
ffffffff812cd965:	73 16                	jae    ffffffff812cd97d <bitmap_ord_to_pos+0x39>
ffffffff812cd967:	85 db                	test   %ebx,%ebx
ffffffff812cd969:	74 12                	je     ffffffff812cd97d <bitmap_ord_to_pos+0x39>
	     pos < nbits && ord;
	     pos = find_next_bit(buf, nbits, pos + 1))
ffffffff812cd96b:	8d 50 01             	lea    0x1(%rax),%edx
ffffffff812cd96e:	4c 89 e6             	mov    %r12,%rsi
ffffffff812cd971:	4c 89 ef             	mov    %r13,%rdi
		ord--;
ffffffff812cd974:	ff cb                	dec    %ebx
{
	unsigned int pos;

	for (pos = find_first_bit(buf, nbits);
	     pos < nbits && ord;
	     pos = find_next_bit(buf, nbits, pos + 1))
ffffffff812cd976:	e8 9d 33 00 00       	callq  ffffffff812d0d18 <find_next_bit>
ffffffff812cd97b:	eb e5                	jmp    ffffffff812cd962 <bitmap_ord_to_pos+0x1e>
		ord--;

	return pos;
}
ffffffff812cd97d:	5b                   	pop    %rbx
ffffffff812cd97e:	41 5c                	pop    %r12
ffffffff812cd980:	41 5d                	pop    %r13
ffffffff812cd982:	41 5e                	pop    %r14
ffffffff812cd984:	5d                   	pop    %rbp
ffffffff812cd985:	c3                   	retq   

ffffffff812cd986 <bitmap_remap>:
		const unsigned long *old, const unsigned long *new,
		unsigned int nbits)
{
	unsigned int oldbit, w;

	if (dst == src)		/* following doesn't handle inplace remaps */
ffffffff812cd986:	48 39 f7             	cmp    %rsi,%rdi
ffffffff812cd989:	0f 84 a9 00 00 00    	je     ffffffff812cda38 <bitmap_remap+0xb2>
 * 13 and 15 set.
 */
void bitmap_remap(unsigned long *dst, const unsigned long *src,
		const unsigned long *old, const unsigned long *new,
		unsigned int nbits)
{
ffffffff812cd98f:	55                   	push   %rbp
ffffffff812cd990:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cd993:	41 57                	push   %r15
ffffffff812cd995:	41 56                	push   %r14
ffffffff812cd997:	41 55                	push   %r13
ffffffff812cd999:	41 54                	push   %r12
ffffffff812cd99b:	49 89 ce             	mov    %rcx,%r14
ffffffff812cd99e:	53                   	push   %rbx
ffffffff812cd99f:	49 89 fc             	mov    %rdi,%r12
ffffffff812cd9a2:	44 89 c3             	mov    %r8d,%ebx
ffffffff812cd9a5:	48 83 ec 28          	sub    $0x28,%rsp
ffffffff812cd9a9:	48 89 75 c8          	mov    %rsi,-0x38(%rbp)
	unsigned int oldbit, w;

	if (dst == src)		/* following doesn't handle inplace remaps */
		return;
	bitmap_zero(dst, nbits);
ffffffff812cd9ad:	44 89 c6             	mov    %r8d,%esi
ffffffff812cd9b0:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
ffffffff812cd9b4:	e8 15 fa ff ff       	callq  ffffffff812cd3ce <bitmap_zero>

	w = bitmap_weight(new, nbits);
ffffffff812cd9b9:	4c 89 f7             	mov    %r14,%rdi
ffffffff812cd9bc:	44 89 c6             	mov    %r8d,%esi
ffffffff812cd9bf:	e8 4a fb ff ff       	callq  ffffffff812cd50e <bitmap_weight>
	for_each_set_bit(oldbit, src, nbits) {
ffffffff812cd9c4:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi

	if (dst == src)		/* following doesn't handle inplace remaps */
		return;
	bitmap_zero(dst, nbits);

	w = bitmap_weight(new, nbits);
ffffffff812cd9c8:	41 89 c7             	mov    %eax,%r15d
	for_each_set_bit(oldbit, src, nbits) {
ffffffff812cd9cb:	89 de                	mov    %ebx,%esi
ffffffff812cd9cd:	89 d8                	mov    %ebx,%eax
ffffffff812cd9cf:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812cd9d3:	e8 74 32 00 00       	callq  ffffffff812d0c4c <find_first_bit>
ffffffff812cd9d8:	39 d8                	cmp    %ebx,%eax
ffffffff812cd9da:	41 89 c5             	mov    %eax,%r13d
ffffffff812cd9dd:	73 4b                	jae    ffffffff812cda2a <bitmap_remap+0xa4>
		int n = bitmap_pos_to_ord(old, oldbit, nbits);
ffffffff812cd9df:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cd9e3:	89 da                	mov    %ebx,%edx
ffffffff812cd9e5:	44 89 ee             	mov    %r13d,%esi
ffffffff812cd9e8:	e8 2c fb ff ff       	callq  ffffffff812cd519 <bitmap_pos_to_ord>

		if (n < 0 || w == 0)
ffffffff812cd9ed:	89 c1                	mov    %eax,%ecx
ffffffff812cd9ef:	c1 e9 1f             	shr    $0x1f,%ecx
ffffffff812cd9f2:	75 05                	jne    ffffffff812cd9f9 <bitmap_remap+0x73>
ffffffff812cd9f4:	45 85 ff             	test   %r15d,%r15d
ffffffff812cd9f7:	75 05                	jne    ffffffff812cd9fe <bitmap_remap+0x78>
			set_bit(oldbit, dst);	/* identity map */
ffffffff812cd9f9:	44 89 e8             	mov    %r13d,%eax
ffffffff812cd9fc:	eb 13                	jmp    ffffffff812cda11 <bitmap_remap+0x8b>
		else
			set_bit(bitmap_ord_to_pos(new, n % w, nbits), dst);
ffffffff812cd9fe:	31 d2                	xor    %edx,%edx
ffffffff812cda00:	4c 89 f7             	mov    %r14,%rdi
ffffffff812cda03:	41 f7 f7             	div    %r15d
ffffffff812cda06:	89 d6                	mov    %edx,%esi
ffffffff812cda08:	89 da                	mov    %ebx,%edx
ffffffff812cda0a:	e8 35 ff ff ff       	callq  ffffffff812cd944 <bitmap_ord_to_pos>
ffffffff812cda0f:	89 c0                	mov    %eax,%eax
ffffffff812cda11:	f0 49 0f ab 04 24    	lock bts %rax,(%r12)
	if (dst == src)		/* following doesn't handle inplace remaps */
		return;
	bitmap_zero(dst, nbits);

	w = bitmap_weight(new, nbits);
	for_each_set_bit(oldbit, src, nbits) {
ffffffff812cda17:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
ffffffff812cda1b:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
ffffffff812cda1f:	41 8d 55 01          	lea    0x1(%r13),%edx
ffffffff812cda23:	e8 f0 32 00 00       	callq  ffffffff812d0d18 <find_next_bit>
ffffffff812cda28:	eb ae                	jmp    ffffffff812cd9d8 <bitmap_remap+0x52>
		if (n < 0 || w == 0)
			set_bit(oldbit, dst);	/* identity map */
		else
			set_bit(bitmap_ord_to_pos(new, n % w, nbits), dst);
	}
}
ffffffff812cda2a:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812cda2e:	5b                   	pop    %rbx
ffffffff812cda2f:	41 5c                	pop    %r12
ffffffff812cda31:	41 5d                	pop    %r13
ffffffff812cda33:	41 5e                	pop    %r14
ffffffff812cda35:	41 5f                	pop    %r15
ffffffff812cda37:	5d                   	pop    %rbp
ffffffff812cda38:	c3                   	retq   

ffffffff812cda39 <bitmap_bitremap>:
 * bit positions unchanged.  So if say @oldbit is 5, then this routine
 * returns 13.
 */
int bitmap_bitremap(int oldbit, const unsigned long *old,
				const unsigned long *new, int bits)
{
ffffffff812cda39:	55                   	push   %rbp
ffffffff812cda3a:	41 89 f9             	mov    %edi,%r9d
ffffffff812cda3d:	41 89 ca             	mov    %ecx,%r10d
	int w = bitmap_weight(new, bits);
ffffffff812cda40:	48 89 d7             	mov    %rdx,%rdi
 * bit positions unchanged.  So if say @oldbit is 5, then this routine
 * returns 13.
 */
int bitmap_bitremap(int oldbit, const unsigned long *old,
				const unsigned long *new, int bits)
{
ffffffff812cda43:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cda46:	41 54                	push   %r12
ffffffff812cda48:	53                   	push   %rbx
ffffffff812cda49:	49 89 f4             	mov    %rsi,%r12
	int w = bitmap_weight(new, bits);
ffffffff812cda4c:	89 ce                	mov    %ecx,%esi
 * bit positions unchanged.  So if say @oldbit is 5, then this routine
 * returns 13.
 */
int bitmap_bitremap(int oldbit, const unsigned long *old,
				const unsigned long *new, int bits)
{
ffffffff812cda4e:	48 89 d3             	mov    %rdx,%rbx
	int w = bitmap_weight(new, bits);
ffffffff812cda51:	e8 b8 fa ff ff       	callq  ffffffff812cd50e <bitmap_weight>
	int n = bitmap_pos_to_ord(old, oldbit, bits);
ffffffff812cda56:	4c 89 e7             	mov    %r12,%rdi
ffffffff812cda59:	44 89 d2             	mov    %r10d,%edx
ffffffff812cda5c:	44 89 ce             	mov    %r9d,%esi
 * returns 13.
 */
int bitmap_bitremap(int oldbit, const unsigned long *old,
				const unsigned long *new, int bits)
{
	int w = bitmap_weight(new, bits);
ffffffff812cda5f:	41 89 c3             	mov    %eax,%r11d
	int n = bitmap_pos_to_ord(old, oldbit, bits);
ffffffff812cda62:	e8 b2 fa ff ff       	callq  ffffffff812cd519 <bitmap_pos_to_ord>
	if (n < 0 || w == 0)
ffffffff812cda67:	89 c1                	mov    %eax,%ecx
ffffffff812cda69:	c1 e9 1f             	shr    $0x1f,%ecx
ffffffff812cda6c:	75 18                	jne    ffffffff812cda86 <bitmap_bitremap+0x4d>
ffffffff812cda6e:	45 85 db             	test   %r11d,%r11d
ffffffff812cda71:	74 13                	je     ffffffff812cda86 <bitmap_bitremap+0x4d>
		return oldbit;
	else
		return bitmap_ord_to_pos(new, n % w, bits);
ffffffff812cda73:	99                   	cltd   
ffffffff812cda74:	48 89 df             	mov    %rbx,%rdi
ffffffff812cda77:	41 f7 fb             	idiv   %r11d
ffffffff812cda7a:	89 d6                	mov    %edx,%esi
ffffffff812cda7c:	44 89 d2             	mov    %r10d,%edx
ffffffff812cda7f:	e8 c0 fe ff ff       	callq  ffffffff812cd944 <bitmap_ord_to_pos>
ffffffff812cda84:	eb 03                	jmp    ffffffff812cda89 <bitmap_bitremap+0x50>
				const unsigned long *new, int bits)
{
	int w = bitmap_weight(new, bits);
	int n = bitmap_pos_to_ord(old, oldbit, bits);
	if (n < 0 || w == 0)
		return oldbit;
ffffffff812cda86:	44 89 c8             	mov    %r9d,%eax
	else
		return bitmap_ord_to_pos(new, n % w, bits);
}
ffffffff812cda89:	5b                   	pop    %rbx
ffffffff812cda8a:	41 5c                	pop    %r12
ffffffff812cda8c:	5d                   	pop    %rbp
ffffffff812cda8d:	c3                   	retq   

ffffffff812cda8e <__sg_page_iter_start>:
EXPORT_SYMBOL(sg_alloc_table_from_pages);

void __sg_page_iter_start(struct sg_page_iter *piter,
			  struct scatterlist *sglist, unsigned int nents,
			  unsigned long pgoffset)
{
ffffffff812cda8e:	55                   	push   %rbp
	piter->__pg_advance = 0;
ffffffff812cda8f:	c7 47 10 00 00 00 00 	movl   $0x0,0x10(%rdi)
	piter->__nents = nents;
ffffffff812cda96:	89 57 0c             	mov    %edx,0xc(%rdi)

	piter->sg = sglist;
ffffffff812cda99:	48 89 37             	mov    %rsi,(%rdi)
EXPORT_SYMBOL(sg_alloc_table_from_pages);

void __sg_page_iter_start(struct sg_page_iter *piter,
			  struct scatterlist *sglist, unsigned int nents,
			  unsigned long pgoffset)
{
ffffffff812cda9c:	48 89 e5             	mov    %rsp,%rbp
	piter->__pg_advance = 0;
	piter->__nents = nents;

	piter->sg = sglist;
	piter->sg_pgoffset = pgoffset;
ffffffff812cda9f:	89 4f 08             	mov    %ecx,0x8(%rdi)
}
ffffffff812cdaa2:	5d                   	pop    %rbp
ffffffff812cdaa3:	c3                   	retq   

ffffffff812cdaa4 <sg_next>:
{
#ifdef CONFIG_DEBUG_SG
	BUG_ON(sg->sg_magic != SG_MAGIC);
#endif
	if (sg_is_last(sg))
		return NULL;
ffffffff812cdaa4:	31 c0                	xor    %eax,%eax
struct scatterlist *sg_next(struct scatterlist *sg)
{
#ifdef CONFIG_DEBUG_SG
	BUG_ON(sg->sg_magic != SG_MAGIC);
#endif
	if (sg_is_last(sg))
ffffffff812cdaa6:	f6 07 02             	testb  $0x2,(%rdi)
 *   of a chained scatterlist, it could jump to the start of a new
 *   scatterlist array.
 *
 **/
struct scatterlist *sg_next(struct scatterlist *sg)
{
ffffffff812cdaa9:	55                   	push   %rbp
ffffffff812cdaaa:	48 89 e5             	mov    %rsp,%rbp
#ifdef CONFIG_DEBUG_SG
	BUG_ON(sg->sg_magic != SG_MAGIC);
#endif
	if (sg_is_last(sg))
ffffffff812cdaad:	75 14                	jne    ffffffff812cdac3 <sg_next+0x1f>
		return NULL;

	sg++;
	if (unlikely(sg_is_chain(sg)))
ffffffff812cdaaf:	48 8b 57 20          	mov    0x20(%rdi),%rdx
	BUG_ON(sg->sg_magic != SG_MAGIC);
#endif
	if (sg_is_last(sg))
		return NULL;

	sg++;
ffffffff812cdab3:	48 8d 47 20          	lea    0x20(%rdi),%rax
	if (unlikely(sg_is_chain(sg)))
ffffffff812cdab7:	f6 c2 01             	test   $0x1,%dl
ffffffff812cdaba:	74 07                	je     ffffffff812cdac3 <sg_next+0x1f>
		sg = sg_chain_ptr(sg);
ffffffff812cdabc:	48 89 d0             	mov    %rdx,%rax
ffffffff812cdabf:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax

	return sg;
}
ffffffff812cdac3:	5d                   	pop    %rbp
ffffffff812cdac4:	c3                   	retq   

ffffffff812cdac5 <sg_nents>:
 * Allows to know how many entries are in sg, taking into acount
 * chaining as well
 *
 **/
int sg_nents(struct scatterlist *sg)
{
ffffffff812cdac5:	55                   	push   %rbp
	int nents;
	for (nents = 0; sg; sg = sg_next(sg))
ffffffff812cdac6:	31 c9                	xor    %ecx,%ecx
 * Allows to know how many entries are in sg, taking into acount
 * chaining as well
 *
 **/
int sg_nents(struct scatterlist *sg)
{
ffffffff812cdac8:	48 89 e5             	mov    %rsp,%rbp
	int nents;
	for (nents = 0; sg; sg = sg_next(sg))
ffffffff812cdacb:	48 85 ff             	test   %rdi,%rdi
ffffffff812cdace:	74 0c                	je     ffffffff812cdadc <sg_nents+0x17>
		nents++;
ffffffff812cdad0:	ff c1                	inc    %ecx
 *
 **/
int sg_nents(struct scatterlist *sg)
{
	int nents;
	for (nents = 0; sg; sg = sg_next(sg))
ffffffff812cdad2:	e8 cd ff ff ff       	callq  ffffffff812cdaa4 <sg_next>
ffffffff812cdad7:	48 89 c7             	mov    %rax,%rdi
ffffffff812cdada:	eb ef                	jmp    ffffffff812cdacb <sg_nents+0x6>
		nents++;
	return nents;
}
ffffffff812cdadc:	89 c8                	mov    %ecx,%eax
ffffffff812cdade:	5d                   	pop    %rbp
ffffffff812cdadf:	c3                   	retq   

ffffffff812cdae0 <sg_last>:
 *   the important bit is that @nents@ denotes the number of entries that
 *   exist from @sgl@.
 *
 **/
struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)
{
ffffffff812cdae0:	55                   	push   %rbp
ffffffff812cdae1:	49 89 f8             	mov    %rdi,%r8
ffffffff812cdae4:	89 f6                	mov    %esi,%esi
	struct scatterlist *ret = &sgl[nents - 1];
#else
	struct scatterlist *sg, *ret = NULL;
	unsigned int i;

	for_each_sg(sgl, sg, nents, i)
ffffffff812cdae6:	31 c9                	xor    %ecx,%ecx
struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)
{
#ifndef CONFIG_ARCH_HAS_SG_CHAIN
	struct scatterlist *ret = &sgl[nents - 1];
#else
	struct scatterlist *sg, *ret = NULL;
ffffffff812cdae8:	31 d2                	xor    %edx,%edx
 *   the important bit is that @nents@ denotes the number of entries that
 *   exist from @sgl@.
 *
 **/
struct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)
{
ffffffff812cdaea:	48 89 e5             	mov    %rsp,%rbp
	struct scatterlist *ret = &sgl[nents - 1];
#else
	struct scatterlist *sg, *ret = NULL;
	unsigned int i;

	for_each_sg(sgl, sg, nents, i)
ffffffff812cdaed:	48 39 f1             	cmp    %rsi,%rcx
ffffffff812cdaf0:	74 13                	je     ffffffff812cdb05 <sg_last+0x25>
ffffffff812cdaf2:	4c 89 c7             	mov    %r8,%rdi
ffffffff812cdaf5:	e8 aa ff ff ff       	callq  ffffffff812cdaa4 <sg_next>
		ret = sg;
ffffffff812cdafa:	4c 89 c2             	mov    %r8,%rdx
ffffffff812cdafd:	48 ff c1             	inc    %rcx
	struct scatterlist *ret = &sgl[nents - 1];
#else
	struct scatterlist *sg, *ret = NULL;
	unsigned int i;

	for_each_sg(sgl, sg, nents, i)
ffffffff812cdb00:	49 89 c0             	mov    %rax,%r8
ffffffff812cdb03:	eb e8                	jmp    ffffffff812cdaed <sg_last+0xd>
#ifdef CONFIG_DEBUG_SG
	BUG_ON(sgl[0].sg_magic != SG_MAGIC);
	BUG_ON(!sg_is_last(ret));
#endif
	return ret;
}
ffffffff812cdb05:	48 89 d0             	mov    %rdx,%rax
ffffffff812cdb08:	5d                   	pop    %rbp
ffffffff812cdb09:	c3                   	retq   

ffffffff812cdb0a <__sg_free_table>:
void __sg_free_table(struct sg_table *table, unsigned int max_ents,
		     bool skip_first_chunk, sg_free_fn *free_fn)
{
	struct scatterlist *sgl, *next;

	if (unlikely(!table->sgl))
ffffffff812cdb0a:	4c 8b 07             	mov    (%rdi),%r8
ffffffff812cdb0d:	4d 85 c0             	test   %r8,%r8
ffffffff812cdb10:	74 78                	je     ffffffff812cdb8a <__sg_free_table+0x80>
 *    that previously used with __sg_alloc_table().
 *
 **/
void __sg_free_table(struct sg_table *table, unsigned int max_ents,
		     bool skip_first_chunk, sg_free_fn *free_fn)
{
ffffffff812cdb12:	55                   	push   %rbp
ffffffff812cdb13:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cdb16:	41 57                	push   %r15
ffffffff812cdb18:	41 56                	push   %r14
		 * then assign 'next' to the sg table after the current one.
		 * sg_size is then one less than alloc size, since the last
		 * element is the chain pointer.
		 */
		if (alloc_size > max_ents) {
			next = sg_chain_ptr(&sgl[max_ents - 1]);
ffffffff812cdb1a:	44 8d 76 ff          	lea    -0x1(%rsi),%r14d
 *    that previously used with __sg_alloc_table().
 *
 **/
void __sg_free_table(struct sg_table *table, unsigned int max_ents,
		     bool skip_first_chunk, sg_free_fn *free_fn)
{
ffffffff812cdb1e:	41 55                	push   %r13
ffffffff812cdb20:	41 54                	push   %r12
ffffffff812cdb22:	53                   	push   %rbx
ffffffff812cdb23:	49 89 cf             	mov    %rcx,%r15
ffffffff812cdb26:	41 89 f4             	mov    %esi,%r12d
		 * then assign 'next' to the sg table after the current one.
		 * sg_size is then one less than alloc size, since the last
		 * element is the chain pointer.
		 */
		if (alloc_size > max_ents) {
			next = sg_chain_ptr(&sgl[max_ents - 1]);
ffffffff812cdb29:	4c 89 f1             	mov    %r14,%rcx
ffffffff812cdb2c:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cdb2f:	49 c1 e6 05          	shl    $0x5,%r14
 *    that previously used with __sg_alloc_table().
 *
 **/
void __sg_free_table(struct sg_table *table, unsigned int max_ents,
		     bool skip_first_chunk, sg_free_fn *free_fn)
{
ffffffff812cdb33:	48 83 ec 18          	sub    $0x18,%rsp

	if (unlikely(!table->sgl))
		return;

	sgl = table->sgl;
	while (table->orig_nents) {
ffffffff812cdb37:	8b 43 0c             	mov    0xc(%rbx),%eax
ffffffff812cdb3a:	85 c0                	test   %eax,%eax
ffffffff812cdb3c:	74 37                	je     ffffffff812cdb75 <__sg_free_table+0x6b>
		 * If we have more than max_ents segments left,
		 * then assign 'next' to the sg table after the current one.
		 * sg_size is then one less than alloc size, since the last
		 * element is the chain pointer.
		 */
		if (alloc_size > max_ents) {
ffffffff812cdb3e:	44 39 e0             	cmp    %r12d,%eax
ffffffff812cdb41:	76 0f                	jbe    ffffffff812cdb52 <__sg_free_table+0x48>
			next = sg_chain_ptr(&sgl[max_ents - 1]);
ffffffff812cdb43:	4f 8b 2c 30          	mov    (%r8,%r14,1),%r13
			alloc_size = max_ents;
			sg_size = alloc_size - 1;
ffffffff812cdb47:	89 cf                	mov    %ecx,%edi
ffffffff812cdb49:	44 89 e6             	mov    %r12d,%esi
		 * then assign 'next' to the sg table after the current one.
		 * sg_size is then one less than alloc size, since the last
		 * element is the chain pointer.
		 */
		if (alloc_size > max_ents) {
			next = sg_chain_ptr(&sgl[max_ents - 1]);
ffffffff812cdb4c:	49 83 e5 fc          	and    $0xfffffffffffffffc,%r13
ffffffff812cdb50:	eb 07                	jmp    ffffffff812cdb59 <__sg_free_table+0x4f>
ffffffff812cdb52:	89 c7                	mov    %eax,%edi
ffffffff812cdb54:	89 c6                	mov    %eax,%esi
			alloc_size = max_ents;
			sg_size = alloc_size - 1;
		} else {
			sg_size = alloc_size;
			next = NULL;
ffffffff812cdb56:	45 31 ed             	xor    %r13d,%r13d
		}

		table->orig_nents -= sg_size;
ffffffff812cdb59:	29 f8                	sub    %edi,%eax
		if (skip_first_chunk)
ffffffff812cdb5b:	84 d2                	test   %dl,%dl
		} else {
			sg_size = alloc_size;
			next = NULL;
		}

		table->orig_nents -= sg_size;
ffffffff812cdb5d:	89 43 0c             	mov    %eax,0xc(%rbx)
		if (skip_first_chunk)
ffffffff812cdb60:	75 0c                	jne    ffffffff812cdb6e <__sg_free_table+0x64>
ffffffff812cdb62:	89 4d cc             	mov    %ecx,-0x34(%rbp)
			skip_first_chunk = false;
		else
			free_fn(sgl, alloc_size);
ffffffff812cdb65:	4c 89 c7             	mov    %r8,%rdi
ffffffff812cdb68:	41 ff d7             	callq  *%r15
ffffffff812cdb6b:	8b 4d cc             	mov    -0x34(%rbp),%ecx
ffffffff812cdb6e:	31 d2                	xor    %edx,%edx
			next = sg_chain_ptr(&sgl[max_ents - 1]);
			alloc_size = max_ents;
			sg_size = alloc_size - 1;
		} else {
			sg_size = alloc_size;
			next = NULL;
ffffffff812cdb70:	4d 89 e8             	mov    %r13,%r8
ffffffff812cdb73:	eb c2                	jmp    ffffffff812cdb37 <__sg_free_table+0x2d>
		else
			free_fn(sgl, alloc_size);
		sgl = next;
	}

	table->sgl = NULL;
ffffffff812cdb75:	48 c7 03 00 00 00 00 	movq   $0x0,(%rbx)
}
ffffffff812cdb7c:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812cdb80:	5b                   	pop    %rbx
ffffffff812cdb81:	41 5c                	pop    %r12
ffffffff812cdb83:	41 5d                	pop    %r13
ffffffff812cdb85:	41 5e                	pop    %r14
ffffffff812cdb87:	41 5f                	pop    %r15
ffffffff812cdb89:	5d                   	pop    %rbp
ffffffff812cdb8a:	c3                   	retq   

ffffffff812cdb8b <sg_free_table>:
 * sg_free_table - Free a previously allocated sg table
 * @table:	The mapped sg table header
 *
 **/
void sg_free_table(struct sg_table *table)
{
ffffffff812cdb8b:	55                   	push   %rbp
	__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);
ffffffff812cdb8c:	48 c7 c1 a4 db 2c 81 	mov    $0xffffffff812cdba4,%rcx
ffffffff812cdb93:	31 d2                	xor    %edx,%edx
ffffffff812cdb95:	be 80 00 00 00       	mov    $0x80,%esi
 * sg_free_table - Free a previously allocated sg table
 * @table:	The mapped sg table header
 *
 **/
void sg_free_table(struct sg_table *table)
{
ffffffff812cdb9a:	48 89 e5             	mov    %rsp,%rbp
	__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);
ffffffff812cdb9d:	e8 68 ff ff ff       	callq  ffffffff812cdb0a <__sg_free_table>
}
ffffffff812cdba2:	5d                   	pop    %rbp
ffffffff812cdba3:	c3                   	retq   

ffffffff812cdba4 <sg_kfree>:
	} else
		return kmalloc(nents * sizeof(struct scatterlist), gfp_mask);
}

static void sg_kfree(struct scatterlist *sg, unsigned int nents)
{
ffffffff812cdba4:	55                   	push   %rbp
	if (nents == SG_MAX_SINGLE_ALLOC) {
ffffffff812cdba5:	83 c6 80             	add    $0xffffff80,%esi
	} else
		return kmalloc(nents * sizeof(struct scatterlist), gfp_mask);
}

static void sg_kfree(struct scatterlist *sg, unsigned int nents)
{
ffffffff812cdba8:	48 89 e5             	mov    %rsp,%rbp
	if (nents == SG_MAX_SINGLE_ALLOC) {
ffffffff812cdbab:	75 09                	jne    ffffffff812cdbb6 <sg_kfree+0x12>
		kmemleak_free(sg);
		free_page((unsigned long) sg);
ffffffff812cdbad:	31 f6                	xor    %esi,%esi
ffffffff812cdbaf:	e8 9e b3 e0 ff       	callq  ffffffff810d8f52 <free_pages>
ffffffff812cdbb4:	eb 05                	jmp    ffffffff812cdbbb <sg_kfree+0x17>
	} else
		kfree(sg);
ffffffff812cdbb6:	e8 ce 2d e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812cdbbb:	5d                   	pop    %rbp
ffffffff812cdbbc:	c3                   	retq   

ffffffff812cdbbd <sg_kmalloc>:
/*
 * The default behaviour of sg_alloc_table() is to use these kmalloc/kfree
 * helpers.
 */
static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
{
ffffffff812cdbbd:	55                   	push   %rbp
	if (nents == SG_MAX_SINGLE_ALLOC) {
ffffffff812cdbbe:	81 ff 80 00 00 00    	cmp    $0x80,%edi
/*
 * The default behaviour of sg_alloc_table() is to use these kmalloc/kfree
 * helpers.
 */
static struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)
{
ffffffff812cdbc4:	89 f0                	mov    %esi,%eax
ffffffff812cdbc6:	48 89 e5             	mov    %rsp,%rbp
	if (nents == SG_MAX_SINGLE_ALLOC) {
ffffffff812cdbc9:	75 0b                	jne    ffffffff812cdbd6 <sg_kmalloc+0x19>
		 * kmalloc (tracked by kmemleak), in order to for that last
		 * allocation not to become decoupled (and thus a
		 * false-positive) we need to inform kmemleak of all the
		 * intermediate allocations.
		 */
		void *ptr = (void *) __get_free_page(gfp_mask);
ffffffff812cdbcb:	31 f6                	xor    %esi,%esi
ffffffff812cdbcd:	89 c7                	mov    %eax,%edi
ffffffff812cdbcf:	e8 f0 c4 e0 ff       	callq  ffffffff810da0c4 <__get_free_pages>
		kmemleak_alloc(ptr, PAGE_SIZE, 1, gfp_mask);
		return ptr;
ffffffff812cdbd4:	eb 0b                	jmp    ffffffff812cdbe1 <sg_kmalloc+0x24>
ffffffff812cdbd6:	89 ff                	mov    %edi,%edi
ffffffff812cdbd8:	48 c1 e7 05          	shl    $0x5,%rdi
ffffffff812cdbdc:	e8 aa 35 e3 ff       	callq  ffffffff8110118b <__kmalloc>
	} else
		return kmalloc(nents * sizeof(struct scatterlist), gfp_mask);
}
ffffffff812cdbe1:	5d                   	pop    %rbp
ffffffff812cdbe2:	c3                   	retq   

ffffffff812cdbe3 <sg_miter_start>:
 * Context:
 *   Don't care.
 */
void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
		    unsigned int nents, unsigned int flags)
{
ffffffff812cdbe3:	55                   	push   %rbp
	memset(miter, 0, sizeof(struct sg_mapping_iter));
ffffffff812cdbe4:	31 c0                	xor    %eax,%eax
 * Context:
 *   Don't care.
 */
void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
		    unsigned int nents, unsigned int flags)
{
ffffffff812cdbe6:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cdbe9:	41 54                	push   %r12
ffffffff812cdbeb:	41 89 cc             	mov    %ecx,%r12d
ffffffff812cdbee:	53                   	push   %rbx
	memset(miter, 0, sizeof(struct sg_mapping_iter));
ffffffff812cdbef:	b9 12 00 00 00       	mov    $0x12,%ecx
 * Context:
 *   Don't care.
 */
void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
		    unsigned int nents, unsigned int flags)
{
ffffffff812cdbf4:	48 89 fb             	mov    %rdi,%rbx
	memset(miter, 0, sizeof(struct sg_mapping_iter));

	__sg_page_iter_start(&miter->piter, sgl, nents, 0);
	WARN_ON(!(flags & (SG_MITER_TO_SG | SG_MITER_FROM_SG)));
ffffffff812cdbf7:	41 f6 c4 06          	test   $0x6,%r12b
 *   Don't care.
 */
void sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,
		    unsigned int nents, unsigned int flags)
{
	memset(miter, 0, sizeof(struct sg_mapping_iter));
ffffffff812cdbfb:	f3 ab                	rep stos %eax,%es:(%rdi)
void __sg_page_iter_start(struct sg_page_iter *piter,
			  struct scatterlist *sglist, unsigned int nents,
			  unsigned long pgoffset)
{
	piter->__pg_advance = 0;
	piter->__nents = nents;
ffffffff812cdbfd:	89 53 2c             	mov    %edx,0x2c(%rbx)

	piter->sg = sglist;
ffffffff812cdc00:	48 89 73 20          	mov    %rsi,0x20(%rbx)
		    unsigned int nents, unsigned int flags)
{
	memset(miter, 0, sizeof(struct sg_mapping_iter));

	__sg_page_iter_start(&miter->piter, sgl, nents, 0);
	WARN_ON(!(flags & (SG_MITER_TO_SG | SG_MITER_FROM_SG)));
ffffffff812cdc04:	75 11                	jne    ffffffff812cdc17 <sg_miter_start+0x34>
ffffffff812cdc06:	be ce 01 00 00       	mov    $0x1ce,%esi
ffffffff812cdc0b:	48 c7 c7 17 6e 7b 81 	mov    $0xffffffff817b6e17,%rdi
ffffffff812cdc12:	e8 9e 87 d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>
	miter->__flags = flags;
ffffffff812cdc17:	44 89 63 40          	mov    %r12d,0x40(%rbx)
}
ffffffff812cdc1b:	5b                   	pop    %rbx
ffffffff812cdc1c:	41 5c                	pop    %r12
ffffffff812cdc1e:	5d                   	pop    %rbp
ffffffff812cdc1f:	c3                   	retq   

ffffffff812cdc20 <sg_miter_stop>:
 * Context:
 *   Preemption disabled if the SG_MITER_ATOMIC is set.  Don't care
 *   otherwise.
 */
void sg_miter_stop(struct sg_mapping_iter *miter)
{
ffffffff812cdc20:	55                   	push   %rbp
ffffffff812cdc21:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cdc24:	53                   	push   %rbx
ffffffff812cdc25:	52                   	push   %rdx
	WARN_ON(miter->consumed > miter->length);
ffffffff812cdc26:	48 8b 47 10          	mov    0x10(%rdi),%rax
ffffffff812cdc2a:	48 39 47 18          	cmp    %rax,0x18(%rdi)
 * Context:
 *   Preemption disabled if the SG_MITER_ATOMIC is set.  Don't care
 *   otherwise.
 */
void sg_miter_stop(struct sg_mapping_iter *miter)
{
ffffffff812cdc2e:	48 89 fb             	mov    %rdi,%rbx
	WARN_ON(miter->consumed > miter->length);
ffffffff812cdc31:	76 11                	jbe    ffffffff812cdc44 <sg_miter_stop+0x24>
ffffffff812cdc33:	be 47 02 00 00       	mov    $0x247,%esi
ffffffff812cdc38:	48 c7 c7 17 6e 7b 81 	mov    $0xffffffff817b6e17,%rdi
ffffffff812cdc3f:	e8 71 87 d9 ff       	callq  ffffffff810663b5 <warn_slowpath_null>

	/* drop resources from the last iteration */
	if (miter->addr) {
ffffffff812cdc44:	48 83 7b 08 00       	cmpq   $0x0,0x8(%rbx)
ffffffff812cdc49:	74 41                	je     ffffffff812cdc8c <sg_miter_stop+0x6c>
		miter->__offset += miter->consumed;
ffffffff812cdc4b:	48 8b 43 18          	mov    0x18(%rbx),%rax
ffffffff812cdc4f:	01 43 38             	add    %eax,0x38(%rbx)
		miter->__remaining -= miter->consumed;
ffffffff812cdc52:	29 43 3c             	sub    %eax,0x3c(%rbx)

		if ((miter->__flags & SG_MITER_TO_SG) &&
ffffffff812cdc55:	8b 43 40             	mov    0x40(%rbx),%eax
ffffffff812cdc58:	a8 02                	test   $0x2,%al
ffffffff812cdc5a:	74 06                	je     ffffffff812cdc62 <sg_miter_stop+0x42>
		    !PageSlab(miter->page))
ffffffff812cdc5c:	48 8b 13             	mov    (%rbx),%rdx
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
ffffffff812cdc5f:	48 8b 12             	mov    (%rdx),%rdx
			flush_kernel_dcache_page(miter->page);

		if (miter->__flags & SG_MITER_ATOMIC) {
ffffffff812cdc62:	a8 01                	test   $0x1,%al
ffffffff812cdc64:	74 07                	je     ffffffff812cdc6d <sg_miter_stop+0x4d>
ffffffff812cdc66:	65 ff 0d 2b cd d3 7e 	decl   %gs:0x7ed3cd2b(%rip)        # a998 <__preempt_count>
			WARN_ON_ONCE(preemptible());
			kunmap_atomic(miter->addr);
		} else
			kunmap(miter->page);

		miter->page = NULL;
ffffffff812cdc6d:	48 c7 03 00 00 00 00 	movq   $0x0,(%rbx)
		miter->addr = NULL;
ffffffff812cdc74:	48 c7 43 08 00 00 00 	movq   $0x0,0x8(%rbx)
ffffffff812cdc7b:	00 
		miter->length = 0;
ffffffff812cdc7c:	48 c7 43 10 00 00 00 	movq   $0x0,0x10(%rbx)
ffffffff812cdc83:	00 
		miter->consumed = 0;
ffffffff812cdc84:	48 c7 43 18 00 00 00 	movq   $0x0,0x18(%rbx)
ffffffff812cdc8b:	00 
	}
}
ffffffff812cdc8c:	58                   	pop    %rax
ffffffff812cdc8d:	5b                   	pop    %rbx
ffffffff812cdc8e:	5d                   	pop    %rbp
ffffffff812cdc8f:	c3                   	retq   

ffffffff812cdc90 <__sg_page_iter_next>:
	return PAGE_ALIGN(sg->offset + sg->length) >> PAGE_SHIFT;
}

bool __sg_page_iter_next(struct sg_page_iter *piter)
{
	if (!piter->__nents || !piter->sg)
ffffffff812cdc90:	83 7f 0c 00          	cmpl   $0x0,0xc(%rdi)
ffffffff812cdc94:	74 5d                	je     ffffffff812cdcf3 <__sg_page_iter_next+0x63>
ffffffff812cdc96:	48 83 3f 00          	cmpq   $0x0,(%rdi)
ffffffff812cdc9a:	74 57                	je     ffffffff812cdcf3 <__sg_page_iter_next+0x63>
{
	return PAGE_ALIGN(sg->offset + sg->length) >> PAGE_SHIFT;
}

bool __sg_page_iter_next(struct sg_page_iter *piter)
{
ffffffff812cdc9c:	55                   	push   %rbp
ffffffff812cdc9d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cdca0:	53                   	push   %rbx
	if (!piter->__nents || !piter->sg)
		return false;

	piter->sg_pgoffset += piter->__pg_advance;
ffffffff812cdca1:	8b 47 10             	mov    0x10(%rdi),%eax
ffffffff812cdca4:	01 47 08             	add    %eax,0x8(%rdi)
ffffffff812cdca7:	48 89 fb             	mov    %rdi,%rbx
	piter->__pg_advance = 1;
ffffffff812cdcaa:	c7 47 10 01 00 00 00 	movl   $0x1,0x10(%rdi)

	while (piter->sg_pgoffset >= sg_page_count(piter->sg)) {
ffffffff812cdcb1:	48 8b 3b             	mov    (%rbx),%rdi
ffffffff812cdcb4:	8b 53 08             	mov    0x8(%rbx),%edx
}
EXPORT_SYMBOL(__sg_page_iter_start);

static int sg_page_count(struct scatterlist *sg)
{
	return PAGE_ALIGN(sg->offset + sg->length) >> PAGE_SHIFT;
ffffffff812cdcb7:	8b 47 0c             	mov    0xc(%rdi),%eax
ffffffff812cdcba:	03 47 08             	add    0x8(%rdi),%eax
ffffffff812cdcbd:	05 ff 0f 00 00       	add    $0xfff,%eax
ffffffff812cdcc2:	c1 e8 0c             	shr    $0xc,%eax
		return false;

	piter->sg_pgoffset += piter->__pg_advance;
	piter->__pg_advance = 1;

	while (piter->sg_pgoffset >= sg_page_count(piter->sg)) {
ffffffff812cdcc5:	39 c2                	cmp    %eax,%edx
ffffffff812cdcc7:	72 25                	jb     ffffffff812cdcee <__sg_page_iter_next+0x5e>
		piter->sg_pgoffset -= sg_page_count(piter->sg);
ffffffff812cdcc9:	29 c2                	sub    %eax,%edx
ffffffff812cdccb:	89 53 08             	mov    %edx,0x8(%rbx)
		piter->sg = sg_next(piter->sg);
ffffffff812cdcce:	e8 d1 fd ff ff       	callq  ffffffff812cdaa4 <sg_next>
		if (!--piter->__nents || !piter->sg)
ffffffff812cdcd3:	8b 4b 0c             	mov    0xc(%rbx),%ecx
ffffffff812cdcd6:	48 85 c0             	test   %rax,%rax
	piter->sg_pgoffset += piter->__pg_advance;
	piter->__pg_advance = 1;

	while (piter->sg_pgoffset >= sg_page_count(piter->sg)) {
		piter->sg_pgoffset -= sg_page_count(piter->sg);
		piter->sg = sg_next(piter->sg);
ffffffff812cdcd9:	48 89 03             	mov    %rax,(%rbx)
		if (!--piter->__nents || !piter->sg)
ffffffff812cdcdc:	8d 51 ff             	lea    -0x1(%rcx),%edx
ffffffff812cdcdf:	89 53 0c             	mov    %edx,0xc(%rbx)
ffffffff812cdce2:	75 04                	jne    ffffffff812cdce8 <__sg_page_iter_next+0x58>
}

bool __sg_page_iter_next(struct sg_page_iter *piter)
{
	if (!piter->__nents || !piter->sg)
		return false;
ffffffff812cdce4:	31 c0                	xor    %eax,%eax
ffffffff812cdce6:	eb 08                	jmp    ffffffff812cdcf0 <__sg_page_iter_next+0x60>
	piter->__pg_advance = 1;

	while (piter->sg_pgoffset >= sg_page_count(piter->sg)) {
		piter->sg_pgoffset -= sg_page_count(piter->sg);
		piter->sg = sg_next(piter->sg);
		if (!--piter->__nents || !piter->sg)
ffffffff812cdce8:	85 d2                	test   %edx,%edx
ffffffff812cdcea:	75 c5                	jne    ffffffff812cdcb1 <__sg_page_iter_next+0x21>
ffffffff812cdcec:	eb f6                	jmp    ffffffff812cdce4 <__sg_page_iter_next+0x54>
			return false;
	}

	return true;
ffffffff812cdcee:	b0 01                	mov    $0x1,%al
}
ffffffff812cdcf0:	5b                   	pop    %rbx
ffffffff812cdcf1:	5d                   	pop    %rbp
ffffffff812cdcf2:	c3                   	retq   
}

bool __sg_page_iter_next(struct sg_page_iter *piter)
{
	if (!piter->__nents || !piter->sg)
		return false;
ffffffff812cdcf3:	31 c0                	xor    %eax,%eax
		if (!--piter->__nents || !piter->sg)
			return false;
	}

	return true;
}
ffffffff812cdcf5:	c3                   	retq   

ffffffff812cdcf6 <sg_miter_get_next_page>:
}
EXPORT_SYMBOL(sg_miter_start);

static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
{
	if (!miter->__remaining) {
ffffffff812cdcf6:	44 8b 47 3c          	mov    0x3c(%rdi),%r8d
				(pgoffset << PAGE_SHIFT) - miter->__offset;
		miter->__remaining = min_t(unsigned long, miter->__remaining,
					   PAGE_SIZE - miter->__offset);
	}

	return true;
ffffffff812cdcfa:	b0 01                	mov    $0x1,%al
}
EXPORT_SYMBOL(sg_miter_start);

static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
{
	if (!miter->__remaining) {
ffffffff812cdcfc:	45 85 c0             	test   %r8d,%r8d
ffffffff812cdcff:	75 48                	jne    ffffffff812cdd49 <sg_miter_get_next_page+0x53>
	miter->__flags = flags;
}
EXPORT_SYMBOL(sg_miter_start);

static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
{
ffffffff812cdd01:	55                   	push   %rbp
ffffffff812cdd02:	48 89 fe             	mov    %rdi,%rsi
	if (!miter->__remaining) {
		struct scatterlist *sg;
		unsigned long pgoffset;

		if (!__sg_page_iter_next(&miter->piter))
ffffffff812cdd05:	48 8d 7f 20          	lea    0x20(%rdi),%rdi
	miter->__flags = flags;
}
EXPORT_SYMBOL(sg_miter_start);

static bool sg_miter_get_next_page(struct sg_mapping_iter *miter)
{
ffffffff812cdd09:	48 89 e5             	mov    %rsp,%rbp
	if (!miter->__remaining) {
		struct scatterlist *sg;
		unsigned long pgoffset;

		if (!__sg_page_iter_next(&miter->piter))
ffffffff812cdd0c:	e8 7f ff ff ff       	callq  ffffffff812cdc90 <__sg_page_iter_next>
ffffffff812cdd11:	84 c0                	test   %al,%al
ffffffff812cdd13:	74 33                	je     ffffffff812cdd48 <sg_miter_get_next_page+0x52>
			return false;

		sg = miter->piter.sg;
		pgoffset = miter->piter.sg_pgoffset;
ffffffff812cdd15:	8b 4e 28             	mov    0x28(%rsi),%ecx
		unsigned long pgoffset;

		if (!__sg_page_iter_next(&miter->piter))
			return false;

		sg = miter->piter.sg;
ffffffff812cdd18:	48 8b 7e 20          	mov    0x20(%rsi),%rdi
		pgoffset = miter->piter.sg_pgoffset;

		miter->__offset = pgoffset ? 0 : sg->offset;
ffffffff812cdd1c:	85 c9                	test   %ecx,%ecx
ffffffff812cdd1e:	75 04                	jne    ffffffff812cdd24 <sg_miter_get_next_page+0x2e>
ffffffff812cdd20:	44 8b 47 08          	mov    0x8(%rdi),%r8d
ffffffff812cdd24:	44 89 46 38          	mov    %r8d,0x38(%rsi)
		miter->__remaining = sg->offset + sg->length -
				(pgoffset << PAGE_SHIFT) - miter->__offset;
		miter->__remaining = min_t(unsigned long, miter->__remaining,
ffffffff812cdd28:	8b 57 0c             	mov    0xc(%rdi),%edx
ffffffff812cdd2b:	c1 e1 0c             	shl    $0xc,%ecx
ffffffff812cdd2e:	03 57 08             	add    0x8(%rdi),%edx
ffffffff812cdd31:	29 ca                	sub    %ecx,%edx
ffffffff812cdd33:	b9 00 10 00 00       	mov    $0x1000,%ecx
ffffffff812cdd38:	44 29 c2             	sub    %r8d,%edx
ffffffff812cdd3b:	4c 29 c1             	sub    %r8,%rcx
ffffffff812cdd3e:	48 39 ca             	cmp    %rcx,%rdx
ffffffff812cdd41:	48 0f 47 d1          	cmova  %rcx,%rdx
ffffffff812cdd45:	89 56 3c             	mov    %edx,0x3c(%rsi)
					   PAGE_SIZE - miter->__offset);
	}

	return true;
}
ffffffff812cdd48:	5d                   	pop    %rbp
ffffffff812cdd49:	c3                   	retq   

ffffffff812cdd4a <sg_miter_skip>:
 * Returns:
 *   true if @miter contains the valid mapping.  false if end of sg
 *   list is reached.
 */
bool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)
{
ffffffff812cdd4a:	55                   	push   %rbp
ffffffff812cdd4b:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cdd4e:	41 54                	push   %r12
ffffffff812cdd50:	53                   	push   %rbx
ffffffff812cdd51:	49 89 f4             	mov    %rsi,%r12
ffffffff812cdd54:	48 89 fb             	mov    %rdi,%rbx
	sg_miter_stop(miter);
ffffffff812cdd57:	e8 c4 fe ff ff       	callq  ffffffff812cdc20 <sg_miter_stop>

	while (offset) {
ffffffff812cdd5c:	4d 85 e4             	test   %r12,%r12
ffffffff812cdd5f:	74 26                	je     ffffffff812cdd87 <sg_miter_skip+0x3d>
		off_t consumed;

		if (!sg_miter_get_next_page(miter))
ffffffff812cdd61:	48 89 df             	mov    %rbx,%rdi
ffffffff812cdd64:	e8 8d ff ff ff       	callq  ffffffff812cdcf6 <sg_miter_get_next_page>
ffffffff812cdd69:	84 c0                	test   %al,%al
ffffffff812cdd6b:	74 1c                	je     ffffffff812cdd89 <sg_miter_skip+0x3f>
			return false;

		consumed = min_t(off_t, offset, miter->__remaining);
ffffffff812cdd6d:	8b 43 3c             	mov    0x3c(%rbx),%eax
ffffffff812cdd70:	4c 39 e0             	cmp    %r12,%rax
ffffffff812cdd73:	48 89 c2             	mov    %rax,%rdx
ffffffff812cdd76:	49 0f 4f c4          	cmovg  %r12,%rax
		miter->__offset += consumed;
ffffffff812cdd7a:	01 43 38             	add    %eax,0x38(%rbx)
		miter->__remaining -= consumed;
ffffffff812cdd7d:	29 c2                	sub    %eax,%edx
		offset -= consumed;
ffffffff812cdd7f:	49 29 c4             	sub    %rax,%r12
		if (!sg_miter_get_next_page(miter))
			return false;

		consumed = min_t(off_t, offset, miter->__remaining);
		miter->__offset += consumed;
		miter->__remaining -= consumed;
ffffffff812cdd82:	89 53 3c             	mov    %edx,0x3c(%rbx)
ffffffff812cdd85:	eb d5                	jmp    ffffffff812cdd5c <sg_miter_skip+0x12>
		offset -= consumed;
	}

	return true;
ffffffff812cdd87:	b0 01                	mov    $0x1,%al
}
ffffffff812cdd89:	5b                   	pop    %rbx
ffffffff812cdd8a:	41 5c                	pop    %r12
ffffffff812cdd8c:	5d                   	pop    %rbp
ffffffff812cdd8d:	c3                   	retq   

ffffffff812cdd8e <sg_assign_page.isra.12>:
 *   variant.
 *
 **/
static inline void sg_assign_page(struct scatterlist *sg, struct page *page)
{
	unsigned long page_link = sg->page_link & 0x3;
ffffffff812cdd8e:	48 8b 07             	mov    (%rdi),%rax
 * Description:
 *   Assign page to sg entry. Also see sg_set_page(), the most commonly used
 *   variant.
 *
 **/
static inline void sg_assign_page(struct scatterlist *sg, struct page *page)
ffffffff812cdd91:	55                   	push   %rbp
ffffffff812cdd92:	48 89 e5             	mov    %rsp,%rbp
{
	unsigned long page_link = sg->page_link & 0x3;
ffffffff812cdd95:	83 e0 03             	and    $0x3,%eax

	/*
	 * In order for the low bit stealing approach to work, pages
	 * must be aligned at a 32-bit boundary as a minimum.
	 */
	BUG_ON((unsigned long) page & 0x03);
ffffffff812cdd98:	40 f6 c6 03          	test   $0x3,%sil
ffffffff812cdd9c:	74 02                	je     ffffffff812cdda0 <sg_assign_page.isra.12+0x12>
ffffffff812cdd9e:	0f 0b                	ud2    
#ifdef CONFIG_DEBUG_SG
	BUG_ON(sg->sg_magic != SG_MAGIC);
	BUG_ON(sg_is_chain(sg));
#endif
	sg->page_link = page_link | (unsigned long) page;
ffffffff812cdda0:	48 09 c6             	or     %rax,%rsi
ffffffff812cdda3:	48 89 37             	mov    %rsi,(%rdi)
}
ffffffff812cdda6:	5d                   	pop    %rbp
ffffffff812cdda7:	c3                   	retq   

ffffffff812cdda8 <sg_miter_next>:
 * Returns:
 *   true if @miter contains the next mapping.  false if end of sg
 *   list is reached.
 */
bool sg_miter_next(struct sg_mapping_iter *miter)
{
ffffffff812cdda8:	55                   	push   %rbp
ffffffff812cdda9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cddac:	53                   	push   %rbx
ffffffff812cddad:	51                   	push   %rcx
ffffffff812cddae:	48 89 fb             	mov    %rdi,%rbx
	sg_miter_stop(miter);
ffffffff812cddb1:	e8 6a fe ff ff       	callq  ffffffff812cdc20 <sg_miter_stop>

	/*
	 * Get to the next page if necessary.
	 * __remaining, __offset is adjusted by sg_miter_stop
	 */
	if (!sg_miter_get_next_page(miter))
ffffffff812cddb6:	48 89 df             	mov    %rbx,%rdi
ffffffff812cddb9:	e8 38 ff ff ff       	callq  ffffffff812cdcf6 <sg_miter_get_next_page>
ffffffff812cddbe:	84 c0                	test   %al,%al
ffffffff812cddc0:	74 62                	je     ffffffff812cde24 <sg_miter_next+0x7c>
ffffffff812cddc2:	48 8b 53 20          	mov    0x20(%rbx),%rdx
 * sg_page_iter_page - get the current page held by the page iterator
 * @piter:	page iterator holding the page
 */
static inline struct page *sg_page_iter_page(struct sg_page_iter *piter)
{
	return nth_page(sg_page(piter->sg), piter->sg_pgoffset);
ffffffff812cddc6:	8b 4b 28             	mov    0x28(%rbx),%ecx
ffffffff812cddc9:	48 be 00 00 00 00 00 	movabs $0x160000000000,%rsi
ffffffff812cddd0:	16 00 00 
ffffffff812cddd3:	48 8b 12             	mov    (%rdx),%rdx
ffffffff812cddd6:	48 83 e2 fc          	and    $0xfffffffffffffffc,%rdx
ffffffff812cddda:	48 01 f2             	add    %rsi,%rdx
ffffffff812cdddd:	48 c1 fa 06          	sar    $0x6,%rdx
ffffffff812cdde1:	48 01 ca             	add    %rcx,%rdx
ffffffff812cdde4:	48 c1 e2 06          	shl    $0x6,%rdx
ffffffff812cdde8:	48 89 d1             	mov    %rdx,%rcx
ffffffff812cddeb:	48 29 f1             	sub    %rsi,%rcx
		return false;

	miter->page = sg_page_iter_page(&miter->piter);
	miter->consumed = miter->length = miter->__remaining;

	if (miter->__flags & SG_MITER_ATOMIC)
ffffffff812cddee:	f6 43 40 01          	testb  $0x1,0x40(%rbx)
	 * __remaining, __offset is adjusted by sg_miter_stop
	 */
	if (!sg_miter_get_next_page(miter))
		return false;

	miter->page = sg_page_iter_page(&miter->piter);
ffffffff812cddf2:	48 89 0b             	mov    %rcx,(%rbx)
	miter->consumed = miter->length = miter->__remaining;
ffffffff812cddf5:	8b 4b 3c             	mov    0x3c(%rbx),%ecx
ffffffff812cddf8:	48 89 4b 10          	mov    %rcx,0x10(%rbx)
ffffffff812cddfc:	48 89 4b 18          	mov    %rcx,0x18(%rbx)

	if (miter->__flags & SG_MITER_ATOMIC)
ffffffff812cde00:	74 07                	je     ffffffff812cde09 <sg_miter_next+0x61>
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	raw_cpu_add_4(__preempt_count, val);
ffffffff812cde02:	65 ff 05 8f cb d3 7e 	incl   %gs:0x7ed3cb8f(%rip)        # a998 <__preempt_count>
		miter->addr = kmap_atomic(miter->page) + miter->__offset;
	else
		miter->addr = kmap(miter->page) + miter->__offset;
ffffffff812cde09:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
ffffffff812cde10:	88 ff ff 
ffffffff812cde13:	48 c1 e2 06          	shl    $0x6,%rdx
ffffffff812cde17:	48 01 ca             	add    %rcx,%rdx
ffffffff812cde1a:	8b 4b 38             	mov    0x38(%rbx),%ecx
ffffffff812cde1d:	48 01 ca             	add    %rcx,%rdx
ffffffff812cde20:	48 89 53 08          	mov    %rdx,0x8(%rbx)

	return true;
}
ffffffff812cde24:	5a                   	pop    %rdx
ffffffff812cde25:	5b                   	pop    %rbx
ffffffff812cde26:	5d                   	pop    %rbp
ffffffff812cde27:	c3                   	retq   

ffffffff812cde28 <sg_copy_buffer>:
 *
 **/
static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
			     void *buf, size_t buflen, off_t skip,
			     bool to_buffer)
{
ffffffff812cde28:	55                   	push   %rbp
ffffffff812cde29:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cde2c:	41 57                	push   %r15
ffffffff812cde2e:	41 56                	push   %r14
ffffffff812cde30:	41 55                	push   %r13
ffffffff812cde32:	41 54                	push   %r12
ffffffff812cde34:	49 89 d5             	mov    %rdx,%r13
ffffffff812cde37:	53                   	push   %rbx
ffffffff812cde38:	48 89 cb             	mov    %rcx,%rbx
	if (to_buffer)
		sg_flags |= SG_MITER_FROM_SG;
	else
		sg_flags |= SG_MITER_TO_SG;

	sg_miter_start(&miter, sgl, nents, sg_flags);
ffffffff812cde3b:	89 f2                	mov    %esi,%edx
ffffffff812cde3d:	48 89 fe             	mov    %rdi,%rsi
ffffffff812cde40:	48 8d 7d 88          	lea    -0x78(%rbp),%rdi
 *
 **/
static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
			     void *buf, size_t buflen, off_t skip,
			     bool to_buffer)
{
ffffffff812cde44:	45 88 ce             	mov    %r9b,%r14b
ffffffff812cde47:	48 83 ec 68          	sub    $0x68,%rsp
	unsigned int sg_flags = SG_MITER_ATOMIC;

	if (to_buffer)
		sg_flags |= SG_MITER_FROM_SG;
	else
		sg_flags |= SG_MITER_TO_SG;
ffffffff812cde4b:	41 80 f9 01          	cmp    $0x1,%r9b
 *
 **/
static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
			     void *buf, size_t buflen, off_t skip,
			     bool to_buffer)
{
ffffffff812cde4f:	4c 89 85 78 ff ff ff 	mov    %r8,-0x88(%rbp)
	unsigned int sg_flags = SG_MITER_ATOMIC;

	if (to_buffer)
		sg_flags |= SG_MITER_FROM_SG;
	else
		sg_flags |= SG_MITER_TO_SG;
ffffffff812cde56:	19 c9                	sbb    %ecx,%ecx
ffffffff812cde58:	45 31 ff             	xor    %r15d,%r15d
ffffffff812cde5b:	83 e1 fe             	and    $0xfffffffe,%ecx
ffffffff812cde5e:	83 c1 05             	add    $0x5,%ecx

	sg_miter_start(&miter, sgl, nents, sg_flags);
ffffffff812cde61:	e8 7d fd ff ff       	callq  ffffffff812cdbe3 <sg_miter_start>

	if (!sg_miter_skip(&miter, skip))
ffffffff812cde66:	4c 8b 85 78 ff ff ff 	mov    -0x88(%rbp),%r8
ffffffff812cde6d:	48 8d 7d 88          	lea    -0x78(%rbp),%rdi
ffffffff812cde71:	4c 89 c6             	mov    %r8,%rsi
ffffffff812cde74:	e8 d1 fe ff ff       	callq  ffffffff812cdd4a <sg_miter_skip>
ffffffff812cde79:	84 c0                	test   %al,%al
ffffffff812cde7b:	74 66                	je     ffffffff812cdee3 <sg_copy_buffer+0xbb>
	/*
	 * "=rm" is safe here, because "pop" adjusts the stack before
	 * it evaluates its effective address -- this is part of the
	 * documented behavior of the "pop" instruction.
	 */
	asm volatile("# __raw_save_flags\n\t"
ffffffff812cde7d:	9c                   	pushfq 
ffffffff812cde7e:	8f 85 78 ff ff ff    	popq   -0x88(%rbp)
		     :"memory", "cc");
}

static inline void native_irq_disable(void)
{
	asm volatile("cli": : :"memory");
ffffffff812cde84:	fa                   	cli    
 **/
static size_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents,
			     void *buf, size_t buflen, off_t skip,
			     bool to_buffer)
{
	unsigned int offset = 0;
ffffffff812cde85:	45 31 e4             	xor    %r12d,%r12d
	if (!sg_miter_skip(&miter, skip))
		return false;

	local_irq_save(flags);

	while (sg_miter_next(&miter) && offset < buflen) {
ffffffff812cde88:	48 8d 7d 88          	lea    -0x78(%rbp),%rdi
ffffffff812cde8c:	45 89 e7             	mov    %r12d,%r15d
ffffffff812cde8f:	e8 14 ff ff ff       	callq  ffffffff812cdda8 <sg_miter_next>
ffffffff812cde94:	84 c0                	test   %al,%al
ffffffff812cde96:	75 12                	jne    ffffffff812cdeaa <sg_copy_buffer+0x82>
			memcpy(miter.addr, buf + offset, len);

		offset += len;
	}

	sg_miter_stop(&miter);
ffffffff812cde98:	48 8d 7d 88          	lea    -0x78(%rbp),%rdi
ffffffff812cde9c:	e8 7f fd ff ff       	callq  ffffffff812cdc20 <sg_miter_stop>
	return flags;
}

static inline void native_restore_fl(unsigned long flags)
{
	asm volatile("push %0 ; popf"
ffffffff812cdea1:	ff b5 78 ff ff ff    	pushq  -0x88(%rbp)
ffffffff812cdea7:	9d                   	popfq  

	local_irq_restore(flags);
	return offset;
ffffffff812cdea8:	eb 39                	jmp    ffffffff812cdee3 <sg_copy_buffer+0xbb>
	if (!sg_miter_skip(&miter, skip))
		return false;

	local_irq_save(flags);

	while (sg_miter_next(&miter) && offset < buflen) {
ffffffff812cdeaa:	49 39 df             	cmp    %rbx,%r15
ffffffff812cdead:	73 e9                	jae    ffffffff812cde98 <sg_copy_buffer+0x70>
		unsigned int len;

		len = min(miter.length, buflen - offset);
ffffffff812cdeaf:	48 89 d8             	mov    %rbx,%rax
ffffffff812cdeb2:	4f 8d 4c 3d 00       	lea    0x0(%r13,%r15,1),%r9
ffffffff812cdeb7:	4c 8b 45 90          	mov    -0x70(%rbp),%r8
ffffffff812cdebb:	4c 29 f8             	sub    %r15,%rax
ffffffff812cdebe:	48 39 45 98          	cmp    %rax,-0x68(%rbp)
ffffffff812cdec2:	48 0f 46 45 98       	cmovbe -0x68(%rbp),%rax

		if (to_buffer)
ffffffff812cdec7:	45 84 f6             	test   %r14b,%r14b
ffffffff812cdeca:	89 c1                	mov    %eax,%ecx
ffffffff812cdecc:	74 08                	je     ffffffff812cded6 <sg_copy_buffer+0xae>
			memcpy(buf + offset, miter.addr, len);
ffffffff812cdece:	4c 89 cf             	mov    %r9,%rdi
ffffffff812cded1:	4c 89 c6             	mov    %r8,%rsi
ffffffff812cded4:	eb 06                	jmp    ffffffff812cdedc <sg_copy_buffer+0xb4>
		else
			memcpy(miter.addr, buf + offset, len);
ffffffff812cded6:	4c 89 c7             	mov    %r8,%rdi
ffffffff812cded9:	4c 89 ce             	mov    %r9,%rsi
ffffffff812cdedc:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)

		offset += len;
ffffffff812cdede:	41 01 c4             	add    %eax,%r12d
ffffffff812cdee1:	eb a5                	jmp    ffffffff812cde88 <sg_copy_buffer+0x60>

	sg_miter_stop(&miter);

	local_irq_restore(flags);
	return offset;
}
ffffffff812cdee3:	48 83 c4 68          	add    $0x68,%rsp
ffffffff812cdee7:	4c 89 f8             	mov    %r15,%rax
ffffffff812cdeea:	5b                   	pop    %rbx
ffffffff812cdeeb:	41 5c                	pop    %r12
ffffffff812cdeed:	41 5d                	pop    %r13
ffffffff812cdeef:	41 5e                	pop    %r14
ffffffff812cdef1:	41 5f                	pop    %r15
ffffffff812cdef3:	5d                   	pop    %rbp
ffffffff812cdef4:	c3                   	retq   

ffffffff812cdef5 <sg_copy_from_buffer>:
 * Returns the number of copied bytes.
 *
 **/
size_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,
			   void *buf, size_t buflen)
{
ffffffff812cdef5:	55                   	push   %rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, 0, false);
ffffffff812cdef6:	45 31 c9             	xor    %r9d,%r9d
ffffffff812cdef9:	45 31 c0             	xor    %r8d,%r8d
 * Returns the number of copied bytes.
 *
 **/
size_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,
			   void *buf, size_t buflen)
{
ffffffff812cdefc:	48 89 e5             	mov    %rsp,%rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, 0, false);
ffffffff812cdeff:	e8 24 ff ff ff       	callq  ffffffff812cde28 <sg_copy_buffer>
}
ffffffff812cdf04:	5d                   	pop    %rbp
ffffffff812cdf05:	c3                   	retq   

ffffffff812cdf06 <sg_copy_to_buffer>:
 * Returns the number of copied bytes.
 *
 **/
size_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents,
			 void *buf, size_t buflen)
{
ffffffff812cdf06:	55                   	push   %rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, 0, true);
ffffffff812cdf07:	41 b9 01 00 00 00    	mov    $0x1,%r9d
ffffffff812cdf0d:	45 31 c0             	xor    %r8d,%r8d
 * Returns the number of copied bytes.
 *
 **/
size_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents,
			 void *buf, size_t buflen)
{
ffffffff812cdf10:	48 89 e5             	mov    %rsp,%rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, 0, true);
ffffffff812cdf13:	e8 10 ff ff ff       	callq  ffffffff812cde28 <sg_copy_buffer>
}
ffffffff812cdf18:	5d                   	pop    %rbp
ffffffff812cdf19:	c3                   	retq   

ffffffff812cdf1a <sg_pcopy_from_buffer>:
 * Returns the number of copied bytes.
 *
 **/
size_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,
			    void *buf, size_t buflen, off_t skip)
{
ffffffff812cdf1a:	55                   	push   %rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, skip, false);
ffffffff812cdf1b:	45 31 c9             	xor    %r9d,%r9d
 * Returns the number of copied bytes.
 *
 **/
size_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,
			    void *buf, size_t buflen, off_t skip)
{
ffffffff812cdf1e:	48 89 e5             	mov    %rsp,%rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, skip, false);
ffffffff812cdf21:	e8 02 ff ff ff       	callq  ffffffff812cde28 <sg_copy_buffer>
}
ffffffff812cdf26:	5d                   	pop    %rbp
ffffffff812cdf27:	c3                   	retq   

ffffffff812cdf28 <sg_pcopy_to_buffer>:
 * Returns the number of copied bytes.
 *
 **/
size_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,
			  void *buf, size_t buflen, off_t skip)
{
ffffffff812cdf28:	55                   	push   %rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, skip, true);
ffffffff812cdf29:	41 b9 01 00 00 00    	mov    $0x1,%r9d
 * Returns the number of copied bytes.
 *
 **/
size_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,
			  void *buf, size_t buflen, off_t skip)
{
ffffffff812cdf2f:	48 89 e5             	mov    %rsp,%rbp
	return sg_copy_buffer(sgl, nents, buf, buflen, skip, true);
ffffffff812cdf32:	e8 f1 fe ff ff       	callq  ffffffff812cde28 <sg_copy_buffer>
}
ffffffff812cdf37:	5d                   	pop    %rbp
ffffffff812cdf38:	c3                   	retq   

ffffffff812cdf39 <sg_init_table>:
 *   used only on the last table part.
 *
 **/
void sg_init_table(struct scatterlist *sgl, unsigned int nents)
{
	memset(sgl, 0, sizeof(*sgl) * nents);
ffffffff812cdf39:	89 f1                	mov    %esi,%ecx
ffffffff812cdf3b:	31 c0                	xor    %eax,%eax
 *   If this is part of a chained sg table, sg_mark_end() should be
 *   used only on the last table part.
 *
 **/
void sg_init_table(struct scatterlist *sgl, unsigned int nents)
{
ffffffff812cdf3d:	55                   	push   %rbp
	memset(sgl, 0, sizeof(*sgl) * nents);
ffffffff812cdf3e:	48 c1 e1 05          	shl    $0x5,%rcx
 *   If this is part of a chained sg table, sg_mark_end() should be
 *   used only on the last table part.
 *
 **/
void sg_init_table(struct scatterlist *sgl, unsigned int nents)
{
ffffffff812cdf42:	48 89 fa             	mov    %rdi,%rdx
	memset(sgl, 0, sizeof(*sgl) * nents);
ffffffff812cdf45:	f3 aa                	rep stos %al,%es:(%rdi)
		unsigned int i;
		for (i = 0; i < nents; i++)
			sgl[i].sg_magic = SG_MAGIC;
	}
#endif
	sg_mark_end(&sgl[nents - 1]);
ffffffff812cdf47:	8d 7e ff             	lea    -0x1(%rsi),%edi
 *   If this is part of a chained sg table, sg_mark_end() should be
 *   used only on the last table part.
 *
 **/
void sg_init_table(struct scatterlist *sgl, unsigned int nents)
{
ffffffff812cdf4a:	48 89 e5             	mov    %rsp,%rbp
		unsigned int i;
		for (i = 0; i < nents; i++)
			sgl[i].sg_magic = SG_MAGIC;
	}
#endif
	sg_mark_end(&sgl[nents - 1]);
ffffffff812cdf4d:	48 c1 e7 05          	shl    $0x5,%rdi
ffffffff812cdf51:	48 01 d7             	add    %rdx,%rdi
#endif
	/*
	 * Set termination bit, clear potential chain bit
	 */
	sg->page_link |= 0x02;
	sg->page_link &= ~0x01;
ffffffff812cdf54:	48 8b 07             	mov    (%rdi),%rax
ffffffff812cdf57:	48 83 e0 fe          	and    $0xfffffffffffffffe,%rax
ffffffff812cdf5b:	48 83 c8 02          	or     $0x2,%rax
ffffffff812cdf5f:	48 89 07             	mov    %rax,(%rdi)
}
ffffffff812cdf62:	5d                   	pop    %rbp
ffffffff812cdf63:	c3                   	retq   

ffffffff812cdf64 <__sg_alloc_table>:
 *
 **/
int __sg_alloc_table(struct sg_table *table, unsigned int nents,
		     unsigned int max_ents, struct scatterlist *first_chunk,
		     gfp_t gfp_mask, sg_alloc_fn *alloc_fn)
{
ffffffff812cdf64:	55                   	push   %rbp
	struct scatterlist *sg, *prv;
	unsigned int left;

	memset(table, 0, sizeof(*table));
ffffffff812cdf65:	31 c0                	xor    %eax,%eax
 *
 **/
int __sg_alloc_table(struct sg_table *table, unsigned int nents,
		     unsigned int max_ents, struct scatterlist *first_chunk,
		     gfp_t gfp_mask, sg_alloc_fn *alloc_fn)
{
ffffffff812cdf67:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cdf6a:	41 57                	push   %r15
ffffffff812cdf6c:	41 56                	push   %r14
ffffffff812cdf6e:	41 55                	push   %r13
ffffffff812cdf70:	41 54                	push   %r12
ffffffff812cdf72:	41 89 f4             	mov    %esi,%r12d
ffffffff812cdf75:	53                   	push   %rbx
ffffffff812cdf76:	48 89 ce             	mov    %rcx,%rsi
	struct scatterlist *sg, *prv;
	unsigned int left;

	memset(table, 0, sizeof(*table));
ffffffff812cdf79:	b9 04 00 00 00       	mov    $0x4,%ecx
 *
 **/
int __sg_alloc_table(struct sg_table *table, unsigned int nents,
		     unsigned int max_ents, struct scatterlist *first_chunk,
		     gfp_t gfp_mask, sg_alloc_fn *alloc_fn)
{
ffffffff812cdf7e:	48 89 fb             	mov    %rdi,%rbx
ffffffff812cdf81:	48 83 ec 38          	sub    $0x38,%rsp
	struct scatterlist *sg, *prv;
	unsigned int left;

	memset(table, 0, sizeof(*table));

	if (nents == 0)
ffffffff812cdf85:	45 85 e4             	test   %r12d,%r12d
 *
 **/
int __sg_alloc_table(struct sg_table *table, unsigned int nents,
		     unsigned int max_ents, struct scatterlist *first_chunk,
		     gfp_t gfp_mask, sg_alloc_fn *alloc_fn)
{
ffffffff812cdf88:	44 89 45 c0          	mov    %r8d,-0x40(%rbp)
	struct scatterlist *sg, *prv;
	unsigned int left;

	memset(table, 0, sizeof(*table));
ffffffff812cdf8c:	f3 ab                	rep stos %eax,%es:(%rdi)

	if (nents == 0)
		return -EINVAL;
ffffffff812cdf8e:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
	struct scatterlist *sg, *prv;
	unsigned int left;

	memset(table, 0, sizeof(*table));

	if (nents == 0)
ffffffff812cdf93:	0f 84 f8 00 00 00    	je     ffffffff812ce091 <__sg_alloc_table+0x12d>
#endif

	/*
	 * offset and length are unused for chain entry.  Clear them.
	 */
	prv[prv_nents - 1].offset = 0;
ffffffff812cdf99:	44 8d 6a ff          	lea    -0x1(%rdx),%r13d
ffffffff812cdf9d:	41 89 d7             	mov    %edx,%r15d
ffffffff812cdfa0:	45 31 f6             	xor    %r14d,%r14d
ffffffff812cdfa3:	44 89 6d c4          	mov    %r13d,-0x3c(%rbp)
ffffffff812cdfa7:	49 c1 e5 05          	shl    $0x5,%r13
ffffffff812cdfab:	4c 89 6d b8          	mov    %r13,-0x48(%rbp)
	left = nents;
	prv = NULL;
	do {
		unsigned int sg_size, alloc_size = left;

		if (alloc_size > max_ents) {
ffffffff812cdfaf:	45 39 fc             	cmp    %r15d,%r12d
ffffffff812cdfb2:	77 08                	ja     ffffffff812cdfbc <__sg_alloc_table+0x58>
ffffffff812cdfb4:	45 89 e3             	mov    %r12d,%r11d
ffffffff812cdfb7:	45 89 e5             	mov    %r12d,%r13d
ffffffff812cdfba:	eb 07                	jmp    ffffffff812cdfc3 <__sg_alloc_table+0x5f>
			alloc_size = max_ents;
			sg_size = alloc_size - 1;
ffffffff812cdfbc:	44 8b 6d c4          	mov    -0x3c(%rbp),%r13d
ffffffff812cdfc0:	45 89 fb             	mov    %r15d,%r11d
		} else
			sg_size = alloc_size;

		left -= sg_size;
ffffffff812cdfc3:	45 29 ec             	sub    %r13d,%r12d

		if (first_chunk) {
ffffffff812cdfc6:	48 85 f6             	test   %rsi,%rsi
ffffffff812cdfc9:	48 89 75 c8          	mov    %rsi,-0x38(%rbp)
ffffffff812cdfcd:	75 40                	jne    ffffffff812ce00f <__sg_alloc_table+0xab>
			sg = first_chunk;
			first_chunk = NULL;
		} else {
			sg = alloc_fn(alloc_size, gfp_mask);
ffffffff812cdfcf:	44 89 df             	mov    %r11d,%edi
ffffffff812cdfd2:	44 89 5d ac          	mov    %r11d,-0x54(%rbp)
ffffffff812cdfd6:	4c 89 4d b0          	mov    %r9,-0x50(%rbp)
ffffffff812cdfda:	8b 75 c0             	mov    -0x40(%rbp),%esi
ffffffff812cdfdd:	41 ff d1             	callq  *%r9
		}
		if (unlikely(!sg)) {
ffffffff812cdfe0:	48 85 c0             	test   %rax,%rax

		if (first_chunk) {
			sg = first_chunk;
			first_chunk = NULL;
		} else {
			sg = alloc_fn(alloc_size, gfp_mask);
ffffffff812cdfe3:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
		}
		if (unlikely(!sg)) {
ffffffff812cdfe7:	4c 8b 4d b0          	mov    -0x50(%rbp),%r9
ffffffff812cdfeb:	44 8b 5d ac          	mov    -0x54(%rbp),%r11d
ffffffff812cdfef:	75 1e                	jne    ffffffff812ce00f <__sg_alloc_table+0xab>
			 * Adjust entry count to reflect that the last
			 * entry of the previous table won't be used for
			 * linkage.  Without this, sg_kfree() may get
			 * confused.
			 */
			if (prv)
ffffffff812cdff1:	4d 85 f6             	test   %r14,%r14
				table->nents = ++table->orig_nents;

 			return -ENOMEM;
ffffffff812cdff4:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
			 * Adjust entry count to reflect that the last
			 * entry of the previous table won't be used for
			 * linkage.  Without this, sg_kfree() may get
			 * confused.
			 */
			if (prv)
ffffffff812cdff9:	0f 84 92 00 00 00    	je     ffffffff812ce091 <__sg_alloc_table+0x12d>
				table->nents = ++table->orig_nents;
ffffffff812cdfff:	8b 53 0c             	mov    0xc(%rbx),%edx
ffffffff812ce002:	ff c2                	inc    %edx
ffffffff812ce004:	89 53 0c             	mov    %edx,0xc(%rbx)
ffffffff812ce007:	89 53 08             	mov    %edx,0x8(%rbx)
ffffffff812ce00a:	e9 82 00 00 00       	jmpq   ffffffff812ce091 <__sg_alloc_table+0x12d>

 			return -ENOMEM;
		}

		sg_init_table(sg, alloc_size);
ffffffff812ce00f:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
ffffffff812ce013:	44 89 de             	mov    %r11d,%esi
ffffffff812ce016:	4c 89 4d b0          	mov    %r9,-0x50(%rbp)
ffffffff812ce01a:	e8 1a ff ff ff       	callq  ffffffff812cdf39 <sg_init_table>
		table->nents = table->orig_nents += sg_size;
ffffffff812ce01f:	8b 43 0c             	mov    0xc(%rbx),%eax

		/*
		 * If this is the first mapping, assign the sg table header.
		 * If this is not the first mapping, chain previous part.
		 */
		if (prv)
ffffffff812ce022:	4c 8b 4d b0          	mov    -0x50(%rbp),%r9

 			return -ENOMEM;
		}

		sg_init_table(sg, alloc_size);
		table->nents = table->orig_nents += sg_size;
ffffffff812ce026:	44 01 e8             	add    %r13d,%eax

		/*
		 * If this is the first mapping, assign the sg table header.
		 * If this is not the first mapping, chain previous part.
		 */
		if (prv)
ffffffff812ce029:	4d 85 f6             	test   %r14,%r14

 			return -ENOMEM;
		}

		sg_init_table(sg, alloc_size);
		table->nents = table->orig_nents += sg_size;
ffffffff812ce02c:	89 43 0c             	mov    %eax,0xc(%rbx)
ffffffff812ce02f:	89 43 08             	mov    %eax,0x8(%rbx)

		/*
		 * If this is the first mapping, assign the sg table header.
		 * If this is not the first mapping, chain previous part.
		 */
		if (prv)
ffffffff812ce032:	74 27                	je     ffffffff812ce05b <__sg_alloc_table+0xf7>
ffffffff812ce034:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
ffffffff812ce038:	49 8d 0c 06          	lea    (%r14,%rax,1),%rcx

	/*
	 * Set lowest bit to indicate a link pointer, and make sure to clear
	 * the termination bit if it happens to be set.
	 */
	prv[prv_nents - 1].page_link = ((unsigned long) sgl | 0x01) & ~0x02;
ffffffff812ce03c:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
#endif

	/*
	 * offset and length are unused for chain entry.  Clear them.
	 */
	prv[prv_nents - 1].offset = 0;
ffffffff812ce040:	c7 41 08 00 00 00 00 	movl   $0x0,0x8(%rcx)
	prv[prv_nents - 1].length = 0;
ffffffff812ce047:	c7 41 0c 00 00 00 00 	movl   $0x0,0xc(%rcx)

	/*
	 * Set lowest bit to indicate a link pointer, and make sure to clear
	 * the termination bit if it happens to be set.
	 */
	prv[prv_nents - 1].page_link = ((unsigned long) sgl | 0x01) & ~0x02;
ffffffff812ce04e:	48 83 e0 fc          	and    $0xfffffffffffffffc,%rax
ffffffff812ce052:	48 83 c8 01          	or     $0x1,%rax
ffffffff812ce056:	48 89 01             	mov    %rax,(%rcx)
ffffffff812ce059:	eb 07                	jmp    ffffffff812ce062 <__sg_alloc_table+0xfe>
			sg_chain(prv, max_ents, sg);
		else
			table->sgl = sg;
ffffffff812ce05b:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
ffffffff812ce05f:	48 89 03             	mov    %rax,(%rbx)
ffffffff812ce062:	31 f6                	xor    %esi,%esi

		/*
		 * If no more entries after this one, mark the end
		 */
		if (!left)
ffffffff812ce064:	45 85 e4             	test   %r12d,%r12d
ffffffff812ce067:	74 09                	je     ffffffff812ce072 <__sg_alloc_table+0x10e>
ffffffff812ce069:	4c 8b 75 c8          	mov    -0x38(%rbp),%r14
ffffffff812ce06d:	e9 3d ff ff ff       	jmpq   ffffffff812cdfaf <__sg_alloc_table+0x4b>
			sg_mark_end(&sg[sg_size - 1]);
ffffffff812ce072:	41 8d 45 ff          	lea    -0x1(%r13),%eax
ffffffff812ce076:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
ffffffff812ce07a:	48 c1 e0 05          	shl    $0x5,%rax
ffffffff812ce07e:	49 01 c0             	add    %rax,%r8
#endif
	/*
	 * Set termination bit, clear potential chain bit
	 */
	sg->page_link |= 0x02;
	sg->page_link &= ~0x01;
ffffffff812ce081:	49 8b 00             	mov    (%r8),%rax
ffffffff812ce084:	48 83 e0 fe          	and    $0xfffffffffffffffe,%rax
ffffffff812ce088:	48 83 c8 02          	or     $0x2,%rax
ffffffff812ce08c:	49 89 00             	mov    %rax,(%r8)

		prv = sg;
	} while (left);

	return 0;
ffffffff812ce08f:	31 c0                	xor    %eax,%eax
}
ffffffff812ce091:	48 83 c4 38          	add    $0x38,%rsp
ffffffff812ce095:	5b                   	pop    %rbx
ffffffff812ce096:	41 5c                	pop    %r12
ffffffff812ce098:	41 5d                	pop    %r13
ffffffff812ce09a:	41 5e                	pop    %r14
ffffffff812ce09c:	41 5f                	pop    %r15
ffffffff812ce09e:	5d                   	pop    %rbp
ffffffff812ce09f:	c3                   	retq   

ffffffff812ce0a0 <sg_alloc_table>:
 *    Allocate and initialize an sg table. If @nents@ is larger than
 *    SG_MAX_SINGLE_ALLOC a chained sg table will be setup.
 *
 **/
int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
{
ffffffff812ce0a0:	55                   	push   %rbp
ffffffff812ce0a1:	41 89 d0             	mov    %edx,%r8d
	int ret;

	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
ffffffff812ce0a4:	31 c9                	xor    %ecx,%ecx
ffffffff812ce0a6:	49 c7 c1 bd db 2c 81 	mov    $0xffffffff812cdbbd,%r9
ffffffff812ce0ad:	ba 80 00 00 00       	mov    $0x80,%edx
 *    Allocate and initialize an sg table. If @nents@ is larger than
 *    SG_MAX_SINGLE_ALLOC a chained sg table will be setup.
 *
 **/
int sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)
{
ffffffff812ce0b2:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce0b5:	53                   	push   %rbx
ffffffff812ce0b6:	48 89 fb             	mov    %rdi,%rbx
ffffffff812ce0b9:	48 83 ec 18          	sub    $0x18,%rsp
	int ret;

	ret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,
ffffffff812ce0bd:	e8 a2 fe ff ff       	callq  ffffffff812cdf64 <__sg_alloc_table>
			       NULL, gfp_mask, sg_kmalloc);
	if (unlikely(ret))
ffffffff812ce0c2:	85 c0                	test   %eax,%eax
ffffffff812ce0c4:	74 1c                	je     ffffffff812ce0e2 <sg_alloc_table+0x42>
		__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);
ffffffff812ce0c6:	48 c7 c1 a4 db 2c 81 	mov    $0xffffffff812cdba4,%rcx
ffffffff812ce0cd:	31 d2                	xor    %edx,%edx
ffffffff812ce0cf:	be 80 00 00 00       	mov    $0x80,%esi
ffffffff812ce0d4:	48 89 df             	mov    %rbx,%rdi
ffffffff812ce0d7:	89 45 ec             	mov    %eax,-0x14(%rbp)
ffffffff812ce0da:	e8 2b fa ff ff       	callq  ffffffff812cdb0a <__sg_free_table>
ffffffff812ce0df:	8b 45 ec             	mov    -0x14(%rbp),%eax

	return ret;
}
ffffffff812ce0e2:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812ce0e6:	5b                   	pop    %rbx
ffffffff812ce0e7:	5d                   	pop    %rbp
ffffffff812ce0e8:	c3                   	retq   

ffffffff812ce0e9 <sg_alloc_table_from_pages>:
 */
int sg_alloc_table_from_pages(struct sg_table *sgt,
	struct page **pages, unsigned int n_pages,
	unsigned long offset, unsigned long size,
	gfp_t gfp_mask)
{
ffffffff812ce0e9:	55                   	push   %rbp
ffffffff812ce0ea:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce0ed:	41 57                	push   %r15
ffffffff812ce0ef:	49 89 f7             	mov    %rsi,%r15
ffffffff812ce0f2:	41 56                	push   %r14
ffffffff812ce0f4:	49 89 ce             	mov    %rcx,%r14
ffffffff812ce0f7:	41 55                	push   %r13
ffffffff812ce0f9:	48 8d 4e 08          	lea    0x8(%rsi),%rcx
ffffffff812ce0fd:	41 54                	push   %r12
ffffffff812ce0ff:	41 89 d5             	mov    %edx,%r13d
ffffffff812ce102:	53                   	push   %rbx
ffffffff812ce103:	49 89 fc             	mov    %rdi,%r12
ffffffff812ce106:	41 52                	push   %r10
ffffffff812ce108:	4c 89 c3             	mov    %r8,%rbx
	int ret;
	struct scatterlist *s;

	/* compute number of contiguous chunks */
	chunks = 1;
	for (i = 1; i < n_pages; ++i)
ffffffff812ce10b:	bf 01 00 00 00       	mov    $0x1,%edi
	unsigned int cur_page;
	int ret;
	struct scatterlist *s;

	/* compute number of contiguous chunks */
	chunks = 1;
ffffffff812ce110:	be 01 00 00 00       	mov    $0x1,%esi
	for (i = 1; i < n_pages; ++i)
		if (page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1)
ffffffff812ce115:	49 ba 00 00 00 00 00 	movabs $0x160000000000,%r10
ffffffff812ce11c:	16 00 00 
	int ret;
	struct scatterlist *s;

	/* compute number of contiguous chunks */
	chunks = 1;
	for (i = 1; i < n_pages; ++i)
ffffffff812ce11f:	44 39 ef             	cmp    %r13d,%edi
ffffffff812ce122:	73 2b                	jae    ffffffff812ce14f <sg_alloc_table_from_pages+0x66>
		if (page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1)
ffffffff812ce124:	4c 89 d0             	mov    %r10,%rax
ffffffff812ce127:	48 03 41 f8          	add    -0x8(%rcx),%rax
ffffffff812ce12b:	4d 89 d0             	mov    %r10,%r8
ffffffff812ce12e:	4c 03 01             	add    (%rcx),%r8
ffffffff812ce131:	48 c1 f8 06          	sar    $0x6,%rax
ffffffff812ce135:	48 ff c0             	inc    %rax
ffffffff812ce138:	49 c1 f8 06          	sar    $0x6,%r8
			++chunks;
ffffffff812ce13c:	49 39 c0             	cmp    %rax,%r8
ffffffff812ce13f:	0f 95 c0             	setne  %al
	int ret;
	struct scatterlist *s;

	/* compute number of contiguous chunks */
	chunks = 1;
	for (i = 1; i < n_pages; ++i)
ffffffff812ce142:	ff c7                	inc    %edi
ffffffff812ce144:	48 83 c1 08          	add    $0x8,%rcx
		if (page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1)
			++chunks;
ffffffff812ce148:	0f b6 c0             	movzbl %al,%eax
ffffffff812ce14b:	01 c6                	add    %eax,%esi
ffffffff812ce14d:	eb d0                	jmp    ffffffff812ce11f <sg_alloc_table_from_pages+0x36>

	ret = sg_alloc_table(sgt, chunks, gfp_mask);
ffffffff812ce14f:	44 89 ca             	mov    %r9d,%edx
ffffffff812ce152:	4c 89 e7             	mov    %r12,%rdi
ffffffff812ce155:	e8 46 ff ff ff       	callq  ffffffff812ce0a0 <sg_alloc_table>
	if (unlikely(ret))
ffffffff812ce15a:	85 c0                	test   %eax,%eax
ffffffff812ce15c:	89 c2                	mov    %eax,%edx
ffffffff812ce15e:	0f 85 8e 00 00 00    	jne    ffffffff812ce1f2 <sg_alloc_table_from_pages+0x109>
		return ret;

	/* merging chunks and putting them into the scatterlist */
	cur_page = 0;
	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
ffffffff812ce164:	4d 8b 14 24          	mov    (%r12),%r10
ffffffff812ce168:	45 31 c0             	xor    %r8d,%r8d
		unsigned long chunk_size;
		unsigned int j;

		/* look for the end of the current chunk */
		for (j = cur_page + 1; j < n_pages; ++j)
			if (page_to_pfn(pages[j]) !=
ffffffff812ce16b:	49 b9 00 00 00 00 00 	movabs $0x160000000000,%r9
ffffffff812ce172:	16 00 00 
	if (unlikely(ret))
		return ret;

	/* merging chunks and putting them into the scatterlist */
	cur_page = 0;
	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
ffffffff812ce175:	45 3b 44 24 0c       	cmp    0xc(%r12),%r8d
ffffffff812ce17a:	73 74                	jae    ffffffff812ce1f0 <sg_alloc_table_from_pages+0x107>
		unsigned long chunk_size;
		unsigned int j;

		/* look for the end of the current chunk */
		for (j = cur_page + 1; j < n_pages; ++j)
ffffffff812ce17c:	8d 48 01             	lea    0x1(%rax),%ecx
ffffffff812ce17f:	44 39 e9             	cmp    %r13d,%ecx
ffffffff812ce182:	73 26                	jae    ffffffff812ce1aa <sg_alloc_table_from_pages+0xc1>
			if (page_to_pfn(pages[j]) !=
ffffffff812ce184:	89 ca                	mov    %ecx,%edx
ffffffff812ce186:	4c 89 ce             	mov    %r9,%rsi
ffffffff812ce189:	4c 89 cf             	mov    %r9,%rdi
ffffffff812ce18c:	49 03 34 d7          	add    (%r15,%rdx,8),%rsi
			    page_to_pfn(pages[j - 1]) + 1)
ffffffff812ce190:	8d 51 ff             	lea    -0x1(%rcx),%edx
		unsigned long chunk_size;
		unsigned int j;

		/* look for the end of the current chunk */
		for (j = cur_page + 1; j < n_pages; ++j)
			if (page_to_pfn(pages[j]) !=
ffffffff812ce193:	49 03 3c d7          	add    (%r15,%rdx,8),%rdi
ffffffff812ce197:	48 c1 fe 06          	sar    $0x6,%rsi
ffffffff812ce19b:	48 89 fa             	mov    %rdi,%rdx
ffffffff812ce19e:	48 c1 fa 06          	sar    $0x6,%rdx
ffffffff812ce1a2:	48 ff c2             	inc    %rdx
ffffffff812ce1a5:	48 39 d6             	cmp    %rdx,%rsi
ffffffff812ce1a8:	74 42                	je     ffffffff812ce1ec <sg_alloc_table_from_pages+0x103>
			    page_to_pfn(pages[j - 1]) + 1)
				break;

		chunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;
ffffffff812ce1aa:	89 ca                	mov    %ecx,%edx
 *
 **/
static inline void sg_set_page(struct scatterlist *sg, struct page *page,
			       unsigned int len, unsigned int offset)
{
	sg_assign_page(sg, page);
ffffffff812ce1ac:	4c 89 d7             	mov    %r10,%rdi
ffffffff812ce1af:	29 c2                	sub    %eax,%edx
		sg_set_page(s, pages[cur_page], min(size, chunk_size), offset);
ffffffff812ce1b1:	89 c0                	mov    %eax,%eax
ffffffff812ce1b3:	49 8b 34 c7          	mov    (%r15,%rax,8),%rsi
		for (j = cur_page + 1; j < n_pages; ++j)
			if (page_to_pfn(pages[j]) !=
			    page_to_pfn(pages[j - 1]) + 1)
				break;

		chunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;
ffffffff812ce1b7:	c1 e2 0c             	shl    $0xc,%edx
ffffffff812ce1ba:	4c 29 f2             	sub    %r14,%rdx
ffffffff812ce1bd:	e8 cc fb ff ff       	callq  ffffffff812cdd8e <sg_assign_page.isra.12>
	sg->offset = offset;
	sg->length = len;
ffffffff812ce1c2:	48 39 d3             	cmp    %rdx,%rbx
ffffffff812ce1c5:	48 89 d0             	mov    %rdx,%rax
 **/
static inline void sg_set_page(struct scatterlist *sg, struct page *page,
			       unsigned int len, unsigned int offset)
{
	sg_assign_page(sg, page);
	sg->offset = offset;
ffffffff812ce1c8:	45 89 72 08          	mov    %r14d,0x8(%r10)
	sg->length = len;
ffffffff812ce1cc:	48 0f 46 c3          	cmovbe %rbx,%rax
	if (unlikely(ret))
		return ret;

	/* merging chunks and putting them into the scatterlist */
	cur_page = 0;
	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
ffffffff812ce1d0:	4c 89 d7             	mov    %r10,%rdi
			    page_to_pfn(pages[j - 1]) + 1)
				break;

		chunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;
		sg_set_page(s, pages[cur_page], min(size, chunk_size), offset);
		size -= chunk_size;
ffffffff812ce1d3:	48 29 d3             	sub    %rdx,%rbx
ffffffff812ce1d6:	41 89 42 0c          	mov    %eax,0xc(%r10)
	if (unlikely(ret))
		return ret;

	/* merging chunks and putting them into the scatterlist */
	cur_page = 0;
	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
ffffffff812ce1da:	41 ff c0             	inc    %r8d
				break;

		chunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;
		sg_set_page(s, pages[cur_page], min(size, chunk_size), offset);
		size -= chunk_size;
		offset = 0;
ffffffff812ce1dd:	45 31 f6             	xor    %r14d,%r14d
	if (unlikely(ret))
		return ret;

	/* merging chunks and putting them into the scatterlist */
	cur_page = 0;
	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
ffffffff812ce1e0:	e8 bf f8 ff ff       	callq  ffffffff812cdaa4 <sg_next>
ffffffff812ce1e5:	49 89 c2             	mov    %rax,%r10
				break;

		chunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;
		sg_set_page(s, pages[cur_page], min(size, chunk_size), offset);
		size -= chunk_size;
		offset = 0;
ffffffff812ce1e8:	89 c8                	mov    %ecx,%eax
ffffffff812ce1ea:	eb 89                	jmp    ffffffff812ce175 <sg_alloc_table_from_pages+0x8c>
	for_each_sg(sgt->sgl, s, sgt->orig_nents, i) {
		unsigned long chunk_size;
		unsigned int j;

		/* look for the end of the current chunk */
		for (j = cur_page + 1; j < n_pages; ++j)
ffffffff812ce1ec:	ff c1                	inc    %ecx
ffffffff812ce1ee:	eb 8f                	jmp    ffffffff812ce17f <sg_alloc_table_from_pages+0x96>
		size -= chunk_size;
		offset = 0;
		cur_page = j;
	}

	return 0;
ffffffff812ce1f0:	31 d2                	xor    %edx,%edx
}
ffffffff812ce1f2:	89 d0                	mov    %edx,%eax
ffffffff812ce1f4:	5a                   	pop    %rdx
ffffffff812ce1f5:	5b                   	pop    %rbx
ffffffff812ce1f6:	41 5c                	pop    %r12
ffffffff812ce1f8:	41 5d                	pop    %r13
ffffffff812ce1fa:	41 5e                	pop    %r14
ffffffff812ce1fc:	41 5f                	pop    %r15
ffffffff812ce1fe:	5d                   	pop    %rbp
ffffffff812ce1ff:	c3                   	retq   

ffffffff812ce200 <sg_init_one>:
 * @buf:	 Virtual address for IO
 * @buflen:	 IO length
 *
 **/
void sg_init_one(struct scatterlist *sg, const void *buf, unsigned int buflen)
{
ffffffff812ce200:	55                   	push   %rbp
ffffffff812ce201:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce204:	41 55                	push   %r13
ffffffff812ce206:	41 54                	push   %r12
ffffffff812ce208:	53                   	push   %rbx
ffffffff812ce209:	50                   	push   %rax
ffffffff812ce20a:	49 89 f4             	mov    %rsi,%r12
	sg_init_table(sg, 1);
ffffffff812ce20d:	be 01 00 00 00       	mov    $0x1,%esi
 * @buf:	 Virtual address for IO
 * @buflen:	 IO length
 *
 **/
void sg_init_one(struct scatterlist *sg, const void *buf, unsigned int buflen)
{
ffffffff812ce212:	48 89 fb             	mov    %rdi,%rbx
ffffffff812ce215:	41 89 d5             	mov    %edx,%r13d
	sg_init_table(sg, 1);
ffffffff812ce218:	e8 1c fd ff ff       	callq  ffffffff812cdf39 <sg_init_table>
			      unsigned int buflen)
{
#ifdef CONFIG_DEBUG_SG
	BUG_ON(!virt_addr_valid(buf));
#endif
	sg_set_page(sg, virt_to_page(buf), buflen, offset_in_page(buf));
ffffffff812ce21d:	4c 89 e7             	mov    %r12,%rdi
 **/
static inline void sg_set_page(struct scatterlist *sg, struct page *page,
			       unsigned int len, unsigned int offset)
{
	sg_assign_page(sg, page);
	sg->offset = offset;
ffffffff812ce220:	41 81 e4 ff 0f 00 00 	and    $0xfff,%r12d
			      unsigned int buflen)
{
#ifdef CONFIG_DEBUG_SG
	BUG_ON(!virt_addr_valid(buf));
#endif
	sg_set_page(sg, virt_to_page(buf), buflen, offset_in_page(buf));
ffffffff812ce227:	e8 8a 46 d9 ff       	callq  ffffffff810628b6 <__phys_addr>
 *
 **/
static inline void sg_set_page(struct scatterlist *sg, struct page *page,
			       unsigned int len, unsigned int offset)
{
	sg_assign_page(sg, page);
ffffffff812ce22c:	48 c1 e8 0c          	shr    $0xc,%rax
ffffffff812ce230:	48 be 00 00 00 00 00 	movabs $0xffffea0000000000,%rsi
ffffffff812ce237:	ea ff ff 
ffffffff812ce23a:	48 89 df             	mov    %rbx,%rdi
ffffffff812ce23d:	48 c1 e0 06          	shl    $0x6,%rax
ffffffff812ce241:	48 01 c6             	add    %rax,%rsi
ffffffff812ce244:	e8 45 fb ff ff       	callq  ffffffff812cdd8e <sg_assign_page.isra.12>
	sg->offset = offset;
ffffffff812ce249:	44 89 63 08          	mov    %r12d,0x8(%rbx)
	sg->length = len;
ffffffff812ce24d:	44 89 6b 0c          	mov    %r13d,0xc(%rbx)
	sg_set_buf(sg, buf, buflen);
}
ffffffff812ce251:	5a                   	pop    %rdx
ffffffff812ce252:	5b                   	pop    %rbx
ffffffff812ce253:	41 5c                	pop    %r12
ffffffff812ce255:	41 5d                	pop    %r13
ffffffff812ce257:	5d                   	pop    %rbp
ffffffff812ce258:	c3                   	retq   

ffffffff812ce259 <gcd>:
#include <linux/gcd.h>
#include <linux/export.h>

/* Greatest common divisor */
unsigned long gcd(unsigned long a, unsigned long b)
{
ffffffff812ce259:	55                   	push   %rbp
	unsigned long r;

	if (a < b)
ffffffff812ce25a:	48 39 f7             	cmp    %rsi,%rdi
#include <linux/gcd.h>
#include <linux/export.h>

/* Greatest common divisor */
unsigned long gcd(unsigned long a, unsigned long b)
{
ffffffff812ce25d:	48 89 e5             	mov    %rsp,%rbp
	unsigned long r;

	if (a < b)
ffffffff812ce260:	73 09                	jae    ffffffff812ce26b <gcd+0x12>
ffffffff812ce262:	48 89 f8             	mov    %rdi,%rax
ffffffff812ce265:	48 89 f7             	mov    %rsi,%rdi
ffffffff812ce268:	48 89 c6             	mov    %rax,%rsi
		swap(a, b);

	if (!b)
ffffffff812ce26b:	48 85 f6             	test   %rsi,%rsi
		return a;
ffffffff812ce26e:	48 89 f8             	mov    %rdi,%rax
	unsigned long r;

	if (a < b)
		swap(a, b);

	if (!b)
ffffffff812ce271:	74 18                	je     ffffffff812ce28b <gcd+0x32>
		return a;
	while ((r = a % b) != 0) {
ffffffff812ce273:	48 89 f8             	mov    %rdi,%rax
ffffffff812ce276:	31 d2                	xor    %edx,%edx
ffffffff812ce278:	48 89 f7             	mov    %rsi,%rdi
ffffffff812ce27b:	48 f7 f6             	div    %rsi
ffffffff812ce27e:	48 85 d2             	test   %rdx,%rdx
ffffffff812ce281:	74 05                	je     ffffffff812ce288 <gcd+0x2f>
ffffffff812ce283:	48 89 d6             	mov    %rdx,%rsi
ffffffff812ce286:	eb eb                	jmp    ffffffff812ce273 <gcd+0x1a>
ffffffff812ce288:	48 89 f0             	mov    %rsi,%rax
		a = b;
		b = r;
	}
	return b;
}
ffffffff812ce28b:	5d                   	pop    %rbp
ffffffff812ce28c:	c3                   	retq   

ffffffff812ce28d <lcm>:
#include <linux/lcm.h>

/* Lowest common multiple */
unsigned long lcm(unsigned long a, unsigned long b)
{
	if (a && b)
ffffffff812ce28d:	48 85 ff             	test   %rdi,%rdi
ffffffff812ce290:	74 2b                	je     ffffffff812ce2bd <lcm+0x30>
ffffffff812ce292:	48 85 f6             	test   %rsi,%rsi
ffffffff812ce295:	74 26                	je     ffffffff812ce2bd <lcm+0x30>
#include <linux/export.h>
#include <linux/lcm.h>

/* Lowest common multiple */
unsigned long lcm(unsigned long a, unsigned long b)
{
ffffffff812ce297:	55                   	push   %rbp
ffffffff812ce298:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce29b:	41 54                	push   %r12
ffffffff812ce29d:	53                   	push   %rbx
ffffffff812ce29e:	49 89 fc             	mov    %rdi,%r12
ffffffff812ce2a1:	48 89 f3             	mov    %rsi,%rbx
	if (a && b)
		return (a / gcd(a, b)) * b;
ffffffff812ce2a4:	e8 b0 ff ff ff       	callq  ffffffff812ce259 <gcd>
ffffffff812ce2a9:	31 d2                	xor    %edx,%edx
ffffffff812ce2ab:	48 89 c1             	mov    %rax,%rcx
ffffffff812ce2ae:	4c 89 e0             	mov    %r12,%rax
ffffffff812ce2b1:	48 f7 f1             	div    %rcx
ffffffff812ce2b4:	48 0f af c3          	imul   %rbx,%rax
	else
		return 0;
}
ffffffff812ce2b8:	5b                   	pop    %rbx
ffffffff812ce2b9:	41 5c                	pop    %r12
ffffffff812ce2bb:	5d                   	pop    %rbp
ffffffff812ce2bc:	c3                   	retq   
unsigned long lcm(unsigned long a, unsigned long b)
{
	if (a && b)
		return (a / gcd(a, b)) * b;
	else
		return 0;
ffffffff812ce2bd:	31 c0                	xor    %eax,%eax
ffffffff812ce2bf:	c3                   	retq   

ffffffff812ce2c0 <lcm_not_zero>:
}
EXPORT_SYMBOL_GPL(lcm);

unsigned long lcm_not_zero(unsigned long a, unsigned long b)
{
ffffffff812ce2c0:	55                   	push   %rbp
ffffffff812ce2c1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce2c4:	41 54                	push   %r12
ffffffff812ce2c6:	53                   	push   %rbx
ffffffff812ce2c7:	49 89 fc             	mov    %rdi,%r12
ffffffff812ce2ca:	48 89 f3             	mov    %rsi,%rbx
	unsigned long l = lcm(a, b);
ffffffff812ce2cd:	e8 bb ff ff ff       	callq  ffffffff812ce28d <lcm>

	if (l)
ffffffff812ce2d2:	48 85 c0             	test   %rax,%rax
ffffffff812ce2d5:	75 0a                	jne    ffffffff812ce2e1 <lcm_not_zero+0x21>
		return l;

	return (b ? : a);
ffffffff812ce2d7:	48 85 db             	test   %rbx,%rbx
ffffffff812ce2da:	4c 89 e0             	mov    %r12,%rax
ffffffff812ce2dd:	48 0f 45 c3          	cmovne %rbx,%rax
}
ffffffff812ce2e1:	5b                   	pop    %rbx
ffffffff812ce2e2:	41 5c                	pop    %r12
ffffffff812ce2e4:	5d                   	pop    %rbp
ffffffff812ce2e5:	c3                   	retq   

ffffffff812ce2e6 <merge>:
 */
static struct list_head *merge(void *priv,
				int (*cmp)(void *priv, struct list_head *a,
					struct list_head *b),
				struct list_head *a, struct list_head *b)
{
ffffffff812ce2e6:	55                   	push   %rbp
ffffffff812ce2e7:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce2ea:	41 57                	push   %r15
ffffffff812ce2ec:	41 56                	push   %r14
ffffffff812ce2ee:	41 55                	push   %r13
ffffffff812ce2f0:	41 54                	push   %r12
	struct list_head head, *tail = &head;
ffffffff812ce2f2:	4c 8d 6d c0          	lea    -0x40(%rbp),%r13
 */
static struct list_head *merge(void *priv,
				int (*cmp)(void *priv, struct list_head *a,
					struct list_head *b),
				struct list_head *a, struct list_head *b)
{
ffffffff812ce2f6:	53                   	push   %rbx
ffffffff812ce2f7:	49 89 fe             	mov    %rdi,%r14
ffffffff812ce2fa:	49 89 f7             	mov    %rsi,%r15
ffffffff812ce2fd:	49 89 d4             	mov    %rdx,%r12
ffffffff812ce300:	48 89 cb             	mov    %rcx,%rbx
ffffffff812ce303:	48 83 ec 18          	sub    $0x18,%rsp
	struct list_head head, *tail = &head;

	while (a && b) {
ffffffff812ce307:	4d 85 e4             	test   %r12,%r12
ffffffff812ce30a:	74 2c                	je     ffffffff812ce338 <merge+0x52>
ffffffff812ce30c:	48 85 db             	test   %rbx,%rbx
ffffffff812ce30f:	74 27                	je     ffffffff812ce338 <merge+0x52>
		/* if equal, take 'a' -- important for sort stability */
		if ((*cmp)(priv, a, b) <= 0) {
ffffffff812ce311:	48 89 da             	mov    %rbx,%rdx
ffffffff812ce314:	4c 89 e6             	mov    %r12,%rsi
ffffffff812ce317:	4c 89 f7             	mov    %r14,%rdi
ffffffff812ce31a:	41 ff d7             	callq  *%r15
ffffffff812ce31d:	85 c0                	test   %eax,%eax
ffffffff812ce31f:	7f 0a                	jg     ffffffff812ce32b <merge+0x45>
			tail->next = a;
ffffffff812ce321:	4d 89 65 00          	mov    %r12,0x0(%r13)
			a = a->next;
ffffffff812ce325:	4d 8b 24 24          	mov    (%r12),%r12
ffffffff812ce329:	eb 07                	jmp    ffffffff812ce332 <merge+0x4c>
		} else {
			tail->next = b;
ffffffff812ce32b:	49 89 5d 00          	mov    %rbx,0x0(%r13)
			b = b->next;
ffffffff812ce32f:	48 8b 1b             	mov    (%rbx),%rbx
		}
		tail = tail->next;
ffffffff812ce332:	4d 8b 6d 00          	mov    0x0(%r13),%r13
ffffffff812ce336:	eb cf                	jmp    ffffffff812ce307 <merge+0x21>
	}
	tail->next = a?:b;
ffffffff812ce338:	4d 85 e4             	test   %r12,%r12
ffffffff812ce33b:	49 0f 45 dc          	cmovne %r12,%rbx
ffffffff812ce33f:	49 89 5d 00          	mov    %rbx,0x0(%r13)
	return head.next;
}
ffffffff812ce343:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
ffffffff812ce347:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812ce34b:	5b                   	pop    %rbx
ffffffff812ce34c:	41 5c                	pop    %r12
ffffffff812ce34e:	41 5d                	pop    %r13
ffffffff812ce350:	41 5e                	pop    %r14
ffffffff812ce352:	41 5f                	pop    %r15
ffffffff812ce354:	5d                   	pop    %rbp
ffffffff812ce355:	c3                   	retq   

ffffffff812ce356 <list_sort>:
						-- last slot is a sentinel */
	int lev;  /* index into part[] */
	int max_lev = 0;
	struct list_head *list;

	if (list_empty(head))
ffffffff812ce356:	48 3b 36             	cmp    (%rsi),%rsi
ffffffff812ce359:	0f 84 ad 01 00 00    	je     ffffffff812ce50c <list_sort+0x1b6>
 * ordering is to be preserved, @cmp must return 0.
 */
void list_sort(void *priv, struct list_head *head,
		int (*cmp)(void *priv, struct list_head *a,
			struct list_head *b))
{
ffffffff812ce35f:	55                   	push   %rbp
	struct list_head *list;

	if (list_empty(head))
		return;

	memset(part, 0, sizeof(part));
ffffffff812ce360:	31 c0                	xor    %eax,%eax
ffffffff812ce362:	b9 2a 00 00 00       	mov    $0x2a,%ecx
 * ordering is to be preserved, @cmp must return 0.
 */
void list_sort(void *priv, struct list_head *head,
		int (*cmp)(void *priv, struct list_head *a,
			struct list_head *b))
{
ffffffff812ce367:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce36a:	41 57                	push   %r15
ffffffff812ce36c:	41 56                	push   %r14
	struct list_head *list;

	if (list_empty(head))
		return;

	memset(part, 0, sizeof(part));
ffffffff812ce36e:	4c 8d b5 28 ff ff ff 	lea    -0xd8(%rbp),%r14
 * ordering is to be preserved, @cmp must return 0.
 */
void list_sort(void *priv, struct list_head *head,
		int (*cmp)(void *priv, struct list_head *a,
			struct list_head *b))
{
ffffffff812ce375:	41 55                	push   %r13
ffffffff812ce377:	41 54                	push   %r12
ffffffff812ce379:	53                   	push   %rbx
ffffffff812ce37a:	49 89 d5             	mov    %rdx,%r13
ffffffff812ce37d:	48 89 f3             	mov    %rsi,%rbx
	struct list_head *part[MAX_LIST_LENGTH_BITS+1]; /* sorted partial lists
						-- last slot is a sentinel */
	int lev;  /* index into part[] */
	int max_lev = 0;
ffffffff812ce380:	45 31 e4             	xor    %r12d,%r12d
 * ordering is to be preserved, @cmp must return 0.
 */
void list_sort(void *priv, struct list_head *head,
		int (*cmp)(void *priv, struct list_head *a,
			struct list_head *b))
{
ffffffff812ce383:	48 81 ec d8 00 00 00 	sub    $0xd8,%rsp
ffffffff812ce38a:	48 89 bd 18 ff ff ff 	mov    %rdi,-0xe8(%rbp)
	struct list_head *list;

	if (list_empty(head))
		return;

	memset(part, 0, sizeof(part));
ffffffff812ce391:	4c 89 f7             	mov    %r14,%rdi
ffffffff812ce394:	f3 ab                	rep stos %eax,%es:(%rdi)

	head->prev->next = NULL;
ffffffff812ce396:	48 8b 46 08          	mov    0x8(%rsi),%rax
ffffffff812ce39a:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)
	list = head->next;
ffffffff812ce3a1:	48 8b 0e             	mov    (%rsi),%rcx

	while (list) {
ffffffff812ce3a4:	48 85 c9             	test   %rcx,%rcx
ffffffff812ce3a7:	0f 84 98 00 00 00    	je     ffffffff812ce445 <list_sort+0xef>
		struct list_head *cur = list;
		list = list->next;
ffffffff812ce3ad:	48 8b 01             	mov    (%rcx),%rax
		cur->next = NULL;

		for (lev = 0; part[lev]; lev++) {
ffffffff812ce3b0:	45 31 ff             	xor    %r15d,%r15d
	list = head->next;

	while (list) {
		struct list_head *cur = list;
		list = list->next;
		cur->next = NULL;
ffffffff812ce3b3:	48 c7 01 00 00 00 00 	movq   $0x0,(%rcx)
	head->prev->next = NULL;
	list = head->next;

	while (list) {
		struct list_head *cur = list;
		list = list->next;
ffffffff812ce3ba:	48 89 85 10 ff ff ff 	mov    %rax,-0xf0(%rbp)
		cur->next = NULL;

		for (lev = 0; part[lev]; lev++) {
ffffffff812ce3c1:	4b 8b 14 fe          	mov    (%r14,%r15,8),%rdx
ffffffff812ce3c5:	49 63 c7             	movslq %r15d,%rax
ffffffff812ce3c8:	48 85 d2             	test   %rdx,%rdx
ffffffff812ce3cb:	74 1f                	je     ffffffff812ce3ec <list_sort+0x96>
			cur = merge(priv, cmp, part[lev], cur);
ffffffff812ce3cd:	48 8b bd 18 ff ff ff 	mov    -0xe8(%rbp),%rdi
ffffffff812ce3d4:	4c 89 ee             	mov    %r13,%rsi
ffffffff812ce3d7:	e8 0a ff ff ff       	callq  ffffffff812ce2e6 <merge>
			part[lev] = NULL;
ffffffff812ce3dc:	4b c7 04 fe 00 00 00 	movq   $0x0,(%r14,%r15,8)
ffffffff812ce3e3:	00 
		struct list_head *cur = list;
		list = list->next;
		cur->next = NULL;

		for (lev = 0; part[lev]; lev++) {
			cur = merge(priv, cmp, part[lev], cur);
ffffffff812ce3e4:	48 89 c1             	mov    %rax,%rcx
ffffffff812ce3e7:	49 ff c7             	inc    %r15
ffffffff812ce3ea:	eb d5                	jmp    ffffffff812ce3c1 <list_sort+0x6b>
			part[lev] = NULL;
		}
		if (lev > max_lev) {
ffffffff812ce3ec:	44 39 e0             	cmp    %r12d,%eax
ffffffff812ce3ef:	7e 40                	jle    ffffffff812ce431 <list_sort+0xdb>
			if (unlikely(lev >= ARRAY_SIZE(part)-1)) {
ffffffff812ce3f1:	41 83 ff 14          	cmp    $0x14,%r15d
ffffffff812ce3f5:	45 89 fc             	mov    %r15d,%r12d
ffffffff812ce3f8:	75 37                	jne    ffffffff812ce431 <list_sort+0xdb>
				printk_once(KERN_DEBUG "list too long for efficiency\n");
ffffffff812ce3fa:	80 3d 03 b4 78 00 00 	cmpb   $0x0,0x78b403(%rip)        # ffffffff81a59804 <__print_once.3208>
ffffffff812ce401:	75 23                	jne    ffffffff812ce426 <list_sort+0xd0>
ffffffff812ce403:	48 c7 c7 29 6e 7b 81 	mov    $0xffffffff817b6e29,%rdi
ffffffff812ce40a:	31 c0                	xor    %eax,%eax
ffffffff812ce40c:	48 89 8d 08 ff ff ff 	mov    %rcx,-0xf8(%rbp)
ffffffff812ce413:	c6 05 ea b3 78 00 01 	movb   $0x1,0x78b3ea(%rip)        # ffffffff81a59804 <__print_once.3208>
ffffffff812ce41a:	e8 73 4b 16 00       	callq  ffffffff81432f92 <printk>
ffffffff812ce41f:	48 8b 8d 08 ff ff ff 	mov    -0xf8(%rbp),%rcx
				lev--;
ffffffff812ce426:	41 bc 13 00 00 00    	mov    $0x13,%r12d
ffffffff812ce42c:	b8 13 00 00 00       	mov    $0x13,%eax
			}
			max_lev = lev;
		}
		part[lev] = cur;
ffffffff812ce431:	48 89 8c c5 28 ff ff 	mov    %rcx,-0xd8(%rbp,%rax,8)
ffffffff812ce438:	ff 
	head->prev->next = NULL;
	list = head->next;

	while (list) {
		struct list_head *cur = list;
		list = list->next;
ffffffff812ce439:	48 8b 8d 10 ff ff ff 	mov    -0xf0(%rbp),%rcx
ffffffff812ce440:	e9 5f ff ff ff       	jmpq   ffffffff812ce3a4 <list_sort+0x4e>
ffffffff812ce445:	45 31 ff             	xor    %r15d,%r15d
			max_lev = lev;
		}
		part[lev] = cur;
	}

	for (lev = 0; lev < max_lev; lev++)
ffffffff812ce448:	45 39 fc             	cmp    %r15d,%r12d
ffffffff812ce44b:	7e 20                	jle    ffffffff812ce46d <list_sort+0x117>
		if (part[lev])
ffffffff812ce44d:	4b 8b 14 fe          	mov    (%r14,%r15,8),%rdx
ffffffff812ce451:	48 85 d2             	test   %rdx,%rdx
ffffffff812ce454:	74 12                	je     ffffffff812ce468 <list_sort+0x112>
			list = merge(priv, cmp, part[lev], list);
ffffffff812ce456:	48 8b bd 18 ff ff ff 	mov    -0xe8(%rbp),%rdi
ffffffff812ce45d:	4c 89 ee             	mov    %r13,%rsi
ffffffff812ce460:	e8 81 fe ff ff       	callq  ffffffff812ce2e6 <merge>
ffffffff812ce465:	48 89 c1             	mov    %rax,%rcx
ffffffff812ce468:	49 ff c7             	inc    %r15
ffffffff812ce46b:	eb db                	jmp    ffffffff812ce448 <list_sort+0xf2>

	merge_and_restore_back_links(priv, cmp, head, part[max_lev], list);
ffffffff812ce46d:	4d 63 c4             	movslq %r12d,%r8
ffffffff812ce470:	49 89 de             	mov    %rbx,%r14
ffffffff812ce473:	4e 8b bc c5 28 ff ff 	mov    -0xd8(%rbp,%r8,8),%r15
ffffffff812ce47a:	ff 
				struct list_head *a, struct list_head *b)
{
	struct list_head *tail = head;
	u8 count = 0;

	while (a && b) {
ffffffff812ce47b:	4d 85 ff             	test   %r15,%r15
ffffffff812ce47e:	74 42                	je     ffffffff812ce4c2 <list_sort+0x16c>
ffffffff812ce480:	48 85 c9             	test   %rcx,%rcx
ffffffff812ce483:	74 3d                	je     ffffffff812ce4c2 <list_sort+0x16c>
		/* if equal, take 'a' -- important for sort stability */
		if ((*cmp)(priv, a, b) <= 0) {
ffffffff812ce485:	48 89 ca             	mov    %rcx,%rdx
ffffffff812ce488:	48 89 8d 10 ff ff ff 	mov    %rcx,-0xf0(%rbp)
ffffffff812ce48f:	4c 89 fe             	mov    %r15,%rsi
ffffffff812ce492:	48 8b bd 18 ff ff ff 	mov    -0xe8(%rbp),%rdi
ffffffff812ce499:	41 ff d5             	callq  *%r13
ffffffff812ce49c:	85 c0                	test   %eax,%eax
ffffffff812ce49e:	48 8b 8d 10 ff ff ff 	mov    -0xf0(%rbp),%rcx
ffffffff812ce4a5:	7f 0c                	jg     ffffffff812ce4b3 <list_sort+0x15d>
			tail->next = a;
ffffffff812ce4a7:	4d 89 3e             	mov    %r15,(%r14)
			a->prev = tail;
ffffffff812ce4aa:	4d 89 77 08          	mov    %r14,0x8(%r15)
			a = a->next;
ffffffff812ce4ae:	4d 8b 3f             	mov    (%r15),%r15
ffffffff812ce4b1:	eb 0a                	jmp    ffffffff812ce4bd <list_sort+0x167>
		} else {
			tail->next = b;
ffffffff812ce4b3:	49 89 0e             	mov    %rcx,(%r14)
			b->prev = tail;
ffffffff812ce4b6:	4c 89 71 08          	mov    %r14,0x8(%rcx)
			b = b->next;
ffffffff812ce4ba:	48 8b 09             	mov    (%rcx),%rcx
		}
		tail = tail->next;
ffffffff812ce4bd:	4d 8b 36             	mov    (%r14),%r14
ffffffff812ce4c0:	eb b9                	jmp    ffffffff812ce47b <list_sort+0x125>
	}
	tail->next = a ? : b;
ffffffff812ce4c2:	4d 85 ff             	test   %r15,%r15
ffffffff812ce4c5:	49 0f 45 cf          	cmovne %r15,%rcx
					struct list_head *b),
				struct list_head *head,
				struct list_head *a, struct list_head *b)
{
	struct list_head *tail = head;
	u8 count = 0;
ffffffff812ce4c9:	45 31 ff             	xor    %r15d,%r15d
			b->prev = tail;
			b = b->next;
		}
		tail = tail->next;
	}
	tail->next = a ? : b;
ffffffff812ce4cc:	49 89 0e             	mov    %rcx,(%r14)
		 * In worst cases this loop may run many iterations.
		 * Continue callbacks to the client even though no
		 * element comparison is needed, so the client's cmp()
		 * routine can invoke cond_resched() periodically.
		 */
		if (unlikely(!(++count)))
ffffffff812ce4cf:	41 fe c7             	inc    %r15b
ffffffff812ce4d2:	75 10                	jne    ffffffff812ce4e4 <list_sort+0x18e>
			(*cmp)(priv, tail->next, tail->next);
ffffffff812ce4d4:	49 8b 36             	mov    (%r14),%rsi
ffffffff812ce4d7:	48 8b bd 18 ff ff ff 	mov    -0xe8(%rbp),%rdi
ffffffff812ce4de:	48 89 f2             	mov    %rsi,%rdx
ffffffff812ce4e1:	41 ff d5             	callq  *%r13

		tail->next->prev = tail;
ffffffff812ce4e4:	49 8b 06             	mov    (%r14),%rax
ffffffff812ce4e7:	4c 89 70 08          	mov    %r14,0x8(%rax)
		tail = tail->next;
ffffffff812ce4eb:	4d 8b 36             	mov    (%r14),%r14
	} while (tail->next);
ffffffff812ce4ee:	49 83 3e 00          	cmpq   $0x0,(%r14)
ffffffff812ce4f2:	75 db                	jne    ffffffff812ce4cf <list_sort+0x179>

	tail->next = head;
ffffffff812ce4f4:	49 89 1e             	mov    %rbx,(%r14)
	head->prev = tail;
ffffffff812ce4f7:	4c 89 73 08          	mov    %r14,0x8(%rbx)
	for (lev = 0; lev < max_lev; lev++)
		if (part[lev])
			list = merge(priv, cmp, part[lev], list);

	merge_and_restore_back_links(priv, cmp, head, part[max_lev], list);
}
ffffffff812ce4fb:	48 81 c4 d8 00 00 00 	add    $0xd8,%rsp
ffffffff812ce502:	5b                   	pop    %rbx
ffffffff812ce503:	41 5c                	pop    %r12
ffffffff812ce505:	41 5d                	pop    %r13
ffffffff812ce507:	41 5e                	pop    %r14
ffffffff812ce509:	41 5f                	pop    %r15
ffffffff812ce50b:	5d                   	pop    %rbp
ffffffff812ce50c:	c3                   	retq   

ffffffff812ce50d <uuid_le_gen>:
	/* reversion 0b10 */
	b[8] = (b[8] & 0x3F) | 0x80;
}

void uuid_le_gen(uuid_le *lu)
{
ffffffff812ce50d:	55                   	push   %rbp
#include <linux/uuid.h>
#include <linux/random.h>

static void __uuid_gen_common(__u8 b[16])
{
	prandom_bytes(b, 16);
ffffffff812ce50e:	be 10 00 00 00       	mov    $0x10,%esi
	/* reversion 0b10 */
	b[8] = (b[8] & 0x3F) | 0x80;
}

void uuid_le_gen(uuid_le *lu)
{
ffffffff812ce513:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce516:	53                   	push   %rbx
ffffffff812ce517:	50                   	push   %rax
ffffffff812ce518:	48 89 fb             	mov    %rdi,%rbx
#include <linux/uuid.h>
#include <linux/random.h>

static void __uuid_gen_common(__u8 b[16])
{
	prandom_bytes(b, 16);
ffffffff812ce51b:	e8 0c e6 ff ff       	callq  ffffffff812ccb2c <prandom_bytes>
	/* reversion 0b10 */
	b[8] = (b[8] & 0x3F) | 0x80;
ffffffff812ce520:	8a 43 08             	mov    0x8(%rbx),%al
ffffffff812ce523:	83 e0 3f             	and    $0x3f,%eax
ffffffff812ce526:	83 c8 80             	or     $0xffffff80,%eax
ffffffff812ce529:	88 43 08             	mov    %al,0x8(%rbx)

void uuid_le_gen(uuid_le *lu)
{
	__uuid_gen_common(lu->b);
	/* version 4 : random generation */
	lu->b[7] = (lu->b[7] & 0x0F) | 0x40;
ffffffff812ce52c:	8a 43 07             	mov    0x7(%rbx),%al
ffffffff812ce52f:	83 e0 0f             	and    $0xf,%eax
ffffffff812ce532:	83 c8 40             	or     $0x40,%eax
ffffffff812ce535:	88 43 07             	mov    %al,0x7(%rbx)
}
ffffffff812ce538:	5a                   	pop    %rdx
ffffffff812ce539:	5b                   	pop    %rbx
ffffffff812ce53a:	5d                   	pop    %rbp
ffffffff812ce53b:	c3                   	retq   

ffffffff812ce53c <uuid_be_gen>:
EXPORT_SYMBOL_GPL(uuid_le_gen);

void uuid_be_gen(uuid_be *bu)
{
ffffffff812ce53c:	55                   	push   %rbp
#include <linux/uuid.h>
#include <linux/random.h>

static void __uuid_gen_common(__u8 b[16])
{
	prandom_bytes(b, 16);
ffffffff812ce53d:	be 10 00 00 00       	mov    $0x10,%esi
	lu->b[7] = (lu->b[7] & 0x0F) | 0x40;
}
EXPORT_SYMBOL_GPL(uuid_le_gen);

void uuid_be_gen(uuid_be *bu)
{
ffffffff812ce542:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce545:	53                   	push   %rbx
ffffffff812ce546:	50                   	push   %rax
ffffffff812ce547:	48 89 fb             	mov    %rdi,%rbx
#include <linux/uuid.h>
#include <linux/random.h>

static void __uuid_gen_common(__u8 b[16])
{
	prandom_bytes(b, 16);
ffffffff812ce54a:	e8 dd e5 ff ff       	callq  ffffffff812ccb2c <prandom_bytes>
	/* reversion 0b10 */
	b[8] = (b[8] & 0x3F) | 0x80;
ffffffff812ce54f:	8a 43 08             	mov    0x8(%rbx),%al
ffffffff812ce552:	83 e0 3f             	and    $0x3f,%eax
ffffffff812ce555:	83 c8 80             	or     $0xffffff80,%eax
ffffffff812ce558:	88 43 08             	mov    %al,0x8(%rbx)

void uuid_be_gen(uuid_be *bu)
{
	__uuid_gen_common(bu->b);
	/* version 4 : random generation */
	bu->b[6] = (bu->b[6] & 0x0F) | 0x40;
ffffffff812ce55b:	8a 43 06             	mov    0x6(%rbx),%al
ffffffff812ce55e:	83 e0 0f             	and    $0xf,%eax
ffffffff812ce561:	83 c8 40             	or     $0x40,%eax
ffffffff812ce564:	88 43 06             	mov    %al,0x6(%rbx)
}
ffffffff812ce567:	5a                   	pop    %rdx
ffffffff812ce568:	5b                   	pop    %rbx
ffffffff812ce569:	5d                   	pop    %rbp
ffffffff812ce56a:	c3                   	retq   

ffffffff812ce56b <fa_element_to_part_nr>:

struct reciprocal_value reciprocal_value(u32 d);

static inline u32 reciprocal_divide(u32 a, struct reciprocal_value R)
{
	u32 t = (u32)(((u64)a * R.m) >> 32);
ffffffff812ce56b:	8b 57 0c             	mov    0xc(%rdi),%edx
ffffffff812ce56e:	89 f1                	mov    %esi,%ecx
}
EXPORT_SYMBOL(flex_array_alloc);

static int fa_element_to_part_nr(struct flex_array *fa,
					unsigned int element_nr)
{
ffffffff812ce570:	55                   	push   %rbp
ffffffff812ce571:	48 89 e5             	mov    %rsp,%rbp
	 * if element_size == 0 we don't get here, so we never touch
	 * the zeroed fa->reciprocal_elems, which would yield invalid
	 * results
	 */
	return reciprocal_divide(element_nr, fa->reciprocal_elems);
}
ffffffff812ce574:	5d                   	pop    %rbp
ffffffff812ce575:	48 0f af d1          	imul   %rcx,%rdx
	/*
	 * if element_size == 0 we don't get here, so we never touch
	 * the zeroed fa->reciprocal_elems, which would yield invalid
	 * results
	 */
	return reciprocal_divide(element_nr, fa->reciprocal_elems);
ffffffff812ce579:	0f b6 4f 10          	movzbl 0x10(%rdi),%ecx
ffffffff812ce57d:	48 c1 ea 20          	shr    $0x20,%rdx
ffffffff812ce581:	29 d6                	sub    %edx,%esi
ffffffff812ce583:	d3 ee                	shr    %cl,%esi
ffffffff812ce585:	0f b6 4f 11          	movzbl 0x11(%rdi),%ecx
ffffffff812ce589:	8d 04 16             	lea    (%rsi,%rdx,1),%eax
ffffffff812ce58c:	d3 e8                	shr    %cl,%eax
}
ffffffff812ce58e:	c3                   	retq   

ffffffff812ce58f <__fa_get_part.part.2>:
	part_offset = element_nr - part_nr * fa->elems_per_part;
	return part_offset * fa->element_size;
}

static struct flex_array_part *
__fa_get_part(struct flex_array *fa, int part_nr, gfp_t flags)
ffffffff812ce58f:	55                   	push   %rbp
ffffffff812ce590:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce593:	41 55                	push   %r13
ffffffff812ce595:	41 54                	push   %r12
ffffffff812ce597:	4c 63 e6             	movslq %esi,%r12
ffffffff812ce59a:	53                   	push   %rbx

#else /* CONFIG_TRACING */
static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
		gfp_t flags, size_t size)
{
	void *ret = kmem_cache_alloc(s, flags);
ffffffff812ce59b:	89 d6                	mov    %edx,%esi
ffffffff812ce59d:	89 d3                	mov    %edx,%ebx
#endif
			return kmalloc_container(nsproxy, size, flags);
		}
#endif

		if (!(flags & GFP_DMA)) {
ffffffff812ce59f:	80 e2 01             	and    $0x1,%dl
ffffffff812ce5a2:	51                   	push   %rcx
ffffffff812ce5a3:	49 89 fd             	mov    %rdi,%r13
ffffffff812ce5a6:	75 0e                	jne    ffffffff812ce5b6 <__fa_get_part.part.2+0x27>

#else /* CONFIG_TRACING */
static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
		gfp_t flags, size_t size)
{
	void *ret = kmem_cache_alloc(s, flags);
ffffffff812ce5a8:	48 8b 3d f1 40 8d 00 	mov    0x8d40f1(%rip),%rdi        # ffffffff81ba26a0 <kmalloc_caches+0x60>
ffffffff812ce5af:	e8 1d 2b e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
ffffffff812ce5b4:	eb 0a                	jmp    ffffffff812ce5c0 <__fa_get_part.part.2+0x31>
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812ce5b6:	bf 00 10 00 00       	mov    $0x1000,%edi
ffffffff812ce5bb:	e8 cb 2b e3 ff       	callq  ffffffff8110118b <__kmalloc>
ffffffff812ce5c0:	49 89 c0             	mov    %rax,%r8
{
	struct flex_array_part *part = fa->parts[part_nr];
	if (!part) {
		part = kmalloc(sizeof(struct flex_array_part), flags);
		if (!part)
			return NULL;
ffffffff812ce5c3:	31 c0                	xor    %eax,%eax
__fa_get_part(struct flex_array *fa, int part_nr, gfp_t flags)
{
	struct flex_array_part *part = fa->parts[part_nr];
	if (!part) {
		part = kmalloc(sizeof(struct flex_array_part), flags);
		if (!part)
ffffffff812ce5c5:	4d 85 c0             	test   %r8,%r8
ffffffff812ce5c8:	74 19                	je     ffffffff812ce5e3 <__fa_get_part.part.2+0x54>
			return NULL;
		if (!(flags & __GFP_ZERO))
ffffffff812ce5ca:	80 e7 80             	and    $0x80,%bh
ffffffff812ce5cd:	75 0c                	jne    ffffffff812ce5db <__fa_get_part.part.2+0x4c>
			memset(part, FLEX_ARRAY_FREE,
ffffffff812ce5cf:	b9 00 10 00 00       	mov    $0x1000,%ecx
ffffffff812ce5d4:	b0 6c                	mov    $0x6c,%al
ffffffff812ce5d6:	4c 89 c7             	mov    %r8,%rdi
ffffffff812ce5d9:	f3 aa                	rep stos %al,%es:(%rdi)
				sizeof(struct flex_array_part));
		fa->parts[part_nr] = part;
ffffffff812ce5db:	4f 89 44 e5 18       	mov    %r8,0x18(%r13,%r12,8)
ffffffff812ce5e0:	4c 89 c0             	mov    %r8,%rax
	}
	return part;
}
ffffffff812ce5e3:	5a                   	pop    %rdx
ffffffff812ce5e4:	5b                   	pop    %rbx
ffffffff812ce5e5:	41 5c                	pop    %r12
ffffffff812ce5e7:	41 5d                	pop    %r13
ffffffff812ce5e9:	5d                   	pop    %rbp
ffffffff812ce5ea:	c3                   	retq   

ffffffff812ce5eb <flex_array_free_parts>:
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce5eb:	8b 07                	mov    (%rdi),%eax
ffffffff812ce5ed:	0f af 47 04          	imul   0x4(%rdi),%eax
ffffffff812ce5f1:	3d e8 0f 00 00       	cmp    $0xfe8,%eax
ffffffff812ce5f6:	76 2a                	jbe    ffffffff812ce622 <flex_array_free_parts+0x37>
 *
 * This is to be used in cases where the base 'struct flex_array'
 * has been statically allocated and should not be free.
 */
void flex_array_free_parts(struct flex_array *fa)
{
ffffffff812ce5f8:	55                   	push   %rbp
ffffffff812ce5f9:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce5fc:	41 54                	push   %r12
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce5fe:	45 31 e4             	xor    %r12d,%r12d
 *
 * This is to be used in cases where the base 'struct flex_array'
 * has been statically allocated and should not be free.
 */
void flex_array_free_parts(struct flex_array *fa)
{
ffffffff812ce601:	53                   	push   %rbx
ffffffff812ce602:	48 89 fb             	mov    %rdi,%rbx
ffffffff812ce605:	49 83 c4 08          	add    $0x8,%r12
	int part_nr;

	if (elements_fit_in_base(fa))
		return;
	for (part_nr = 0; part_nr < FLEX_ARRAY_NR_BASE_PTRS; part_nr++)
ffffffff812ce609:	49 81 fc f0 0f 00 00 	cmp    $0xff0,%r12
ffffffff812ce610:	74 0c                	je     ffffffff812ce61e <flex_array_free_parts+0x33>
		kfree(fa->parts[part_nr]);
ffffffff812ce612:	4a 8b 7c 23 10       	mov    0x10(%rbx,%r12,1),%rdi
ffffffff812ce617:	e8 6d 23 e3 ff       	callq  ffffffff81100989 <kfree>
ffffffff812ce61c:	eb e7                	jmp    ffffffff812ce605 <flex_array_free_parts+0x1a>
}
ffffffff812ce61e:	5b                   	pop    %rbx
ffffffff812ce61f:	41 5c                	pop    %r12
ffffffff812ce621:	5d                   	pop    %rbp
ffffffff812ce622:	c3                   	retq   

ffffffff812ce623 <flex_array_free>:
EXPORT_SYMBOL(flex_array_free_parts);

void flex_array_free(struct flex_array *fa)
{
ffffffff812ce623:	55                   	push   %rbp
ffffffff812ce624:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce627:	53                   	push   %rbx
ffffffff812ce628:	50                   	push   %rax
ffffffff812ce629:	48 89 fb             	mov    %rdi,%rbx
	flex_array_free_parts(fa);
ffffffff812ce62c:	e8 ba ff ff ff       	callq  ffffffff812ce5eb <flex_array_free_parts>
	kfree(fa);
ffffffff812ce631:	48 89 df             	mov    %rbx,%rdi
ffffffff812ce634:	e8 50 23 e3 ff       	callq  ffffffff81100989 <kfree>
}
ffffffff812ce639:	5a                   	pop    %rdx
ffffffff812ce63a:	5b                   	pop    %rbx
ffffffff812ce63b:	5d                   	pop    %rbp
ffffffff812ce63c:	c3                   	retq   

ffffffff812ce63d <flex_array_put>:
{
	int part_nr = 0;
	struct flex_array_part *part;
	void *dst;

	if (element_nr >= fa->total_nr_elements)
ffffffff812ce63d:	8b 47 04             	mov    0x4(%rdi),%eax
		return -ENOSPC;
ffffffff812ce640:	41 b8 e4 ff ff ff    	mov    $0xffffffe4,%r8d
{
	int part_nr = 0;
	struct flex_array_part *part;
	void *dst;

	if (element_nr >= fa->total_nr_elements)
ffffffff812ce646:	39 f0                	cmp    %esi,%eax
ffffffff812ce648:	0f 86 8f 00 00 00    	jbe    ffffffff812ce6dd <flex_array_put+0xa0>
		return -ENOSPC;
	if (!fa->element_size)
ffffffff812ce64e:	44 8b 07             	mov    (%rdi),%r8d
ffffffff812ce651:	45 85 c0             	test   %r8d,%r8d
ffffffff812ce654:	0f 84 83 00 00 00    	je     ffffffff812ce6dd <flex_array_put+0xa0>
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce65a:	44 0f af c0          	imul   %eax,%r8d
 *
 * Locking must be provided by the caller.
 */
int flex_array_put(struct flex_array *fa, unsigned int element_nr, void *src,
			gfp_t flags)
{
ffffffff812ce65e:	55                   	push   %rbp
ffffffff812ce65f:	41 89 c9             	mov    %ecx,%r9d
ffffffff812ce662:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce665:	41 56                	push   %r14
ffffffff812ce667:	41 55                	push   %r13
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce669:	41 81 f8 e8 0f 00 00 	cmp    $0xfe8,%r8d
 *
 * Locking must be provided by the caller.
 */
int flex_array_put(struct flex_array *fa, unsigned int element_nr, void *src,
			gfp_t flags)
{
ffffffff812ce670:	41 54                	push   %r12
ffffffff812ce672:	49 89 d5             	mov    %rdx,%r13
ffffffff812ce675:	53                   	push   %rbx
ffffffff812ce676:	49 89 fc             	mov    %rdi,%r12
ffffffff812ce679:	89 f3                	mov    %esi,%ebx
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce67b:	77 27                	ja     ffffffff812ce6a4 <flex_array_put+0x67>
	if (element_nr >= fa->total_nr_elements)
		return -ENOSPC;
	if (!fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
ffffffff812ce67d:	48 8d 47 18          	lea    0x18(%rdi),%rax
 * Locking must be provided by the caller.
 */
int flex_array_put(struct flex_array *fa, unsigned int element_nr, void *src,
			gfp_t flags)
{
	int part_nr = 0;
ffffffff812ce681:	45 31 f6             	xor    %r14d,%r14d
		part = __fa_get_part(fa, part_nr, flags);
		if (!part)
			return -ENOMEM;
	}
	dst = &part->elements[index_inside_part(fa, element_nr, part_nr)];
	memcpy(dst, src, fa->element_size);
ffffffff812ce684:	45 0f af 74 24 08    	imul   0x8(%r12),%r14d
					unsigned int part_nr)
{
	unsigned int part_offset;

	part_offset = element_nr - part_nr * fa->elems_per_part;
	return part_offset * fa->element_size;
ffffffff812ce68a:	49 63 0c 24          	movslq (%r12),%rcx
		part = __fa_get_part(fa, part_nr, flags);
		if (!part)
			return -ENOMEM;
	}
	dst = &part->elements[index_inside_part(fa, element_nr, part_nr)];
	memcpy(dst, src, fa->element_size);
ffffffff812ce68e:	4c 89 ee             	mov    %r13,%rsi
	return 0;
ffffffff812ce691:	45 31 c0             	xor    %r8d,%r8d
		part = __fa_get_part(fa, part_nr, flags);
		if (!part)
			return -ENOMEM;
	}
	dst = &part->elements[index_inside_part(fa, element_nr, part_nr)];
	memcpy(dst, src, fa->element_size);
ffffffff812ce694:	44 29 f3             	sub    %r14d,%ebx
ffffffff812ce697:	0f af d9             	imul   %ecx,%ebx
ffffffff812ce69a:	48 01 d8             	add    %rbx,%rax
ffffffff812ce69d:	48 89 c7             	mov    %rax,%rdi
ffffffff812ce6a0:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
ffffffff812ce6a2:	eb 2d                	jmp    ffffffff812ce6d1 <flex_array_put+0x94>
	if (!fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
	else {
		part_nr = fa_element_to_part_nr(fa, element_nr);
ffffffff812ce6a4:	e8 c2 fe ff ff       	callq  ffffffff812ce56b <fa_element_to_part_nr>
ffffffff812ce6a9:	48 98                	cltq   
ffffffff812ce6ab:	49 89 c6             	mov    %rax,%r14
}

static struct flex_array_part *
__fa_get_part(struct flex_array *fa, int part_nr, gfp_t flags)
{
	struct flex_array_part *part = fa->parts[part_nr];
ffffffff812ce6ae:	48 8b 44 c7 18       	mov    0x18(%rdi,%rax,8),%rax
	if (!part) {
ffffffff812ce6b3:	48 85 c0             	test   %rax,%rax
ffffffff812ce6b6:	75 cc                	jne    ffffffff812ce684 <flex_array_put+0x47>
ffffffff812ce6b8:	44 89 ca             	mov    %r9d,%edx
ffffffff812ce6bb:	44 89 f6             	mov    %r14d,%esi
ffffffff812ce6be:	4c 89 e7             	mov    %r12,%rdi
ffffffff812ce6c1:	e8 c9 fe ff ff       	callq  ffffffff812ce58f <__fa_get_part.part.2>
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
	else {
		part_nr = fa_element_to_part_nr(fa, element_nr);
		part = __fa_get_part(fa, part_nr, flags);
		if (!part)
ffffffff812ce6c6:	48 85 c0             	test   %rax,%rax
			return -ENOMEM;
ffffffff812ce6c9:	41 b8 f4 ff ff ff    	mov    $0xfffffff4,%r8d
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
	else {
		part_nr = fa_element_to_part_nr(fa, element_nr);
		part = __fa_get_part(fa, part_nr, flags);
		if (!part)
ffffffff812ce6cf:	75 b3                	jne    ffffffff812ce684 <flex_array_put+0x47>
			return -ENOMEM;
	}
	dst = &part->elements[index_inside_part(fa, element_nr, part_nr)];
	memcpy(dst, src, fa->element_size);
	return 0;
}
ffffffff812ce6d1:	5b                   	pop    %rbx
ffffffff812ce6d2:	44 89 c0             	mov    %r8d,%eax
ffffffff812ce6d5:	41 5c                	pop    %r12
ffffffff812ce6d7:	41 5d                	pop    %r13
ffffffff812ce6d9:	41 5e                	pop    %r14
ffffffff812ce6db:	5d                   	pop    %rbp
ffffffff812ce6dc:	c3                   	retq   
ffffffff812ce6dd:	44 89 c0             	mov    %r8d,%eax
ffffffff812ce6e0:	c3                   	retq   

ffffffff812ce6e1 <flex_array_get>:
void *flex_array_get(struct flex_array *fa, unsigned int element_nr)
{
	int part_nr = 0;
	struct flex_array_part *part;

	if (!fa->element_size)
ffffffff812ce6e1:	44 8b 17             	mov    (%rdi),%r10d
		return NULL;
ffffffff812ce6e4:	31 c0                	xor    %eax,%eax
void *flex_array_get(struct flex_array *fa, unsigned int element_nr)
{
	int part_nr = 0;
	struct flex_array_part *part;

	if (!fa->element_size)
ffffffff812ce6e6:	45 85 d2             	test   %r10d,%r10d
ffffffff812ce6e9:	74 4c                	je     ffffffff812ce737 <flex_array_get+0x56>
		return NULL;
	if (element_nr >= fa->total_nr_elements)
ffffffff812ce6eb:	8b 57 04             	mov    0x4(%rdi),%edx
ffffffff812ce6ee:	39 f2                	cmp    %esi,%edx
ffffffff812ce6f0:	76 45                	jbe    ffffffff812ce737 <flex_array_get+0x56>
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce6f2:	41 0f af d2          	imul   %r10d,%edx
 * may instead wish to use the flex_array_get_ptr helper.
 *
 * Locking must be provided by the caller.
 */
void *flex_array_get(struct flex_array *fa, unsigned int element_nr)
{
ffffffff812ce6f6:	55                   	push   %rbp
ffffffff812ce6f7:	41 89 f0             	mov    %esi,%r8d
ffffffff812ce6fa:	49 89 f9             	mov    %rdi,%r9
ffffffff812ce6fd:	48 89 e5             	mov    %rsp,%rbp
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce700:	81 fa e8 0f 00 00    	cmp    $0xfe8,%edx
ffffffff812ce706:	77 18                	ja     ffffffff812ce720 <flex_array_get+0x3f>
	if (!fa->element_size)
		return NULL;
	if (element_nr >= fa->total_nr_elements)
		return NULL;
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
ffffffff812ce708:	48 8d 4f 18          	lea    0x18(%rdi),%rcx
 *
 * Locking must be provided by the caller.
 */
void *flex_array_get(struct flex_array *fa, unsigned int element_nr)
{
	int part_nr = 0;
ffffffff812ce70c:	31 d2                	xor    %edx,%edx
		part_nr = fa_element_to_part_nr(fa, element_nr);
		part = fa->parts[part_nr];
		if (!part)
			return NULL;
	}
	return &part->elements[index_inside_part(fa, element_nr, part_nr)];
ffffffff812ce70e:	41 0f af 51 08       	imul   0x8(%r9),%edx
ffffffff812ce713:	41 29 d0             	sub    %edx,%r8d
ffffffff812ce716:	45 0f af c2          	imul   %r10d,%r8d
ffffffff812ce71a:	4a 8d 04 01          	lea    (%rcx,%r8,1),%rax
ffffffff812ce71e:	eb 16                	jmp    ffffffff812ce736 <flex_array_get+0x55>
	if (element_nr >= fa->total_nr_elements)
		return NULL;
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
	else {
		part_nr = fa_element_to_part_nr(fa, element_nr);
ffffffff812ce720:	e8 46 fe ff ff       	callq  ffffffff812ce56b <fa_element_to_part_nr>
ffffffff812ce725:	48 98                	cltq   
		part = fa->parts[part_nr];
ffffffff812ce727:	48 8b 4c c7 18       	mov    0x18(%rdi,%rax,8),%rcx
	if (element_nr >= fa->total_nr_elements)
		return NULL;
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
	else {
		part_nr = fa_element_to_part_nr(fa, element_nr);
ffffffff812ce72c:	48 89 c2             	mov    %rax,%rdx
{
	int part_nr = 0;
	struct flex_array_part *part;

	if (!fa->element_size)
		return NULL;
ffffffff812ce72f:	31 c0                	xor    %eax,%eax
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
	else {
		part_nr = fa_element_to_part_nr(fa, element_nr);
		part = fa->parts[part_nr];
		if (!part)
ffffffff812ce731:	48 85 c9             	test   %rcx,%rcx
ffffffff812ce734:	75 d8                	jne    ffffffff812ce70e <flex_array_get+0x2d>
			return NULL;
	}
	return &part->elements[index_inside_part(fa, element_nr, part_nr)];
}
ffffffff812ce736:	5d                   	pop    %rbp
ffffffff812ce737:	c3                   	retq   

ffffffff812ce738 <flex_array_get_ptr>:
 * Returns the pointer placed in the flex array at element_nr using
 * flex_array_put_ptr().  This function should not be called if the
 * element in question was not set using the _put_ptr() helper.
 */
void *flex_array_get_ptr(struct flex_array *fa, unsigned int element_nr)
{
ffffffff812ce738:	55                   	push   %rbp
ffffffff812ce739:	48 89 e5             	mov    %rsp,%rbp
	void **tmp;

	tmp = flex_array_get(fa, element_nr);
ffffffff812ce73c:	e8 a0 ff ff ff       	callq  ffffffff812ce6e1 <flex_array_get>
ffffffff812ce741:	31 d2                	xor    %edx,%edx
	if (!tmp)
ffffffff812ce743:	48 85 c0             	test   %rax,%rax
ffffffff812ce746:	74 03                	je     ffffffff812ce74b <flex_array_get_ptr+0x13>
		return NULL;

	return *tmp;
ffffffff812ce748:	48 8b 10             	mov    (%rax),%rdx
}
ffffffff812ce74b:	48 89 d0             	mov    %rdx,%rax
ffffffff812ce74e:	5d                   	pop    %rbp
ffffffff812ce74f:	c3                   	retq   

ffffffff812ce750 <flex_array_alloc>:
 * capacity in the base structure.  Also note that no effort is made
 * to efficiently pack objects across page boundaries.
 */
struct flex_array *flex_array_alloc(int element_size, unsigned int total,
					gfp_t flags)
{
ffffffff812ce750:	55                   	push   %rbp
ffffffff812ce751:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce754:	41 57                	push   %r15
ffffffff812ce756:	41 56                	push   %r14
ffffffff812ce758:	41 55                	push   %r13
ffffffff812ce75a:	41 54                	push   %r12
ffffffff812ce75c:	41 89 d5             	mov    %edx,%r13d
ffffffff812ce75f:	53                   	push   %rbx
ffffffff812ce760:	41 89 fc             	mov    %edi,%r12d
ffffffff812ce763:	89 f3                	mov    %esi,%ebx
ffffffff812ce765:	48 83 ec 18          	sub    $0x18,%rsp
	struct flex_array *ret;
	int elems_per_part = 0;
	int max_size = 0;
	struct reciprocal_value reciprocal_elems = { 0 };

	if (element_size) {
ffffffff812ce769:	85 ff                	test   %edi,%edi
ffffffff812ce76b:	75 0a                	jne    ffffffff812ce777 <flex_array_alloc+0x27>
					gfp_t flags)
{
	struct flex_array *ret;
	int elems_per_part = 0;
	int max_size = 0;
	struct reciprocal_value reciprocal_elems = { 0 };
ffffffff812ce76d:	45 31 f6             	xor    %r14d,%r14d
struct flex_array *flex_array_alloc(int element_size, unsigned int total,
					gfp_t flags)
{
	struct flex_array *ret;
	int elems_per_part = 0;
	int max_size = 0;
ffffffff812ce770:	31 c0                	xor    %eax,%eax
 */
struct flex_array *flex_array_alloc(int element_size, unsigned int total,
					gfp_t flags)
{
	struct flex_array *ret;
	int elems_per_part = 0;
ffffffff812ce772:	45 31 ff             	xor    %r15d,%r15d
ffffffff812ce775:	eb 27                	jmp    ffffffff812ce79e <flex_array_alloc+0x4e>
	int max_size = 0;
	struct reciprocal_value reciprocal_elems = { 0 };

	if (element_size) {
		elems_per_part = FLEX_ARRAY_ELEMENTS_PER_PART(element_size);
ffffffff812ce777:	48 63 cf             	movslq %edi,%rcx
ffffffff812ce77a:	b8 00 10 00 00       	mov    $0x1000,%eax
ffffffff812ce77f:	31 d2                	xor    %edx,%edx
ffffffff812ce781:	48 f7 f1             	div    %rcx
ffffffff812ce784:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
ffffffff812ce788:	44 8b 7d c8          	mov    -0x38(%rbp),%r15d
		reciprocal_elems = reciprocal_value(elems_per_part);
ffffffff812ce78c:	44 89 ff             	mov    %r15d,%edi
ffffffff812ce78f:	e8 d9 49 00 00       	callq  ffffffff812d316d <reciprocal_value>
ffffffff812ce794:	49 89 c6             	mov    %rax,%r14
		max_size = FLEX_ARRAY_NR_BASE_PTRS * elems_per_part;
ffffffff812ce797:	69 45 c8 fd 01 00 00 	imul   $0x1fd,-0x38(%rbp),%eax
	}

	/* max_size will end up 0 if element_size > PAGE_SIZE */
	if (total > max_size)
ffffffff812ce79e:	39 d8                	cmp    %ebx,%eax
ffffffff812ce7a0:	73 04                	jae    ffffffff812ce7a6 <flex_array_alloc+0x56>
		return NULL;
ffffffff812ce7a2:	31 c0                	xor    %eax,%eax
ffffffff812ce7a4:	eb 65                	jmp    ffffffff812ce80b <flex_array_alloc+0xbb>
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline void *kzalloc(size_t size, gfp_t flags)
{
	return kmalloc(size, flags | __GFP_ZERO);
ffffffff812ce7a6:	44 89 ee             	mov    %r13d,%esi
ffffffff812ce7a9:	81 ce 00 80 00 00    	or     $0x8000,%esi
#endif
			return kmalloc_container(nsproxy, size, flags);
		}
#endif

		if (!(flags & GFP_DMA)) {
ffffffff812ce7af:	41 f6 c5 01          	test   $0x1,%r13b
ffffffff812ce7b3:	75 0e                	jne    ffffffff812ce7c3 <flex_array_alloc+0x73>

#else /* CONFIG_TRACING */
static __always_inline void *kmem_cache_alloc_trace(struct kmem_cache *s,
		gfp_t flags, size_t size)
{
	void *ret = kmem_cache_alloc(s, flags);
ffffffff812ce7b5:	48 8b 3d e4 3e 8d 00 	mov    0x8d3ee4(%rip),%rdi        # ffffffff81ba26a0 <kmalloc_caches+0x60>
ffffffff812ce7bc:	e8 10 29 e3 ff       	callq  ffffffff811010d1 <kmem_cache_alloc>
ffffffff812ce7c1:	eb 0a                	jmp    ffffffff812ce7cd <flex_array_alloc+0x7d>
			return kmem_cache_alloc_trace(kmalloc_caches[index],
					flags, size);
		}
#endif
	}
	return __kmalloc(size, flags);
ffffffff812ce7c3:	bf 00 10 00 00       	mov    $0x1000,%edi
ffffffff812ce7c8:	e8 be 29 e3 ff       	callq  ffffffff8110118b <__kmalloc>
	ret = kzalloc(sizeof(struct flex_array), flags);
	if (!ret)
ffffffff812ce7cd:	48 85 c0             	test   %rax,%rax
ffffffff812ce7d0:	48 89 c2             	mov    %rax,%rdx
ffffffff812ce7d3:	74 cd                	je     ffffffff812ce7a2 <flex_array_alloc+0x52>
		return NULL;
	ret->element_size = element_size;
	ret->total_nr_elements = total;
ffffffff812ce7d5:	89 58 04             	mov    %ebx,0x4(%rax)
	if (total > max_size)
		return NULL;
	ret = kzalloc(sizeof(struct flex_array), flags);
	if (!ret)
		return NULL;
	ret->element_size = element_size;
ffffffff812ce7d8:	44 89 20             	mov    %r12d,(%rax)
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce7db:	41 0f af dc          	imul   %r12d,%ebx
	ret = kzalloc(sizeof(struct flex_array), flags);
	if (!ret)
		return NULL;
	ret->element_size = element_size;
	ret->total_nr_elements = total;
	ret->elems_per_part = elems_per_part;
ffffffff812ce7df:	44 89 78 08          	mov    %r15d,0x8(%rax)
	ret->reciprocal_elems = reciprocal_elems;
ffffffff812ce7e3:	4c 89 70 0c          	mov    %r14,0xc(%rax)
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce7e7:	81 fb e8 0f 00 00    	cmp    $0xfe8,%ebx
ffffffff812ce7ed:	77 19                	ja     ffffffff812ce808 <flex_array_alloc+0xb8>
		return NULL;
	ret->element_size = element_size;
	ret->total_nr_elements = total;
	ret->elems_per_part = elems_per_part;
	ret->reciprocal_elems = reciprocal_elems;
	if (elements_fit_in_base(ret) && !(flags & __GFP_ZERO))
ffffffff812ce7ef:	41 81 e5 00 80 00 00 	and    $0x8000,%r13d
ffffffff812ce7f6:	75 10                	jne    ffffffff812ce808 <flex_array_alloc+0xb8>
		memset(&ret->parts[0], FLEX_ARRAY_FREE,
ffffffff812ce7f8:	48 8d 72 18          	lea    0x18(%rdx),%rsi
ffffffff812ce7fc:	b9 e8 0f 00 00       	mov    $0xfe8,%ecx
ffffffff812ce801:	b0 6c                	mov    $0x6c,%al
ffffffff812ce803:	48 89 f7             	mov    %rsi,%rdi
ffffffff812ce806:	f3 aa                	rep stos %al,%es:(%rdi)
ffffffff812ce808:	48 89 d0             	mov    %rdx,%rax
						FLEX_ARRAY_BASE_BYTES_LEFT);
	return ret;
}
ffffffff812ce80b:	48 83 c4 18          	add    $0x18,%rsp
ffffffff812ce80f:	5b                   	pop    %rbx
ffffffff812ce810:	41 5c                	pop    %r12
ffffffff812ce812:	41 5d                	pop    %r13
ffffffff812ce814:	41 5e                	pop    %r14
ffffffff812ce816:	41 5f                	pop    %r15
ffffffff812ce818:	5d                   	pop    %rbp
ffffffff812ce819:	c3                   	retq   

ffffffff812ce81a <flex_array_shrink>:
{
	struct flex_array_part *part;
	int part_nr;
	int ret = 0;

	if (!fa->total_nr_elements || !fa->element_size)
ffffffff812ce81a:	8b 47 04             	mov    0x4(%rdi),%eax
ffffffff812ce81d:	85 c0                	test   %eax,%eax
ffffffff812ce81f:	74 6e                	je     ffffffff812ce88f <flex_array_shrink+0x75>
ffffffff812ce821:	8b 17                	mov    (%rdi),%edx
ffffffff812ce823:	85 d2                	test   %edx,%edx
ffffffff812ce825:	74 65                	je     ffffffff812ce88c <flex_array_shrink+0x72>
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce827:	0f af d0             	imul   %eax,%edx
	struct flex_array_part *part;
	int part_nr;
	int ret = 0;

	if (!fa->total_nr_elements || !fa->element_size)
		return 0;
ffffffff812ce82a:	31 c0                	xor    %eax,%eax
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce82c:	81 fa e8 0f 00 00    	cmp    $0xfe8,%edx
ffffffff812ce832:	76 5b                	jbe    ffffffff812ce88f <flex_array_shrink+0x75>
 * elements.  Returns the number of pages freed.
 *
 * Locking must be provided by the caller.
 */
int flex_array_shrink(struct flex_array *fa)
{
ffffffff812ce834:	55                   	push   %rbp
ffffffff812ce835:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce838:	41 55                	push   %r13
ffffffff812ce83a:	41 54                	push   %r12
ffffffff812ce83c:	53                   	push   %rbx
ffffffff812ce83d:	51                   	push   %rcx
ffffffff812ce83e:	49 89 fc             	mov    %rdi,%r12
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce841:	31 db                	xor    %ebx,%ebx
ffffffff812ce843:	45 31 ed             	xor    %r13d,%r13d
	if (!fa->total_nr_elements || !fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		return ret;
	for (part_nr = 0; part_nr < FLEX_ARRAY_NR_BASE_PTRS; part_nr++) {
		part = fa->parts[part_nr];
ffffffff812ce846:	4b 8b 7c 2c 18       	mov    0x18(%r12,%r13,1),%rdi
		if (!part)
ffffffff812ce84b:	48 85 ff             	test   %rdi,%rdi
ffffffff812ce84e:	74 24                	je     ffffffff812ce874 <flex_array_shrink+0x5a>
ffffffff812ce850:	31 c0                	xor    %eax,%eax
ffffffff812ce852:	48 ff c0             	inc    %rax
static int part_is_free(struct flex_array_part *part)
{
	int i;

	for (i = 0; i < sizeof(struct flex_array_part); i++)
		if (part->elements[i] != FLEX_ARRAY_FREE)
ffffffff812ce855:	80 7c 07 ff 6c       	cmpb   $0x6c,-0x1(%rdi,%rax,1)
ffffffff812ce85a:	75 18                	jne    ffffffff812ce874 <flex_array_shrink+0x5a>

static int part_is_free(struct flex_array_part *part)
{
	int i;

	for (i = 0; i < sizeof(struct flex_array_part); i++)
ffffffff812ce85c:	48 3d 00 10 00 00    	cmp    $0x1000,%rax
ffffffff812ce862:	75 ee                	jne    ffffffff812ce852 <flex_array_shrink+0x38>
	for (part_nr = 0; part_nr < FLEX_ARRAY_NR_BASE_PTRS; part_nr++) {
		part = fa->parts[part_nr];
		if (!part)
			continue;
		if (part_is_free(part)) {
			fa->parts[part_nr] = NULL;
ffffffff812ce864:	4b c7 44 2c 18 00 00 	movq   $0x0,0x18(%r12,%r13,1)
ffffffff812ce86b:	00 00 
			kfree(part);
			ret++;
ffffffff812ce86d:	ff c3                	inc    %ebx
		part = fa->parts[part_nr];
		if (!part)
			continue;
		if (part_is_free(part)) {
			fa->parts[part_nr] = NULL;
			kfree(part);
ffffffff812ce86f:	e8 15 21 e3 ff       	callq  ffffffff81100989 <kfree>
ffffffff812ce874:	49 83 c5 08          	add    $0x8,%r13

	if (!fa->total_nr_elements || !fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		return ret;
	for (part_nr = 0; part_nr < FLEX_ARRAY_NR_BASE_PTRS; part_nr++) {
ffffffff812ce878:	49 81 fd e8 0f 00 00 	cmp    $0xfe8,%r13
ffffffff812ce87f:	75 c5                	jne    ffffffff812ce846 <flex_array_shrink+0x2c>
			kfree(part);
			ret++;
		}
	}
	return ret;
}
ffffffff812ce881:	5a                   	pop    %rdx

	if (!fa->total_nr_elements || !fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		return ret;
	for (part_nr = 0; part_nr < FLEX_ARRAY_NR_BASE_PTRS; part_nr++) {
ffffffff812ce882:	89 d8                	mov    %ebx,%eax
			kfree(part);
			ret++;
		}
	}
	return ret;
}
ffffffff812ce884:	5b                   	pop    %rbx
ffffffff812ce885:	41 5c                	pop    %r12
ffffffff812ce887:	41 5d                	pop    %r13
ffffffff812ce889:	5d                   	pop    %rbp
ffffffff812ce88a:	eb 03                	jmp    ffffffff812ce88f <flex_array_shrink+0x75>
	struct flex_array_part *part;
	int part_nr;
	int ret = 0;

	if (!fa->total_nr_elements || !fa->element_size)
		return 0;
ffffffff812ce88c:	31 c0                	xor    %eax,%eax
			kfree(part);
			ret++;
		}
	}
	return ret;
}
ffffffff812ce88e:	c3                   	retq   
ffffffff812ce88f:	c3                   	retq   

ffffffff812ce890 <flex_array_clear>:
{
	int part_nr = 0;
	struct flex_array_part *part;
	void *dst;

	if (element_nr >= fa->total_nr_elements)
ffffffff812ce890:	8b 47 04             	mov    0x4(%rdi),%eax
		return -ENOSPC;
ffffffff812ce893:	41 b9 e4 ff ff ff    	mov    $0xffffffe4,%r9d
{
	int part_nr = 0;
	struct flex_array_part *part;
	void *dst;

	if (element_nr >= fa->total_nr_elements)
ffffffff812ce899:	39 f0                	cmp    %esi,%eax
ffffffff812ce89b:	76 5e                	jbe    ffffffff812ce8fb <flex_array_clear+0x6b>
		return -ENOSPC;
	if (!fa->element_size)
ffffffff812ce89d:	44 8b 0f             	mov    (%rdi),%r9d
ffffffff812ce8a0:	45 85 c9             	test   %r9d,%r9d
ffffffff812ce8a3:	74 56                	je     ffffffff812ce8fb <flex_array_clear+0x6b>
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce8a5:	41 0f af c1          	imul   %r9d,%eax
 * @element_nr:	index of the position to clear.
 *
 * Locking must be provided by the caller.
 */
int flex_array_clear(struct flex_array *fa, unsigned int element_nr)
{
ffffffff812ce8a9:	55                   	push   %rbp
ffffffff812ce8aa:	41 89 f0             	mov    %esi,%r8d
ffffffff812ce8ad:	49 89 fa             	mov    %rdi,%r10
ffffffff812ce8b0:	48 89 e5             	mov    %rsp,%rbp
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce8b3:	3d e8 0f 00 00       	cmp    $0xfe8,%eax
ffffffff812ce8b8:	77 24                	ja     ffffffff812ce8de <flex_array_clear+0x4e>
	if (element_nr >= fa->total_nr_elements)
		return -ENOSPC;
	if (!fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
ffffffff812ce8ba:	48 8d 57 18          	lea    0x18(%rdi),%rdx
 *
 * Locking must be provided by the caller.
 */
int flex_array_clear(struct flex_array *fa, unsigned int element_nr)
{
	int part_nr = 0;
ffffffff812ce8be:	31 c0                	xor    %eax,%eax
		part = fa->parts[part_nr];
		if (!part)
			return -EINVAL;
	}
	dst = &part->elements[index_inside_part(fa, element_nr, part_nr)];
	memset(dst, FLEX_ARRAY_FREE, fa->element_size);
ffffffff812ce8c0:	41 0f af 42 08       	imul   0x8(%r10),%eax
ffffffff812ce8c5:	49 63 c9             	movslq %r9d,%rcx
ffffffff812ce8c8:	41 29 c0             	sub    %eax,%r8d
ffffffff812ce8cb:	b0 6c                	mov    $0x6c,%al
ffffffff812ce8cd:	45 0f af c1          	imul   %r9d,%r8d
	return 0;
ffffffff812ce8d1:	45 31 c9             	xor    %r9d,%r9d
		part = fa->parts[part_nr];
		if (!part)
			return -EINVAL;
	}
	dst = &part->elements[index_inside_part(fa, element_nr, part_nr)];
	memset(dst, FLEX_ARRAY_FREE, fa->element_size);
ffffffff812ce8d4:	49 01 d0             	add    %rdx,%r8
ffffffff812ce8d7:	4c 89 c7             	mov    %r8,%rdi
ffffffff812ce8da:	f3 aa                	rep stos %al,%es:(%rdi)
	return 0;
ffffffff812ce8dc:	eb 18                	jmp    ffffffff812ce8f6 <flex_array_clear+0x66>
	if (!fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		part = (struct flex_array_part *)&fa->parts[0];
	else {
		part_nr = fa_element_to_part_nr(fa, element_nr);
ffffffff812ce8de:	e8 88 fc ff ff       	callq  ffffffff812ce56b <fa_element_to_part_nr>
		part = fa->parts[part_nr];
ffffffff812ce8e3:	48 63 d0             	movslq %eax,%rdx
ffffffff812ce8e6:	48 8b 54 d7 18       	mov    0x18(%rdi,%rdx,8),%rdx
		if (!part)
ffffffff812ce8eb:	48 85 d2             	test   %rdx,%rdx
ffffffff812ce8ee:	75 d0                	jne    ffffffff812ce8c0 <flex_array_clear+0x30>
			return -EINVAL;
ffffffff812ce8f0:	41 b9 ea ff ff ff    	mov    $0xffffffea,%r9d
	}
	dst = &part->elements[index_inside_part(fa, element_nr, part_nr)];
	memset(dst, FLEX_ARRAY_FREE, fa->element_size);
	return 0;
}
ffffffff812ce8f6:	44 89 c8             	mov    %r9d,%eax
ffffffff812ce8f9:	5d                   	pop    %rbp
ffffffff812ce8fa:	c3                   	retq   
ffffffff812ce8fb:	44 89 c8             	mov    %r9d,%eax
ffffffff812ce8fe:	c3                   	retq   

ffffffff812ce8ff <flex_array_prealloc>:
	int end_part;
	int part_nr;
	unsigned int end;
	struct flex_array_part *part;

	if (!start && !nr_elements)
ffffffff812ce8ff:	89 f0                	mov    %esi,%eax
ffffffff812ce901:	09 d0                	or     %edx,%eax
ffffffff812ce903:	74 2d                	je     ffffffff812ce932 <flex_array_prealloc+0x33>
		return 0;
	if (start >= fa->total_nr_elements)
ffffffff812ce905:	44 8b 47 04          	mov    0x4(%rdi),%r8d
		return -ENOSPC;
ffffffff812ce909:	b8 e4 ff ff ff       	mov    $0xffffffe4,%eax
	unsigned int end;
	struct flex_array_part *part;

	if (!start && !nr_elements)
		return 0;
	if (start >= fa->total_nr_elements)
ffffffff812ce90e:	44 39 c6             	cmp    %r8d,%esi
ffffffff812ce911:	73 7f                	jae    ffffffff812ce992 <flex_array_prealloc+0x93>
		return -ENOSPC;
	if (!nr_elements)
ffffffff812ce913:	85 d2                	test   %edx,%edx
ffffffff812ce915:	74 1b                	je     ffffffff812ce932 <flex_array_prealloc+0x33>
		return 0;

	end = start + nr_elements - 1;
ffffffff812ce917:	44 8d 4c 16 ff       	lea    -0x1(%rsi,%rdx,1),%r9d

	if (end >= fa->total_nr_elements)
ffffffff812ce91c:	45 39 c8             	cmp    %r9d,%r8d
ffffffff812ce91f:	76 71                	jbe    ffffffff812ce992 <flex_array_prealloc+0x93>
		return -ENOSPC;
	if (!fa->element_size)
ffffffff812ce921:	8b 07                	mov    (%rdi),%eax
ffffffff812ce923:	85 c0                	test   %eax,%eax
ffffffff812ce925:	74 0b                	je     ffffffff812ce932 <flex_array_prealloc+0x33>
 * data.
 */
static inline int elements_fit_in_base(struct flex_array *fa)
{
	int data_size = fa->element_size * fa->total_nr_elements;
	if (data_size <= FLEX_ARRAY_BASE_BYTES_LEFT)
ffffffff812ce927:	41 0f af c0          	imul   %r8d,%eax
ffffffff812ce92b:	3d e8 0f 00 00       	cmp    $0xfe8,%eax
ffffffff812ce930:	77 03                	ja     ffffffff812ce935 <flex_array_prealloc+0x36>
	int part_nr;
	unsigned int end;
	struct flex_array_part *part;

	if (!start && !nr_elements)
		return 0;
ffffffff812ce932:	31 c0                	xor    %eax,%eax
		part = __fa_get_part(fa, part_nr, flags);
		if (!part)
			return -ENOMEM;
	}
	return 0;
}
ffffffff812ce934:	c3                   	retq   
 *
 * Locking must be provided by the caller.
 */
int flex_array_prealloc(struct flex_array *fa, unsigned int start,
			unsigned int nr_elements, gfp_t flags)
{
ffffffff812ce935:	55                   	push   %rbp
ffffffff812ce936:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ce939:	41 56                	push   %r14
ffffffff812ce93b:	41 55                	push   %r13
ffffffff812ce93d:	41 54                	push   %r12
ffffffff812ce93f:	53                   	push   %rbx
ffffffff812ce940:	41 89 cd             	mov    %ecx,%r13d
ffffffff812ce943:	48 89 fb             	mov    %rdi,%rbx
		return -ENOSPC;
	if (!fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		return 0;
	start_part = fa_element_to_part_nr(fa, start);
ffffffff812ce946:	e8 20 fc ff ff       	callq  ffffffff812ce56b <fa_element_to_part_nr>
	end_part = fa_element_to_part_nr(fa, end);
ffffffff812ce94b:	44 89 ce             	mov    %r9d,%esi
		return -ENOSPC;
	if (!fa->element_size)
		return 0;
	if (elements_fit_in_base(fa))
		return 0;
	start_part = fa_element_to_part_nr(fa, start);
ffffffff812ce94e:	41 89 c4             	mov    %eax,%r12d
	end_part = fa_element_to_part_nr(fa, end);
ffffffff812ce951:	e8 15 fc ff ff       	callq  ffffffff812ce56b <fa_element_to_part_nr>
ffffffff812ce956:	41 89 c6             	mov    %eax,%r14d
	for (part_nr = start_part; part_nr <= end_part; part_nr++) {
ffffffff812ce959:	45 39 e6             	cmp    %r12d,%r14d
ffffffff812ce95c:	7d 04                	jge    ffffffff812ce962 <flex_array_prealloc+0x63>
	int part_nr;
	unsigned int end;
	struct flex_array_part *part;

	if (!start && !nr_elements)
		return 0;
ffffffff812ce95e:	31 c0                	xor    %eax,%eax
ffffffff812ce960:	eb 28                	jmp    ffffffff812ce98a <flex_array_prealloc+0x8b>
}

static struct flex_array_part *
__fa_get_part(struct flex_array *fa, int part_nr, gfp_t flags)
{
	struct flex_array_part *part = fa->parts[part_nr];
ffffffff812ce962:	49 63 c4             	movslq %r12d,%rax
	if (!part) {
ffffffff812ce965:	48 83 7c c3 18 00    	cmpq   $0x0,0x18(%rbx,%rax,8)
ffffffff812ce96b:	74 05                	je     ffffffff812ce972 <flex_array_prealloc+0x73>
		return 0;
	if (elements_fit_in_base(fa))
		return 0;
	start_part = fa_element_to_part_nr(fa, start);
	end_part = fa_element_to_part_nr(fa, end);
	for (part_nr = start_part; part_nr <= end_part; part_nr++) {
ffffffff812ce96d:	41 ff c4             	inc    %r12d
ffffffff812ce970:	eb e7                	jmp    ffffffff812ce959 <flex_array_prealloc+0x5a>
ffffffff812ce972:	44 89 ea             	mov    %r13d,%edx
ffffffff812ce975:	44 89 e6             	mov    %r12d,%esi
ffffffff812ce978:	48 89 df             	mov    %rbx,%rdi
ffffffff812ce97b:	e8 0f fc ff ff       	callq  ffffffff812ce58f <__fa_get_part.part.2>
		part = __fa_get_part(fa, part_nr, flags);
		if (!part)
ffffffff812ce980:	48 85 c0             	test   %rax,%rax
ffffffff812ce983:	75 e8                	jne    ffffffff812ce96d <flex_array_prealloc+0x6e>
			return -ENOMEM;
ffffffff812ce985:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
	}
	return 0;
}
ffffffff812ce98a:	5b                   	pop    %rbx
ffffffff812ce98b:	41 5c                	pop    %r12
ffffffff812ce98d:	41 5d                	pop    %r13
ffffffff812ce98f:	41 5e                	pop    %r14
ffffffff812ce991:	5d                   	pop    %rbp
ffffffff812ce992:	c3                   	retq   

ffffffff812ce993 <kmap_atomic>:
static inline void kunmap(struct page *page)
{
}

static inline void *kmap_atomic(struct page *page)
{
ffffffff812ce993:	55                   	push   %rbp
ffffffff812ce994:	65 ff 05 fd bf d3 7e 	incl   %gs:0x7ed3bffd(%rip)        # a998 <__preempt_count>
ffffffff812ce99b:	48 89 e5             	mov    %rsp,%rbp
	pagefault_disable();
	return page_address(page);
ffffffff812ce99e:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
ffffffff812ce9a5:	16 00 00 
ffffffff812ce9a8:	48 01 c7             	add    %rax,%rdi
ffffffff812ce9ab:	48 b8 00 00 00 00 00 	movabs $0xffff880000000000,%rax
ffffffff812ce9b2:	88 ff ff 
ffffffff812ce9b5:	48 c1 ff 06          	sar    $0x6,%rdi
ffffffff812ce9b9:	48 c1 e7 0c          	shl    $0xc,%rdi
ffffffff812ce9bd:	48 01 f8             	add    %rdi,%rax
}
ffffffff812ce9c0:	5d                   	pop    %rbp
ffffffff812ce9c1:	c3                   	retq   

ffffffff812ce9c2 <csum_block_add>:
	return csum16_add(csum, ~addend);
}

static inline __wsum
csum_block_add(__wsum csum, __wsum csum2, int offset)
{
ffffffff812ce9c2:	55                   	push   %rbp
	u32 sum = (__force u32)csum2;
	if (offset&1)
ffffffff812ce9c3:	80 e2 01             	and    $0x1,%dl
	return csum16_add(csum, ~addend);
}

static inline __wsum
csum_block_add(__wsum csum, __wsum csum2, int offset)
{
ffffffff812ce9c6:	48 89 e5             	mov    %rsp,%rbp
	u32 sum = (__force u32)csum2;
	if (offset&1)
ffffffff812ce9c9:	74 15                	je     ffffffff812ce9e0 <csum_block_add+0x1e>
		sum = ((sum&0xFF00FF)<<8)+((sum>>8)&0xFF00FF);
ffffffff812ce9cb:	89 f0                	mov    %esi,%eax
ffffffff812ce9cd:	c1 ee 08             	shr    $0x8,%esi
ffffffff812ce9d0:	25 ff 00 ff 00       	and    $0xff00ff,%eax
ffffffff812ce9d5:	81 e6 ff 00 ff 00    	and    $0xff00ff,%esi
ffffffff812ce9db:	c1 e0 08             	shl    $0x8,%eax
ffffffff812ce9de:	01 c6                	add    %eax,%esi
csum_ipv6_magic(const struct in6_addr *saddr, const struct in6_addr *daddr,
		__u32 len, unsigned short proto, __wsum sum);

static inline unsigned add32_with_carry(unsigned a, unsigned b)
{
	asm("addl %2,%0\n\t"
ffffffff812ce9e0:	89 f8                	mov    %edi,%eax
	return csum_add(csum, (__force __wsum)sum);
}
ffffffff812ce9e2:	5d                   	pop    %rbp
ffffffff812ce9e3:	01 f0                	add    %esi,%eax
ffffffff812ce9e5:	83 d0 00             	adc    $0x0,%eax
ffffffff812ce9e8:	c3                   	retq   

ffffffff812ce9e9 <iov_iter_init>:
EXPORT_SYMBOL(iov_iter_fault_in_multipages_readable);

void iov_iter_init(struct iov_iter *i, int direction,
			const struct iovec *iov, unsigned long nr_segs,
			size_t count)
{
ffffffff812ce9e9:	55                   	push   %rbp
ffffffff812ce9ea:	65 48 8b 04 25 04 22 	mov    %gs:0x12204,%rax
ffffffff812ce9f1:	01 00 
	/* It will get better.  Eventually... */
	if (segment_eq(get_fs(), KERNEL_DS)) {
ffffffff812ce9f3:	48 83 b8 18 c0 ff ff 	cmpq   $0xffffffffffffffff,-0x3fe8(%rax)
ffffffff812ce9fa:	ff 
EXPORT_SYMBOL(iov_iter_fault_in_multipages_readable);

void iov_iter_init(struct iov_iter *i, int direction,
			const struct iovec *iov, unsigned long nr_segs,
			size_t count)
{
ffffffff812ce9fb:	48 89 e5             	mov    %rsp,%rbp
	/* It will get better.  Eventually... */
	if (segment_eq(get_fs(), KERNEL_DS)) {
ffffffff812ce9fe:	75 03                	jne    ffffffff812cea03 <iov_iter_init+0x1a>
		direction |= ITER_KVEC;
		i->type = direction;
ffffffff812cea00:	83 ce 02             	or     $0x2,%esi
		i->kvec = (struct kvec *)iov;
	} else {
		i->type = direction;
ffffffff812cea03:	89 37                	mov    %esi,(%rdi)
		i->iov = iov;
ffffffff812cea05:	48 89 57 18          	mov    %rdx,0x18(%rdi)
	}
	i->nr_segs = nr_segs;
ffffffff812cea09:	48 89 4f 20          	mov    %rcx,0x20(%rdi)
	i->iov_offset = 0;
ffffffff812cea0d:	48 c7 47 08 00 00 00 	movq   $0x0,0x8(%rdi)
ffffffff812cea14:	00 
	i->count = count;
ffffffff812cea15:	4c 89 47 10          	mov    %r8,0x10(%rdi)
}
ffffffff812cea19:	5d                   	pop    %rbp
ffffffff812cea1a:	c3                   	retq   

ffffffff812cea1b <fault_in_pages_readable>:
static inline int fault_in_pages_readable(const char __user *uaddr, int size)
{
	volatile char c;
	int ret;

	if (unlikely(size == 0))
ffffffff812cea1b:	85 f6                	test   %esi,%esi
ffffffff812cea1d:	74 41                	je     ffffffff812cea60 <fault_in_pages_readable+0x45>
	}
	return ret;
}

static inline int fault_in_pages_readable(const char __user *uaddr, int size)
{
ffffffff812cea1f:	55                   	push   %rbp
	int ret;

	if (unlikely(size == 0))
		return 0;

	ret = __get_user(c, uaddr);
ffffffff812cea20:	31 c0                	xor    %eax,%eax
	}
	return ret;
}

static inline int fault_in_pages_readable(const char __user *uaddr, int size)
{
ffffffff812cea22:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cea25:	48 83 ec 10          	sub    $0x10,%rsp
ffffffff812cea29:	90                   	nop
ffffffff812cea2a:	90                   	nop
ffffffff812cea2b:	90                   	nop
	int ret;

	if (unlikely(size == 0))
		return 0;

	ret = __get_user(c, uaddr);
ffffffff812cea2c:	8a 17                	mov    (%rdi),%dl
ffffffff812cea2e:	90                   	nop
ffffffff812cea2f:	90                   	nop
ffffffff812cea30:	90                   	nop
	if (ret == 0) {
ffffffff812cea31:	85 c0                	test   %eax,%eax
	int ret;

	if (unlikely(size == 0))
		return 0;

	ret = __get_user(c, uaddr);
ffffffff812cea33:	88 55 ff             	mov    %dl,-0x1(%rbp)
	if (ret == 0) {
ffffffff812cea36:	75 26                	jne    ffffffff812cea5e <fault_in_pages_readable+0x43>
		const char __user *end = uaddr + size - 1;
ffffffff812cea38:	48 63 f6             	movslq %esi,%rsi
ffffffff812cea3b:	48 8d 54 37 ff       	lea    -0x1(%rdi,%rsi,1),%rdx

		if (((unsigned long)uaddr & PAGE_MASK) !=
ffffffff812cea40:	48 31 d7             	xor    %rdx,%rdi
ffffffff812cea43:	48 f7 c7 00 f0 ff ff 	test   $0xfffffffffffff000,%rdi
ffffffff812cea4a:	75 04                	jne    ffffffff812cea50 <fault_in_pages_readable+0x35>
{
	volatile char c;
	int ret;

	if (unlikely(size == 0))
		return 0;
ffffffff812cea4c:	31 c0                	xor    %eax,%eax
ffffffff812cea4e:	eb 0e                	jmp    ffffffff812cea5e <fault_in_pages_readable+0x43>
ffffffff812cea50:	90                   	nop
ffffffff812cea51:	90                   	nop
ffffffff812cea52:	90                   	nop
	if (ret == 0) {
		const char __user *end = uaddr + size - 1;

		if (((unsigned long)uaddr & PAGE_MASK) !=
				((unsigned long)end & PAGE_MASK)) {
			ret = __get_user(c, end);
ffffffff812cea53:	8a 12                	mov    (%rdx),%dl
ffffffff812cea55:	90                   	nop
ffffffff812cea56:	90                   	nop
ffffffff812cea57:	90                   	nop
ffffffff812cea58:	88 55 ff             	mov    %dl,-0x1(%rbp)
			(void)c;
ffffffff812cea5b:	8a 55 ff             	mov    -0x1(%rbp),%dl
		}
	}
	return ret;
}
ffffffff812cea5e:	c9                   	leaveq 
ffffffff812cea5f:	c3                   	retq   
{
	volatile char c;
	int ret;

	if (unlikely(size == 0))
		return 0;
ffffffff812cea60:	31 c0                	xor    %eax,%eax
			ret = __get_user(c, end);
			(void)c;
		}
	}
	return ret;
}
ffffffff812cea62:	c3                   	retq   

ffffffff812cea63 <fault_in_multipages_readable>:
static inline int fault_in_multipages_readable(const char __user *uaddr,
					       int size)
{
	volatile char c;
	int ret = 0;
	const char __user *end = uaddr + size - 1;
ffffffff812cea63:	48 63 c6             	movslq %esi,%rax

	if (unlikely(size == 0))
ffffffff812cea66:	85 f6                	test   %esi,%esi
static inline int fault_in_multipages_readable(const char __user *uaddr,
					       int size)
{
	volatile char c;
	int ret = 0;
	const char __user *end = uaddr + size - 1;
ffffffff812cea68:	48 8d 54 07 ff       	lea    -0x1(%rdi,%rax,1),%rdx

	if (unlikely(size == 0))
ffffffff812cea6d:	74 49                	je     ffffffff812ceab8 <fault_in_multipages_readable+0x55>
	return ret;
}

static inline int fault_in_multipages_readable(const char __user *uaddr,
					       int size)
{
ffffffff812cea6f:	55                   	push   %rbp

	if (unlikely(size == 0))
		return ret;

	while (uaddr <= end) {
		ret = __get_user(c, uaddr);
ffffffff812cea70:	31 c9                	xor    %ecx,%ecx
	return ret;
}

static inline int fault_in_multipages_readable(const char __user *uaddr,
					       int size)
{
ffffffff812cea72:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cea75:	48 83 ec 10          	sub    $0x10,%rsp
	const char __user *end = uaddr + size - 1;

	if (unlikely(size == 0))
		return ret;

	while (uaddr <= end) {
ffffffff812cea79:	48 39 d7             	cmp    %rdx,%rdi
ffffffff812cea7c:	77 1c                	ja     ffffffff812cea9a <fault_in_multipages_readable+0x37>
		ret = __get_user(c, uaddr);
ffffffff812cea7e:	89 c8                	mov    %ecx,%eax
ffffffff812cea80:	90                   	nop
ffffffff812cea81:	90                   	nop
ffffffff812cea82:	90                   	nop
ffffffff812cea83:	40 8a 37             	mov    (%rdi),%sil
ffffffff812cea86:	90                   	nop
ffffffff812cea87:	90                   	nop
ffffffff812cea88:	90                   	nop
		if (ret != 0)
ffffffff812cea89:	85 c0                	test   %eax,%eax

	if (unlikely(size == 0))
		return ret;

	while (uaddr <= end) {
		ret = __get_user(c, uaddr);
ffffffff812cea8b:	40 88 75 ff          	mov    %sil,-0x1(%rbp)
		if (ret != 0)
ffffffff812cea8f:	75 25                	jne    ffffffff812ceab6 <fault_in_multipages_readable+0x53>
			return ret;
		uaddr += PAGE_SIZE;
ffffffff812cea91:	48 81 c7 00 10 00 00 	add    $0x1000,%rdi
ffffffff812cea98:	eb df                	jmp    ffffffff812cea79 <fault_in_multipages_readable+0x16>
	}

	/* Check whether the range spilled into the next page. */
	if (((unsigned long)uaddr & PAGE_MASK) ==
ffffffff812cea9a:	48 31 d7             	xor    %rdx,%rdi
			((unsigned long)end & PAGE_MASK)) {
		ret = __get_user(c, end);
ffffffff812cea9d:	31 c0                	xor    %eax,%eax
			return ret;
		uaddr += PAGE_SIZE;
	}

	/* Check whether the range spilled into the next page. */
	if (((unsigned long)uaddr & PAGE_MASK) ==
ffffffff812cea9f:	48 f7 c7 00 f0 ff ff 	test   $0xfffffffffffff000,%rdi
ffffffff812ceaa6:	75 0e                	jne    ffffffff812ceab6 <fault_in_multipages_readable+0x53>
ffffffff812ceaa8:	90                   	nop
ffffffff812ceaa9:	90                   	nop
ffffffff812ceaaa:	90                   	nop
			((unsigned long)end & PAGE_MASK)) {
		ret = __get_user(c, end);
ffffffff812ceaab:	8a 12                	mov    (%rdx),%dl
ffffffff812ceaad:	90                   	nop
ffffffff812ceaae:	90                   	nop
ffffffff812ceaaf:	90                   	nop
ffffffff812ceab0:	88 55 ff             	mov    %dl,-0x1(%rbp)
		(void)c;
ffffffff812ceab3:	8a 55 ff             	mov    -0x1(%rbp),%dl
	}

	return ret;
}
ffffffff812ceab6:	c9                   	leaveq 
ffffffff812ceab7:	c3                   	retq   
	volatile char c;
	int ret = 0;
	const char __user *end = uaddr + size - 1;

	if (unlikely(size == 0))
		return ret;
ffffffff812ceab8:	31 c0                	xor    %eax,%eax
		ret = __get_user(c, end);
		(void)c;
	}

	return ret;
}
ffffffff812ceaba:	c3                   	retq   

ffffffff812ceabb <iov_iter_fault_in_multipages_readable>:
	size_t skip = i->iov_offset;
	const struct iovec *iov;
	int err;
	struct iovec v;

	if (!(i->type & (ITER_BVEC|ITER_KVEC))) {
ffffffff812ceabb:	f6 07 06             	testb  $0x6,(%rdi)
ffffffff812ceabe:	75 7c                	jne    ffffffff812ceb3c <iov_iter_fault_in_multipages_readable+0x81>
 *
 * Return 0 on success, or non-zero if the memory could not be accessed (i.e.
 * because it is an invalid address).
 */
int iov_iter_fault_in_multipages_readable(struct iov_iter *i, size_t bytes)
{
ffffffff812ceac0:	55                   	push   %rbp
ffffffff812ceac1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ceac4:	41 57                	push   %r15
ffffffff812ceac6:	41 56                	push   %r14
ffffffff812ceac8:	53                   	push   %rbx
	const struct iovec *iov;
	int err;
	struct iovec v;

	if (!(i->type & (ITER_BVEC|ITER_KVEC))) {
		iterate_iovec(i, bytes, v, iov, skip, ({
ffffffff812ceac9:	4c 8b 77 18          	mov    0x18(%rdi),%r14
ffffffff812ceacd:	48 89 f3             	mov    %rsi,%rbx
 * Return 0 on success, or non-zero if the memory could not be accessed (i.e.
 * because it is an invalid address).
 */
int iov_iter_fault_in_multipages_readable(struct iov_iter *i, size_t bytes)
{
	size_t skip = i->iov_offset;
ffffffff812cead0:	48 8b 47 08          	mov    0x8(%rdi),%rax
	const struct iovec *iov;
	int err;
	struct iovec v;

	if (!(i->type & (ITER_BVEC|ITER_KVEC))) {
		iterate_iovec(i, bytes, v, iov, skip, ({
ffffffff812cead4:	4d 8b 4e 08          	mov    0x8(%r14),%r9
ffffffff812cead8:	49 29 c1             	sub    %rax,%r9
ffffffff812ceadb:	49 39 f1             	cmp    %rsi,%r9
ffffffff812ceade:	4d 89 cf             	mov    %r9,%r15
ffffffff812ceae1:	4c 0f 47 fe          	cmova  %rsi,%r15
ffffffff812ceae5:	4d 85 ff             	test   %r15,%r15
ffffffff812ceae8:	74 17                	je     ffffffff812ceb01 <iov_iter_fault_in_multipages_readable+0x46>
ffffffff812ceaea:	49 03 06             	add    (%r14),%rax
ffffffff812ceaed:	44 89 fe             	mov    %r15d,%esi
ffffffff812ceaf0:	48 89 c7             	mov    %rax,%rdi
ffffffff812ceaf3:	e8 6b ff ff ff       	callq  ffffffff812cea63 <fault_in_multipages_readable>
ffffffff812ceaf8:	85 c0                	test   %eax,%eax
ffffffff812ceafa:	89 c2                	mov    %eax,%edx
ffffffff812ceafc:	75 35                	jne    ffffffff812ceb33 <iov_iter_fault_in_multipages_readable+0x78>
ffffffff812ceafe:	4c 29 fb             	sub    %r15,%rbx
ffffffff812ceb01:	48 85 db             	test   %rbx,%rbx
ffffffff812ceb04:	75 04                	jne    ffffffff812ceb0a <iov_iter_fault_in_multipages_readable+0x4f>
					v.iov_len);
			if (unlikely(err))
			return err;
		0;}))
	}
	return 0;
ffffffff812ceb06:	31 d2                	xor    %edx,%edx
ffffffff812ceb08:	eb 29                	jmp    ffffffff812ceb33 <iov_iter_fault_in_multipages_readable+0x78>
	const struct iovec *iov;
	int err;
	struct iovec v;

	if (!(i->type & (ITER_BVEC|ITER_KVEC))) {
		iterate_iovec(i, bytes, v, iov, skip, ({
ffffffff812ceb0a:	49 83 c6 10          	add    $0x10,%r14
ffffffff812ceb0e:	49 89 d9             	mov    %rbx,%r9
ffffffff812ceb11:	49 39 5e 08          	cmp    %rbx,0x8(%r14)
ffffffff812ceb15:	4d 0f 46 4e 08       	cmovbe 0x8(%r14),%r9
ffffffff812ceb1a:	4d 85 c9             	test   %r9,%r9
ffffffff812ceb1d:	4d 89 cf             	mov    %r9,%r15
ffffffff812ceb20:	74 df                	je     ffffffff812ceb01 <iov_iter_fault_in_multipages_readable+0x46>
ffffffff812ceb22:	49 8b 3e             	mov    (%r14),%rdi
ffffffff812ceb25:	44 89 ce             	mov    %r9d,%esi
ffffffff812ceb28:	e8 36 ff ff ff       	callq  ffffffff812cea63 <fault_in_multipages_readable>
ffffffff812ceb2d:	85 c0                	test   %eax,%eax
ffffffff812ceb2f:	89 c2                	mov    %eax,%edx
ffffffff812ceb31:	74 cb                	je     ffffffff812ceafe <iov_iter_fault_in_multipages_readable+0x43>
			if (unlikely(err))
			return err;
		0;}))
	}
	return 0;
}
ffffffff812ceb33:	5b                   	pop    %rbx
ffffffff812ceb34:	89 d0                	mov    %edx,%eax
ffffffff812ceb36:	41 5e                	pop    %r14
ffffffff812ceb38:	41 5f                	pop    %r15
ffffffff812ceb3a:	5d                   	pop    %rbp
ffffffff812ceb3b:	c3                   	retq   
					v.iov_len);
			if (unlikely(err))
			return err;
		0;}))
	}
	return 0;
ffffffff812ceb3c:	31 d2                	xor    %edx,%edx
}
ffffffff812ceb3e:	89 d0                	mov    %edx,%eax
ffffffff812ceb40:	c3                   	retq   

ffffffff812ceb41 <iov_iter_advance>:
}
EXPORT_SYMBOL(iov_iter_copy_from_user_atomic);

void iov_iter_advance(struct iov_iter *i, size_t size)
{
	iterate_and_advance(i, size, v, 0, 0, 0)
ffffffff812ceb41:	8b 17                	mov    (%rdi),%edx
	return bytes;
}
EXPORT_SYMBOL(iov_iter_copy_from_user_atomic);

void iov_iter_advance(struct iov_iter *i, size_t size)
{
ffffffff812ceb43:	55                   	push   %rbp
	iterate_and_advance(i, size, v, 0, 0, 0)
ffffffff812ceb44:	48 8b 47 08          	mov    0x8(%rdi),%rax
	return bytes;
}
EXPORT_SYMBOL(iov_iter_copy_from_user_atomic);

void iov_iter_advance(struct iov_iter *i, size_t size)
{
ffffffff812ceb48:	48 89 e5             	mov    %rsp,%rbp
	iterate_and_advance(i, size, v, 0, 0, 0)
ffffffff812ceb4b:	f6 c2 04             	test   $0x4,%dl
ffffffff812ceb4e:	74 5e                	je     ffffffff812cebae <iov_iter_advance+0x6d>
ffffffff812ceb50:	4c 8b 4f 18          	mov    0x18(%rdi),%r9
ffffffff812ceb54:	48 89 f1             	mov    %rsi,%rcx
ffffffff812ceb57:	41 8b 51 08          	mov    0x8(%r9),%edx
ffffffff812ceb5b:	48 29 c2             	sub    %rax,%rdx
ffffffff812ceb5e:	48 39 f2             	cmp    %rsi,%rdx
ffffffff812ceb61:	48 0f 47 d6          	cmova  %rsi,%rdx
ffffffff812ceb65:	85 d2                	test   %edx,%edx
ffffffff812ceb67:	74 08                	je     ffffffff812ceb71 <iov_iter_advance+0x30>
ffffffff812ceb69:	89 d2                	mov    %edx,%edx
ffffffff812ceb6b:	48 01 d0             	add    %rdx,%rax
ffffffff812ceb6e:	48 29 d1             	sub    %rdx,%rcx
ffffffff812ceb71:	4c 89 ca             	mov    %r9,%rdx
ffffffff812ceb74:	48 85 c9             	test   %rcx,%rcx
ffffffff812ceb77:	74 1c                	je     ffffffff812ceb95 <iov_iter_advance+0x54>
ffffffff812ceb79:	48 83 c2 10          	add    $0x10,%rdx
ffffffff812ceb7d:	44 8b 42 08          	mov    0x8(%rdx),%r8d
ffffffff812ceb81:	49 39 c8             	cmp    %rcx,%r8
ffffffff812ceb84:	4c 0f 47 c1          	cmova  %rcx,%r8
ffffffff812ceb88:	4d 85 c0             	test   %r8,%r8
ffffffff812ceb8b:	74 e7                	je     ffffffff812ceb74 <iov_iter_advance+0x33>
ffffffff812ceb8d:	4c 29 c1             	sub    %r8,%rcx
ffffffff812ceb90:	4c 89 c0             	mov    %r8,%rax
ffffffff812ceb93:	eb df                	jmp    ffffffff812ceb74 <iov_iter_advance+0x33>
ffffffff812ceb95:	8b 4a 08             	mov    0x8(%rdx),%ecx
ffffffff812ceb98:	48 39 c8             	cmp    %rcx,%rax
ffffffff812ceb9b:	75 06                	jne    ffffffff812ceba3 <iov_iter_advance+0x62>
ffffffff812ceb9d:	48 83 c2 10          	add    $0x10,%rdx
ffffffff812ceba1:	31 c0                	xor    %eax,%eax
ffffffff812ceba3:	48 89 d1             	mov    %rdx,%rcx
ffffffff812ceba6:	4c 29 c9             	sub    %r9,%rcx
ffffffff812ceba9:	e9 99 00 00 00       	jmpq   ffffffff812cec47 <iov_iter_advance+0x106>
ffffffff812cebae:	4c 8b 47 18          	mov    0x18(%rdi),%r8
ffffffff812cebb2:	80 e2 02             	and    $0x2,%dl
ffffffff812cebb5:	49 8b 50 08          	mov    0x8(%r8),%rdx
ffffffff812cebb9:	74 3d                	je     ffffffff812cebf8 <iov_iter_advance+0xb7>
ffffffff812cebbb:	48 29 c2             	sub    %rax,%rdx
ffffffff812cebbe:	48 89 f1             	mov    %rsi,%rcx
ffffffff812cebc1:	48 39 f2             	cmp    %rsi,%rdx
ffffffff812cebc4:	48 0f 47 d6          	cmova  %rsi,%rdx
ffffffff812cebc8:	48 85 d2             	test   %rdx,%rdx
ffffffff812cebcb:	74 06                	je     ffffffff812cebd3 <iov_iter_advance+0x92>
ffffffff812cebcd:	48 01 d0             	add    %rdx,%rax
ffffffff812cebd0:	48 29 d1             	sub    %rdx,%rcx
ffffffff812cebd3:	4c 89 c2             	mov    %r8,%rdx
ffffffff812cebd6:	48 85 c9             	test   %rcx,%rcx
ffffffff812cebd9:	74 5a                	je     ffffffff812cec35 <iov_iter_advance+0xf4>
ffffffff812cebdb:	48 83 c2 10          	add    $0x10,%rdx
ffffffff812cebdf:	49 89 c9             	mov    %rcx,%r9
ffffffff812cebe2:	48 39 4a 08          	cmp    %rcx,0x8(%rdx)
ffffffff812cebe6:	4c 0f 46 4a 08       	cmovbe 0x8(%rdx),%r9
ffffffff812cebeb:	4d 85 c9             	test   %r9,%r9
ffffffff812cebee:	74 e6                	je     ffffffff812cebd6 <iov_iter_advance+0x95>
ffffffff812cebf0:	4c 29 c9             	sub    %r9,%rcx
ffffffff812cebf3:	4c 89 c8             	mov    %r9,%rax
ffffffff812cebf6:	eb de                	jmp    ffffffff812cebd6 <iov_iter_advance+0x95>
ffffffff812cebf8:	48 29 c2             	sub    %rax,%rdx
ffffffff812cebfb:	48 89 f1             	mov    %rsi,%rcx
ffffffff812cebfe:	48 39 f2             	cmp    %rsi,%rdx
ffffffff812cec01:	48 0f 47 d6          	cmova  %rsi,%rdx
ffffffff812cec05:	48 85 d2             	test   %rdx,%rdx
ffffffff812cec08:	74 06                	je     ffffffff812cec10 <iov_iter_advance+0xcf>
ffffffff812cec0a:	48 01 d0             	add    %rdx,%rax
ffffffff812cec0d:	48 29 d1             	sub    %rdx,%rcx
ffffffff812cec10:	4c 89 c2             	mov    %r8,%rdx
ffffffff812cec13:	48 85 c9             	test   %rcx,%rcx
ffffffff812cec16:	74 1d                	je     ffffffff812cec35 <iov_iter_advance+0xf4>
ffffffff812cec18:	48 83 c2 10          	add    $0x10,%rdx
ffffffff812cec1c:	49 89 c9             	mov    %rcx,%r9
ffffffff812cec1f:	48 39 4a 08          	cmp    %rcx,0x8(%rdx)
ffffffff812cec23:	4c 0f 46 4a 08       	cmovbe 0x8(%rdx),%r9
ffffffff812cec28:	4d 85 c9             	test   %r9,%r9
ffffffff812cec2b:	74 e6                	je     ffffffff812cec13 <iov_iter_advance+0xd2>
ffffffff812cec2d:	4c 29 c9             	sub    %r9,%rcx
ffffffff812cec30:	4c 89 c8             	mov    %r9,%rax
ffffffff812cec33:	eb de                	jmp    ffffffff812cec13 <iov_iter_advance+0xd2>
ffffffff812cec35:	48 3b 42 08          	cmp    0x8(%rdx),%rax
ffffffff812cec39:	75 06                	jne    ffffffff812cec41 <iov_iter_advance+0x100>
ffffffff812cec3b:	48 83 c2 10          	add    $0x10,%rdx
ffffffff812cec3f:	31 c0                	xor    %eax,%eax
ffffffff812cec41:	48 89 d1             	mov    %rdx,%rcx
ffffffff812cec44:	4c 29 c1             	sub    %r8,%rcx
ffffffff812cec47:	48 c1 f9 04          	sar    $0x4,%rcx
ffffffff812cec4b:	48 89 57 18          	mov    %rdx,0x18(%rdi)
ffffffff812cec4f:	48 29 4f 20          	sub    %rcx,0x20(%rdi)
ffffffff812cec53:	48 29 77 10          	sub    %rsi,0x10(%rdi)
ffffffff812cec57:	48 89 47 08          	mov    %rax,0x8(%rdi)
}
ffffffff812cec5b:	5d                   	pop    %rbp
ffffffff812cec5c:	c3                   	retq   

ffffffff812cec5d <iov_iter_alignment>:
	i->count = count;
}
EXPORT_SYMBOL(iov_iter_bvec);

unsigned long iov_iter_alignment(const struct iov_iter *i)
{
ffffffff812cec5d:	55                   	push   %rbp
	unsigned long res = 0;
	size_t size = i->count;
ffffffff812cec5e:	48 8b 57 10          	mov    0x10(%rdi),%rdx

	if (!size)
		return 0;
ffffffff812cec62:	31 c0                	xor    %eax,%eax
	i->count = count;
}
EXPORT_SYMBOL(iov_iter_bvec);

unsigned long iov_iter_alignment(const struct iov_iter *i)
{
ffffffff812cec64:	48 89 e5             	mov    %rsp,%rbp
	unsigned long res = 0;
	size_t size = i->count;

	if (!size)
ffffffff812cec67:	48 85 d2             	test   %rdx,%rdx
ffffffff812cec6a:	0f 84 cb 00 00 00    	je     ffffffff812ced3b <iov_iter_alignment+0xde>
		return 0;

	iterate_all_kinds(i, size, v,
ffffffff812cec70:	8b 07                	mov    (%rdi),%eax
ffffffff812cec72:	4c 8b 47 08          	mov    0x8(%rdi),%r8
ffffffff812cec76:	48 8b 77 18          	mov    0x18(%rdi),%rsi
ffffffff812cec7a:	a8 04                	test   $0x4,%al
ffffffff812cec7c:	74 46                	je     ffffffff812cecc4 <iov_iter_alignment+0x67>
ffffffff812cec7e:	8b 4e 08             	mov    0x8(%rsi),%ecx
ffffffff812cec81:	4c 29 c1             	sub    %r8,%rcx
ffffffff812cec84:	48 39 d1             	cmp    %rdx,%rcx
ffffffff812cec87:	48 0f 47 ca          	cmova  %rdx,%rcx
}
EXPORT_SYMBOL(iov_iter_bvec);

unsigned long iov_iter_alignment(const struct iov_iter *i)
{
	unsigned long res = 0;
ffffffff812cec8b:	31 c0                	xor    %eax,%eax
	size_t size = i->count;

	if (!size)
		return 0;

	iterate_all_kinds(i, size, v,
ffffffff812cec8d:	85 c9                	test   %ecx,%ecx
ffffffff812cec8f:	74 0d                	je     ffffffff812cec9e <iov_iter_alignment+0x41>
ffffffff812cec91:	8b 46 0c             	mov    0xc(%rsi),%eax
ffffffff812cec94:	44 01 c0             	add    %r8d,%eax
ffffffff812cec97:	09 c8                	or     %ecx,%eax
ffffffff812cec99:	89 c9                	mov    %ecx,%ecx
ffffffff812cec9b:	48 29 ca             	sub    %rcx,%rdx
ffffffff812cec9e:	48 85 d2             	test   %rdx,%rdx
ffffffff812ceca1:	0f 84 94 00 00 00    	je     ffffffff812ced3b <iov_iter_alignment+0xde>
ffffffff812ceca7:	48 83 c6 10          	add    $0x10,%rsi
ffffffff812cecab:	8b 4e 08             	mov    0x8(%rsi),%ecx
ffffffff812cecae:	48 39 d1             	cmp    %rdx,%rcx
ffffffff812cecb1:	48 0f 47 ca          	cmova  %rdx,%rcx
ffffffff812cecb5:	48 85 c9             	test   %rcx,%rcx
ffffffff812cecb8:	74 e4                	je     ffffffff812cec9e <iov_iter_alignment+0x41>
ffffffff812cecba:	89 cf                	mov    %ecx,%edi
ffffffff812cecbc:	0b 7e 0c             	or     0xc(%rsi),%edi
ffffffff812cecbf:	48 09 f8             	or     %rdi,%rax
ffffffff812cecc2:	eb d7                	jmp    ffffffff812cec9b <iov_iter_alignment+0x3e>
ffffffff812cecc4:	48 8b 4e 08          	mov    0x8(%rsi),%rcx
ffffffff812cecc8:	4c 29 c1             	sub    %r8,%rcx
ffffffff812ceccb:	48 39 d1             	cmp    %rdx,%rcx
ffffffff812cecce:	48 0f 47 ca          	cmova  %rdx,%rcx
ffffffff812cecd2:	a8 02                	test   $0x2,%al
}
EXPORT_SYMBOL(iov_iter_bvec);

unsigned long iov_iter_alignment(const struct iov_iter *i)
{
	unsigned long res = 0;
ffffffff812cecd4:	b8 00 00 00 00       	mov    $0x0,%eax
	size_t size = i->count;

	if (!size)
		return 0;

	iterate_all_kinds(i, size, v,
ffffffff812cecd9:	74 30                	je     ffffffff812ced0b <iov_iter_alignment+0xae>
ffffffff812cecdb:	48 85 c9             	test   %rcx,%rcx
ffffffff812cecde:	74 0c                	je     ffffffff812cecec <iov_iter_alignment+0x8f>
ffffffff812cece0:	4c 89 c0             	mov    %r8,%rax
ffffffff812cece3:	48 03 06             	add    (%rsi),%rax
ffffffff812cece6:	48 09 c8             	or     %rcx,%rax
ffffffff812cece9:	48 29 ca             	sub    %rcx,%rdx
ffffffff812cecec:	48 85 d2             	test   %rdx,%rdx
ffffffff812cecef:	74 4a                	je     ffffffff812ced3b <iov_iter_alignment+0xde>
ffffffff812cecf1:	48 83 c6 10          	add    $0x10,%rsi
ffffffff812cecf5:	48 89 d1             	mov    %rdx,%rcx
ffffffff812cecf8:	48 39 56 08          	cmp    %rdx,0x8(%rsi)
ffffffff812cecfc:	48 0f 46 4e 08       	cmovbe 0x8(%rsi),%rcx
ffffffff812ced01:	48 85 c9             	test   %rcx,%rcx
ffffffff812ced04:	74 e6                	je     ffffffff812cecec <iov_iter_alignment+0x8f>
ffffffff812ced06:	48 0b 06             	or     (%rsi),%rax
ffffffff812ced09:	eb db                	jmp    ffffffff812cece6 <iov_iter_alignment+0x89>
ffffffff812ced0b:	48 85 c9             	test   %rcx,%rcx
ffffffff812ced0e:	74 0c                	je     ffffffff812ced1c <iov_iter_alignment+0xbf>
ffffffff812ced10:	4c 89 c0             	mov    %r8,%rax
ffffffff812ced13:	48 03 06             	add    (%rsi),%rax
ffffffff812ced16:	48 09 c8             	or     %rcx,%rax
ffffffff812ced19:	48 29 ca             	sub    %rcx,%rdx
ffffffff812ced1c:	48 85 d2             	test   %rdx,%rdx
ffffffff812ced1f:	74 1a                	je     ffffffff812ced3b <iov_iter_alignment+0xde>
ffffffff812ced21:	48 83 c6 10          	add    $0x10,%rsi
ffffffff812ced25:	48 89 d1             	mov    %rdx,%rcx
ffffffff812ced28:	48 39 56 08          	cmp    %rdx,0x8(%rsi)
ffffffff812ced2c:	48 0f 46 4e 08       	cmovbe 0x8(%rsi),%rcx
ffffffff812ced31:	48 85 c9             	test   %rcx,%rcx
ffffffff812ced34:	74 e6                	je     ffffffff812ced1c <iov_iter_alignment+0xbf>
ffffffff812ced36:	48 0b 06             	or     (%rsi),%rax
ffffffff812ced39:	eb db                	jmp    ffffffff812ced16 <iov_iter_alignment+0xb9>
		(res |= (unsigned long)v.iov_base | v.iov_len, 0),
		res |= v.bv_offset | v.bv_len,
		res |= (unsigned long)v.iov_base | v.iov_len
	)
	return res;
}
ffffffff812ced3b:	5d                   	pop    %rbp
ffffffff812ced3c:	c3                   	retq   

ffffffff812ced3d <iov_iter_npages>:
	return bytes;
}
EXPORT_SYMBOL(csum_and_copy_to_iter);

int iov_iter_npages(const struct iov_iter *i, int maxpages)
{
ffffffff812ced3d:	55                   	push   %rbp
	size_t size = i->count;
ffffffff812ced3e:	48 8b 57 10          	mov    0x10(%rdi),%rdx
	int npages = 0;

	if (!size)
		return 0;
ffffffff812ced42:	31 c0                	xor    %eax,%eax
	return bytes;
}
EXPORT_SYMBOL(csum_and_copy_to_iter);

int iov_iter_npages(const struct iov_iter *i, int maxpages)
{
ffffffff812ced44:	48 89 e5             	mov    %rsp,%rbp
	size_t size = i->count;
	int npages = 0;

	if (!size)
ffffffff812ced47:	48 85 d2             	test   %rdx,%rdx
ffffffff812ced4a:	0f 84 4a 01 00 00    	je     ffffffff812cee9a <iov_iter_npages+0x15d>
		return 0;

	iterate_all_kinds(i, size, v, ({
ffffffff812ced50:	8b 0f                	mov    (%rdi),%ecx
ffffffff812ced52:	48 8b 47 08          	mov    0x8(%rdi),%rax
ffffffff812ced56:	f6 c1 04             	test   $0x4,%cl
ffffffff812ced59:	74 5d                	je     ffffffff812cedb8 <iov_iter_npages+0x7b>
ffffffff812ced5b:	48 8b 7f 18          	mov    0x18(%rdi),%rdi
ffffffff812ced5f:	8b 4f 08             	mov    0x8(%rdi),%ecx
ffffffff812ced62:	48 29 c1             	sub    %rax,%rcx
ffffffff812ced65:	48 39 d1             	cmp    %rdx,%rcx
ffffffff812ced68:	48 0f 47 ca          	cmova  %rdx,%rcx
ffffffff812ced6c:	85 c9                	test   %ecx,%ecx
ffffffff812ced6e:	74 17                	je     ffffffff812ced87 <iov_iter_npages+0x4a>
ffffffff812ced70:	83 fe 01             	cmp    $0x1,%esi
ffffffff812ced73:	89 f0                	mov    %esi,%eax
ffffffff812ced75:	0f 8e 1f 01 00 00    	jle    ffffffff812cee9a <iov_iter_npages+0x15d>
ffffffff812ced7b:	89 c9                	mov    %ecx,%ecx
ffffffff812ced7d:	b8 01 00 00 00       	mov    $0x1,%eax
ffffffff812ced82:	48 29 ca             	sub    %rcx,%rdx
ffffffff812ced85:	eb 17                	jmp    ffffffff812ced9e <iov_iter_npages+0x61>
EXPORT_SYMBOL(csum_and_copy_to_iter);

int iov_iter_npages(const struct iov_iter *i, int maxpages)
{
	size_t size = i->count;
	int npages = 0;
ffffffff812ced87:	31 c0                	xor    %eax,%eax
ffffffff812ced89:	eb 13                	jmp    ffffffff812ced9e <iov_iter_npages+0x61>

	if (!size)
		return 0;

	iterate_all_kinds(i, size, v, ({
ffffffff812ced8b:	48 83 c7 10          	add    $0x10,%rdi
ffffffff812ced8f:	8b 4f 08             	mov    0x8(%rdi),%ecx
ffffffff812ced92:	48 39 d1             	cmp    %rdx,%rcx
ffffffff812ced95:	48 0f 47 ca          	cmova  %rdx,%rcx
ffffffff812ced99:	48 85 c9             	test   %rcx,%rcx
ffffffff812ced9c:	75 0b                	jne    ffffffff812ceda9 <iov_iter_npages+0x6c>
ffffffff812ced9e:	48 85 d2             	test   %rdx,%rdx
ffffffff812ceda1:	0f 84 f3 00 00 00    	je     ffffffff812cee9a <iov_iter_npages+0x15d>
ffffffff812ceda7:	eb e2                	jmp    ffffffff812ced8b <iov_iter_npages+0x4e>
ffffffff812ceda9:	ff c0                	inc    %eax
ffffffff812cedab:	39 c6                	cmp    %eax,%esi
ffffffff812cedad:	0f 8e e5 00 00 00    	jle    ffffffff812cee98 <iov_iter_npages+0x15b>
ffffffff812cedb3:	48 29 ca             	sub    %rcx,%rdx
ffffffff812cedb6:	eb e6                	jmp    ffffffff812ced9e <iov_iter_npages+0x61>
ffffffff812cedb8:	4c 8b 47 18          	mov    0x18(%rdi),%r8
ffffffff812cedbc:	49 8b 78 08          	mov    0x8(%r8),%rdi
ffffffff812cedc0:	48 29 c7             	sub    %rax,%rdi
ffffffff812cedc3:	48 39 d7             	cmp    %rdx,%rdi
ffffffff812cedc6:	48 0f 47 fa          	cmova  %rdx,%rdi
ffffffff812cedca:	80 e1 02             	and    $0x2,%cl
ffffffff812cedcd:	74 66                	je     ffffffff812cee35 <iov_iter_npages+0xf8>
ffffffff812cedcf:	48 85 ff             	test   %rdi,%rdi
ffffffff812cedd2:	74 20                	je     ffffffff812cedf4 <iov_iter_npages+0xb7>
ffffffff812cedd4:	49 03 00             	add    (%r8),%rax
ffffffff812cedd7:	48 8d 8c 07 ff 0f 00 	lea    0xfff(%rdi,%rax,1),%rcx
ffffffff812cedde:	00 
ffffffff812ceddf:	48 c1 e8 0c          	shr    $0xc,%rax
ffffffff812cede3:	48 c1 e9 0c          	shr    $0xc,%rcx
ffffffff812cede7:	29 c1                	sub    %eax,%ecx
ffffffff812cede9:	89 f0                	mov    %esi,%eax
ffffffff812cedeb:	39 ce                	cmp    %ecx,%esi
ffffffff812ceded:	7f 41                	jg     ffffffff812cee30 <iov_iter_npages+0xf3>
ffffffff812cedef:	e9 a6 00 00 00       	jmpq   ffffffff812cee9a <iov_iter_npages+0x15d>
EXPORT_SYMBOL(csum_and_copy_to_iter);

int iov_iter_npages(const struct iov_iter *i, int maxpages)
{
	size_t size = i->count;
	int npages = 0;
ffffffff812cedf4:	31 c9                	xor    %ecx,%ecx

	if (!size)
		return 0;

	iterate_all_kinds(i, size, v, ({
ffffffff812cedf6:	48 85 d2             	test   %rdx,%rdx
ffffffff812cedf9:	0f 84 95 00 00 00    	je     ffffffff812cee94 <iov_iter_npages+0x157>
ffffffff812cedff:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cee03:	48 89 d7             	mov    %rdx,%rdi
ffffffff812cee06:	49 39 50 08          	cmp    %rdx,0x8(%r8)
ffffffff812cee0a:	49 0f 46 78 08       	cmovbe 0x8(%r8),%rdi
ffffffff812cee0f:	48 85 ff             	test   %rdi,%rdi
ffffffff812cee12:	74 e2                	je     ffffffff812cedf6 <iov_iter_npages+0xb9>
ffffffff812cee14:	49 8b 00             	mov    (%r8),%rax
ffffffff812cee17:	4c 8d 8c 07 ff 0f 00 	lea    0xfff(%rdi,%rax,1),%r9
ffffffff812cee1e:	00 
ffffffff812cee1f:	48 c1 e8 0c          	shr    $0xc,%rax
ffffffff812cee23:	29 c1                	sub    %eax,%ecx
ffffffff812cee25:	49 c1 e9 0c          	shr    $0xc,%r9
ffffffff812cee29:	44 01 c9             	add    %r9d,%ecx
ffffffff812cee2c:	39 ce                	cmp    %ecx,%esi
ffffffff812cee2e:	7e 68                	jle    ffffffff812cee98 <iov_iter_npages+0x15b>
ffffffff812cee30:	48 29 fa             	sub    %rdi,%rdx
ffffffff812cee33:	eb c1                	jmp    ffffffff812cedf6 <iov_iter_npages+0xb9>
ffffffff812cee35:	48 85 ff             	test   %rdi,%rdi
ffffffff812cee38:	74 1d                	je     ffffffff812cee57 <iov_iter_npages+0x11a>
ffffffff812cee3a:	49 03 00             	add    (%r8),%rax
ffffffff812cee3d:	48 8d 8c 07 ff 0f 00 	lea    0xfff(%rdi,%rax,1),%rcx
ffffffff812cee44:	00 
ffffffff812cee45:	48 c1 e8 0c          	shr    $0xc,%rax
ffffffff812cee49:	48 c1 e9 0c          	shr    $0xc,%rcx
ffffffff812cee4d:	29 c1                	sub    %eax,%ecx
ffffffff812cee4f:	89 f0                	mov    %esi,%eax
ffffffff812cee51:	39 ce                	cmp    %ecx,%esi
ffffffff812cee53:	7e 45                	jle    ffffffff812cee9a <iov_iter_npages+0x15d>
ffffffff812cee55:	eb 38                	jmp    ffffffff812cee8f <iov_iter_npages+0x152>
EXPORT_SYMBOL(csum_and_copy_to_iter);

int iov_iter_npages(const struct iov_iter *i, int maxpages)
{
	size_t size = i->count;
	int npages = 0;
ffffffff812cee57:	31 c9                	xor    %ecx,%ecx

	if (!size)
		return 0;

	iterate_all_kinds(i, size, v, ({
ffffffff812cee59:	48 85 d2             	test   %rdx,%rdx
ffffffff812cee5c:	74 36                	je     ffffffff812cee94 <iov_iter_npages+0x157>
ffffffff812cee5e:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cee62:	48 89 d7             	mov    %rdx,%rdi
ffffffff812cee65:	49 39 50 08          	cmp    %rdx,0x8(%r8)
ffffffff812cee69:	49 0f 46 78 08       	cmovbe 0x8(%r8),%rdi
ffffffff812cee6e:	48 85 ff             	test   %rdi,%rdi
ffffffff812cee71:	74 e6                	je     ffffffff812cee59 <iov_iter_npages+0x11c>
ffffffff812cee73:	49 8b 00             	mov    (%r8),%rax
ffffffff812cee76:	4c 8d 8c 07 ff 0f 00 	lea    0xfff(%rdi,%rax,1),%r9
ffffffff812cee7d:	00 
ffffffff812cee7e:	48 c1 e8 0c          	shr    $0xc,%rax
ffffffff812cee82:	29 c1                	sub    %eax,%ecx
ffffffff812cee84:	49 c1 e9 0c          	shr    $0xc,%r9
ffffffff812cee88:	44 01 c9             	add    %r9d,%ecx
ffffffff812cee8b:	39 ce                	cmp    %ecx,%esi
ffffffff812cee8d:	7e 09                	jle    ffffffff812cee98 <iov_iter_npages+0x15b>
ffffffff812cee8f:	48 29 fa             	sub    %rdi,%rdx
ffffffff812cee92:	eb c5                	jmp    ffffffff812cee59 <iov_iter_npages+0x11c>
ffffffff812cee94:	89 c8                	mov    %ecx,%eax
ffffffff812cee96:	eb 02                	jmp    ffffffff812cee9a <iov_iter_npages+0x15d>
ffffffff812cee98:	89 f0                	mov    %esi,%eax
		if (npages >= maxpages)
			return maxpages;
	})
	)
	return npages;
}
ffffffff812cee9a:	5d                   	pop    %rbp
ffffffff812cee9b:	c3                   	retq   

ffffffff812cee9c <memcpy_from_page>:
	i->count = count;
}
EXPORT_SYMBOL(iov_iter_init);

static void memcpy_from_page(char *to, struct page *page, size_t offset, size_t len)
{
ffffffff812cee9c:	55                   	push   %rbp
ffffffff812cee9d:	49 89 f8             	mov    %rdi,%r8
	char *from = kmap_atomic(page);
ffffffff812ceea0:	48 89 f7             	mov    %rsi,%rdi
	i->count = count;
}
EXPORT_SYMBOL(iov_iter_init);

static void memcpy_from_page(char *to, struct page *page, size_t offset, size_t len)
{
ffffffff812ceea3:	48 89 e5             	mov    %rsp,%rbp
	char *from = kmap_atomic(page);
ffffffff812ceea6:	e8 e8 fa ff ff       	callq  ffffffff812ce993 <kmap_atomic>
	memcpy(to, from + offset, len);
ffffffff812ceeab:	48 8d 34 10          	lea    (%rax,%rdx,1),%rsi
ffffffff812ceeaf:	4c 89 c7             	mov    %r8,%rdi
ffffffff812ceeb2:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
}

static __always_inline void __preempt_count_sub(int val)
{
	raw_cpu_add_4(__preempt_count, -val);
ffffffff812ceeb4:	65 ff 0d dd ba d3 7e 	decl   %gs:0x7ed3badd(%rip)        # a998 <__preempt_count>
	kunmap_atomic(from);
}
ffffffff812ceebb:	5d                   	pop    %rbp
ffffffff812ceebc:	c3                   	retq   

ffffffff812ceebd <copy_from_iter>:
	return bytes;
}
EXPORT_SYMBOL(copy_to_iter);

size_t copy_from_iter(void *addr, size_t bytes, struct iov_iter *i)
{
ffffffff812ceebd:	55                   	push   %rbp
ffffffff812ceebe:	48 89 e5             	mov    %rsp,%rbp
ffffffff812ceec1:	41 57                	push   %r15
ffffffff812ceec3:	41 56                	push   %r14
ffffffff812ceec5:	41 55                	push   %r13
ffffffff812ceec7:	41 54                	push   %r12
ffffffff812ceec9:	53                   	push   %rbx
ffffffff812ceeca:	48 83 ec 10          	sub    $0x10,%rsp
	char *to = addr;
	if (unlikely(bytes > i->count))
ffffffff812ceece:	4c 8b 6a 10          	mov    0x10(%rdx),%r13
ffffffff812ceed2:	49 39 f5             	cmp    %rsi,%r13
ffffffff812ceed5:	4c 0f 47 ee          	cmova  %rsi,%r13
ffffffff812ceed9:	31 c0                	xor    %eax,%eax
		bytes = i->count;

	if (unlikely(!bytes))
ffffffff812ceedb:	4d 85 ed             	test   %r13,%r13
ffffffff812ceede:	0f 84 22 02 00 00    	je     ffffffff812cf106 <copy_from_iter+0x249>
		return 0;

	iterate_and_advance(i, bytes, v,
ffffffff812ceee4:	8b 02                	mov    (%rdx),%eax
ffffffff812ceee6:	49 89 fe             	mov    %rdi,%r14
ffffffff812ceee9:	49 89 d4             	mov    %rdx,%r12
ffffffff812ceeec:	48 8b 5a 08          	mov    0x8(%rdx),%rbx
ffffffff812ceef0:	a8 04                	test   $0x4,%al
ffffffff812ceef2:	0f 84 9d 00 00 00    	je     ffffffff812cef95 <copy_from_iter+0xd8>
ffffffff812ceef8:	4c 8b 7a 18          	mov    0x18(%rdx),%r15
ffffffff812ceefc:	4c 89 6d c8          	mov    %r13,-0x38(%rbp)
ffffffff812cef00:	45 8b 57 08          	mov    0x8(%r15),%r10d
ffffffff812cef04:	49 29 da             	sub    %rbx,%r10
ffffffff812cef07:	4d 39 ea             	cmp    %r13,%r10
ffffffff812cef0a:	4d 0f 47 d5          	cmova  %r13,%r10
ffffffff812cef0e:	45 85 d2             	test   %r10d,%r10d
ffffffff812cef11:	74 6f                	je     ffffffff812cef82 <copy_from_iter+0xc5>
ffffffff812cef13:	44 89 d0             	mov    %r10d,%eax
ffffffff812cef16:	89 da                	mov    %ebx,%edx
ffffffff812cef18:	41 03 57 0c          	add    0xc(%r15),%edx
ffffffff812cef1c:	49 01 c6             	add    %rax,%r14
ffffffff812cef1f:	49 8b 37             	mov    (%r15),%rsi
ffffffff812cef22:	48 89 c1             	mov    %rax,%rcx
ffffffff812cef25:	4c 89 f7             	mov    %r14,%rdi
ffffffff812cef28:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
ffffffff812cef2c:	48 29 c7             	sub    %rax,%rdi
ffffffff812cef2f:	e8 68 ff ff ff       	callq  ffffffff812cee9c <memcpy_from_page>
ffffffff812cef34:	4d 89 eb             	mov    %r13,%r11
ffffffff812cef37:	4c 2b 5d d0          	sub    -0x30(%rbp),%r11
ffffffff812cef3b:	48 03 5d d0          	add    -0x30(%rbp),%rbx
ffffffff812cef3f:	4c 89 5d c8          	mov    %r11,-0x38(%rbp)
ffffffff812cef43:	eb 3d                	jmp    ffffffff812cef82 <copy_from_iter+0xc5>
ffffffff812cef45:	49 83 c7 10          	add    $0x10,%r15
ffffffff812cef49:	45 8b 57 08          	mov    0x8(%r15),%r10d
ffffffff812cef4d:	4c 3b 55 c8          	cmp    -0x38(%rbp),%r10
ffffffff812cef51:	4c 0f 47 55 c8       	cmova  -0x38(%rbp),%r10
ffffffff812cef56:	4d 85 d2             	test   %r10,%r10
ffffffff812cef59:	4c 89 55 d0          	mov    %r10,-0x30(%rbp)
ffffffff812cef5d:	74 23                	je     ffffffff812cef82 <copy_from_iter+0xc5>
ffffffff812cef5f:	41 8b 57 0c          	mov    0xc(%r15),%edx
ffffffff812cef63:	4d 01 d6             	add    %r10,%r14
ffffffff812cef66:	49 8b 37             	mov    (%r15),%rsi
ffffffff812cef69:	4c 89 f7             	mov    %r14,%rdi
ffffffff812cef6c:	4c 89 d1             	mov    %r10,%rcx
ffffffff812cef6f:	4c 29 d7             	sub    %r10,%rdi
ffffffff812cef72:	e8 25 ff ff ff       	callq  ffffffff812cee9c <memcpy_from_page>
ffffffff812cef77:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
ffffffff812cef7b:	48 29 4d c8          	sub    %rcx,-0x38(%rbp)
ffffffff812cef7f:	48 89 cb             	mov    %rcx,%rbx
ffffffff812cef82:	48 83 7d c8 00       	cmpq   $0x0,-0x38(%rbp)
ffffffff812cef87:	75 bc                	jne    ffffffff812cef45 <copy_from_iter+0x88>
ffffffff812cef89:	41 8b 47 08          	mov    0x8(%r15),%eax
ffffffff812cef8d:	48 39 c3             	cmp    %rax,%rbx
ffffffff812cef90:	e9 46 01 00 00       	jmpq   ffffffff812cf0db <copy_from_iter+0x21e>
ffffffff812cef95:	a8 02                	test   $0x2,%al
ffffffff812cef97:	0f 84 97 00 00 00    	je     ffffffff812cf034 <copy_from_iter+0x177>
ffffffff812cef9d:	48 8b 52 18          	mov    0x18(%rdx),%rdx
ffffffff812cefa1:	4d 89 e8             	mov    %r13,%r8
ffffffff812cefa4:	48 8b 42 08          	mov    0x8(%rdx),%rax
ffffffff812cefa8:	48 29 d8             	sub    %rbx,%rax
ffffffff812cefab:	4c 39 e8             	cmp    %r13,%rax
ffffffff812cefae:	49 0f 47 c5          	cmova  %r13,%rax
ffffffff812cefb2:	48 85 c0             	test   %rax,%rax
ffffffff812cefb5:	74 51                	je     ffffffff812cf008 <copy_from_iter+0x14b>
ffffffff812cefb7:	49 01 c6             	add    %rax,%r14
ffffffff812cefba:	48 89 de             	mov    %rbx,%rsi
ffffffff812cefbd:	48 03 32             	add    (%rdx),%rsi
ffffffff812cefc0:	4d 89 f0             	mov    %r14,%r8
ffffffff812cefc3:	48 89 c1             	mov    %rax,%rcx
ffffffff812cefc6:	48 01 c3             	add    %rax,%rbx
ffffffff812cefc9:	49 29 c0             	sub    %rax,%r8
ffffffff812cefcc:	4c 89 c7             	mov    %r8,%rdi
ffffffff812cefcf:	4d 89 e8             	mov    %r13,%r8
ffffffff812cefd2:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
ffffffff812cefd4:	49 29 c0             	sub    %rax,%r8
ffffffff812cefd7:	eb 2f                	jmp    ffffffff812cf008 <copy_from_iter+0x14b>
ffffffff812cefd9:	48 83 c2 10          	add    $0x10,%rdx
ffffffff812cefdd:	4c 89 c0             	mov    %r8,%rax
ffffffff812cefe0:	4c 39 42 08          	cmp    %r8,0x8(%rdx)
ffffffff812cefe4:	48 0f 46 42 08       	cmovbe 0x8(%rdx),%rax
ffffffff812cefe9:	48 85 c0             	test   %rax,%rax
ffffffff812cefec:	74 1a                	je     ffffffff812cf008 <copy_from_iter+0x14b>
ffffffff812cefee:	49 01 c6             	add    %rax,%r14
ffffffff812ceff1:	48 8b 32             	mov    (%rdx),%rsi
ffffffff812ceff4:	48 89 c1             	mov    %rax,%rcx
ffffffff812ceff7:	4d 89 f1             	mov    %r14,%r9
ffffffff812ceffa:	49 29 c0             	sub    %rax,%r8
ffffffff812ceffd:	48 89 c3             	mov    %rax,%rbx
ffffffff812cf000:	49 29 c1             	sub    %rax,%r9
ffffffff812cf003:	4c 89 cf             	mov    %r9,%rdi
ffffffff812cf006:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
ffffffff812cf008:	4d 85 c0             	test   %r8,%r8
ffffffff812cf00b:	75 cc                	jne    ffffffff812cefd9 <copy_from_iter+0x11c>
ffffffff812cf00d:	48 3b 5a 08          	cmp    0x8(%rdx),%rbx
ffffffff812cf011:	75 06                	jne    ffffffff812cf019 <copy_from_iter+0x15c>
ffffffff812cf013:	48 83 c2 10          	add    $0x10,%rdx
ffffffff812cf017:	31 db                	xor    %ebx,%ebx
ffffffff812cf019:	48 89 d0             	mov    %rdx,%rax
ffffffff812cf01c:	49 2b 44 24 18       	sub    0x18(%r12),%rax
ffffffff812cf021:	49 89 54 24 18       	mov    %rdx,0x18(%r12)
ffffffff812cf026:	48 c1 f8 04          	sar    $0x4,%rax
ffffffff812cf02a:	49 29 44 24 20       	sub    %rax,0x20(%r12)
ffffffff812cf02f:	e9 c5 00 00 00       	jmpq   ffffffff812cf0f9 <copy_from_iter+0x23c>
ffffffff812cf034:	4c 8b 7a 18          	mov    0x18(%rdx),%r15
ffffffff812cf038:	49 8b 47 08          	mov    0x8(%r15),%rax
ffffffff812cf03c:	48 29 d8             	sub    %rbx,%rax
ffffffff812cf03f:	4c 39 e8             	cmp    %r13,%rax
ffffffff812cf042:	49 0f 47 c5          	cmova  %r13,%rax
ffffffff812cf046:	48 85 c0             	test   %rax,%rax
ffffffff812cf049:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
ffffffff812cf04d:	74 2f                	je     ffffffff812cf07e <copy_from_iter+0x1c1>
ffffffff812cf04f:	49 01 c6             	add    %rax,%r14
	/*
	 * If CPU has ERMS feature, use copy_user_enhanced_fast_string.
	 * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.
	 * Otherwise, use copy_user_generic_unrolled.
	 */
	alternative_call_2(copy_user_generic_unrolled,
ffffffff812cf052:	48 89 de             	mov    %rbx,%rsi
ffffffff812cf055:	8b 55 d0             	mov    -0x30(%rbp),%edx
ffffffff812cf058:	4c 89 f7             	mov    %r14,%rdi
ffffffff812cf05b:	49 03 37             	add    (%r15),%rsi
ffffffff812cf05e:	48 29 c7             	sub    %rax,%rdi
ffffffff812cf061:	e8 ca ba ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
ffffffff812cf066:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
ffffffff812cf06a:	48 98                	cltq   
ffffffff812cf06c:	4c 89 e9             	mov    %r13,%rcx
ffffffff812cf06f:	48 29 c2             	sub    %rax,%rdx
ffffffff812cf072:	48 29 d1             	sub    %rdx,%rcx
ffffffff812cf075:	48 01 d3             	add    %rdx,%rbx
ffffffff812cf078:	48 89 4d d0          	mov    %rcx,-0x30(%rbp)
ffffffff812cf07c:	eb 50                	jmp    ffffffff812cf0ce <copy_from_iter+0x211>
ffffffff812cf07e:	4c 89 6d d0          	mov    %r13,-0x30(%rbp)
ffffffff812cf082:	31 c0                	xor    %eax,%eax
ffffffff812cf084:	eb 48                	jmp    ffffffff812cf0ce <copy_from_iter+0x211>
ffffffff812cf086:	48 83 7d d0 00       	cmpq   $0x0,-0x30(%rbp)
ffffffff812cf08b:	74 46                	je     ffffffff812cf0d3 <copy_from_iter+0x216>
ffffffff812cf08d:	49 83 c7 10          	add    $0x10,%r15
ffffffff812cf091:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
ffffffff812cf095:	49 39 47 08          	cmp    %rax,0x8(%r15)
ffffffff812cf099:	49 0f 46 47 08       	cmovbe 0x8(%r15),%rax
ffffffff812cf09e:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
ffffffff812cf0a2:	31 c0                	xor    %eax,%eax
ffffffff812cf0a4:	48 83 7d c8 00       	cmpq   $0x0,-0x38(%rbp)
ffffffff812cf0a9:	74 23                	je     ffffffff812cf0ce <copy_from_iter+0x211>
ffffffff812cf0ab:	4c 03 75 c8          	add    -0x38(%rbp),%r14
ffffffff812cf0af:	49 8b 37             	mov    (%r15),%rsi
ffffffff812cf0b2:	8b 55 c8             	mov    -0x38(%rbp),%edx
ffffffff812cf0b5:	4c 89 f7             	mov    %r14,%rdi
ffffffff812cf0b8:	48 2b 7d c8          	sub    -0x38(%rbp),%rdi
ffffffff812cf0bc:	e8 6f ba ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
ffffffff812cf0c1:	48 8b 5d c8          	mov    -0x38(%rbp),%rbx
ffffffff812cf0c5:	48 98                	cltq   
ffffffff812cf0c7:	48 29 c3             	sub    %rax,%rbx
ffffffff812cf0ca:	48 29 5d d0          	sub    %rbx,-0x30(%rbp)
ffffffff812cf0ce:	48 85 c0             	test   %rax,%rax
ffffffff812cf0d1:	74 b3                	je     ffffffff812cf086 <copy_from_iter+0x1c9>
ffffffff812cf0d3:	4c 2b 6d d0          	sub    -0x30(%rbp),%r13
ffffffff812cf0d7:	49 3b 5f 08          	cmp    0x8(%r15),%rbx
ffffffff812cf0db:	75 06                	jne    ffffffff812cf0e3 <copy_from_iter+0x226>
ffffffff812cf0dd:	49 83 c7 10          	add    $0x10,%r15
ffffffff812cf0e1:	31 db                	xor    %ebx,%ebx
ffffffff812cf0e3:	4c 89 f8             	mov    %r15,%rax
ffffffff812cf0e6:	49 2b 44 24 18       	sub    0x18(%r12),%rax
ffffffff812cf0eb:	4d 89 7c 24 18       	mov    %r15,0x18(%r12)
ffffffff812cf0f0:	48 c1 f8 04          	sar    $0x4,%rax
ffffffff812cf0f4:	49 29 44 24 20       	sub    %rax,0x20(%r12)
ffffffff812cf0f9:	4d 29 6c 24 10       	sub    %r13,0x10(%r12)
ffffffff812cf0fe:	49 89 5c 24 08       	mov    %rbx,0x8(%r12)
		memcpy_from_page((to += v.bv_len) - v.bv_len, v.bv_page,
				 v.bv_offset, v.bv_len),
		memcpy((to += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)
	)

	return bytes;
ffffffff812cf103:	4c 89 e8             	mov    %r13,%rax
}
ffffffff812cf106:	5a                   	pop    %rdx
ffffffff812cf107:	59                   	pop    %rcx
ffffffff812cf108:	5b                   	pop    %rbx
ffffffff812cf109:	41 5c                	pop    %r12
ffffffff812cf10b:	41 5d                	pop    %r13
ffffffff812cf10d:	41 5e                	pop    %r14
ffffffff812cf10f:	41 5f                	pop    %r15
ffffffff812cf111:	5d                   	pop    %rbp
ffffffff812cf112:	c3                   	retq   

ffffffff812cf113 <iov_iter_copy_from_user_atomic>:
}
EXPORT_SYMBOL(iov_iter_zero);

size_t iov_iter_copy_from_user_atomic(struct page *page,
		struct iov_iter *i, unsigned long offset, size_t bytes)
{
ffffffff812cf113:	55                   	push   %rbp
ffffffff812cf114:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cf117:	41 57                	push   %r15
ffffffff812cf119:	41 56                	push   %r14
ffffffff812cf11b:	41 55                	push   %r13
ffffffff812cf11d:	41 54                	push   %r12
ffffffff812cf11f:	49 89 cc             	mov    %rcx,%r12
ffffffff812cf122:	53                   	push   %rbx
ffffffff812cf123:	48 89 d3             	mov    %rdx,%rbx
	char *kaddr = kmap_atomic(page), *p = kaddr + offset;
ffffffff812cf126:	e8 68 f8 ff ff       	callq  ffffffff812ce993 <kmap_atomic>
ffffffff812cf12b:	48 01 c3             	add    %rax,%rbx
	iterate_all_kinds(i, bytes, v,
ffffffff812cf12e:	8b 06                	mov    (%rsi),%eax
ffffffff812cf130:	48 8b 56 08          	mov    0x8(%rsi),%rdx
ffffffff812cf134:	a8 04                	test   $0x4,%al
ffffffff812cf136:	74 7a                	je     ffffffff812cf1b2 <iov_iter_copy_from_user_atomic+0x9f>
ffffffff812cf138:	4c 8b 76 18          	mov    0x18(%rsi),%r14
ffffffff812cf13c:	49 89 cf             	mov    %rcx,%r15
ffffffff812cf13f:	45 8b 4e 08          	mov    0x8(%r14),%r9d
ffffffff812cf143:	49 29 d1             	sub    %rdx,%r9
ffffffff812cf146:	49 39 c9             	cmp    %rcx,%r9
ffffffff812cf149:	4c 0f 47 c9          	cmova  %rcx,%r9
ffffffff812cf14d:	45 85 c9             	test   %r9d,%r9d
ffffffff812cf150:	74 55                	je     ffffffff812cf1a7 <iov_iter_copy_from_user_atomic+0x94>
ffffffff812cf152:	45 89 cf             	mov    %r9d,%r15d
ffffffff812cf155:	41 03 56 0c          	add    0xc(%r14),%edx
ffffffff812cf159:	49 8b 36             	mov    (%r14),%rsi
ffffffff812cf15c:	4c 01 fb             	add    %r15,%rbx
ffffffff812cf15f:	4c 89 f9             	mov    %r15,%rcx
ffffffff812cf162:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf165:	4c 29 ff             	sub    %r15,%rdi
ffffffff812cf168:	e8 2f fd ff ff       	callq  ffffffff812cee9c <memcpy_from_page>
ffffffff812cf16d:	4d 89 e3             	mov    %r12,%r11
ffffffff812cf170:	4d 29 fb             	sub    %r15,%r11
ffffffff812cf173:	4d 89 df             	mov    %r11,%r15
ffffffff812cf176:	eb 2f                	jmp    ffffffff812cf1a7 <iov_iter_copy_from_user_atomic+0x94>
ffffffff812cf178:	49 83 c6 10          	add    $0x10,%r14
ffffffff812cf17c:	45 8b 4e 08          	mov    0x8(%r14),%r9d
ffffffff812cf180:	4d 39 f9             	cmp    %r15,%r9
ffffffff812cf183:	4d 0f 47 cf          	cmova  %r15,%r9
ffffffff812cf187:	4d 85 c9             	test   %r9,%r9
ffffffff812cf18a:	74 1b                	je     ffffffff812cf1a7 <iov_iter_copy_from_user_atomic+0x94>
ffffffff812cf18c:	41 8b 56 0c          	mov    0xc(%r14),%edx
ffffffff812cf190:	4c 01 cb             	add    %r9,%rbx
ffffffff812cf193:	49 8b 36             	mov    (%r14),%rsi
ffffffff812cf196:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf199:	4c 89 c9             	mov    %r9,%rcx
ffffffff812cf19c:	4c 29 cf             	sub    %r9,%rdi
ffffffff812cf19f:	e8 f8 fc ff ff       	callq  ffffffff812cee9c <memcpy_from_page>
ffffffff812cf1a4:	4d 29 cf             	sub    %r9,%r15
ffffffff812cf1a7:	4d 85 ff             	test   %r15,%r15
ffffffff812cf1aa:	0f 84 f6 00 00 00    	je     ffffffff812cf2a6 <iov_iter_copy_from_user_atomic+0x193>
ffffffff812cf1b0:	eb c6                	jmp    ffffffff812cf178 <iov_iter_copy_from_user_atomic+0x65>
ffffffff812cf1b2:	a8 02                	test   $0x2,%al
ffffffff812cf1b4:	74 6b                	je     ffffffff812cf221 <iov_iter_copy_from_user_atomic+0x10e>
ffffffff812cf1b6:	4c 8b 46 18          	mov    0x18(%rsi),%r8
ffffffff812cf1ba:	49 89 c9             	mov    %rcx,%r9
ffffffff812cf1bd:	49 8b 40 08          	mov    0x8(%r8),%rax
ffffffff812cf1c1:	48 29 d0             	sub    %rdx,%rax
ffffffff812cf1c4:	48 39 c8             	cmp    %rcx,%rax
ffffffff812cf1c7:	48 0f 47 c1          	cmova  %rcx,%rax
ffffffff812cf1cb:	48 85 c0             	test   %rax,%rax
ffffffff812cf1ce:	74 1d                	je     ffffffff812cf1ed <iov_iter_copy_from_user_atomic+0xda>
ffffffff812cf1d0:	48 01 c3             	add    %rax,%rbx
ffffffff812cf1d3:	48 89 d6             	mov    %rdx,%rsi
ffffffff812cf1d6:	49 03 30             	add    (%r8),%rsi
ffffffff812cf1d9:	49 89 d9             	mov    %rbx,%r9
ffffffff812cf1dc:	48 89 c1             	mov    %rax,%rcx
ffffffff812cf1df:	49 29 c1             	sub    %rax,%r9
ffffffff812cf1e2:	4c 89 cf             	mov    %r9,%rdi
ffffffff812cf1e5:	4d 89 e1             	mov    %r12,%r9
ffffffff812cf1e8:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
ffffffff812cf1ea:	49 29 c1             	sub    %rax,%r9
ffffffff812cf1ed:	4d 85 c9             	test   %r9,%r9
ffffffff812cf1f0:	0f 84 b0 00 00 00    	je     ffffffff812cf2a6 <iov_iter_copy_from_user_atomic+0x193>
ffffffff812cf1f6:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cf1fa:	4c 89 c8             	mov    %r9,%rax
ffffffff812cf1fd:	4d 39 48 08          	cmp    %r9,0x8(%r8)
ffffffff812cf201:	49 0f 46 40 08       	cmovbe 0x8(%r8),%rax
ffffffff812cf206:	48 85 c0             	test   %rax,%rax
ffffffff812cf209:	74 e2                	je     ffffffff812cf1ed <iov_iter_copy_from_user_atomic+0xda>
ffffffff812cf20b:	48 01 c3             	add    %rax,%rbx
ffffffff812cf20e:	49 8b 30             	mov    (%r8),%rsi
ffffffff812cf211:	48 89 c1             	mov    %rax,%rcx
ffffffff812cf214:	48 89 da             	mov    %rbx,%rdx
ffffffff812cf217:	48 29 c2             	sub    %rax,%rdx
ffffffff812cf21a:	48 89 d7             	mov    %rdx,%rdi
ffffffff812cf21d:	f3 a4                	rep movsb %ds:(%rsi),%es:(%rdi)
ffffffff812cf21f:	eb c9                	jmp    ffffffff812cf1ea <iov_iter_copy_from_user_atomic+0xd7>
ffffffff812cf221:	4c 8b 76 18          	mov    0x18(%rsi),%r14
ffffffff812cf225:	4d 8b 6e 08          	mov    0x8(%r14),%r13
ffffffff812cf229:	49 29 d5             	sub    %rdx,%r13
ffffffff812cf22c:	49 39 cd             	cmp    %rcx,%r13
ffffffff812cf22f:	4c 0f 47 e9          	cmova  %rcx,%r13
ffffffff812cf233:	4d 85 ed             	test   %r13,%r13
ffffffff812cf236:	74 27                	je     ffffffff812cf25f <iov_iter_copy_from_user_atomic+0x14c>
ffffffff812cf238:	4c 01 eb             	add    %r13,%rbx
ffffffff812cf23b:	48 89 d6             	mov    %rdx,%rsi
ffffffff812cf23e:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf241:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf244:	49 03 36             	add    (%r14),%rsi
ffffffff812cf247:	4c 29 ef             	sub    %r13,%rdi
ffffffff812cf24a:	e8 e1 b8 ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
ffffffff812cf24f:	48 98                	cltq   
ffffffff812cf251:	4c 89 e1             	mov    %r12,%rcx
ffffffff812cf254:	49 29 c5             	sub    %rax,%r13
ffffffff812cf257:	4c 29 e9             	sub    %r13,%rcx
ffffffff812cf25a:	49 89 cd             	mov    %rcx,%r13
ffffffff812cf25d:	eb 3f                	jmp    ffffffff812cf29e <iov_iter_copy_from_user_atomic+0x18b>
ffffffff812cf25f:	49 89 cd             	mov    %rcx,%r13
ffffffff812cf262:	31 c0                	xor    %eax,%eax
ffffffff812cf264:	eb 38                	jmp    ffffffff812cf29e <iov_iter_copy_from_user_atomic+0x18b>
ffffffff812cf266:	4d 85 ed             	test   %r13,%r13
ffffffff812cf269:	74 38                	je     ffffffff812cf2a3 <iov_iter_copy_from_user_atomic+0x190>
ffffffff812cf26b:	49 83 c6 10          	add    $0x10,%r14
ffffffff812cf26f:	4d 89 ef             	mov    %r13,%r15
ffffffff812cf272:	4d 39 6e 08          	cmp    %r13,0x8(%r14)
ffffffff812cf276:	4d 0f 46 7e 08       	cmovbe 0x8(%r14),%r15
ffffffff812cf27b:	31 c0                	xor    %eax,%eax
ffffffff812cf27d:	4d 85 ff             	test   %r15,%r15
ffffffff812cf280:	74 1c                	je     ffffffff812cf29e <iov_iter_copy_from_user_atomic+0x18b>
ffffffff812cf282:	4c 01 fb             	add    %r15,%rbx
ffffffff812cf285:	49 8b 36             	mov    (%r14),%rsi
ffffffff812cf288:	44 89 fa             	mov    %r15d,%edx
ffffffff812cf28b:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf28e:	4c 29 ff             	sub    %r15,%rdi
ffffffff812cf291:	e8 9a b8 ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
ffffffff812cf296:	48 98                	cltq   
ffffffff812cf298:	49 29 c7             	sub    %rax,%r15
ffffffff812cf29b:	4d 29 fd             	sub    %r15,%r13
ffffffff812cf29e:	48 85 c0             	test   %rax,%rax
ffffffff812cf2a1:	74 c3                	je     ffffffff812cf266 <iov_iter_copy_from_user_atomic+0x153>
ffffffff812cf2a3:	4d 29 ec             	sub    %r13,%r12
ffffffff812cf2a6:	65 ff 0d eb b6 d3 7e 	decl   %gs:0x7ed3b6eb(%rip)        # a998 <__preempt_count>
				 v.bv_offset, v.bv_len),
		memcpy((p += v.iov_len) - v.iov_len, v.iov_base, v.iov_len)
	)
	kunmap_atomic(kaddr);
	return bytes;
}
ffffffff812cf2ad:	5b                   	pop    %rbx
ffffffff812cf2ae:	4c 89 e0             	mov    %r12,%rax
ffffffff812cf2b1:	41 5c                	pop    %r12
ffffffff812cf2b3:	41 5d                	pop    %r13
ffffffff812cf2b5:	41 5e                	pop    %r14
ffffffff812cf2b7:	41 5f                	pop    %r15
ffffffff812cf2b9:	5d                   	pop    %rbp
ffffffff812cf2ba:	c3                   	retq   

ffffffff812cf2bb <get_pages_array>:
	return 0;
}
EXPORT_SYMBOL(iov_iter_get_pages);

static struct page **get_pages_array(size_t n)
{
ffffffff812cf2bb:	55                   	push   %rbp
ffffffff812cf2bc:	be d0 00 00 00       	mov    $0xd0,%esi
ffffffff812cf2c1:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cf2c4:	53                   	push   %rbx
	struct page **p = kmalloc(n * sizeof(struct page *), GFP_KERNEL);
ffffffff812cf2c5:	48 8d 1c fd 00 00 00 	lea    0x0(,%rdi,8),%rbx
ffffffff812cf2cc:	00 
	return 0;
}
EXPORT_SYMBOL(iov_iter_get_pages);

static struct page **get_pages_array(size_t n)
{
ffffffff812cf2cd:	51                   	push   %rcx
ffffffff812cf2ce:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf2d1:	e8 b5 1e e3 ff       	callq  ffffffff8110118b <__kmalloc>
	struct page **p = kmalloc(n * sizeof(struct page *), GFP_KERNEL);
	if (!p)
ffffffff812cf2d6:	48 85 c0             	test   %rax,%rax
ffffffff812cf2d9:	75 08                	jne    ffffffff812cf2e3 <get_pages_array+0x28>
		p = vmalloc(n * sizeof(struct page *));
ffffffff812cf2db:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf2de:	e8 b4 a9 e2 ff       	callq  ffffffff810f9c97 <vmalloc>
	return p;
}
ffffffff812cf2e3:	5a                   	pop    %rdx
ffffffff812cf2e4:	5b                   	pop    %rbx
ffffffff812cf2e5:	5d                   	pop    %rbp
ffffffff812cf2e6:	c3                   	retq   

ffffffff812cf2e7 <csum_and_copy_from_iter>:
}
EXPORT_SYMBOL(iov_iter_get_pages_alloc);

size_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,
			       struct iov_iter *i)
{
ffffffff812cf2e7:	55                   	push   %rbp
ffffffff812cf2e8:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cf2eb:	41 57                	push   %r15
ffffffff812cf2ed:	41 56                	push   %r14
ffffffff812cf2ef:	41 55                	push   %r13
ffffffff812cf2f1:	41 54                	push   %r12
ffffffff812cf2f3:	53                   	push   %rbx
ffffffff812cf2f4:	48 83 ec 48          	sub    $0x48,%rsp
	char *to = addr;
	__wsum sum, next;
	size_t off = 0;
	if (unlikely(bytes > i->count))
ffffffff812cf2f8:	4c 8b 61 10          	mov    0x10(%rcx),%r12
}
EXPORT_SYMBOL(iov_iter_get_pages_alloc);

size_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,
			       struct iov_iter *i)
{
ffffffff812cf2fc:	48 89 7d b8          	mov    %rdi,-0x48(%rbp)
ffffffff812cf300:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
ffffffff812cf304:	49 39 f4             	cmp    %rsi,%r12
ffffffff812cf307:	4c 0f 47 e6          	cmova  %rsi,%r12
ffffffff812cf30b:	31 c0                	xor    %eax,%eax
	__wsum sum, next;
	size_t off = 0;
	if (unlikely(bytes > i->count))
		bytes = i->count;

	if (unlikely(!bytes))
ffffffff812cf30d:	4d 85 e4             	test   %r12,%r12
ffffffff812cf310:	0f 84 57 03 00 00    	je     ffffffff812cf66d <csum_and_copy_from_iter+0x386>
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf316:	8b 01                	mov    (%rcx),%eax
ffffffff812cf318:	49 89 ce             	mov    %rcx,%r14
		bytes = i->count;

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
ffffffff812cf31b:	44 8b 3a             	mov    (%rdx),%r15d
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf31e:	48 8b 59 08          	mov    0x8(%rcx),%rbx
ffffffff812cf322:	a8 04                	test   $0x4,%al
ffffffff812cf324:	0f 84 06 01 00 00    	je     ffffffff812cf430 <csum_and_copy_from_iter+0x149>
ffffffff812cf32a:	4c 8b 41 18          	mov    0x18(%rcx),%r8
ffffffff812cf32e:	41 8b 50 08          	mov    0x8(%r8),%edx
ffffffff812cf332:	48 29 da             	sub    %rbx,%rdx
ffffffff812cf335:	4c 39 e2             	cmp    %r12,%rdx
ffffffff812cf338:	49 0f 47 d4          	cmova  %r12,%rdx
ffffffff812cf33c:	48 89 55 b0          	mov    %rdx,-0x50(%rbp)
ffffffff812cf340:	83 7d b0 00          	cmpl   $0x0,-0x50(%rbp)
ffffffff812cf344:	74 5a                	je     ffffffff812cf3a0 <csum_and_copy_from_iter+0xb9>
ffffffff812cf346:	41 8b 48 0c          	mov    0xc(%r8),%ecx
ffffffff812cf34a:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cf34d:	4c 89 45 98          	mov    %r8,-0x68(%rbp)
ffffffff812cf351:	01 d9                	add    %ebx,%ecx
ffffffff812cf353:	89 4d a0             	mov    %ecx,-0x60(%rbp)
ffffffff812cf356:	e8 38 f6 ff ff       	callq  ffffffff812ce993 <kmap_atomic>
ffffffff812cf35b:	44 8b 6d b0          	mov    -0x50(%rbp),%r13d
ffffffff812cf35f:	8b 4d a0             	mov    -0x60(%rbp),%ecx
ffffffff812cf362:	4c 01 6d b8          	add    %r13,-0x48(%rbp)
ffffffff812cf366:	8b 55 b0             	mov    -0x50(%rbp),%edx
ffffffff812cf369:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cf36d:	48 8d 3c 08          	lea    (%rax,%rcx,1),%rdi
ffffffff812cf371:	31 c9                	xor    %ecx,%ecx
ffffffff812cf373:	4c 29 ee             	sub    %r13,%rsi
ffffffff812cf376:	e8 9c 10 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf37b:	31 d2                	xor    %edx,%edx
ffffffff812cf37d:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf380:	89 c6                	mov    %eax,%esi
ffffffff812cf382:	65 ff 0d 0f b6 d3 7e 	decl   %gs:0x7ed3b60f(%rip)        # a998 <__preempt_count>
ffffffff812cf389:	e8 34 f6 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf38e:	4d 89 e3             	mov    %r12,%r11
ffffffff812cf391:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf394:	4c 01 eb             	add    %r13,%rbx
ffffffff812cf397:	4d 29 eb             	sub    %r13,%r11
ffffffff812cf39a:	4c 8b 45 98          	mov    -0x68(%rbp),%r8
ffffffff812cf39e:	eb 7f                	jmp    ffffffff812cf41f <csum_and_copy_from_iter+0x138>
ffffffff812cf3a0:	4d 89 e3             	mov    %r12,%r11
size_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,
			       struct iov_iter *i)
{
	char *to = addr;
	__wsum sum, next;
	size_t off = 0;
ffffffff812cf3a3:	45 31 ed             	xor    %r13d,%r13d
ffffffff812cf3a6:	eb 77                	jmp    ffffffff812cf41f <csum_and_copy_from_iter+0x138>

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf3a8:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cf3ac:	45 8b 50 08          	mov    0x8(%r8),%r10d
ffffffff812cf3b0:	4d 39 da             	cmp    %r11,%r10
ffffffff812cf3b3:	4d 0f 47 d3          	cmova  %r11,%r10
ffffffff812cf3b7:	4d 85 d2             	test   %r10,%r10
ffffffff812cf3ba:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
ffffffff812cf3be:	74 5f                	je     ffffffff812cf41f <csum_and_copy_from_iter+0x138>
ffffffff812cf3c0:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cf3c3:	41 8b 58 0c          	mov    0xc(%r8),%ebx
ffffffff812cf3c7:	4c 89 5d 98          	mov    %r11,-0x68(%rbp)
ffffffff812cf3cb:	4c 89 45 a0          	mov    %r8,-0x60(%rbp)
ffffffff812cf3cf:	e8 bf f5 ff ff       	callq  ffffffff812ce993 <kmap_atomic>
ffffffff812cf3d4:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812cf3d8:	4c 01 55 b8          	add    %r10,-0x48(%rbp)
ffffffff812cf3dc:	89 da                	mov    %ebx,%edx
ffffffff812cf3de:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cf3e2:	48 8d 3c 10          	lea    (%rax,%rdx,1),%rdi
ffffffff812cf3e6:	31 c9                	xor    %ecx,%ecx
ffffffff812cf3e8:	44 89 d2             	mov    %r10d,%edx
ffffffff812cf3eb:	4c 29 d6             	sub    %r10,%rsi
ffffffff812cf3ee:	e8 24 10 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf3f3:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf3f6:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf3f9:	89 c6                	mov    %eax,%esi
ffffffff812cf3fb:	65 ff 0d 96 b5 d3 7e 	decl   %gs:0x7ed3b596(%rip)        # a998 <__preempt_count>
ffffffff812cf402:	e8 bb f5 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf407:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812cf40b:	4c 8b 5d 98          	mov    -0x68(%rbp),%r11
ffffffff812cf40f:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf412:	4c 8b 45 a0          	mov    -0x60(%rbp),%r8
ffffffff812cf416:	4d 01 d5             	add    %r10,%r13
ffffffff812cf419:	4d 29 d3             	sub    %r10,%r11
ffffffff812cf41c:	4c 89 d3             	mov    %r10,%rbx
ffffffff812cf41f:	4d 85 db             	test   %r11,%r11
ffffffff812cf422:	75 84                	jne    ffffffff812cf3a8 <csum_and_copy_from_iter+0xc1>
ffffffff812cf424:	41 8b 40 08          	mov    0x8(%r8),%eax
ffffffff812cf428:	48 39 c3             	cmp    %rax,%rbx
ffffffff812cf42b:	e9 c2 00 00 00       	jmpq   ffffffff812cf4f2 <csum_and_copy_from_iter+0x20b>
ffffffff812cf430:	a8 02                	test   $0x2,%al
ffffffff812cf432:	0f 84 da 00 00 00    	je     ffffffff812cf512 <csum_and_copy_from_iter+0x22b>
ffffffff812cf438:	4c 8b 41 18          	mov    0x18(%rcx),%r8
ffffffff812cf43c:	4d 89 e3             	mov    %r12,%r11
ffffffff812cf43f:	4d 8b 68 08          	mov    0x8(%r8),%r13
ffffffff812cf443:	49 29 dd             	sub    %rbx,%r13
ffffffff812cf446:	4d 39 e5             	cmp    %r12,%r13
ffffffff812cf449:	4d 0f 47 ec          	cmova  %r12,%r13
ffffffff812cf44d:	4d 85 ed             	test   %r13,%r13
ffffffff812cf450:	74 3b                	je     ffffffff812cf48d <csum_and_copy_from_iter+0x1a6>
ffffffff812cf452:	4c 01 6d b8          	add    %r13,-0x48(%rbp)
ffffffff812cf456:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf459:	49 03 38             	add    (%r8),%rdi
ffffffff812cf45c:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cf460:	31 c9                	xor    %ecx,%ecx
ffffffff812cf462:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf465:	4c 89 45 b0          	mov    %r8,-0x50(%rbp)
ffffffff812cf469:	4c 01 eb             	add    %r13,%rbx
ffffffff812cf46c:	4c 29 ee             	sub    %r13,%rsi
ffffffff812cf46f:	e8 a3 0f 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf474:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf477:	31 d2                	xor    %edx,%edx
ffffffff812cf479:	89 c6                	mov    %eax,%esi
ffffffff812cf47b:	e8 42 f5 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf480:	4d 89 e3             	mov    %r12,%r11
ffffffff812cf483:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf486:	4d 29 eb             	sub    %r13,%r11
ffffffff812cf489:	4c 8b 45 b0          	mov    -0x50(%rbp),%r8
ffffffff812cf48d:	4d 85 db             	test   %r11,%r11
ffffffff812cf490:	74 5c                	je     ffffffff812cf4ee <csum_and_copy_from_iter+0x207>
ffffffff812cf492:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cf496:	4d 89 da             	mov    %r11,%r10
ffffffff812cf499:	4d 39 58 08          	cmp    %r11,0x8(%r8)
ffffffff812cf49d:	4d 0f 46 50 08       	cmovbe 0x8(%r8),%r10
ffffffff812cf4a2:	4d 85 d2             	test   %r10,%r10
ffffffff812cf4a5:	74 e6                	je     ffffffff812cf48d <csum_and_copy_from_iter+0x1a6>
ffffffff812cf4a7:	4c 01 55 b8          	add    %r10,-0x48(%rbp)
ffffffff812cf4ab:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cf4ae:	44 89 d2             	mov    %r10d,%edx
ffffffff812cf4b1:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cf4b5:	31 c9                	xor    %ecx,%ecx
ffffffff812cf4b7:	4c 89 5d 98          	mov    %r11,-0x68(%rbp)
ffffffff812cf4bb:	4c 89 55 a0          	mov    %r10,-0x60(%rbp)
ffffffff812cf4bf:	4c 89 45 b0          	mov    %r8,-0x50(%rbp)
ffffffff812cf4c3:	4c 29 d6             	sub    %r10,%rsi
ffffffff812cf4c6:	e8 4c 0f 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf4cb:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf4ce:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf4d1:	89 c6                	mov    %eax,%esi
ffffffff812cf4d3:	e8 ea f4 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf4d8:	4c 8b 55 a0          	mov    -0x60(%rbp),%r10
ffffffff812cf4dc:	4c 8b 5d 98          	mov    -0x68(%rbp),%r11
ffffffff812cf4e0:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf4e3:	4d 01 d5             	add    %r10,%r13
ffffffff812cf4e6:	4d 29 d3             	sub    %r10,%r11
ffffffff812cf4e9:	4c 89 d3             	mov    %r10,%rbx
ffffffff812cf4ec:	eb 9b                	jmp    ffffffff812cf489 <csum_and_copy_from_iter+0x1a2>
ffffffff812cf4ee:	49 3b 58 08          	cmp    0x8(%r8),%rbx
ffffffff812cf4f2:	75 06                	jne    ffffffff812cf4fa <csum_and_copy_from_iter+0x213>
ffffffff812cf4f4:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cf4f8:	31 db                	xor    %ebx,%ebx
ffffffff812cf4fa:	4c 89 c0             	mov    %r8,%rax
ffffffff812cf4fd:	49 2b 46 18          	sub    0x18(%r14),%rax
ffffffff812cf501:	4d 89 46 18          	mov    %r8,0x18(%r14)
ffffffff812cf505:	48 c1 f8 04          	sar    $0x4,%rax
ffffffff812cf509:	49 29 46 20          	sub    %rax,0x20(%r14)
ffffffff812cf50d:	e9 49 01 00 00       	jmpq   ffffffff812cf65b <csum_and_copy_from_iter+0x374>
ffffffff812cf512:	4c 8b 59 18          	mov    0x18(%rcx),%r11
ffffffff812cf516:	4d 8b 6b 08          	mov    0x8(%r11),%r13
ffffffff812cf51a:	49 29 dd             	sub    %rbx,%r13
ffffffff812cf51d:	4d 39 e5             	cmp    %r12,%r13
ffffffff812cf520:	4d 0f 47 ec          	cmova  %r12,%r13
ffffffff812cf524:	4d 85 ed             	test   %r13,%r13
ffffffff812cf527:	74 65                	je     ffffffff812cf58e <csum_and_copy_from_iter+0x2a7>
ffffffff812cf529:	4c 01 6d b8          	add    %r13,-0x48(%rbp)
ffffffff812cf52d:	48 89 df             	mov    %rbx,%rdi
ffffffff812cf530:	49 03 3b             	add    (%r11),%rdi
ffffffff812cf533:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cf537:	4c 8d 45 cc          	lea    -0x34(%rbp),%r8
ffffffff812cf53b:	31 c9                	xor    %ecx,%ecx
ffffffff812cf53d:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf540:	4c 89 5d b0          	mov    %r11,-0x50(%rbp)
ffffffff812cf544:	c7 45 cc 00 00 00 00 	movl   $0x0,-0x34(%rbp)
ffffffff812cf54b:	4c 29 ee             	sub    %r13,%rsi
ffffffff812cf54e:	e8 d5 0e 16 00       	callq  ffffffff81430428 <csum_partial_copy_from_user>
ffffffff812cf553:	83 7d cc 00          	cmpl   $0x0,-0x34(%rbp)
ffffffff812cf557:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
ffffffff812cf55b:	75 1b                	jne    ffffffff812cf578 <csum_and_copy_from_iter+0x291>
ffffffff812cf55d:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf560:	31 d2                	xor    %edx,%edx
ffffffff812cf562:	89 c6                	mov    %eax,%esi
ffffffff812cf564:	e8 59 f4 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf569:	45 31 d2             	xor    %r10d,%r10d
ffffffff812cf56c:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf56f:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
ffffffff812cf573:	4c 89 e8             	mov    %r13,%rax
ffffffff812cf576:	eb 05                	jmp    ffffffff812cf57d <csum_and_copy_from_iter+0x296>
ffffffff812cf578:	4d 89 ea             	mov    %r13,%r10
size_t csum_and_copy_from_iter(void *addr, size_t bytes, __wsum *csum,
			       struct iov_iter *i)
{
	char *to = addr;
	__wsum sum, next;
	size_t off = 0;
ffffffff812cf57b:	31 c0                	xor    %eax,%eax

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf57d:	4d 29 d5             	sub    %r10,%r13
ffffffff812cf580:	4d 89 e1             	mov    %r12,%r9
ffffffff812cf583:	4c 01 eb             	add    %r13,%rbx
ffffffff812cf586:	4d 29 e9             	sub    %r13,%r9
ffffffff812cf589:	49 89 c5             	mov    %rax,%r13
ffffffff812cf58c:	eb 26                	jmp    ffffffff812cf5b4 <csum_and_copy_from_iter+0x2cd>
ffffffff812cf58e:	45 31 d2             	xor    %r10d,%r10d
ffffffff812cf591:	4d 89 e1             	mov    %r12,%r9
ffffffff812cf594:	eb 1e                	jmp    ffffffff812cf5b4 <csum_and_copy_from_iter+0x2cd>
ffffffff812cf596:	4d 85 c9             	test   %r9,%r9
ffffffff812cf599:	0f 84 9a 00 00 00    	je     ffffffff812cf639 <csum_and_copy_from_iter+0x352>
ffffffff812cf59f:	49 83 c3 10          	add    $0x10,%r11
ffffffff812cf5a3:	4d 89 ca             	mov    %r9,%r10
ffffffff812cf5a6:	4d 39 4b 08          	cmp    %r9,0x8(%r11)
ffffffff812cf5aa:	4d 0f 46 53 08       	cmovbe 0x8(%r11),%r10
ffffffff812cf5af:	4d 85 d2             	test   %r10,%r10
ffffffff812cf5b2:	75 07                	jne    ffffffff812cf5bb <csum_and_copy_from_iter+0x2d4>
ffffffff812cf5b4:	4d 85 d2             	test   %r10,%r10
ffffffff812cf5b7:	74 dd                	je     ffffffff812cf596 <csum_and_copy_from_iter+0x2af>
ffffffff812cf5b9:	eb 7e                	jmp    ffffffff812cf639 <csum_and_copy_from_iter+0x352>
ffffffff812cf5bb:	4c 01 55 b8          	add    %r10,-0x48(%rbp)
ffffffff812cf5bf:	49 8b 3b             	mov    (%r11),%rdi
ffffffff812cf5c2:	4c 8d 45 cc          	lea    -0x34(%rbp),%r8
ffffffff812cf5c6:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
ffffffff812cf5ca:	44 89 d2             	mov    %r10d,%edx
ffffffff812cf5cd:	31 c9                	xor    %ecx,%ecx
ffffffff812cf5cf:	4c 89 4d 98          	mov    %r9,-0x68(%rbp)
ffffffff812cf5d3:	4c 89 5d a0          	mov    %r11,-0x60(%rbp)
ffffffff812cf5d7:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
ffffffff812cf5db:	4c 29 d6             	sub    %r10,%rsi
ffffffff812cf5de:	c7 45 cc 00 00 00 00 	movl   $0x0,-0x34(%rbp)
ffffffff812cf5e5:	e8 3e 0e 16 00       	callq  ffffffff81430428 <csum_partial_copy_from_user>
ffffffff812cf5ea:	83 7d cc 00          	cmpl   $0x0,-0x34(%rbp)
ffffffff812cf5ee:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812cf5f2:	4c 8b 5d a0          	mov    -0x60(%rbp),%r11
ffffffff812cf5f6:	4c 8b 4d 98          	mov    -0x68(%rbp),%r9
ffffffff812cf5fa:	4c 89 d2             	mov    %r10,%rdx
ffffffff812cf5fd:	75 29                	jne    ffffffff812cf628 <csum_and_copy_from_iter+0x341>
ffffffff812cf5ff:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf602:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf605:	89 c6                	mov    %eax,%esi
ffffffff812cf607:	4c 89 55 98          	mov    %r10,-0x68(%rbp)
ffffffff812cf60b:	4c 89 4d b0          	mov    %r9,-0x50(%rbp)
ffffffff812cf60f:	e8 ae f3 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf614:	4c 8b 55 98          	mov    -0x68(%rbp),%r10
ffffffff812cf618:	4c 8b 5d a0          	mov    -0x60(%rbp),%r11
ffffffff812cf61c:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf61f:	4c 8b 4d b0          	mov    -0x50(%rbp),%r9
ffffffff812cf623:	31 d2                	xor    %edx,%edx
ffffffff812cf625:	4d 01 d5             	add    %r10,%r13
ffffffff812cf628:	4c 89 d3             	mov    %r10,%rbx
ffffffff812cf62b:	49 89 d2             	mov    %rdx,%r10
ffffffff812cf62e:	48 29 d3             	sub    %rdx,%rbx
ffffffff812cf631:	49 29 d9             	sub    %rbx,%r9
ffffffff812cf634:	e9 7b ff ff ff       	jmpq   ffffffff812cf5b4 <csum_and_copy_from_iter+0x2cd>
ffffffff812cf639:	4d 29 cc             	sub    %r9,%r12
ffffffff812cf63c:	49 3b 5b 08          	cmp    0x8(%r11),%rbx
ffffffff812cf640:	75 06                	jne    ffffffff812cf648 <csum_and_copy_from_iter+0x361>
ffffffff812cf642:	49 83 c3 10          	add    $0x10,%r11
ffffffff812cf646:	31 db                	xor    %ebx,%ebx
ffffffff812cf648:	4c 89 d8             	mov    %r11,%rax
ffffffff812cf64b:	49 2b 46 18          	sub    0x18(%r14),%rax
ffffffff812cf64f:	4d 89 5e 18          	mov    %r11,0x18(%r14)
ffffffff812cf653:	48 c1 f8 04          	sar    $0x4,%rax
ffffffff812cf657:	49 29 46 20          	sub    %rax,0x20(%r14)
						 v.iov_len, 0);
		sum = csum_block_add(sum, next, off);
		off += v.iov_len;
	})
	)
	*csum = sum;
ffffffff812cf65b:	48 8b 45 a8          	mov    -0x58(%rbp),%rax

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf65f:	4d 29 66 10          	sub    %r12,0x10(%r14)
ffffffff812cf663:	49 89 5e 08          	mov    %rbx,0x8(%r14)
						 v.iov_len, 0);
		sum = csum_block_add(sum, next, off);
		off += v.iov_len;
	})
	)
	*csum = sum;
ffffffff812cf667:	44 89 38             	mov    %r15d,(%rax)
	return bytes;
ffffffff812cf66a:	4c 89 e0             	mov    %r12,%rax
}
ffffffff812cf66d:	48 83 c4 48          	add    $0x48,%rsp
ffffffff812cf671:	5b                   	pop    %rbx
ffffffff812cf672:	41 5c                	pop    %r12
ffffffff812cf674:	41 5d                	pop    %r13
ffffffff812cf676:	41 5e                	pop    %r14
ffffffff812cf678:	41 5f                	pop    %r15
ffffffff812cf67a:	5d                   	pop    %rbp
ffffffff812cf67b:	c3                   	retq   

ffffffff812cf67c <csum_and_copy_to_iter>:
EXPORT_SYMBOL(csum_and_copy_from_iter);

size_t csum_and_copy_to_iter(void *addr, size_t bytes, __wsum *csum,
			     struct iov_iter *i)
{
ffffffff812cf67c:	55                   	push   %rbp
ffffffff812cf67d:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cf680:	41 57                	push   %r15
ffffffff812cf682:	41 56                	push   %r14
ffffffff812cf684:	41 55                	push   %r13
ffffffff812cf686:	41 54                	push   %r12
ffffffff812cf688:	53                   	push   %rbx
ffffffff812cf689:	48 83 ec 48          	sub    $0x48,%rsp
	char *from = addr;
	__wsum sum, next;
	size_t off = 0;
	if (unlikely(bytes > i->count))
ffffffff812cf68d:	4c 8b 61 10          	mov    0x10(%rcx),%r12
}
EXPORT_SYMBOL(csum_and_copy_from_iter);

size_t csum_and_copy_to_iter(void *addr, size_t bytes, __wsum *csum,
			     struct iov_iter *i)
{
ffffffff812cf691:	48 89 7d b8          	mov    %rdi,-0x48(%rbp)
ffffffff812cf695:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
ffffffff812cf699:	49 39 f4             	cmp    %rsi,%r12
ffffffff812cf69c:	4c 0f 47 e6          	cmova  %rsi,%r12
ffffffff812cf6a0:	31 c0                	xor    %eax,%eax
	__wsum sum, next;
	size_t off = 0;
	if (unlikely(bytes > i->count))
		bytes = i->count;

	if (unlikely(!bytes))
ffffffff812cf6a2:	4d 85 e4             	test   %r12,%r12
ffffffff812cf6a5:	0f 84 57 03 00 00    	je     ffffffff812cfa02 <csum_and_copy_to_iter+0x386>
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf6ab:	8b 01                	mov    (%rcx),%eax
ffffffff812cf6ad:	49 89 ce             	mov    %rcx,%r14
		bytes = i->count;

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
ffffffff812cf6b0:	44 8b 3a             	mov    (%rdx),%r15d
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf6b3:	48 8b 59 08          	mov    0x8(%rcx),%rbx
ffffffff812cf6b7:	a8 04                	test   $0x4,%al
ffffffff812cf6b9:	0f 84 06 01 00 00    	je     ffffffff812cf7c5 <csum_and_copy_to_iter+0x149>
ffffffff812cf6bf:	4c 8b 41 18          	mov    0x18(%rcx),%r8
ffffffff812cf6c3:	41 8b 50 08          	mov    0x8(%r8),%edx
ffffffff812cf6c7:	48 29 da             	sub    %rbx,%rdx
ffffffff812cf6ca:	4c 39 e2             	cmp    %r12,%rdx
ffffffff812cf6cd:	49 0f 47 d4          	cmova  %r12,%rdx
ffffffff812cf6d1:	48 89 55 b0          	mov    %rdx,-0x50(%rbp)
ffffffff812cf6d5:	83 7d b0 00          	cmpl   $0x0,-0x50(%rbp)
ffffffff812cf6d9:	74 5a                	je     ffffffff812cf735 <csum_and_copy_to_iter+0xb9>
ffffffff812cf6db:	41 8b 48 0c          	mov    0xc(%r8),%ecx
ffffffff812cf6df:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cf6e2:	4c 89 45 98          	mov    %r8,-0x68(%rbp)
ffffffff812cf6e6:	01 d9                	add    %ebx,%ecx
ffffffff812cf6e8:	89 4d a0             	mov    %ecx,-0x60(%rbp)
ffffffff812cf6eb:	e8 a3 f2 ff ff       	callq  ffffffff812ce993 <kmap_atomic>
ffffffff812cf6f0:	44 8b 6d b0          	mov    -0x50(%rbp),%r13d
ffffffff812cf6f4:	8b 4d a0             	mov    -0x60(%rbp),%ecx
ffffffff812cf6f7:	4c 01 6d b8          	add    %r13,-0x48(%rbp)
ffffffff812cf6fb:	8b 55 b0             	mov    -0x50(%rbp),%edx
ffffffff812cf6fe:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cf702:	48 8d 34 08          	lea    (%rax,%rcx,1),%rsi
ffffffff812cf706:	31 c9                	xor    %ecx,%ecx
ffffffff812cf708:	4c 29 ef             	sub    %r13,%rdi
ffffffff812cf70b:	e8 07 0d 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf710:	31 d2                	xor    %edx,%edx
ffffffff812cf712:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf715:	89 c6                	mov    %eax,%esi
ffffffff812cf717:	65 ff 0d 7a b2 d3 7e 	decl   %gs:0x7ed3b27a(%rip)        # a998 <__preempt_count>
ffffffff812cf71e:	e8 9f f2 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf723:	4d 89 e3             	mov    %r12,%r11
ffffffff812cf726:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf729:	4c 01 eb             	add    %r13,%rbx
ffffffff812cf72c:	4d 29 eb             	sub    %r13,%r11
ffffffff812cf72f:	4c 8b 45 98          	mov    -0x68(%rbp),%r8
ffffffff812cf733:	eb 7f                	jmp    ffffffff812cf7b4 <csum_and_copy_to_iter+0x138>
ffffffff812cf735:	4d 89 e3             	mov    %r12,%r11
size_t csum_and_copy_to_iter(void *addr, size_t bytes, __wsum *csum,
			     struct iov_iter *i)
{
	char *from = addr;
	__wsum sum, next;
	size_t off = 0;
ffffffff812cf738:	45 31 ed             	xor    %r13d,%r13d
ffffffff812cf73b:	eb 77                	jmp    ffffffff812cf7b4 <csum_and_copy_to_iter+0x138>

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf73d:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cf741:	45 8b 50 08          	mov    0x8(%r8),%r10d
ffffffff812cf745:	4d 39 da             	cmp    %r11,%r10
ffffffff812cf748:	4d 0f 47 d3          	cmova  %r11,%r10
ffffffff812cf74c:	4d 85 d2             	test   %r10,%r10
ffffffff812cf74f:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
ffffffff812cf753:	74 5f                	je     ffffffff812cf7b4 <csum_and_copy_to_iter+0x138>
ffffffff812cf755:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cf758:	41 8b 58 0c          	mov    0xc(%r8),%ebx
ffffffff812cf75c:	4c 89 5d 98          	mov    %r11,-0x68(%rbp)
ffffffff812cf760:	4c 89 45 a0          	mov    %r8,-0x60(%rbp)
ffffffff812cf764:	e8 2a f2 ff ff       	callq  ffffffff812ce993 <kmap_atomic>
ffffffff812cf769:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812cf76d:	4c 01 55 b8          	add    %r10,-0x48(%rbp)
ffffffff812cf771:	89 da                	mov    %ebx,%edx
ffffffff812cf773:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cf777:	48 8d 34 10          	lea    (%rax,%rdx,1),%rsi
ffffffff812cf77b:	31 c9                	xor    %ecx,%ecx
ffffffff812cf77d:	44 89 d2             	mov    %r10d,%edx
ffffffff812cf780:	4c 29 d7             	sub    %r10,%rdi
ffffffff812cf783:	e8 8f 0c 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf788:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf78b:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf78e:	89 c6                	mov    %eax,%esi
ffffffff812cf790:	65 ff 0d 01 b2 d3 7e 	decl   %gs:0x7ed3b201(%rip)        # a998 <__preempt_count>
ffffffff812cf797:	e8 26 f2 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf79c:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812cf7a0:	4c 8b 5d 98          	mov    -0x68(%rbp),%r11
ffffffff812cf7a4:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf7a7:	4c 8b 45 a0          	mov    -0x60(%rbp),%r8
ffffffff812cf7ab:	4d 01 d5             	add    %r10,%r13
ffffffff812cf7ae:	4d 29 d3             	sub    %r10,%r11
ffffffff812cf7b1:	4c 89 d3             	mov    %r10,%rbx
ffffffff812cf7b4:	4d 85 db             	test   %r11,%r11
ffffffff812cf7b7:	75 84                	jne    ffffffff812cf73d <csum_and_copy_to_iter+0xc1>
ffffffff812cf7b9:	41 8b 40 08          	mov    0x8(%r8),%eax
ffffffff812cf7bd:	48 39 c3             	cmp    %rax,%rbx
ffffffff812cf7c0:	e9 c2 00 00 00       	jmpq   ffffffff812cf887 <csum_and_copy_to_iter+0x20b>
ffffffff812cf7c5:	a8 02                	test   $0x2,%al
ffffffff812cf7c7:	0f 84 da 00 00 00    	je     ffffffff812cf8a7 <csum_and_copy_to_iter+0x22b>
ffffffff812cf7cd:	4c 8b 41 18          	mov    0x18(%rcx),%r8
ffffffff812cf7d1:	4d 89 e3             	mov    %r12,%r11
ffffffff812cf7d4:	4d 8b 68 08          	mov    0x8(%r8),%r13
ffffffff812cf7d8:	49 29 dd             	sub    %rbx,%r13
ffffffff812cf7db:	4d 39 e5             	cmp    %r12,%r13
ffffffff812cf7de:	4d 0f 47 ec          	cmova  %r12,%r13
ffffffff812cf7e2:	4d 85 ed             	test   %r13,%r13
ffffffff812cf7e5:	74 3b                	je     ffffffff812cf822 <csum_and_copy_to_iter+0x1a6>
ffffffff812cf7e7:	4c 01 6d b8          	add    %r13,-0x48(%rbp)
ffffffff812cf7eb:	48 89 de             	mov    %rbx,%rsi
ffffffff812cf7ee:	49 03 30             	add    (%r8),%rsi
ffffffff812cf7f1:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cf7f5:	31 c9                	xor    %ecx,%ecx
ffffffff812cf7f7:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf7fa:	4c 89 45 b0          	mov    %r8,-0x50(%rbp)
ffffffff812cf7fe:	4c 01 eb             	add    %r13,%rbx
ffffffff812cf801:	4c 29 ef             	sub    %r13,%rdi
ffffffff812cf804:	e8 0e 0c 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf809:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf80c:	31 d2                	xor    %edx,%edx
ffffffff812cf80e:	89 c6                	mov    %eax,%esi
ffffffff812cf810:	e8 ad f1 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf815:	4d 89 e3             	mov    %r12,%r11
ffffffff812cf818:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf81b:	4d 29 eb             	sub    %r13,%r11
ffffffff812cf81e:	4c 8b 45 b0          	mov    -0x50(%rbp),%r8
ffffffff812cf822:	4d 85 db             	test   %r11,%r11
ffffffff812cf825:	74 5c                	je     ffffffff812cf883 <csum_and_copy_to_iter+0x207>
ffffffff812cf827:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cf82b:	4d 89 da             	mov    %r11,%r10
ffffffff812cf82e:	4d 39 58 08          	cmp    %r11,0x8(%r8)
ffffffff812cf832:	4d 0f 46 50 08       	cmovbe 0x8(%r8),%r10
ffffffff812cf837:	4d 85 d2             	test   %r10,%r10
ffffffff812cf83a:	74 e6                	je     ffffffff812cf822 <csum_and_copy_to_iter+0x1a6>
ffffffff812cf83c:	4c 01 55 b8          	add    %r10,-0x48(%rbp)
ffffffff812cf840:	49 8b 30             	mov    (%r8),%rsi
ffffffff812cf843:	44 89 d2             	mov    %r10d,%edx
ffffffff812cf846:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cf84a:	31 c9                	xor    %ecx,%ecx
ffffffff812cf84c:	4c 89 5d 98          	mov    %r11,-0x68(%rbp)
ffffffff812cf850:	4c 89 55 a0          	mov    %r10,-0x60(%rbp)
ffffffff812cf854:	4c 89 45 b0          	mov    %r8,-0x50(%rbp)
ffffffff812cf858:	4c 29 d7             	sub    %r10,%rdi
ffffffff812cf85b:	e8 b7 0b 16 00       	callq  ffffffff81430417 <csum_partial_copy_nocheck>
ffffffff812cf860:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf863:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf866:	89 c6                	mov    %eax,%esi
ffffffff812cf868:	e8 55 f1 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf86d:	4c 8b 55 a0          	mov    -0x60(%rbp),%r10
ffffffff812cf871:	4c 8b 5d 98          	mov    -0x68(%rbp),%r11
ffffffff812cf875:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf878:	4d 01 d5             	add    %r10,%r13
ffffffff812cf87b:	4d 29 d3             	sub    %r10,%r11
ffffffff812cf87e:	4c 89 d3             	mov    %r10,%rbx
ffffffff812cf881:	eb 9b                	jmp    ffffffff812cf81e <csum_and_copy_to_iter+0x1a2>
ffffffff812cf883:	49 3b 58 08          	cmp    0x8(%r8),%rbx
ffffffff812cf887:	75 06                	jne    ffffffff812cf88f <csum_and_copy_to_iter+0x213>
ffffffff812cf889:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cf88d:	31 db                	xor    %ebx,%ebx
ffffffff812cf88f:	4c 89 c0             	mov    %r8,%rax
ffffffff812cf892:	49 2b 46 18          	sub    0x18(%r14),%rax
ffffffff812cf896:	4d 89 46 18          	mov    %r8,0x18(%r14)
ffffffff812cf89a:	48 c1 f8 04          	sar    $0x4,%rax
ffffffff812cf89e:	49 29 46 20          	sub    %rax,0x20(%r14)
ffffffff812cf8a2:	e9 49 01 00 00       	jmpq   ffffffff812cf9f0 <csum_and_copy_to_iter+0x374>
ffffffff812cf8a7:	4c 8b 59 18          	mov    0x18(%rcx),%r11
ffffffff812cf8ab:	4d 8b 6b 08          	mov    0x8(%r11),%r13
ffffffff812cf8af:	49 29 dd             	sub    %rbx,%r13
ffffffff812cf8b2:	4d 39 e5             	cmp    %r12,%r13
ffffffff812cf8b5:	4d 0f 47 ec          	cmova  %r12,%r13
ffffffff812cf8b9:	4d 85 ed             	test   %r13,%r13
ffffffff812cf8bc:	74 65                	je     ffffffff812cf923 <csum_and_copy_to_iter+0x2a7>
ffffffff812cf8be:	4c 01 6d b8          	add    %r13,-0x48(%rbp)
ffffffff812cf8c2:	48 89 de             	mov    %rbx,%rsi
ffffffff812cf8c5:	49 03 33             	add    (%r11),%rsi
ffffffff812cf8c8:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cf8cc:	4c 8d 45 cc          	lea    -0x34(%rbp),%r8
ffffffff812cf8d0:	31 c9                	xor    %ecx,%ecx
ffffffff812cf8d2:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf8d5:	4c 89 5d b0          	mov    %r11,-0x50(%rbp)
ffffffff812cf8d9:	c7 45 cc 00 00 00 00 	movl   $0x0,-0x34(%rbp)
ffffffff812cf8e0:	4c 29 ef             	sub    %r13,%rdi
ffffffff812cf8e3:	e8 48 0c 16 00       	callq  ffffffff81430530 <csum_partial_copy_to_user>
ffffffff812cf8e8:	83 7d cc 00          	cmpl   $0x0,-0x34(%rbp)
ffffffff812cf8ec:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
ffffffff812cf8f0:	75 1b                	jne    ffffffff812cf90d <csum_and_copy_to_iter+0x291>
ffffffff812cf8f2:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf8f5:	31 d2                	xor    %edx,%edx
ffffffff812cf8f7:	89 c6                	mov    %eax,%esi
ffffffff812cf8f9:	e8 c4 f0 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf8fe:	45 31 d2             	xor    %r10d,%r10d
ffffffff812cf901:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf904:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
ffffffff812cf908:	4c 89 e8             	mov    %r13,%rax
ffffffff812cf90b:	eb 05                	jmp    ffffffff812cf912 <csum_and_copy_to_iter+0x296>
ffffffff812cf90d:	4d 89 ea             	mov    %r13,%r10
size_t csum_and_copy_to_iter(void *addr, size_t bytes, __wsum *csum,
			     struct iov_iter *i)
{
	char *from = addr;
	__wsum sum, next;
	size_t off = 0;
ffffffff812cf910:	31 c0                	xor    %eax,%eax

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf912:	4d 29 d5             	sub    %r10,%r13
ffffffff812cf915:	4d 89 e1             	mov    %r12,%r9
ffffffff812cf918:	4c 01 eb             	add    %r13,%rbx
ffffffff812cf91b:	4d 29 e9             	sub    %r13,%r9
ffffffff812cf91e:	49 89 c5             	mov    %rax,%r13
ffffffff812cf921:	eb 26                	jmp    ffffffff812cf949 <csum_and_copy_to_iter+0x2cd>
ffffffff812cf923:	45 31 d2             	xor    %r10d,%r10d
ffffffff812cf926:	4d 89 e1             	mov    %r12,%r9
ffffffff812cf929:	eb 1e                	jmp    ffffffff812cf949 <csum_and_copy_to_iter+0x2cd>
ffffffff812cf92b:	4d 85 c9             	test   %r9,%r9
ffffffff812cf92e:	0f 84 9a 00 00 00    	je     ffffffff812cf9ce <csum_and_copy_to_iter+0x352>
ffffffff812cf934:	49 83 c3 10          	add    $0x10,%r11
ffffffff812cf938:	4d 89 ca             	mov    %r9,%r10
ffffffff812cf93b:	4d 39 4b 08          	cmp    %r9,0x8(%r11)
ffffffff812cf93f:	4d 0f 46 53 08       	cmovbe 0x8(%r11),%r10
ffffffff812cf944:	4d 85 d2             	test   %r10,%r10
ffffffff812cf947:	75 07                	jne    ffffffff812cf950 <csum_and_copy_to_iter+0x2d4>
ffffffff812cf949:	4d 85 d2             	test   %r10,%r10
ffffffff812cf94c:	74 dd                	je     ffffffff812cf92b <csum_and_copy_to_iter+0x2af>
ffffffff812cf94e:	eb 7e                	jmp    ffffffff812cf9ce <csum_and_copy_to_iter+0x352>
ffffffff812cf950:	4c 01 55 b8          	add    %r10,-0x48(%rbp)
ffffffff812cf954:	49 8b 33             	mov    (%r11),%rsi
ffffffff812cf957:	4c 8d 45 cc          	lea    -0x34(%rbp),%r8
ffffffff812cf95b:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cf95f:	44 89 d2             	mov    %r10d,%edx
ffffffff812cf962:	31 c9                	xor    %ecx,%ecx
ffffffff812cf964:	4c 89 4d 98          	mov    %r9,-0x68(%rbp)
ffffffff812cf968:	4c 89 5d a0          	mov    %r11,-0x60(%rbp)
ffffffff812cf96c:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
ffffffff812cf970:	4c 29 d7             	sub    %r10,%rdi
ffffffff812cf973:	c7 45 cc 00 00 00 00 	movl   $0x0,-0x34(%rbp)
ffffffff812cf97a:	e8 b1 0b 16 00       	callq  ffffffff81430530 <csum_partial_copy_to_user>
ffffffff812cf97f:	83 7d cc 00          	cmpl   $0x0,-0x34(%rbp)
ffffffff812cf983:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
ffffffff812cf987:	4c 8b 5d a0          	mov    -0x60(%rbp),%r11
ffffffff812cf98b:	4c 8b 4d 98          	mov    -0x68(%rbp),%r9
ffffffff812cf98f:	4c 89 d2             	mov    %r10,%rdx
ffffffff812cf992:	75 29                	jne    ffffffff812cf9bd <csum_and_copy_to_iter+0x341>
ffffffff812cf994:	44 89 ea             	mov    %r13d,%edx
ffffffff812cf997:	44 89 ff             	mov    %r15d,%edi
ffffffff812cf99a:	89 c6                	mov    %eax,%esi
ffffffff812cf99c:	4c 89 55 98          	mov    %r10,-0x68(%rbp)
ffffffff812cf9a0:	4c 89 4d b0          	mov    %r9,-0x50(%rbp)
ffffffff812cf9a4:	e8 19 f0 ff ff       	callq  ffffffff812ce9c2 <csum_block_add>
ffffffff812cf9a9:	4c 8b 55 98          	mov    -0x68(%rbp),%r10
ffffffff812cf9ad:	4c 8b 5d a0          	mov    -0x60(%rbp),%r11
ffffffff812cf9b1:	41 89 c7             	mov    %eax,%r15d
ffffffff812cf9b4:	4c 8b 4d b0          	mov    -0x50(%rbp),%r9
ffffffff812cf9b8:	31 d2                	xor    %edx,%edx
ffffffff812cf9ba:	4d 01 d5             	add    %r10,%r13
ffffffff812cf9bd:	4c 89 d3             	mov    %r10,%rbx
ffffffff812cf9c0:	49 89 d2             	mov    %rdx,%r10
ffffffff812cf9c3:	48 29 d3             	sub    %rdx,%rbx
ffffffff812cf9c6:	49 29 d9             	sub    %rbx,%r9
ffffffff812cf9c9:	e9 7b ff ff ff       	jmpq   ffffffff812cf949 <csum_and_copy_to_iter+0x2cd>
ffffffff812cf9ce:	4d 29 cc             	sub    %r9,%r12
ffffffff812cf9d1:	49 3b 5b 08          	cmp    0x8(%r11),%rbx
ffffffff812cf9d5:	75 06                	jne    ffffffff812cf9dd <csum_and_copy_to_iter+0x361>
ffffffff812cf9d7:	49 83 c3 10          	add    $0x10,%r11
ffffffff812cf9db:	31 db                	xor    %ebx,%ebx
ffffffff812cf9dd:	4c 89 d8             	mov    %r11,%rax
ffffffff812cf9e0:	49 2b 46 18          	sub    0x18(%r14),%rax
ffffffff812cf9e4:	4d 89 5e 18          	mov    %r11,0x18(%r14)
ffffffff812cf9e8:	48 c1 f8 04          	sar    $0x4,%rax
ffffffff812cf9ec:	49 29 46 20          	sub    %rax,0x20(%r14)
						 v.iov_len, 0);
		sum = csum_block_add(sum, next, off);
		off += v.iov_len;
	})
	)
	*csum = sum;
ffffffff812cf9f0:	48 8b 45 a8          	mov    -0x58(%rbp),%rax

	if (unlikely(!bytes))
		return 0;

	sum = *csum;
	iterate_and_advance(i, bytes, v, ({
ffffffff812cf9f4:	4d 29 66 10          	sub    %r12,0x10(%r14)
ffffffff812cf9f8:	49 89 5e 08          	mov    %rbx,0x8(%r14)
						 v.iov_len, 0);
		sum = csum_block_add(sum, next, off);
		off += v.iov_len;
	})
	)
	*csum = sum;
ffffffff812cf9fc:	44 89 38             	mov    %r15d,(%rax)
	return bytes;
ffffffff812cf9ff:	4c 89 e0             	mov    %r12,%rax
}
ffffffff812cfa02:	48 83 c4 48          	add    $0x48,%rsp
ffffffff812cfa06:	5b                   	pop    %rbx
ffffffff812cfa07:	41 5c                	pop    %r12
ffffffff812cfa09:	41 5d                	pop    %r13
ffffffff812cfa0b:	41 5e                	pop    %r14
ffffffff812cfa0d:	41 5f                	pop    %r15
ffffffff812cfa0f:	5d                   	pop    %rbp
ffffffff812cfa10:	c3                   	retq   

ffffffff812cfa11 <import_iovec>:
EXPORT_SYMBOL(dup_iter);

int import_iovec(int type, const struct iovec __user * uvector,
		 unsigned nr_segs, unsigned fast_segs,
		 struct iovec **iov, struct iov_iter *i)
{
ffffffff812cfa11:	55                   	push   %rbp
	ssize_t n;
	struct iovec *p;
	n = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,
ffffffff812cfa12:	89 c9                	mov    %ecx,%ecx
EXPORT_SYMBOL(dup_iter);

int import_iovec(int type, const struct iovec __user * uvector,
		 unsigned nr_segs, unsigned fast_segs,
		 struct iovec **iov, struct iov_iter *i)
{
ffffffff812cfa14:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cfa17:	41 56                	push   %r14
ffffffff812cfa19:	41 55                	push   %r13
ffffffff812cfa1b:	41 54                	push   %r12
ffffffff812cfa1d:	53                   	push   %rbx
ffffffff812cfa1e:	4c 89 c3             	mov    %r8,%rbx
ffffffff812cfa21:	4d 89 cd             	mov    %r9,%r13
	ssize_t n;
	struct iovec *p;
	n = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,
ffffffff812cfa24:	41 89 d4             	mov    %edx,%r12d
ffffffff812cfa27:	4c 8d 4d d8          	lea    -0x28(%rbp),%r9
EXPORT_SYMBOL(dup_iter);

int import_iovec(int type, const struct iovec __user * uvector,
		 unsigned nr_segs, unsigned fast_segs,
		 struct iovec **iov, struct iov_iter *i)
{
ffffffff812cfa2b:	48 83 ec 20          	sub    $0x20,%rsp
	ssize_t n;
	struct iovec *p;
	n = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,
ffffffff812cfa2f:	4d 8b 00             	mov    (%r8),%r8
ffffffff812cfa32:	4c 89 e2             	mov    %r12,%rdx
EXPORT_SYMBOL(dup_iter);

int import_iovec(int type, const struct iovec __user * uvector,
		 unsigned nr_segs, unsigned fast_segs,
		 struct iovec **iov, struct iov_iter *i)
{
ffffffff812cfa35:	41 89 fe             	mov    %edi,%r14d
	ssize_t n;
	struct iovec *p;
	n = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,
ffffffff812cfa38:	e8 54 9f e3 ff       	callq  ffffffff81109991 <rw_copy_check_uvector>
				  *iov, &p);
	if (n < 0) {
ffffffff812cfa3d:	48 85 c0             	test   %rax,%rax
		 unsigned nr_segs, unsigned fast_segs,
		 struct iovec **iov, struct iov_iter *i)
{
	ssize_t n;
	struct iovec *p;
	n = rw_copy_check_uvector(type, uvector, nr_segs, fast_segs,
ffffffff812cfa40:	49 89 c0             	mov    %rax,%r8
ffffffff812cfa43:	48 8b 55 d8          	mov    -0x28(%rbp),%rdx
				  *iov, &p);
	if (n < 0) {
ffffffff812cfa47:	79 21                	jns    ffffffff812cfa6a <import_iovec+0x59>
		if (p != *iov)
ffffffff812cfa49:	48 39 13             	cmp    %rdx,(%rbx)
ffffffff812cfa4c:	74 10                	je     ffffffff812cfa5e <import_iovec+0x4d>
			kfree(p);
ffffffff812cfa4e:	48 89 d7             	mov    %rdx,%rdi
ffffffff812cfa51:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
ffffffff812cfa55:	e8 2f 0f e3 ff       	callq  ffffffff81100989 <kfree>
ffffffff812cfa5a:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
		*iov = NULL;
ffffffff812cfa5e:	48 c7 03 00 00 00 00 	movq   $0x0,(%rbx)
		return n;
ffffffff812cfa65:	44 89 c0             	mov    %r8d,%eax
ffffffff812cfa68:	eb 23                	jmp    ffffffff812cfa8d <import_iovec+0x7c>
	}
	iov_iter_init(i, type, p, nr_segs, n);
ffffffff812cfa6a:	4c 89 e1             	mov    %r12,%rcx
ffffffff812cfa6d:	44 89 f6             	mov    %r14d,%esi
ffffffff812cfa70:	4c 89 ef             	mov    %r13,%rdi
ffffffff812cfa73:	e8 71 ef ff ff       	callq  ffffffff812ce9e9 <iov_iter_init>
	*iov = p == *iov ? NULL : p;
ffffffff812cfa78:	48 8b 45 d8          	mov    -0x28(%rbp),%rax
ffffffff812cfa7c:	48 39 03             	cmp    %rax,(%rbx)
ffffffff812cfa7f:	ba 00 00 00 00       	mov    $0x0,%edx
ffffffff812cfa84:	48 0f 44 c2          	cmove  %rdx,%rax
ffffffff812cfa88:	48 89 03             	mov    %rax,(%rbx)
	return 0;
ffffffff812cfa8b:	31 c0                	xor    %eax,%eax
}
ffffffff812cfa8d:	48 83 c4 20          	add    $0x20,%rsp
ffffffff812cfa91:	5b                   	pop    %rbx
ffffffff812cfa92:	41 5c                	pop    %r12
ffffffff812cfa94:	41 5d                	pop    %r13
ffffffff812cfa96:	41 5e                	pop    %r14
ffffffff812cfa98:	5d                   	pop    %rbp
ffffffff812cfa99:	c3                   	retq   

ffffffff812cfa9a <iov_iter_single_seg_count>:
/*
 * Return the count of just the current iov_iter segment.
 */
size_t iov_iter_single_seg_count(const struct iov_iter *i)
{
	if (i->nr_segs == 1)
ffffffff812cfa9a:	48 83 7f 20 01       	cmpq   $0x1,0x20(%rdi)

/*
 * Return the count of just the current iov_iter segment.
 */
size_t iov_iter_single_seg_count(const struct iov_iter *i)
{
ffffffff812cfa9f:	55                   	push   %rbp
ffffffff812cfaa0:	48 8b 47 10          	mov    0x10(%rdi),%rax
ffffffff812cfaa4:	48 89 e5             	mov    %rsp,%rbp
	if (i->nr_segs == 1)
ffffffff812cfaa7:	74 1d                	je     ffffffff812cfac6 <iov_iter_single_seg_count+0x2c>
		return i->count;
	else if (i->type & ITER_BVEC)
ffffffff812cfaa9:	f6 07 04             	testb  $0x4,(%rdi)
		return min(i->count, i->bvec->bv_len - i->iov_offset);
ffffffff812cfaac:	48 8b 57 18          	mov    0x18(%rdi),%rdx
 */
size_t iov_iter_single_seg_count(const struct iov_iter *i)
{
	if (i->nr_segs == 1)
		return i->count;
	else if (i->type & ITER_BVEC)
ffffffff812cfab0:	74 05                	je     ffffffff812cfab7 <iov_iter_single_seg_count+0x1d>
		return min(i->count, i->bvec->bv_len - i->iov_offset);
ffffffff812cfab2:	8b 52 08             	mov    0x8(%rdx),%edx
ffffffff812cfab5:	eb 04                	jmp    ffffffff812cfabb <iov_iter_single_seg_count+0x21>
	else
		return min(i->count, i->iov->iov_len - i->iov_offset);
ffffffff812cfab7:	48 8b 52 08          	mov    0x8(%rdx),%rdx
ffffffff812cfabb:	48 2b 57 08          	sub    0x8(%rdi),%rdx
ffffffff812cfabf:	48 39 c2             	cmp    %rax,%rdx
ffffffff812cfac2:	48 0f 46 c2          	cmovbe %rdx,%rax
}
ffffffff812cfac6:	5d                   	pop    %rbp
ffffffff812cfac7:	c3                   	retq   

ffffffff812cfac8 <iov_iter_fault_in_readable>:
 * would be possible (callers must not rely on the fact that _only_ the
 * first iovec will be faulted with the current implementation).
 */
int iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)
{
	if (!(i->type & (ITER_BVEC|ITER_KVEC))) {
ffffffff812cfac8:	f6 07 06             	testb  $0x6,(%rdi)
ffffffff812cfacb:	75 24                	jne    ffffffff812cfaf1 <iov_iter_fault_in_readable+0x29>
 * writev-intensive code may want this to prefault several iovecs -- that
 * would be possible (callers must not rely on the fact that _only_ the
 * first iovec will be faulted with the current implementation).
 */
int iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)
{
ffffffff812cfacd:	55                   	push   %rbp
	if (!(i->type & (ITER_BVEC|ITER_KVEC))) {
		char __user *buf = i->iov->iov_base + i->iov_offset;
ffffffff812cface:	48 8b 57 18          	mov    0x18(%rdi),%rdx
ffffffff812cfad2:	48 8b 7f 08          	mov    0x8(%rdi),%rdi
 * writev-intensive code may want this to prefault several iovecs -- that
 * would be possible (callers must not rely on the fact that _only_ the
 * first iovec will be faulted with the current implementation).
 */
int iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)
{
ffffffff812cfad6:	48 89 e5             	mov    %rsp,%rbp
	if (!(i->type & (ITER_BVEC|ITER_KVEC))) {
		char __user *buf = i->iov->iov_base + i->iov_offset;
		bytes = min(bytes, i->iov->iov_len - i->iov_offset);
		return fault_in_pages_readable(buf, bytes);
ffffffff812cfad9:	48 8b 42 08          	mov    0x8(%rdx),%rax
ffffffff812cfadd:	48 29 f8             	sub    %rdi,%rax
ffffffff812cfae0:	48 39 f0             	cmp    %rsi,%rax
ffffffff812cfae3:	48 0f 46 f0          	cmovbe %rax,%rsi
ffffffff812cfae7:	48 03 3a             	add    (%rdx),%rdi
ffffffff812cfaea:	e8 2c ef ff ff       	callq  ffffffff812cea1b <fault_in_pages_readable>
	}
	return 0;
}
ffffffff812cfaef:	5d                   	pop    %rbp
ffffffff812cfaf0:	c3                   	retq   
ffffffff812cfaf1:	31 c0                	xor    %eax,%eax
ffffffff812cfaf3:	c3                   	retq   

ffffffff812cfaf4 <copy_page_from_iter>:
}
EXPORT_SYMBOL(copy_page_to_iter);

size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
			 struct iov_iter *i)
{
ffffffff812cfaf4:	55                   	push   %rbp
ffffffff812cfaf5:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cfaf8:	41 57                	push   %r15
ffffffff812cfafa:	41 56                	push   %r14
ffffffff812cfafc:	41 55                	push   %r13
ffffffff812cfafe:	41 54                	push   %r12
ffffffff812cfb00:	49 89 ce             	mov    %rcx,%r14
ffffffff812cfb03:	53                   	push   %rbx
ffffffff812cfb04:	48 89 d3             	mov    %rdx,%rbx
ffffffff812cfb07:	48 83 ec 28          	sub    $0x28,%rsp
	if (i->type & (ITER_BVEC|ITER_KVEC)) {
ffffffff812cfb0b:	f6 01 06             	testb  $0x6,(%rcx)
}
EXPORT_SYMBOL(copy_page_to_iter);

size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
			 struct iov_iter *i)
{
ffffffff812cfb0e:	48 89 7d b8          	mov    %rdi,-0x48(%rbp)
ffffffff812cfb12:	48 89 75 c0          	mov    %rsi,-0x40(%rbp)
	if (i->type & (ITER_BVEC|ITER_KVEC)) {
ffffffff812cfb16:	74 23                	je     ffffffff812cfb3b <copy_page_from_iter+0x47>
		void *kaddr = kmap_atomic(page);
ffffffff812cfb18:	e8 76 ee ff ff       	callq  ffffffff812ce993 <kmap_atomic>
		size_t wanted = copy_from_iter(kaddr + offset, bytes, i);
ffffffff812cfb1d:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
ffffffff812cfb21:	48 89 ca             	mov    %rcx,%rdx
ffffffff812cfb24:	48 89 de             	mov    %rbx,%rsi
ffffffff812cfb27:	48 01 c7             	add    %rax,%rdi
ffffffff812cfb2a:	e8 8e f3 ff ff       	callq  ffffffff812ceebd <copy_from_iter>
ffffffff812cfb2f:	65 ff 0d 62 ae d3 7e 	decl   %gs:0x7ed3ae62(%rip)        # a998 <__preempt_count>
ffffffff812cfb36:	e9 d2 01 00 00       	jmpq   ffffffff812cfd0d <copy_page_from_iter+0x219>
	size_t skip, copy, left, wanted;
	const struct iovec *iov;
	char __user *buf;
	void *kaddr, *to;

	if (unlikely(bytes > i->count))
ffffffff812cfb3b:	48 8b 41 10          	mov    0x10(%rcx),%rax
ffffffff812cfb3f:	48 89 d6             	mov    %rdx,%rsi
ffffffff812cfb42:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cfb45:	48 0f 46 f0          	cmovbe %rax,%rsi
		bytes = i->count;

	if (unlikely(!bytes))
		return 0;
ffffffff812cfb49:	31 c0                	xor    %eax,%eax
	void *kaddr, *to;

	if (unlikely(bytes > i->count))
		bytes = i->count;

	if (unlikely(!bytes))
ffffffff812cfb4b:	48 85 f6             	test   %rsi,%rsi
ffffffff812cfb4e:	48 89 75 d0          	mov    %rsi,-0x30(%rbp)
ffffffff812cfb52:	0f 84 b5 01 00 00    	je     ffffffff812cfd0d <copy_page_from_iter+0x219>
		return 0;

	wanted = bytes;
	iov = i->iov;
ffffffff812cfb58:	4c 8b 61 18          	mov    0x18(%rcx),%r12
	skip = i->iov_offset;
ffffffff812cfb5c:	4c 8b 69 08          	mov    0x8(%rcx),%r13
	buf = iov->iov_base + skip;
	copy = min(bytes, iov->iov_len - skip);
ffffffff812cfb60:	49 8b 44 24 08       	mov    0x8(%r12),%rax
		return 0;

	wanted = bytes;
	iov = i->iov;
	skip = i->iov_offset;
	buf = iov->iov_base + skip;
ffffffff812cfb65:	4d 89 ef             	mov    %r13,%r15
ffffffff812cfb68:	4d 03 3c 24          	add    (%r12),%r15
	copy = min(bytes, iov->iov_len - skip);
ffffffff812cfb6c:	4c 29 e8             	sub    %r13,%rax
ffffffff812cfb6f:	48 39 f0             	cmp    %rsi,%rax

	if (!fault_in_pages_readable(buf, copy)) {
ffffffff812cfb72:	4c 89 ff             	mov    %r15,%rdi

	wanted = bytes;
	iov = i->iov;
	skip = i->iov_offset;
	buf = iov->iov_base + skip;
	copy = min(bytes, iov->iov_len - skip);
ffffffff812cfb75:	48 0f 47 c6          	cmova  %rsi,%rax
ffffffff812cfb79:	48 89 45 c8          	mov    %rax,-0x38(%rbp)

	if (!fault_in_pages_readable(buf, copy)) {
ffffffff812cfb7d:	8b 75 c8             	mov    -0x38(%rbp),%esi
ffffffff812cfb80:	e8 96 ee ff ff       	callq  ffffffff812cea1b <fault_in_pages_readable>
ffffffff812cfb85:	85 c0                	test   %eax,%eax
ffffffff812cfb87:	48 8b 5d d0          	mov    -0x30(%rbp),%rbx
ffffffff812cfb8b:	0f 85 ba 00 00 00    	jne    ffffffff812cfc4b <copy_page_from_iter+0x157>
		kaddr = kmap_atomic(page);
ffffffff812cfb91:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
ffffffff812cfb95:	e8 f9 ed ff ff       	callq  ffffffff812ce993 <kmap_atomic>
		to = kaddr + offset;
ffffffff812cfb9a:	48 8b 5d c0          	mov    -0x40(%rbp),%rbx
	skip = i->iov_offset;
	buf = iov->iov_base + skip;
	copy = min(bytes, iov->iov_len - skip);

	if (!fault_in_pages_readable(buf, copy)) {
		kaddr = kmap_atomic(page);
ffffffff812cfb9e:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
ffffffff812cfba2:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cfba5:	8b 55 c8             	mov    -0x38(%rbp),%edx
		to = kaddr + offset;
ffffffff812cfba8:	48 01 c3             	add    %rax,%rbx
ffffffff812cfbab:	48 89 df             	mov    %rbx,%rdi
ffffffff812cfbae:	e8 7d af ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>

		/* first chunk, usually the only one */
		left = __copy_from_user_inatomic(to, buf, copy);
		copy -= left;
ffffffff812cfbb3:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
	if (!fault_in_pages_readable(buf, copy)) {
		kaddr = kmap_atomic(page);
		to = kaddr + offset;

		/* first chunk, usually the only one */
		left = __copy_from_user_inatomic(to, buf, copy);
ffffffff812cfbb7:	48 98                	cltq   
		copy -= left;
ffffffff812cfbb9:	48 29 c2             	sub    %rax,%rdx
		skip += copy;
		to += copy;
ffffffff812cfbbc:	48 8d 0c 13          	lea    (%rbx,%rdx,1),%rcx
		bytes -= copy;
ffffffff812cfbc0:	48 8b 5d d0          	mov    -0x30(%rbp),%rbx
		to = kaddr + offset;

		/* first chunk, usually the only one */
		left = __copy_from_user_inatomic(to, buf, copy);
		copy -= left;
		skip += copy;
ffffffff812cfbc4:	49 01 d5             	add    %rdx,%r13
		to += copy;
ffffffff812cfbc7:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
		bytes -= copy;
ffffffff812cfbcb:	48 29 d3             	sub    %rdx,%rbx

		while (unlikely(!left && bytes)) {
ffffffff812cfbce:	48 85 c0             	test   %rax,%rax
ffffffff812cfbd1:	75 3e                	jne    ffffffff812cfc11 <copy_page_from_iter+0x11d>
ffffffff812cfbd3:	48 85 db             	test   %rbx,%rbx
ffffffff812cfbd6:	74 39                	je     ffffffff812cfc11 <copy_page_from_iter+0x11d>
			iov++;
ffffffff812cfbd8:	49 83 c4 10          	add    $0x10,%r12
			buf = iov->iov_base;
			copy = min(bytes, iov->iov_len);
ffffffff812cfbdc:	49 89 dd             	mov    %rbx,%r13
		to += copy;
		bytes -= copy;

		while (unlikely(!left && bytes)) {
			iov++;
			buf = iov->iov_base;
ffffffff812cfbdf:	4d 8b 3c 24          	mov    (%r12),%r15
			copy = min(bytes, iov->iov_len);
ffffffff812cfbe3:	49 39 5c 24 08       	cmp    %rbx,0x8(%r12)
ffffffff812cfbe8:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
ffffffff812cfbec:	4d 0f 46 6c 24 08    	cmovbe 0x8(%r12),%r13
ffffffff812cfbf2:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cfbf5:	44 89 ea             	mov    %r13d,%edx
ffffffff812cfbf8:	e8 33 af ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
			left = __copy_from_user_inatomic(to, buf, copy);
			copy -= left;
ffffffff812cfbfd:	4c 89 ea             	mov    %r13,%rdx

		while (unlikely(!left && bytes)) {
			iov++;
			buf = iov->iov_base;
			copy = min(bytes, iov->iov_len);
			left = __copy_from_user_inatomic(to, buf, copy);
ffffffff812cfc00:	48 98                	cltq   
			copy -= left;
ffffffff812cfc02:	48 29 c2             	sub    %rax,%rdx
			skip = copy;
			to += copy;
ffffffff812cfc05:	48 01 55 c8          	add    %rdx,-0x38(%rbp)
			bytes -= copy;
ffffffff812cfc09:	48 29 d3             	sub    %rdx,%rbx
			iov++;
			buf = iov->iov_base;
			copy = min(bytes, iov->iov_len);
			left = __copy_from_user_inatomic(to, buf, copy);
			copy -= left;
			skip = copy;
ffffffff812cfc0c:	49 89 d5             	mov    %rdx,%r13
ffffffff812cfc0f:	eb bd                	jmp    ffffffff812cfbce <copy_page_from_iter+0xda>
			to += copy;
			bytes -= copy;
		}
		if (likely(!bytes)) {
ffffffff812cfc11:	48 85 db             	test   %rbx,%rbx
ffffffff812cfc14:	75 0c                	jne    ffffffff812cfc22 <copy_page_from_iter+0x12e>
ffffffff812cfc16:	65 ff 0d 7b ad d3 7e 	decl   %gs:0x7ed3ad7b(%rip)        # a998 <__preempt_count>
ffffffff812cfc1d:	e9 b4 00 00 00       	jmpq   ffffffff812cfcd6 <copy_page_from_iter+0x1e2>
			kunmap_atomic(kaddr);
			goto done;
		}
		offset = to - kaddr;
ffffffff812cfc22:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
ffffffff812cfc26:	4c 2b 45 b0          	sub    -0x50(%rbp),%r8
		buf += copy;
ffffffff812cfc2a:	49 01 d7             	add    %rdx,%r15
		}
		if (likely(!bytes)) {
			kunmap_atomic(kaddr);
			goto done;
		}
		offset = to - kaddr;
ffffffff812cfc2d:	4c 89 45 c0          	mov    %r8,-0x40(%rbp)
		buf += copy;
		kunmap_atomic(kaddr);
		copy = min(bytes, iov->iov_len - skip);
ffffffff812cfc31:	49 8b 44 24 08       	mov    0x8(%r12),%rax
ffffffff812cfc36:	65 ff 0d 5b ad d3 7e 	decl   %gs:0x7ed3ad5b(%rip)        # a998 <__preempt_count>
ffffffff812cfc3d:	4c 29 e8             	sub    %r13,%rax
ffffffff812cfc40:	48 39 d8             	cmp    %rbx,%rax
ffffffff812cfc43:	48 0f 47 c3          	cmova  %rbx,%rax
ffffffff812cfc47:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	}
	/* Too bad - revert to non-atomic kmap */
	kaddr = kmap(page);
	to = kaddr + offset;
ffffffff812cfc4b:	48 ba 00 00 00 00 00 	movabs $0x160000000000,%rdx
ffffffff812cfc52:	16 00 00 
ffffffff812cfc55:	48 03 55 b8          	add    -0x48(%rbp),%rdx
ffffffff812cfc59:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
ffffffff812cfc5d:	48 bf 00 00 00 00 00 	movabs $0xffff880000000000,%rdi
ffffffff812cfc64:	88 ff ff 
ffffffff812cfc67:	4c 89 fe             	mov    %r15,%rsi
ffffffff812cfc6a:	48 c1 fa 06          	sar    $0x6,%rdx
ffffffff812cfc6e:	48 c1 e2 0c          	shl    $0xc,%rdx
ffffffff812cfc72:	48 01 fa             	add    %rdi,%rdx
ffffffff812cfc75:	48 01 d0             	add    %rdx,%rax
ffffffff812cfc78:	8b 55 c8             	mov    -0x38(%rbp),%edx
ffffffff812cfc7b:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
ffffffff812cfc7f:	48 89 c7             	mov    %rax,%rdi
ffffffff812cfc82:	e8 a9 ae ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
	left = __copy_from_user(to, buf, copy);
	copy -= left;
ffffffff812cfc87:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
	skip += copy;
	to += copy;
ffffffff812cfc8b:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
		copy = min(bytes, iov->iov_len - skip);
	}
	/* Too bad - revert to non-atomic kmap */
	kaddr = kmap(page);
	to = kaddr + offset;
	left = __copy_from_user(to, buf, copy);
ffffffff812cfc8f:	48 98                	cltq   
	copy -= left;
ffffffff812cfc91:	48 29 c2             	sub    %rax,%rdx
	skip += copy;
	to += copy;
ffffffff812cfc94:	4c 8d 3c 11          	lea    (%rcx,%rdx,1),%r15
	/* Too bad - revert to non-atomic kmap */
	kaddr = kmap(page);
	to = kaddr + offset;
	left = __copy_from_user(to, buf, copy);
	copy -= left;
	skip += copy;
ffffffff812cfc98:	49 01 d5             	add    %rdx,%r13
	to += copy;
	bytes -= copy;
ffffffff812cfc9b:	48 29 d3             	sub    %rdx,%rbx
	while (unlikely(!left && bytes)) {
ffffffff812cfc9e:	48 85 c0             	test   %rax,%rax
ffffffff812cfca1:	75 33                	jne    ffffffff812cfcd6 <copy_page_from_iter+0x1e2>
ffffffff812cfca3:	48 85 db             	test   %rbx,%rbx
ffffffff812cfca6:	74 2e                	je     ffffffff812cfcd6 <copy_page_from_iter+0x1e2>
		iov++;
ffffffff812cfca8:	49 83 c4 10          	add    $0x10,%r12
		buf = iov->iov_base;
		copy = min(bytes, iov->iov_len);
ffffffff812cfcac:	49 89 dd             	mov    %rbx,%r13
ffffffff812cfcaf:	4c 89 ff             	mov    %r15,%rdi
ffffffff812cfcb2:	49 39 5c 24 08       	cmp    %rbx,0x8(%r12)
ffffffff812cfcb7:	49 8b 34 24          	mov    (%r12),%rsi
ffffffff812cfcbb:	4d 0f 46 6c 24 08    	cmovbe 0x8(%r12),%r13
ffffffff812cfcc1:	44 89 ea             	mov    %r13d,%edx
ffffffff812cfcc4:	e8 67 ae ff ff       	callq  ffffffff812cab30 <copy_user_generic_unrolled>
		left = __copy_from_user(to, buf, copy);
ffffffff812cfcc9:	48 98                	cltq   
		copy -= left;
ffffffff812cfccb:	49 29 c5             	sub    %rax,%r13
		skip = copy;
		to += copy;
ffffffff812cfcce:	4d 01 ef             	add    %r13,%r15
		bytes -= copy;
ffffffff812cfcd1:	4c 29 eb             	sub    %r13,%rbx
ffffffff812cfcd4:	eb c8                	jmp    ffffffff812cfc9e <copy_page_from_iter+0x1aa>
	}
	kunmap(page);
done:
	if (skip == iov->iov_len) {
ffffffff812cfcd6:	4d 39 6c 24 08       	cmp    %r13,0x8(%r12)
ffffffff812cfcdb:	75 07                	jne    ffffffff812cfce4 <copy_page_from_iter+0x1f0>
		iov++;
ffffffff812cfcdd:	49 83 c4 10          	add    $0x10,%r12
		skip = 0;
ffffffff812cfce1:	45 31 ed             	xor    %r13d,%r13d
	}
	i->count -= wanted - bytes;
ffffffff812cfce4:	48 89 d8             	mov    %rbx,%rax
ffffffff812cfce7:	48 2b 45 d0          	sub    -0x30(%rbp),%rax
	i->nr_segs -= iov - i->iov;
	i->iov = iov;
	i->iov_offset = skip;
ffffffff812cfceb:	4d 89 6e 08          	mov    %r13,0x8(%r14)
done:
	if (skip == iov->iov_len) {
		iov++;
		skip = 0;
	}
	i->count -= wanted - bytes;
ffffffff812cfcef:	49 01 46 10          	add    %rax,0x10(%r14)
	i->nr_segs -= iov - i->iov;
ffffffff812cfcf3:	4c 89 e0             	mov    %r12,%rax
ffffffff812cfcf6:	49 2b 46 18          	sub    0x18(%r14),%rax
	i->iov = iov;
ffffffff812cfcfa:	4d 89 66 18          	mov    %r12,0x18(%r14)
	if (skip == iov->iov_len) {
		iov++;
		skip = 0;
	}
	i->count -= wanted - bytes;
	i->nr_segs -= iov - i->iov;
ffffffff812cfcfe:	48 c1 f8 04          	sar    $0x4,%rax
ffffffff812cfd02:	49 29 46 20          	sub    %rax,0x20(%r14)
	i->iov = iov;
	i->iov_offset = skip;
	return wanted - bytes;
ffffffff812cfd06:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
ffffffff812cfd0a:	48 29 d8             	sub    %rbx,%rax
		size_t wanted = copy_from_iter(kaddr + offset, bytes, i);
		kunmap_atomic(kaddr);
		return wanted;
	} else
		return copy_page_from_iter_iovec(page, offset, bytes, i);
}
ffffffff812cfd0d:	48 83 c4 28          	add    $0x28,%rsp
ffffffff812cfd11:	5b                   	pop    %rbx
ffffffff812cfd12:	41 5c                	pop    %r12
ffffffff812cfd14:	41 5d                	pop    %r13
ffffffff812cfd16:	41 5e                	pop    %r14
ffffffff812cfd18:	41 5f                	pop    %r15
ffffffff812cfd1a:	5d                   	pop    %rbp
ffffffff812cfd1b:	c3                   	retq   

ffffffff812cfd1c <iov_iter_kvec>:
EXPORT_SYMBOL(iov_iter_single_seg_count);

void iov_iter_kvec(struct iov_iter *i, int direction,
			const struct kvec *kvec, unsigned long nr_segs,
			size_t count)
{
ffffffff812cfd1c:	55                   	push   %rbp
	BUG_ON(!(direction & ITER_KVEC));
ffffffff812cfd1d:	40 f6 c6 02          	test   $0x2,%sil
EXPORT_SYMBOL(iov_iter_single_seg_count);

void iov_iter_kvec(struct iov_iter *i, int direction,
			const struct kvec *kvec, unsigned long nr_segs,
			size_t count)
{
ffffffff812cfd21:	48 89 e5             	mov    %rsp,%rbp
	BUG_ON(!(direction & ITER_KVEC));
ffffffff812cfd24:	75 02                	jne    ffffffff812cfd28 <iov_iter_kvec+0xc>
ffffffff812cfd26:	0f 0b                	ud2    
	i->type = direction;
ffffffff812cfd28:	89 37                	mov    %esi,(%rdi)
	i->kvec = kvec;
ffffffff812cfd2a:	48 89 57 18          	mov    %rdx,0x18(%rdi)
	i->nr_segs = nr_segs;
ffffffff812cfd2e:	48 89 4f 20          	mov    %rcx,0x20(%rdi)
	i->iov_offset = 0;
ffffffff812cfd32:	48 c7 47 08 00 00 00 	movq   $0x0,0x8(%rdi)
ffffffff812cfd39:	00 
	i->count = count;
ffffffff812cfd3a:	4c 89 47 10          	mov    %r8,0x10(%rdi)
}
ffffffff812cfd3e:	5d                   	pop    %rbp
ffffffff812cfd3f:	c3                   	retq   

ffffffff812cfd40 <iov_iter_bvec>:
EXPORT_SYMBOL(iov_iter_kvec);

void iov_iter_bvec(struct iov_iter *i, int direction,
			const struct bio_vec *bvec, unsigned long nr_segs,
			size_t count)
{
ffffffff812cfd40:	55                   	push   %rbp
	BUG_ON(!(direction & ITER_BVEC));
ffffffff812cfd41:	40 f6 c6 04          	test   $0x4,%sil
EXPORT_SYMBOL(iov_iter_kvec);

void iov_iter_bvec(struct iov_iter *i, int direction,
			const struct bio_vec *bvec, unsigned long nr_segs,
			size_t count)
{
ffffffff812cfd45:	48 89 e5             	mov    %rsp,%rbp
	BUG_ON(!(direction & ITER_BVEC));
ffffffff812cfd48:	75 02                	jne    ffffffff812cfd4c <iov_iter_bvec+0xc>
ffffffff812cfd4a:	0f 0b                	ud2    
	i->type = direction;
ffffffff812cfd4c:	89 37                	mov    %esi,(%rdi)
	i->bvec = bvec;
ffffffff812cfd4e:	48 89 57 18          	mov    %rdx,0x18(%rdi)
	i->nr_segs = nr_segs;
ffffffff812cfd52:	48 89 4f 20          	mov    %rcx,0x20(%rdi)
	i->iov_offset = 0;
ffffffff812cfd56:	48 c7 47 08 00 00 00 	movq   $0x0,0x8(%rdi)
ffffffff812cfd5d:	00 
	i->count = count;
ffffffff812cfd5e:	4c 89 47 10          	mov    %r8,0x10(%rdi)
}
ffffffff812cfd62:	5d                   	pop    %rbp
ffffffff812cfd63:	c3                   	retq   

ffffffff812cfd64 <get_page>:
}

extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
ffffffff812cfd64:	55                   	push   %rbp
ffffffff812cfd65:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cfd68:	53                   	push   %rbx
ffffffff812cfd69:	52                   	push   %rdx
ffffffff812cfd6a:	48 8b 07             	mov    (%rdi),%rax
ffffffff812cfd6d:	48 89 fb             	mov    %rdi,%rbx
	if (unlikely(PageTail(page)))
ffffffff812cfd70:	f6 c4 80             	test   $0x80,%ah
ffffffff812cfd73:	75 18                	jne    ffffffff812cfd8d <get_page+0x29>
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return ACCESS_ONCE((v)->counter);
ffffffff812cfd75:	8b 43 1c             	mov    0x1c(%rbx),%eax
			return;
	/*
	 * Getting a normal page or the head of a compound page
	 * requires to already have an elevated page->_count.
	 */
	VM_BUG_ON_PAGE(atomic_read(&page->_count) <= 0, page);
ffffffff812cfd78:	85 c0                	test   %eax,%eax
ffffffff812cfd7a:	7f 1c                	jg     ffffffff812cfd98 <get_page+0x34>
ffffffff812cfd7c:	48 c7 c6 75 4b 78 81 	mov    $0xffffffff81784b75,%rsi
ffffffff812cfd83:	48 89 df             	mov    %rbx,%rdi
ffffffff812cfd86:	e8 9b bd e1 ff       	callq  ffffffff810ebb26 <dump_page>
ffffffff812cfd8b:	0f 0b                	ud2    
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
		if (likely(__get_page_tail(page)))
ffffffff812cfd8d:	e8 02 d6 e0 ff       	callq  ffffffff810dd394 <__get_page_tail>
ffffffff812cfd92:	84 c0                	test   %al,%al
ffffffff812cfd94:	75 06                	jne    ffffffff812cfd9c <get_page+0x38>
ffffffff812cfd96:	eb dd                	jmp    ffffffff812cfd75 <get_page+0x11>
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
ffffffff812cfd98:	f0 ff 43 1c          	lock incl 0x1c(%rbx)
	 * Getting a normal page or the head of a compound page
	 * requires to already have an elevated page->_count.
	 */
	VM_BUG_ON_PAGE(atomic_read(&page->_count) <= 0, page);
	atomic_inc(&page->_count);
}
ffffffff812cfd9c:	58                   	pop    %rax
ffffffff812cfd9d:	5b                   	pop    %rbx
ffffffff812cfd9e:	5d                   	pop    %rbp
ffffffff812cfd9f:	c3                   	retq   

ffffffff812cfda0 <iov_iter_get_pages>:

ssize_t iov_iter_get_pages(struct iov_iter *i,
		   struct page **pages, size_t maxsize, unsigned maxpages,
		   size_t *start)
{
	if (maxsize > i->count)
ffffffff812cfda0:	4c 8b 4f 10          	mov    0x10(%rdi),%r9
ffffffff812cfda4:	49 39 d1             	cmp    %rdx,%r9
ffffffff812cfda7:	49 0f 46 d1          	cmovbe %r9,%rdx
		maxsize = i->count;

	if (!maxsize)
		return 0;
ffffffff812cfdab:	31 c0                	xor    %eax,%eax
		   size_t *start)
{
	if (maxsize > i->count)
		maxsize = i->count;

	if (!maxsize)
ffffffff812cfdad:	48 85 d2             	test   %rdx,%rdx
ffffffff812cfdb0:	0f 84 a4 01 00 00    	je     ffffffff812cff5a <iov_iter_get_pages+0x1ba>
EXPORT_SYMBOL(iov_iter_alignment);

ssize_t iov_iter_get_pages(struct iov_iter *i,
		   struct page **pages, size_t maxsize, unsigned maxpages,
		   size_t *start)
{
ffffffff812cfdb6:	55                   	push   %rbp
ffffffff812cfdb7:	41 89 cb             	mov    %ecx,%r11d
ffffffff812cfdba:	49 89 fa             	mov    %rdi,%r10
ffffffff812cfdbd:	48 89 f1             	mov    %rsi,%rcx
ffffffff812cfdc0:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cfdc3:	41 55                	push   %r13
ffffffff812cfdc5:	41 54                	push   %r12
ffffffff812cfdc7:	53                   	push   %rbx
ffffffff812cfdc8:	41 51                	push   %r9
ffffffff812cfdca:	4d 89 c4             	mov    %r8,%r12
		maxsize = i->count;

	if (!maxsize)
		return 0;

	iterate_all_kinds(i, maxsize, v, ({
ffffffff812cfdcd:	8b 07                	mov    (%rdi),%eax
ffffffff812cfdcf:	48 8b 77 08          	mov    0x8(%rdi),%rsi
ffffffff812cfdd3:	a8 04                	test   $0x4,%al
ffffffff812cfdd5:	74 67                	je     ffffffff812cfe3e <iov_iter_get_pages+0x9e>
ffffffff812cfdd7:	4c 8b 47 18          	mov    0x18(%rdi),%r8
ffffffff812cfddb:	41 8b 40 08          	mov    0x8(%r8),%eax
ffffffff812cfddf:	48 29 f0             	sub    %rsi,%rax
ffffffff812cfde2:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cfde5:	48 0f 47 c2          	cmova  %rdx,%rax
ffffffff812cfde9:	85 c0                	test   %eax,%eax
ffffffff812cfdeb:	48 89 c3             	mov    %rax,%rbx
ffffffff812cfdee:	74 1c                	je     ffffffff812cfe0c <iov_iter_get_pages+0x6c>
ffffffff812cfdf0:	41 03 70 0c          	add    0xc(%r8),%esi
ffffffff812cfdf4:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cfdf7:	89 f0                	mov    %esi,%eax
ffffffff812cfdf9:	49 89 04 24          	mov    %rax,(%r12)
ffffffff812cfdfd:	48 89 39             	mov    %rdi,(%rcx)
ffffffff812cfe00:	e8 5f ff ff ff       	callq  ffffffff812cfd64 <get_page>
ffffffff812cfe05:	89 d8                	mov    %ebx,%eax
ffffffff812cfe07:	e9 47 01 00 00       	jmpq   ffffffff812cff53 <iov_iter_get_pages+0x1b3>
ffffffff812cfe0c:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cfe10:	41 8b 40 08          	mov    0x8(%r8),%eax
ffffffff812cfe14:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cfe17:	48 0f 47 c2          	cmova  %rdx,%rax
ffffffff812cfe1b:	48 85 c0             	test   %rax,%rax
ffffffff812cfe1e:	48 89 c3             	mov    %rax,%rbx
ffffffff812cfe21:	74 e9                	je     ffffffff812cfe0c <iov_iter_get_pages+0x6c>
ffffffff812cfe23:	41 8b 40 0c          	mov    0xc(%r8),%eax
ffffffff812cfe27:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cfe2a:	49 89 04 24          	mov    %rax,(%r12)
ffffffff812cfe2e:	48 89 39             	mov    %rdi,(%rcx)
ffffffff812cfe31:	e8 2e ff ff ff       	callq  ffffffff812cfd64 <get_page>
ffffffff812cfe36:	48 89 d8             	mov    %rbx,%rax
ffffffff812cfe39:	e9 15 01 00 00       	jmpq   ffffffff812cff53 <iov_iter_get_pages+0x1b3>
ffffffff812cfe3e:	a8 02                	test   $0x2,%al
ffffffff812cfe40:	74 46                	je     ffffffff812cfe88 <iov_iter_get_pages+0xe8>
ffffffff812cfe42:	48 8b 7f 18          	mov    0x18(%rdi),%rdi
ffffffff812cfe46:	48 8b 47 08          	mov    0x8(%rdi),%rax
ffffffff812cfe4a:	48 29 f0             	sub    %rsi,%rax
ffffffff812cfe4d:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cfe50:	48 0f 47 c2          	cmova  %rdx,%rax
ffffffff812cfe54:	48 89 c1             	mov    %rax,%rcx
ffffffff812cfe57:	48 c7 c0 f2 ff ff ff 	mov    $0xfffffffffffffff2,%rax
ffffffff812cfe5e:	48 85 c9             	test   %rcx,%rcx
ffffffff812cfe61:	0f 85 ec 00 00 00    	jne    ffffffff812cff53 <iov_iter_get_pages+0x1b3>
ffffffff812cfe67:	48 83 c7 10          	add    $0x10,%rdi
ffffffff812cfe6b:	48 89 d0             	mov    %rdx,%rax
ffffffff812cfe6e:	48 39 57 08          	cmp    %rdx,0x8(%rdi)
ffffffff812cfe72:	48 0f 46 47 08       	cmovbe 0x8(%rdi),%rax
ffffffff812cfe77:	48 85 c0             	test   %rax,%rax
ffffffff812cfe7a:	74 eb                	je     ffffffff812cfe67 <iov_iter_get_pages+0xc7>
ffffffff812cfe7c:	48 c7 c0 f2 ff ff ff 	mov    $0xfffffffffffffff2,%rax
ffffffff812cfe83:	e9 cb 00 00 00       	jmpq   ffffffff812cff53 <iov_iter_get_pages+0x1b3>
ffffffff812cfe88:	4c 8b 47 18          	mov    0x18(%rdi),%r8
ffffffff812cfe8c:	4d 8b 48 08          	mov    0x8(%r8),%r9
ffffffff812cfe90:	49 29 f1             	sub    %rsi,%r9
ffffffff812cfe93:	49 39 d1             	cmp    %rdx,%r9
ffffffff812cfe96:	4c 0f 47 ca          	cmova  %rdx,%r9
ffffffff812cfe9a:	4d 85 c9             	test   %r9,%r9
ffffffff812cfe9d:	74 40                	je     ffffffff812cfedf <iov_iter_get_pages+0x13f>
ffffffff812cfe9f:	49 03 30             	add    (%r8),%rsi
ffffffff812cfea2:	49 c1 e3 0c          	shl    $0xc,%r11
ffffffff812cfea6:	48 89 f3             	mov    %rsi,%rbx
ffffffff812cfea9:	81 e3 ff 0f 00 00    	and    $0xfff,%ebx
ffffffff812cfeaf:	49 01 d9             	add    %rbx,%r9
ffffffff812cfeb2:	49 89 1c 24          	mov    %rbx,(%r12)
ffffffff812cfeb6:	4c 89 db             	mov    %r11,%rbx
ffffffff812cfeb9:	4d 39 d9             	cmp    %r11,%r9
ffffffff812cfebc:	8b 17                	mov    (%rdi),%edx
ffffffff812cfebe:	49 0f 46 d9          	cmovbe %r9,%rbx
ffffffff812cfec2:	48 81 e6 00 f0 ff ff 	and    $0xfffffffffffff000,%rsi
ffffffff812cfec9:	4c 8d ab ff 0f 00 00 	lea    0xfff(%rbx),%r13
ffffffff812cfed0:	48 89 f7             	mov    %rsi,%rdi
ffffffff812cfed3:	83 e2 01             	and    $0x1,%edx
ffffffff812cfed6:	49 c1 ed 0c          	shr    $0xc,%r13
ffffffff812cfeda:	83 f2 01             	xor    $0x1,%edx
ffffffff812cfedd:	eb 51                	jmp    ffffffff812cff30 <iov_iter_get_pages+0x190>
ffffffff812cfedf:	49 83 c0 10          	add    $0x10,%r8
ffffffff812cfee3:	48 89 d3             	mov    %rdx,%rbx
ffffffff812cfee6:	49 39 50 08          	cmp    %rdx,0x8(%r8)
ffffffff812cfeea:	49 0f 46 58 08       	cmovbe 0x8(%r8),%rbx
ffffffff812cfeef:	48 85 db             	test   %rbx,%rbx
ffffffff812cfef2:	74 eb                	je     ffffffff812cfedf <iov_iter_get_pages+0x13f>
ffffffff812cfef4:	49 8b 38             	mov    (%r8),%rdi
ffffffff812cfef7:	49 c1 e3 0c          	shl    $0xc,%r11
ffffffff812cfefb:	48 89 fa             	mov    %rdi,%rdx
ffffffff812cfefe:	81 e2 ff 0f 00 00    	and    $0xfff,%edx
ffffffff812cff04:	49 89 14 24          	mov    %rdx,(%r12)
ffffffff812cff08:	48 01 da             	add    %rbx,%rdx
ffffffff812cff0b:	4c 89 db             	mov    %r11,%rbx
ffffffff812cff0e:	4c 39 da             	cmp    %r11,%rdx
ffffffff812cff11:	48 0f 46 da          	cmovbe %rdx,%rbx
ffffffff812cff15:	41 8b 12             	mov    (%r10),%edx
ffffffff812cff18:	48 81 e7 00 f0 ff ff 	and    $0xfffffffffffff000,%rdi
ffffffff812cff1f:	4c 8d ab ff 0f 00 00 	lea    0xfff(%rbx),%r13
ffffffff812cff26:	83 e2 01             	and    $0x1,%edx
ffffffff812cff29:	49 c1 ed 0c          	shr    $0xc,%r13
ffffffff812cff2d:	83 f2 01             	xor    $0x1,%edx
ffffffff812cff30:	44 89 ee             	mov    %r13d,%esi
ffffffff812cff33:	e8 7c 30 d9 ff       	callq  ffffffff81062fb4 <get_user_pages_fast>
ffffffff812cff38:	85 c0                	test   %eax,%eax
ffffffff812cff3a:	79 04                	jns    ffffffff812cff40 <iov_iter_get_pages+0x1a0>
ffffffff812cff3c:	48 98                	cltq   
ffffffff812cff3e:	eb 13                	jmp    ffffffff812cff53 <iov_iter_get_pages+0x1b3>
ffffffff812cff40:	41 39 c5             	cmp    %eax,%r13d
ffffffff812cff43:	74 07                	je     ffffffff812cff4c <iov_iter_get_pages+0x1ac>
ffffffff812cff45:	48 63 d8             	movslq %eax,%rbx
ffffffff812cff48:	48 c1 e3 0c          	shl    $0xc,%rbx
ffffffff812cff4c:	48 89 d8             	mov    %rbx,%rax
ffffffff812cff4f:	49 2b 04 24          	sub    (%r12),%rax
	}),({
		return -EFAULT;
	})
	)
	return 0;
}
ffffffff812cff53:	5a                   	pop    %rdx
ffffffff812cff54:	5b                   	pop    %rbx
ffffffff812cff55:	41 5c                	pop    %r12
ffffffff812cff57:	41 5d                	pop    %r13
ffffffff812cff59:	5d                   	pop    %rbp
ffffffff812cff5a:	c3                   	retq   

ffffffff812cff5b <iov_iter_get_pages_alloc>:
		   struct page ***pages, size_t maxsize,
		   size_t *start)
{
	struct page **p;

	if (maxsize > i->count)
ffffffff812cff5b:	48 8b 47 10          	mov    0x10(%rdi),%rax
ffffffff812cff5f:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cff62:	48 0f 46 d0          	cmovbe %rax,%rdx
		maxsize = i->count;

	if (!maxsize)
		return 0;
ffffffff812cff66:	31 c0                	xor    %eax,%eax
	struct page **p;

	if (maxsize > i->count)
		maxsize = i->count;

	if (!maxsize)
ffffffff812cff68:	48 85 d2             	test   %rdx,%rdx
ffffffff812cff6b:	0f 84 1c 02 00 00    	je     ffffffff812d018d <iov_iter_get_pages_alloc+0x232>
}

ssize_t iov_iter_get_pages_alloc(struct iov_iter *i,
		   struct page ***pages, size_t maxsize,
		   size_t *start)
{
ffffffff812cff71:	55                   	push   %rbp
ffffffff812cff72:	48 89 e5             	mov    %rsp,%rbp
ffffffff812cff75:	41 57                	push   %r15
ffffffff812cff77:	41 56                	push   %r14
ffffffff812cff79:	41 55                	push   %r13
ffffffff812cff7b:	41 54                	push   %r12
ffffffff812cff7d:	49 89 f5             	mov    %rsi,%r13
ffffffff812cff80:	53                   	push   %rbx
ffffffff812cff81:	49 89 cc             	mov    %rcx,%r12
ffffffff812cff84:	49 89 ff             	mov    %rdi,%r15
ffffffff812cff87:	48 83 ec 18          	sub    $0x18,%rsp
		maxsize = i->count;

	if (!maxsize)
		return 0;

	iterate_all_kinds(i, maxsize, v, ({
ffffffff812cff8b:	8b 07                	mov    (%rdi),%eax
ffffffff812cff8d:	48 8b 4f 08          	mov    0x8(%rdi),%rcx
ffffffff812cff91:	48 8b 77 18          	mov    0x18(%rdi),%rsi
ffffffff812cff95:	a8 04                	test   $0x4,%al
ffffffff812cff97:	0f 84 97 00 00 00    	je     ffffffff812d0034 <iov_iter_get_pages_alloc+0xd9>
ffffffff812cff9d:	8b 46 08             	mov    0x8(%rsi),%eax
ffffffff812cffa0:	48 29 c8             	sub    %rcx,%rax
ffffffff812cffa3:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cffa6:	48 0f 47 c2          	cmova  %rdx,%rax
ffffffff812cffaa:	85 c0                	test   %eax,%eax
ffffffff812cffac:	48 89 c3             	mov    %rax,%rbx
ffffffff812cffaf:	74 3d                	je     ffffffff812cffee <iov_iter_get_pages_alloc+0x93>
ffffffff812cffb1:	03 4e 0c             	add    0xc(%rsi),%ecx
ffffffff812cffb4:	4c 8b 36             	mov    (%rsi),%r14
ffffffff812cffb7:	bf 01 00 00 00       	mov    $0x1,%edi
ffffffff812cffbc:	89 c8                	mov    %ecx,%eax
ffffffff812cffbe:	49 89 04 24          	mov    %rax,(%r12)
ffffffff812cffc2:	e8 f4 f2 ff ff       	callq  ffffffff812cf2bb <get_pages_array>
ffffffff812cffc7:	48 85 c0             	test   %rax,%rax
ffffffff812cffca:	49 89 45 00          	mov    %rax,0x0(%r13)
ffffffff812cffce:	75 0c                	jne    ffffffff812cffdc <iov_iter_get_pages_alloc+0x81>
ffffffff812cffd0:	48 c7 c0 f4 ff ff ff 	mov    $0xfffffffffffffff4,%rax
ffffffff812cffd7:	e9 a3 01 00 00       	jmpq   ffffffff812d017f <iov_iter_get_pages_alloc+0x224>
ffffffff812cffdc:	4c 89 30             	mov    %r14,(%rax)
ffffffff812cffdf:	4c 89 f7             	mov    %r14,%rdi
ffffffff812cffe2:	e8 7d fd ff ff       	callq  ffffffff812cfd64 <get_page>
ffffffff812cffe7:	89 d8                	mov    %ebx,%eax
ffffffff812cffe9:	e9 91 01 00 00       	jmpq   ffffffff812d017f <iov_iter_get_pages_alloc+0x224>
ffffffff812cffee:	48 83 c6 10          	add    $0x10,%rsi
ffffffff812cfff2:	8b 46 08             	mov    0x8(%rsi),%eax
ffffffff812cfff5:	48 39 d0             	cmp    %rdx,%rax
ffffffff812cfff8:	48 0f 47 c2          	cmova  %rdx,%rax
ffffffff812cfffc:	48 85 c0             	test   %rax,%rax
ffffffff812cffff:	48 89 c3             	mov    %rax,%rbx
